0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
7ef7df7df2e216a11e7af4f940863b8e0ce30b81;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		stats "k8s.io/kubernetes/pkg/kubelet/apis/stats/v1alpha1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// Interval to poll /stats/container on a node
0000000000000000000000000000000000000000;;		containerStatsPollingPeriod = 10 * time.Second
0000000000000000000000000000000000000000;;		// The monitoring time for one test.
0000000000000000000000000000000000000000;;		monitoringTime = 20 * time.Minute
0000000000000000000000000000000000000000;;		// The periodic reporting period.
0000000000000000000000000000000000000000;;		reportingPeriod = 5 * time.Minute
0000000000000000000000000000000000000000;;		// Timeout for waiting for the image prepulling to complete.
0000000000000000000000000000000000000000;;		imagePrePullingLongTimeout = time.Minute * 8
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type resourceTest struct {
0000000000000000000000000000000000000000;;		podsPerNode int
0000000000000000000000000000000000000000;;		cpuLimits   framework.ContainersCPUSummary
0000000000000000000000000000000000000000;;		memLimits   framework.ResourceUsagePerContainer
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func logPodsOnNodes(c clientset.Interface, nodeNames []string) {
0000000000000000000000000000000000000000;;		for _, n := range nodeNames {
0000000000000000000000000000000000000000;;			podList, err := framework.GetKubeletRunningPods(c, n)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				framework.Logf("Unable to retrieve kubelet pods for node %v", n)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.Logf("%d pods are running on node %v", len(podList.Items), n)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func runResourceTrackingTest(f *framework.Framework, podsPerNode int, nodeNames sets.String, rm *framework.ResourceMonitor,
0000000000000000000000000000000000000000;;		expectedCPU map[string]map[float64]float64, expectedMemory framework.ResourceUsagePerContainer) {
0000000000000000000000000000000000000000;;		numNodes := nodeNames.Len()
0000000000000000000000000000000000000000;;		totalPods := podsPerNode * numNodes
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Creating a RC of %d pods and wait until all pods of this RC are running", totalPods))
0000000000000000000000000000000000000000;;		rcName := fmt.Sprintf("resource%d-%s", totalPods, string(uuid.NewUUID()))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Use a more realistic workload
0000000000000000000000000000000000000000;;		Expect(framework.RunRC(testutils.RCConfig{
0000000000000000000000000000000000000000;;			Client:         f.ClientSet,
0000000000000000000000000000000000000000;;			InternalClient: f.InternalClientset,
0000000000000000000000000000000000000000;;			Name:           rcName,
0000000000000000000000000000000000000000;;			Namespace:      f.Namespace.Name,
0000000000000000000000000000000000000000;;			Image:          framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;			Replicas:       totalPods,
0000000000000000000000000000000000000000;;		})).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Log once and flush the stats.
0000000000000000000000000000000000000000;;		rm.LogLatest()
0000000000000000000000000000000000000000;;		rm.Reset()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Start monitoring resource usage")
0000000000000000000000000000000000000000;;		// Periodically dump the cpu summary until the deadline is met.
0000000000000000000000000000000000000000;;		// Note that without calling framework.ResourceMonitor.Reset(), the stats
0000000000000000000000000000000000000000;;		// would occupy increasingly more memory. This should be fine
0000000000000000000000000000000000000000;;		// for the current test duration, but we should reclaim the
0000000000000000000000000000000000000000;;		// entries if we plan to monitor longer (e.g., 8 hours).
0000000000000000000000000000000000000000;;		deadline := time.Now().Add(monitoringTime)
0000000000000000000000000000000000000000;;		for time.Now().Before(deadline) {
0000000000000000000000000000000000000000;;			timeLeft := deadline.Sub(time.Now())
0000000000000000000000000000000000000000;;			framework.Logf("Still running...%v left", timeLeft)
0000000000000000000000000000000000000000;;			if timeLeft < reportingPeriod {
0000000000000000000000000000000000000000;;				time.Sleep(timeLeft)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				time.Sleep(reportingPeriod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			logPodsOnNodes(f.ClientSet, nodeNames.List())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Reporting overall resource usage")
0000000000000000000000000000000000000000;;		logPodsOnNodes(f.ClientSet, nodeNames.List())
0000000000000000000000000000000000000000;;		usageSummary, err := rm.GetLatest()
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		// TODO(random-liu): Remove the original log when we migrate to new perfdash
0000000000000000000000000000000000000000;;		framework.Logf("%s", rm.FormatResourceUsage(usageSummary))
0000000000000000000000000000000000000000;;		// Log perf result
0000000000000000000000000000000000000000;;		framework.PrintPerfData(framework.ResourceUsageToPerfData(rm.GetMasterNodeLatest(usageSummary)))
0000000000000000000000000000000000000000;;		verifyMemoryLimits(f.ClientSet, expectedMemory, usageSummary)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cpuSummary := rm.GetCPUSummary()
0000000000000000000000000000000000000000;;		framework.Logf("%s", rm.FormatCPUSummary(cpuSummary))
0000000000000000000000000000000000000000;;		// Log perf result
0000000000000000000000000000000000000000;;		framework.PrintPerfData(framework.CPUUsageToPerfData(rm.GetMasterNodeCPUSummary(cpuSummary)))
0000000000000000000000000000000000000000;;		verifyCPULimits(expectedCPU, cpuSummary)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Deleting the RC")
0000000000000000000000000000000000000000;;		framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, rcName)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func verifyMemoryLimits(c clientset.Interface, expected framework.ResourceUsagePerContainer, actual framework.ResourceUsagePerNode) {
0000000000000000000000000000000000000000;;		if expected == nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var errList []string
0000000000000000000000000000000000000000;;		for nodeName, nodeSummary := range actual {
0000000000000000000000000000000000000000;;			var nodeErrs []string
0000000000000000000000000000000000000000;;			for cName, expectedResult := range expected {
0000000000000000000000000000000000000000;;				container, ok := nodeSummary[cName]
0000000000000000000000000000000000000000;;				if !ok {
0000000000000000000000000000000000000000;;					nodeErrs = append(nodeErrs, fmt.Sprintf("container %q: missing", cName))
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				expectedValue := expectedResult.MemoryRSSInBytes
0000000000000000000000000000000000000000;;				actualValue := container.MemoryRSSInBytes
0000000000000000000000000000000000000000;;				if expectedValue != 0 && actualValue > expectedValue {
0000000000000000000000000000000000000000;;					nodeErrs = append(nodeErrs, fmt.Sprintf("container %q: expected RSS memory (MB) < %d; got %d",
0000000000000000000000000000000000000000;;						cName, expectedValue, actualValue))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(nodeErrs) > 0 {
0000000000000000000000000000000000000000;;				errList = append(errList, fmt.Sprintf("node %v:\n %s", nodeName, strings.Join(nodeErrs, ", ")))
0000000000000000000000000000000000000000;;				heapStats, err := framework.GetKubeletHeapStats(c, nodeName)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					framework.Logf("Unable to get heap stats from %q", nodeName)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					framework.Logf("Heap stats on %q\n:%v", nodeName, heapStats)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(errList) > 0 {
0000000000000000000000000000000000000000;;			framework.Failf("Memory usage exceeding limits:\n %s", strings.Join(errList, "\n"))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func verifyCPULimits(expected framework.ContainersCPUSummary, actual framework.NodesCPUSummary) {
0000000000000000000000000000000000000000;;		if expected == nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var errList []string
0000000000000000000000000000000000000000;;		for nodeName, perNodeSummary := range actual {
0000000000000000000000000000000000000000;;			var nodeErrs []string
0000000000000000000000000000000000000000;;			for cName, expectedResult := range expected {
0000000000000000000000000000000000000000;;				perContainerSummary, ok := perNodeSummary[cName]
0000000000000000000000000000000000000000;;				if !ok {
0000000000000000000000000000000000000000;;					nodeErrs = append(nodeErrs, fmt.Sprintf("container %q: missing", cName))
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for p, expectedValue := range expectedResult {
0000000000000000000000000000000000000000;;					actualValue, ok := perContainerSummary[p]
0000000000000000000000000000000000000000;;					if !ok {
0000000000000000000000000000000000000000;;						nodeErrs = append(nodeErrs, fmt.Sprintf("container %q: missing percentile %v", cName, p))
0000000000000000000000000000000000000000;;						continue
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if actualValue > expectedValue {
0000000000000000000000000000000000000000;;						nodeErrs = append(nodeErrs, fmt.Sprintf("container %q: expected %.0fth%% usage < %.3f; got %.3f",
0000000000000000000000000000000000000000;;							cName, p*100, expectedValue, actualValue))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(nodeErrs) > 0 {
0000000000000000000000000000000000000000;;				errList = append(errList, fmt.Sprintf("node %v:\n %s", nodeName, strings.Join(nodeErrs, ", ")))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(errList) > 0 {
0000000000000000000000000000000000000000;;			framework.Failf("CPU usage exceeding limits:\n %s", strings.Join(errList, "\n"))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Slow by design (1 hour)
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Kubelet [Serial] [Slow]", func() {
0000000000000000000000000000000000000000;;		var nodeNames sets.String
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("kubelet-perf")
0000000000000000000000000000000000000000;;		var om *framework.RuntimeOperationMonitor
0000000000000000000000000000000000000000;;		var rm *framework.ResourceMonitor
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			// Wait until image prepull pod has completed so that they wouldn't
0000000000000000000000000000000000000000;;			// affect the runtime cpu usage. Fail the test if prepulling cannot
0000000000000000000000000000000000000000;;			// finish in time.
0000000000000000000000000000000000000000;;			if err := framework.WaitForPodsSuccess(f.ClientSet, metav1.NamespaceSystem, framework.ImagePullerLabels, imagePrePullingLongTimeout); err != nil {
0000000000000000000000000000000000000000;;				framework.Failf("Image puller didn't complete in %v, not running resource usage test since the metrics might be adultrated", imagePrePullingLongTimeout)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			nodes := framework.GetReadySchedulableNodesOrDie(f.ClientSet)
0000000000000000000000000000000000000000;;			nodeNames = sets.NewString()
0000000000000000000000000000000000000000;;			for _, node := range nodes.Items {
0000000000000000000000000000000000000000;;				nodeNames.Insert(node.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			om = framework.NewRuntimeOperationMonitor(f.ClientSet)
0000000000000000000000000000000000000000;;			rm = framework.NewResourceMonitor(f.ClientSet, framework.TargetContainers(), containerStatsPollingPeriod)
0000000000000000000000000000000000000000;;			rm.Start()
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			rm.Stop()
0000000000000000000000000000000000000000;;			result := om.GetLatestRuntimeOperationErrorRate()
0000000000000000000000000000000000000000;;			framework.Logf("runtime operation error metrics:\n%s", framework.FormatRuntimeOperationErrorRate(result))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		framework.KubeDescribe("regular resource usage tracking", func() {
0000000000000000000000000000000000000000;;			// We assume that the scheduler will make reasonable scheduling choices
0000000000000000000000000000000000000000;;			// and assign ~N pods on the node.
0000000000000000000000000000000000000000;;			// Although we want to track N pods per node, there are N + add-on pods
0000000000000000000000000000000000000000;;			// in the cluster. The cluster add-on pods can be distributed unevenly
0000000000000000000000000000000000000000;;			// among the nodes because they are created during the cluster
0000000000000000000000000000000000000000;;			// initialization. This *noise* is obvious when N is small. We
0000000000000000000000000000000000000000;;			// deliberately set higher resource usage limits to account for the
0000000000000000000000000000000000000000;;			// noise.
0000000000000000000000000000000000000000;;			//
0000000000000000000000000000000000000000;;			// We set all resource limits generously because this test is mainly
0000000000000000000000000000000000000000;;			// used to catch resource leaks in the soak cluster. For tracking
0000000000000000000000000000000000000000;;			// kubelet/runtime resource usage, please see the node e2e benchmark
0000000000000000000000000000000000000000;;			// dashboard. http://node-perf-dash.k8s.io/
0000000000000000000000000000000000000000;;			//
0000000000000000000000000000000000000000;;			// TODO(#36621): Deprecate this test once we have a node e2e soak
0000000000000000000000000000000000000000;;			// cluster.
0000000000000000000000000000000000000000;;			rTests := []resourceTest{
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					podsPerNode: 0,
0000000000000000000000000000000000000000;;					cpuLimits: framework.ContainersCPUSummary{
0000000000000000000000000000000000000000;;						stats.SystemContainerKubelet: {0.50: 0.10, 0.95: 0.20},
0000000000000000000000000000000000000000;;						stats.SystemContainerRuntime: {0.50: 0.10, 0.95: 0.20},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					memLimits: framework.ResourceUsagePerContainer{
0000000000000000000000000000000000000000;;						stats.SystemContainerKubelet: &framework.ContainerResourceUsage{MemoryRSSInBytes: 200 * 1024 * 1024},
0000000000000000000000000000000000000000;;						// The detail can be found at https://github.com/kubernetes/kubernetes/issues/28384#issuecomment-244158892
0000000000000000000000000000000000000000;;						stats.SystemContainerRuntime: &framework.ContainerResourceUsage{MemoryRSSInBytes: 125 * 1024 * 1024},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					cpuLimits: framework.ContainersCPUSummary{
0000000000000000000000000000000000000000;;						stats.SystemContainerKubelet: {0.50: 0.35, 0.95: 0.50},
0000000000000000000000000000000000000000;;						stats.SystemContainerRuntime: {0.50: 0.10, 0.95: 0.50},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					podsPerNode: 100,
0000000000000000000000000000000000000000;;					memLimits: framework.ResourceUsagePerContainer{
0000000000000000000000000000000000000000;;						stats.SystemContainerKubelet: &framework.ContainerResourceUsage{MemoryRSSInBytes: 300 * 1024 * 1024},
0000000000000000000000000000000000000000;;						stats.SystemContainerRuntime: &framework.ContainerResourceUsage{MemoryRSSInBytes: 300 * 1024 * 1024},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, testArg := range rTests {
0000000000000000000000000000000000000000;;				itArg := testArg
0000000000000000000000000000000000000000;;				podsPerNode := itArg.podsPerNode
0000000000000000000000000000000000000000;;				name := fmt.Sprintf(
0000000000000000000000000000000000000000;;					"resource tracking for %d pods per node", podsPerNode)
0000000000000000000000000000000000000000;;				It(name, func() {
0000000000000000000000000000000000000000;;					runResourceTrackingTest(f, podsPerNode, nodeNames, rm, itArg.cpuLimits, itArg.memLimits)
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		framework.KubeDescribe("experimental resource usage tracking [Feature:ExperimentalResourceUsageTracking]", func() {
0000000000000000000000000000000000000000;;			density := []int{100}
0000000000000000000000000000000000000000;;			for i := range density {
0000000000000000000000000000000000000000;;				podsPerNode := density[i]
0000000000000000000000000000000000000000;;				name := fmt.Sprintf(
0000000000000000000000000000000000000000;;					"resource tracking for %d pods per node", podsPerNode)
0000000000000000000000000000000000000000;;				It(name, func() {
0000000000000000000000000000000000000000;;					runResourceTrackingTest(f, podsPerNode, nodeNames, rm, nil, nil)
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
