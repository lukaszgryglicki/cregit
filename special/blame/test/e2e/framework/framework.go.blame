0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
7ba6bcba5bf5332fdb40a3d6bd9bdda873d137dd;test/e2e/framework.go[test/e2e/framework.go][test/e2e/framework/framework.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package framework
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"bufio"
0000000000000000000000000000000000000000;;		"bytes"
0000000000000000000000000000000000000000;;		"encoding/json"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"path"
0000000000000000000000000000000000000000;;		"reflect"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		apierrors "k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime/schema"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/intstr"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/dynamic"
0000000000000000000000000000000000000000;;		staging "k8s.io/client-go/kubernetes"
0000000000000000000000000000000000000000;;		clientreporestclient "k8s.io/client-go/rest"
0000000000000000000000000000000000000000;;		restclient "k8s.io/client-go/rest"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/metrics"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		maxKubectlExecRetries = 5
0000000000000000000000000000000000000000;;		// TODO(mikedanese): reset this to 5 minutes once #47135 is resolved.
0000000000000000000000000000000000000000;;		// ref https://github.com/kubernetes/kubernetes/issues/47135
0000000000000000000000000000000000000000;;		DefaultNamespaceDeletionTimeout = 10 * time.Minute
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Framework supports common operations used by e2e tests; it will keep a client & a namespace for you.
0000000000000000000000000000000000000000;;	// Eventual goal is to merge this with integration test framework.
0000000000000000000000000000000000000000;;	type Framework struct {
0000000000000000000000000000000000000000;;		BaseName string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// ClientSet uses internal objects, you should use ClientSet where possible.
0000000000000000000000000000000000000000;;		ClientSet clientset.Interface
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		InternalClientset *internalclientset.Clientset
0000000000000000000000000000000000000000;;		StagingClient     *staging.Clientset
0000000000000000000000000000000000000000;;		ClientPool        dynamic.ClientPool
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		SkipNamespaceCreation    bool            // Whether to skip creating a namespace
0000000000000000000000000000000000000000;;		Namespace                *v1.Namespace   // Every test has at least one namespace unless creation is skipped
0000000000000000000000000000000000000000;;		namespacesToDelete       []*v1.Namespace // Some tests have more than one.
0000000000000000000000000000000000000000;;		NamespaceDeletionTimeout time.Duration
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		gatherer *containerResourceGatherer
0000000000000000000000000000000000000000;;		// Constraints that passed to a check which is executed after data is gathered to
0000000000000000000000000000000000000000;;		// see if 99% of results are within acceptable bounds. It has to be injected in the test,
0000000000000000000000000000000000000000;;		// as expectations vary greatly. Constraints are grouped by the container names.
0000000000000000000000000000000000000000;;		AddonResourceConstraints map[string]ResourceConstraint
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		logsSizeWaitGroup    sync.WaitGroup
0000000000000000000000000000000000000000;;		logsSizeCloseChannel chan bool
0000000000000000000000000000000000000000;;		logsSizeVerifier     *LogsSizeVerifier
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// To make sure that this framework cleans up after itself, no matter what,
0000000000000000000000000000000000000000;;		// we install a Cleanup action before each test and clear it after.  If we
0000000000000000000000000000000000000000;;		// should abort, the AfterSuite hook should run all Cleanup actions.
0000000000000000000000000000000000000000;;		cleanupHandle CleanupActionHandle
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// configuration for framework's client
0000000000000000000000000000000000000000;;		Options FrameworkOptions
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Place where various additional data is stored during test run to be printed to ReportDir,
0000000000000000000000000000000000000000;;		// or stdout if ReportDir is not set once test ends.
0000000000000000000000000000000000000000;;		TestSummaries []TestDataSummary
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type TestDataSummary interface {
0000000000000000000000000000000000000000;;		SummaryKind() string
0000000000000000000000000000000000000000;;		PrintHumanReadable() string
0000000000000000000000000000000000000000;;		PrintJSON() string
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type FrameworkOptions struct {
0000000000000000000000000000000000000000;;		ClientQPS    float32
0000000000000000000000000000000000000000;;		ClientBurst  int
0000000000000000000000000000000000000000;;		GroupVersion *schema.GroupVersion
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewFramework makes a new framework and sets up a BeforeEach/AfterEach for
0000000000000000000000000000000000000000;;	// you (you can write additional before/after each functions).
0000000000000000000000000000000000000000;;	func NewDefaultFramework(baseName string) *Framework {
0000000000000000000000000000000000000000;;		options := FrameworkOptions{
0000000000000000000000000000000000000000;;			ClientQPS:   20,
0000000000000000000000000000000000000000;;			ClientBurst: 50,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return NewFramework(baseName, options, nil)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewFramework(baseName string, options FrameworkOptions, client clientset.Interface) *Framework {
0000000000000000000000000000000000000000;;		f := &Framework{
0000000000000000000000000000000000000000;;			BaseName:                 baseName,
0000000000000000000000000000000000000000;;			AddonResourceConstraints: make(map[string]ResourceConstraint),
0000000000000000000000000000000000000000;;			Options:                  options,
0000000000000000000000000000000000000000;;			ClientSet:                client,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(f.BeforeEach)
0000000000000000000000000000000000000000;;		AfterEach(f.AfterEach)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return f
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getClientRepoConfig copies k8s.io/kubernetes/pkg/client/restclient.Config to
0000000000000000000000000000000000000000;;	// a k8s.io/client-go/pkg/client/restclient.Config. It's not a deep copy. Two
0000000000000000000000000000000000000000;;	// configs may share some common struct.
0000000000000000000000000000000000000000;;	func getClientRepoConfig(src *restclient.Config) (dst *clientreporestclient.Config) {
0000000000000000000000000000000000000000;;		skippedFields := sets.NewString("Transport", "WrapTransport", "RateLimiter", "AuthConfigPersister")
0000000000000000000000000000000000000000;;		dst = &clientreporestclient.Config{}
0000000000000000000000000000000000000000;;		dst.Transport = src.Transport
0000000000000000000000000000000000000000;;		dst.WrapTransport = src.WrapTransport
0000000000000000000000000000000000000000;;		dst.RateLimiter = src.RateLimiter
0000000000000000000000000000000000000000;;		dst.AuthConfigPersister = src.AuthConfigPersister
0000000000000000000000000000000000000000;;		sv := reflect.ValueOf(src).Elem()
0000000000000000000000000000000000000000;;		dv := reflect.ValueOf(dst).Elem()
0000000000000000000000000000000000000000;;		for i := 0; i < sv.NumField(); i++ {
0000000000000000000000000000000000000000;;			if skippedFields.Has(sv.Type().Field(i).Name) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			sf := sv.Field(i).Interface()
0000000000000000000000000000000000000000;;			data, err := json.Marshal(sf)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if !dv.Field(i).CanAddr() {
0000000000000000000000000000000000000000;;				Failf("unaddressable field: %v", dv.Type().Field(i).Name)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				if err := json.Unmarshal(data, dv.Field(i).Addr().Interface()); err != nil {
0000000000000000000000000000000000000000;;					Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return dst
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// BeforeEach gets a client and makes a namespace.
0000000000000000000000000000000000000000;;	func (f *Framework) BeforeEach() {
0000000000000000000000000000000000000000;;		// The fact that we need this feels like a bug in ginkgo.
0000000000000000000000000000000000000000;;		// https://github.com/onsi/ginkgo/issues/222
0000000000000000000000000000000000000000;;		f.cleanupHandle = AddCleanupAction(f.AfterEach)
0000000000000000000000000000000000000000;;		if f.ClientSet == nil {
0000000000000000000000000000000000000000;;			By("Creating a kubernetes client")
0000000000000000000000000000000000000000;;			config, err := LoadConfig()
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			config.QPS = f.Options.ClientQPS
0000000000000000000000000000000000000000;;			config.Burst = f.Options.ClientBurst
0000000000000000000000000000000000000000;;			if f.Options.GroupVersion != nil {
0000000000000000000000000000000000000000;;				config.GroupVersion = f.Options.GroupVersion
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if TestContext.KubeAPIContentType != "" {
0000000000000000000000000000000000000000;;				config.ContentType = TestContext.KubeAPIContentType
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			f.ClientSet, err = clientset.NewForConfig(config)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			f.InternalClientset, err = internalclientset.NewForConfig(config)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			clientRepoConfig := getClientRepoConfig(config)
0000000000000000000000000000000000000000;;			f.StagingClient, err = staging.NewForConfig(clientRepoConfig)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			f.ClientPool = dynamic.NewClientPool(config, api.Registry.RESTMapper(), dynamic.LegacyAPIPathResolverFunc)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if !f.SkipNamespaceCreation {
0000000000000000000000000000000000000000;;			By("Building a namespace api object")
0000000000000000000000000000000000000000;;			namespace, err := f.CreateNamespace(f.BaseName, map[string]string{
0000000000000000000000000000000000000000;;				"e2e-framework": f.BaseName,
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			f.Namespace = namespace
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if TestContext.VerifyServiceAccount {
0000000000000000000000000000000000000000;;				By("Waiting for a default service account to be provisioned in namespace")
0000000000000000000000000000000000000000;;				err = WaitForDefaultServiceAccountInNamespace(f.ClientSet, namespace.Name)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				Logf("Skipping waiting for service account")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if TestContext.GatherKubeSystemResourceUsageData != "false" && TestContext.GatherKubeSystemResourceUsageData != "none" {
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			f.gatherer, err = NewResourceUsageGatherer(f.ClientSet, ResourceGathererOptions{
0000000000000000000000000000000000000000;;				inKubemark: ProviderIs("kubemark"),
0000000000000000000000000000000000000000;;				masterOnly: TestContext.GatherKubeSystemResourceUsageData == "master",
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Error while creating NewResourceUsageGatherer: %v", err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				go f.gatherer.startGatheringData()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if TestContext.GatherLogsSizes {
0000000000000000000000000000000000000000;;			f.logsSizeWaitGroup = sync.WaitGroup{}
0000000000000000000000000000000000000000;;			f.logsSizeWaitGroup.Add(1)
0000000000000000000000000000000000000000;;			f.logsSizeCloseChannel = make(chan bool)
0000000000000000000000000000000000000000;;			f.logsSizeVerifier = NewLogsVerifier(f.ClientSet, f.logsSizeCloseChannel)
0000000000000000000000000000000000000000;;			go func() {
0000000000000000000000000000000000000000;;				f.logsSizeVerifier.Run()
0000000000000000000000000000000000000000;;				f.logsSizeWaitGroup.Done()
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// AfterEach deletes the namespace, after reading its events.
0000000000000000000000000000000000000000;;	func (f *Framework) AfterEach() {
0000000000000000000000000000000000000000;;		RemoveCleanupAction(f.cleanupHandle)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// DeleteNamespace at the very end in defer, to avoid any
0000000000000000000000000000000000000000;;		// expectation failures preventing deleting the namespace.
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			nsDeletionErrors := map[string]error{}
0000000000000000000000000000000000000000;;			// Whether to delete namespace is determined by 3 factors: delete-namespace flag, delete-namespace-on-failure flag and the test result
0000000000000000000000000000000000000000;;			// if delete-namespace set to false, namespace will always be preserved.
0000000000000000000000000000000000000000;;			// if delete-namespace is true and delete-namespace-on-failure is false, namespace will be preserved if test failed.
0000000000000000000000000000000000000000;;			if TestContext.DeleteNamespace && (TestContext.DeleteNamespaceOnFailure || !CurrentGinkgoTestDescription().Failed) {
0000000000000000000000000000000000000000;;				for _, ns := range f.namespacesToDelete {
0000000000000000000000000000000000000000;;					By(fmt.Sprintf("Destroying namespace %q for this suite.", ns.Name))
0000000000000000000000000000000000000000;;					timeout := DefaultNamespaceDeletionTimeout
0000000000000000000000000000000000000000;;					if f.NamespaceDeletionTimeout != 0 {
0000000000000000000000000000000000000000;;						timeout = f.NamespaceDeletionTimeout
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if err := deleteNS(f.ClientSet, f.ClientPool, ns.Name, timeout); err != nil {
0000000000000000000000000000000000000000;;						if !apierrors.IsNotFound(err) {
0000000000000000000000000000000000000000;;							nsDeletionErrors[ns.Name] = err
0000000000000000000000000000000000000000;;						} else {
0000000000000000000000000000000000000000;;							Logf("Namespace %v was already deleted", ns.Name)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				if !TestContext.DeleteNamespace {
0000000000000000000000000000000000000000;;					Logf("Found DeleteNamespace=false, skipping namespace deletion!")
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					Logf("Found DeleteNamespaceOnFailure=false and current test failed, skipping namespace deletion!")
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Paranoia-- prevent reuse!
0000000000000000000000000000000000000000;;			f.Namespace = nil
0000000000000000000000000000000000000000;;			f.ClientSet = nil
0000000000000000000000000000000000000000;;			f.namespacesToDelete = nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// if we had errors deleting, report them now.
0000000000000000000000000000000000000000;;			if len(nsDeletionErrors) != 0 {
0000000000000000000000000000000000000000;;				messages := []string{}
0000000000000000000000000000000000000000;;				for namespaceKey, namespaceErr := range nsDeletionErrors {
0000000000000000000000000000000000000000;;					messages = append(messages, fmt.Sprintf("Couldn't delete ns: %q: %s (%#v)", namespaceKey, namespaceErr, namespaceErr))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				Failf(strings.Join(messages, ","))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Print events if the test failed.
0000000000000000000000000000000000000000;;		if CurrentGinkgoTestDescription().Failed && TestContext.DumpLogsOnFailure {
0000000000000000000000000000000000000000;;			// Pass both unversioned client and and versioned clientset, till we have removed all uses of the unversioned client.
0000000000000000000000000000000000000000;;			if !f.SkipNamespaceCreation {
0000000000000000000000000000000000000000;;				DumpAllNamespaceInfo(f.ClientSet, f.Namespace.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			logFunc := Logf
0000000000000000000000000000000000000000;;			if TestContext.ReportDir != "" {
0000000000000000000000000000000000000000;;				filePath := path.Join(TestContext.ReportDir, "image-puller.txt")
0000000000000000000000000000000000000000;;				file, err := os.Create(filePath)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					By(fmt.Sprintf("Failed to create a file with image-puller data %v: %v\nPrinting to stdout", filePath, err))
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					By(fmt.Sprintf("Dumping a list of prepulled images on each node to file %v", filePath))
0000000000000000000000000000000000000000;;					defer file.Close()
0000000000000000000000000000000000000000;;					if err = file.Chmod(0644); err != nil {
0000000000000000000000000000000000000000;;						Logf("Failed to chmod to 644 of %v: %v", filePath, err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					logFunc = GetLogToFileFunc(file)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				By("Dumping a list of prepulled images on each node...")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			LogContainersInPodsWithLabels(f.ClientSet, metav1.NamespaceSystem, ImagePullerLabels, "image-puller", logFunc)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if TestContext.GatherKubeSystemResourceUsageData != "false" && TestContext.GatherKubeSystemResourceUsageData != "none" && f.gatherer != nil {
0000000000000000000000000000000000000000;;			By("Collecting resource usage data")
0000000000000000000000000000000000000000;;			summary, resourceViolationError := f.gatherer.stopAndSummarize([]int{90, 99, 100}, f.AddonResourceConstraints)
0000000000000000000000000000000000000000;;			defer ExpectNoError(resourceViolationError)
0000000000000000000000000000000000000000;;			f.TestSummaries = append(f.TestSummaries, summary)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if TestContext.GatherLogsSizes {
0000000000000000000000000000000000000000;;			By("Gathering log sizes data")
0000000000000000000000000000000000000000;;			close(f.logsSizeCloseChannel)
0000000000000000000000000000000000000000;;			f.logsSizeWaitGroup.Wait()
0000000000000000000000000000000000000000;;			f.TestSummaries = append(f.TestSummaries, f.logsSizeVerifier.GetSummary())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if TestContext.GatherMetricsAfterTest {
0000000000000000000000000000000000000000;;			By("Gathering metrics")
0000000000000000000000000000000000000000;;			// Grab apiserver, scheduler, controller-manager metrics and nodes' kubelet metrics (for non-kubemark case).
0000000000000000000000000000000000000000;;			grabber, err := metrics.NewMetricsGrabber(f.ClientSet, !ProviderIs("kubemark"), true, true, true)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Failed to create MetricsGrabber (skipping metrics gathering): %v", err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				received, err := grabber.Grab()
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					Logf("MetricsGrabber failed to grab metrics (skipping metrics gathering): %v", err)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					f.TestSummaries = append(f.TestSummaries, (*MetricsForE2E)(&received))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		PrintSummaries(f.TestSummaries, f.BaseName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check whether all nodes are ready after the test.
0000000000000000000000000000000000000000;;		// This is explicitly done at the very end of the test, to avoid
0000000000000000000000000000000000000000;;		// e.g. not removing namespace in case of this failure.
0000000000000000000000000000000000000000;;		if err := AllNodesReady(f.ClientSet, 3*time.Minute); err != nil {
0000000000000000000000000000000000000000;;			Failf("All nodes should be ready after test, %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (f *Framework) CreateNamespace(baseName string, labels map[string]string) (*v1.Namespace, error) {
0000000000000000000000000000000000000000;;		createTestingNS := TestContext.CreateTestingNS
0000000000000000000000000000000000000000;;		if createTestingNS == nil {
0000000000000000000000000000000000000000;;			createTestingNS = CreateTestingNS
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ns, err := createTestingNS(baseName, f.ClientSet, labels)
0000000000000000000000000000000000000000;;		// check ns instead of err to see if it's nil as we may
0000000000000000000000000000000000000000;;		// fail to create serviceAccount in it.
0000000000000000000000000000000000000000;;		// In this case, we should not forget to delete the namespace.
0000000000000000000000000000000000000000;;		if ns != nil {
0000000000000000000000000000000000000000;;			f.namespacesToDelete = append(f.namespacesToDelete, ns)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return ns, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPodTerminated waits for the pod to be terminated with the given reason.
0000000000000000000000000000000000000000;;	func (f *Framework) WaitForPodTerminated(podName, reason string) error {
0000000000000000000000000000000000000000;;		return waitForPodTerminatedInNamespace(f.ClientSet, podName, reason, f.Namespace.Name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPodRunning waits for the pod to run in the namespace.
0000000000000000000000000000000000000000;;	func (f *Framework) WaitForPodRunning(podName string) error {
0000000000000000000000000000000000000000;;		return WaitForPodNameRunningInNamespace(f.ClientSet, podName, f.Namespace.Name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPodReady waits for the pod to flip to ready in the namespace.
0000000000000000000000000000000000000000;;	func (f *Framework) WaitForPodReady(podName string) error {
0000000000000000000000000000000000000000;;		return waitTimeoutForPodReadyInNamespace(f.ClientSet, podName, f.Namespace.Name, PodStartTimeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPodRunningSlow waits for the pod to run in the namespace.
0000000000000000000000000000000000000000;;	// It has a longer timeout then WaitForPodRunning (util.slowPodStartTimeout).
0000000000000000000000000000000000000000;;	func (f *Framework) WaitForPodRunningSlow(podName string) error {
0000000000000000000000000000000000000000;;		return waitForPodRunningInNamespaceSlow(f.ClientSet, podName, f.Namespace.Name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPodNoLongerRunning waits for the pod to no longer be running in the namespace, for either
0000000000000000000000000000000000000000;;	// success or failure.
0000000000000000000000000000000000000000;;	func (f *Framework) WaitForPodNoLongerRunning(podName string) error {
0000000000000000000000000000000000000000;;		return WaitForPodNoLongerRunningInNamespace(f.ClientSet, podName, f.Namespace.Name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestContainerOutput runs the given pod in the given namespace and waits
0000000000000000000000000000000000000000;;	// for all of the containers in the podSpec to move into the 'Success' status, and tests
0000000000000000000000000000000000000000;;	// the specified container log against the given expected output using a substring matcher.
0000000000000000000000000000000000000000;;	func (f *Framework) TestContainerOutput(scenarioName string, pod *v1.Pod, containerIndex int, expectedOutput []string) {
0000000000000000000000000000000000000000;;		f.testContainerOutputMatcher(scenarioName, pod, containerIndex, expectedOutput, ContainSubstring)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestContainerOutputRegexp runs the given pod in the given namespace and waits
0000000000000000000000000000000000000000;;	// for all of the containers in the podSpec to move into the 'Success' status, and tests
0000000000000000000000000000000000000000;;	// the specified container log against the given expected output using a regexp matcher.
0000000000000000000000000000000000000000;;	func (f *Framework) TestContainerOutputRegexp(scenarioName string, pod *v1.Pod, containerIndex int, expectedOutput []string) {
0000000000000000000000000000000000000000;;		f.testContainerOutputMatcher(scenarioName, pod, containerIndex, expectedOutput, MatchRegexp)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Write a file using kubectl exec echo <contents> > <path> via specified container
0000000000000000000000000000000000000000;;	// Because of the primitive technique we're using here, we only allow ASCII alphanumeric characters
0000000000000000000000000000000000000000;;	func (f *Framework) WriteFileViaContainer(podName, containerName string, path string, contents string) error {
0000000000000000000000000000000000000000;;		By("writing a file in the container")
0000000000000000000000000000000000000000;;		allowedCharacters := "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
0000000000000000000000000000000000000000;;		for _, c := range contents {
0000000000000000000000000000000000000000;;			if !strings.ContainsRune(allowedCharacters, c) {
0000000000000000000000000000000000000000;;				return fmt.Errorf("Unsupported character in string to write: %v", c)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		command := fmt.Sprintf("echo '%s' > '%s'", contents, path)
0000000000000000000000000000000000000000;;		stdout, stderr, err := kubectlExecWithRetry(f.Namespace.Name, podName, containerName, "--", "/bin/sh", "-c", command)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("error running kubectl exec to write file: %v\nstdout=%v\nstderr=%v)", err, string(stdout), string(stderr))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Read a file using kubectl exec cat <path>
0000000000000000000000000000000000000000;;	func (f *Framework) ReadFileViaContainer(podName, containerName string, path string) (string, error) {
0000000000000000000000000000000000000000;;		By("reading a file in the container")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		stdout, stderr, err := kubectlExecWithRetry(f.Namespace.Name, podName, containerName, "--", "cat", path)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("error running kubectl exec to read file: %v\nstdout=%v\nstderr=%v)", err, string(stdout), string(stderr))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return string(stdout), err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (f *Framework) CheckFileSizeViaContainer(podName, containerName, path string) (string, error) {
0000000000000000000000000000000000000000;;		By("checking a file size in the container")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		stdout, stderr, err := kubectlExecWithRetry(f.Namespace.Name, podName, containerName, "--", "ls", "-l", path)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("error running kubectl exec to read file: %v\nstdout=%v\nstderr=%v)", err, string(stdout), string(stderr))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return string(stdout), err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CreateServiceForSimpleAppWithPods is a convenience wrapper to create a service and its matching pods all at once.
0000000000000000000000000000000000000000;;	func (f *Framework) CreateServiceForSimpleAppWithPods(contPort int, svcPort int, appName string, podSpec func(n v1.Node) v1.PodSpec, count int, block bool) (error, *v1.Service) {
0000000000000000000000000000000000000000;;		var err error = nil
0000000000000000000000000000000000000000;;		theService := f.CreateServiceForSimpleApp(contPort, svcPort, appName)
0000000000000000000000000000000000000000;;		f.CreatePodsPerNodeForSimpleApp(appName, podSpec, count)
0000000000000000000000000000000000000000;;		if block {
0000000000000000000000000000000000000000;;			err = testutils.WaitForPodsWithLabelRunning(f.ClientSet, f.Namespace.Name, labels.SelectorFromSet(labels.Set(theService.Spec.Selector)))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return err, theService
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CreateServiceForSimpleApp returns a service that selects/exposes pods (send -1 ports if no exposure needed) with an app label.
0000000000000000000000000000000000000000;;	func (f *Framework) CreateServiceForSimpleApp(contPort, svcPort int, appName string) *v1.Service {
0000000000000000000000000000000000000000;;		if appName == "" {
0000000000000000000000000000000000000000;;			panic(fmt.Sprintf("no app name provided"))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		serviceSelector := map[string]string{
0000000000000000000000000000000000000000;;			"app": appName + "-pod",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// For convenience, user sending ports are optional.
0000000000000000000000000000000000000000;;		portsFunc := func() []v1.ServicePort {
0000000000000000000000000000000000000000;;			if contPort < 1 || svcPort < 1 {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				return []v1.ServicePort{{
0000000000000000000000000000000000000000;;					Protocol:   "TCP",
0000000000000000000000000000000000000000;;					Port:       int32(svcPort),
0000000000000000000000000000000000000000;;					TargetPort: intstr.FromInt(contPort),
0000000000000000000000000000000000000000;;				}}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("Creating a service-for-%v for selecting app=%v-pod", appName, appName)
0000000000000000000000000000000000000000;;		service, err := f.ClientSet.Core().Services(f.Namespace.Name).Create(&v1.Service{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name: "service-for-" + appName,
0000000000000000000000000000000000000000;;				Labels: map[string]string{
0000000000000000000000000000000000000000;;					"app": appName + "-service",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.ServiceSpec{
0000000000000000000000000000000000000000;;				Ports:    portsFunc(),
0000000000000000000000000000000000000000;;				Selector: serviceSelector,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;		return service
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CreatePodsPerNodeForSimpleApp Creates pods w/ labels.  Useful for tests which make a bunch of pods w/o any networking.
0000000000000000000000000000000000000000;;	func (f *Framework) CreatePodsPerNodeForSimpleApp(appName string, podSpec func(n v1.Node) v1.PodSpec, maxCount int) map[string]string {
0000000000000000000000000000000000000000;;		nodes := GetReadySchedulableNodesOrDie(f.ClientSet)
0000000000000000000000000000000000000000;;		labels := map[string]string{
0000000000000000000000000000000000000000;;			"app": appName + "-pod",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i, node := range nodes.Items {
0000000000000000000000000000000000000000;;			// one per node, but no more than maxCount.
0000000000000000000000000000000000000000;;			if i <= maxCount {
0000000000000000000000000000000000000000;;				Logf("%v/%v : Creating container with label app=%v-pod", i, maxCount, appName)
0000000000000000000000000000000000000000;;				_, err := f.ClientSet.Core().Pods(f.Namespace.Name).Create(&v1.Pod{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;						Name:   fmt.Sprintf(appName+"-pod-%v", i),
0000000000000000000000000000000000000000;;						Labels: labels,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					Spec: podSpec(node),
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;				ExpectNoError(err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return labels
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type KubeUser struct {
0000000000000000000000000000000000000000;;		Name string `yaml:"name"`
0000000000000000000000000000000000000000;;		User struct {
0000000000000000000000000000000000000000;;			Username string `yaml:"username"`
0000000000000000000000000000000000000000;;			Password string `yaml:"password"`
0000000000000000000000000000000000000000;;			Token    string `yaml:"token"`
0000000000000000000000000000000000000000;;		} `yaml:"user"`
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type KubeCluster struct {
0000000000000000000000000000000000000000;;		Name    string `yaml:"name"`
0000000000000000000000000000000000000000;;		Cluster struct {
0000000000000000000000000000000000000000;;			CertificateAuthorityData string `yaml:"certificate-authority-data"`
0000000000000000000000000000000000000000;;			Server                   string `yaml:"server"`
0000000000000000000000000000000000000000;;		} `yaml:"cluster"`
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type KubeConfig struct {
0000000000000000000000000000000000000000;;		Contexts []struct {
0000000000000000000000000000000000000000;;			Name    string `yaml:"name"`
0000000000000000000000000000000000000000;;			Context struct {
0000000000000000000000000000000000000000;;				Cluster string `yaml:"cluster"`
0000000000000000000000000000000000000000;;				User    string
0000000000000000000000000000000000000000;;			} `yaml:"context"`
0000000000000000000000000000000000000000;;		} `yaml:"contexts"`
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Clusters []KubeCluster `yaml:"clusters"`
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Users []KubeUser `yaml:"users"`
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (kc *KubeConfig) FindUser(name string) *KubeUser {
0000000000000000000000000000000000000000;;		for _, user := range kc.Users {
0000000000000000000000000000000000000000;;			if user.Name == name {
0000000000000000000000000000000000000000;;				return &user
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (kc *KubeConfig) FindCluster(name string) *KubeCluster {
0000000000000000000000000000000000000000;;		for _, cluster := range kc.Clusters {
0000000000000000000000000000000000000000;;			if cluster.Name == name {
0000000000000000000000000000000000000000;;				return &cluster
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func kubectlExecWithRetry(namespace string, podName, containerName string, args ...string) ([]byte, []byte, error) {
0000000000000000000000000000000000000000;;		for numRetries := 0; numRetries < maxKubectlExecRetries; numRetries++ {
0000000000000000000000000000000000000000;;			if numRetries > 0 {
0000000000000000000000000000000000000000;;				Logf("Retrying kubectl exec (retry count=%v/%v)", numRetries+1, maxKubectlExecRetries)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			stdOutBytes, stdErrBytes, err := kubectlExec(namespace, podName, containerName, args...)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				if strings.Contains(strings.ToLower(string(stdErrBytes)), "i/o timeout") {
0000000000000000000000000000000000000000;;					// Retry on "i/o timeout" errors
0000000000000000000000000000000000000000;;					Logf("Warning: kubectl exec encountered i/o timeout.\nerr=%v\nstdout=%v\nstderr=%v)", err, string(stdOutBytes), string(stdErrBytes))
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if strings.Contains(strings.ToLower(string(stdErrBytes)), "container not found") {
0000000000000000000000000000000000000000;;					// Retry on "container not found" errors
0000000000000000000000000000000000000000;;					Logf("Warning: kubectl exec encountered container not found.\nerr=%v\nstdout=%v\nstderr=%v)", err, string(stdOutBytes), string(stdErrBytes))
0000000000000000000000000000000000000000;;					time.Sleep(2 * time.Second)
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			return stdOutBytes, stdErrBytes, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err := fmt.Errorf("Failed: kubectl exec failed %d times with \"i/o timeout\". Giving up.", maxKubectlExecRetries)
0000000000000000000000000000000000000000;;		return nil, nil, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func kubectlExec(namespace string, podName, containerName string, args ...string) ([]byte, []byte, error) {
0000000000000000000000000000000000000000;;		var stdout, stderr bytes.Buffer
0000000000000000000000000000000000000000;;		cmdArgs := []string{
0000000000000000000000000000000000000000;;			"exec",
0000000000000000000000000000000000000000;;			fmt.Sprintf("--namespace=%v", namespace),
0000000000000000000000000000000000000000;;			podName,
0000000000000000000000000000000000000000;;			fmt.Sprintf("-c=%v", containerName),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		cmdArgs = append(cmdArgs, args...)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cmd := KubectlCmd(cmdArgs...)
0000000000000000000000000000000000000000;;		cmd.Stdout, cmd.Stderr = &stdout, &stderr
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Logf("Running '%s %s'", cmd.Path, strings.Join(cmdArgs, " "))
0000000000000000000000000000000000000000;;		err := cmd.Run()
0000000000000000000000000000000000000000;;		return stdout.Bytes(), stderr.Bytes(), err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Wrapper function for ginkgo describe.  Adds namespacing.
0000000000000000000000000000000000000000;;	// TODO: Support type safe tagging as well https://github.com/kubernetes/kubernetes/pull/22401.
0000000000000000000000000000000000000000;;	func KubeDescribe(text string, body func()) bool {
0000000000000000000000000000000000000000;;		return Describe("[k8s.io] "+text, body)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodStateVerification represents a verification of pod state.
0000000000000000000000000000000000000000;;	// Any time you have a set of pods that you want to operate against or query,
0000000000000000000000000000000000000000;;	// this struct can be used to declaratively identify those pods.
0000000000000000000000000000000000000000;;	type PodStateVerification struct {
0000000000000000000000000000000000000000;;		// Optional: only pods that have k=v labels will pass this filter.
0000000000000000000000000000000000000000;;		Selectors map[string]string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Required: The phases which are valid for your pod.
0000000000000000000000000000000000000000;;		ValidPhases []v1.PodPhase
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Optional: only pods passing this function will pass the filter
0000000000000000000000000000000000000000;;		// Verify a pod.
0000000000000000000000000000000000000000;;		// As an optimization, in addition to specfying filter (boolean),
0000000000000000000000000000000000000000;;		// this function allows specifying an error as well.
0000000000000000000000000000000000000000;;		// The error indicates that the polling of the pod spectrum should stop.
0000000000000000000000000000000000000000;;		Verify func(v1.Pod) (bool, error)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Optional: only pods with this name will pass the filter.
0000000000000000000000000000000000000000;;		PodName string
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type ClusterVerification struct {
0000000000000000000000000000000000000000;;		client    clientset.Interface
0000000000000000000000000000000000000000;;		namespace *v1.Namespace // pointer rather than string, since ns isn't created until before each.
0000000000000000000000000000000000000000;;		podState  PodStateVerification
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (f *Framework) NewClusterVerification(namespace *v1.Namespace, filter PodStateVerification) *ClusterVerification {
0000000000000000000000000000000000000000;;		return &ClusterVerification{
0000000000000000000000000000000000000000;;			f.ClientSet,
0000000000000000000000000000000000000000;;			namespace,
0000000000000000000000000000000000000000;;			filter,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func passesPodNameFilter(pod v1.Pod, name string) bool {
0000000000000000000000000000000000000000;;		return name == "" || strings.Contains(pod.Name, name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func passesVerifyFilter(pod v1.Pod, verify func(p v1.Pod) (bool, error)) (bool, error) {
0000000000000000000000000000000000000000;;		if verify == nil {
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			verified, err := verify(pod)
0000000000000000000000000000000000000000;;			// If an error is returned, by definition, pod verification fails
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				return verified, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func passesPhasesFilter(pod v1.Pod, validPhases []v1.PodPhase) bool {
0000000000000000000000000000000000000000;;		passesPhaseFilter := false
0000000000000000000000000000000000000000;;		for _, phase := range validPhases {
0000000000000000000000000000000000000000;;			if pod.Status.Phase == phase {
0000000000000000000000000000000000000000;;				passesPhaseFilter = true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return passesPhaseFilter
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// filterLabels returns a list of pods which have labels.
0000000000000000000000000000000000000000;;	func filterLabels(selectors map[string]string, cli clientset.Interface, ns string) (*v1.PodList, error) {
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		var selector labels.Selector
0000000000000000000000000000000000000000;;		var pl *v1.PodList
0000000000000000000000000000000000000000;;		// List pods based on selectors.  This might be a tiny optimization rather then filtering
0000000000000000000000000000000000000000;;		// everything manually.
0000000000000000000000000000000000000000;;		if len(selectors) > 0 {
0000000000000000000000000000000000000000;;			selector = labels.SelectorFromSet(labels.Set(selectors))
0000000000000000000000000000000000000000;;			options := metav1.ListOptions{LabelSelector: selector.String()}
0000000000000000000000000000000000000000;;			pl, err = cli.Core().Pods(ns).List(options)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			pl, err = cli.Core().Pods(ns).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pl, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// filter filters pods which pass a filter.  It can be used to compose
0000000000000000000000000000000000000000;;	// the more useful abstractions like ForEach, WaitFor, and so on, which
0000000000000000000000000000000000000000;;	// can be used directly by tests.
0000000000000000000000000000000000000000;;	func (p *PodStateVerification) filter(c clientset.Interface, namespace *v1.Namespace) ([]v1.Pod, error) {
0000000000000000000000000000000000000000;;		if len(p.ValidPhases) == 0 || namespace == nil {
0000000000000000000000000000000000000000;;			panic(fmt.Errorf("Need to specify a valid pod phases (%v) and namespace (%v). ", p.ValidPhases, namespace))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ns := namespace.Name
0000000000000000000000000000000000000000;;		pl, err := filterLabels(p.Selectors, c, ns) // Build an v1.PodList to operate against.
0000000000000000000000000000000000000000;;		Logf("Selector matched %v pods for %v", len(pl.Items), p.Selectors)
0000000000000000000000000000000000000000;;		if len(pl.Items) == 0 || err != nil {
0000000000000000000000000000000000000000;;			return pl.Items, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		unfilteredPods := pl.Items
0000000000000000000000000000000000000000;;		filteredPods := []v1.Pod{}
0000000000000000000000000000000000000000;;	ReturnPodsSoFar:
0000000000000000000000000000000000000000;;		// Next: Pod must match at least one of the states that the user specified
0000000000000000000000000000000000000000;;		for _, pod := range unfilteredPods {
0000000000000000000000000000000000000000;;			if !(passesPhasesFilter(pod, p.ValidPhases) && passesPodNameFilter(pod, p.PodName)) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			passesVerify, err := passesVerifyFilter(pod, p.Verify)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Error detected on %v : %v !", pod.Name, err)
0000000000000000000000000000000000000000;;				break ReturnPodsSoFar
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if passesVerify {
0000000000000000000000000000000000000000;;				filteredPods = append(filteredPods, pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return filteredPods, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitFor waits for some minimum number of pods to be verified, according to the PodStateVerification
0000000000000000000000000000000000000000;;	// definition.
0000000000000000000000000000000000000000;;	func (cl *ClusterVerification) WaitFor(atLeast int, timeout time.Duration) ([]v1.Pod, error) {
0000000000000000000000000000000000000000;;		pods := []v1.Pod{}
0000000000000000000000000000000000000000;;		var returnedErr error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := wait.Poll(1*time.Second, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			pods, returnedErr = cl.podState.filter(cl.client, cl.namespace)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Failure
0000000000000000000000000000000000000000;;			if returnedErr != nil {
0000000000000000000000000000000000000000;;				Logf("Cutting polling short: We got an error from the pod filtering layer.")
0000000000000000000000000000000000000000;;				// stop polling if the pod filtering returns an error.  that should never happen.
0000000000000000000000000000000000000000;;				// it indicates, for example, that the client is broken or something non-pod related.
0000000000000000000000000000000000000000;;				return false, returnedErr
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("Found %v / %v", len(pods), atLeast)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Success
0000000000000000000000000000000000000000;;			if len(pods) >= atLeast {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Keep trying...
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		Logf("WaitFor completed with timeout %v.  Pods found = %v out of %v", timeout, len(pods), atLeast)
0000000000000000000000000000000000000000;;		return pods, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForOrFail provides a shorthand WaitFor with failure as an option if anything goes wrong.
0000000000000000000000000000000000000000;;	func (cl *ClusterVerification) WaitForOrFail(atLeast int, timeout time.Duration) {
0000000000000000000000000000000000000000;;		pods, err := cl.WaitFor(atLeast, timeout)
0000000000000000000000000000000000000000;;		if err != nil || len(pods) < atLeast {
0000000000000000000000000000000000000000;;			Failf("Verified %v of %v pods , error : %v", len(pods), atLeast, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ForEach runs a function against every verifiable pod.  Be warned that this doesn't wait for "n" pods to verifiy,
0000000000000000000000000000000000000000;;	// so it may return very quickly if you have strict pod state requirements.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// For example, if you require at least 5 pods to be running before your test will pass,
0000000000000000000000000000000000000000;;	// its smart to first call "clusterVerification.WaitFor(5)" before you call clusterVerification.ForEach.
0000000000000000000000000000000000000000;;	func (cl *ClusterVerification) ForEach(podFunc func(v1.Pod)) error {
0000000000000000000000000000000000000000;;		pods, err := cl.podState.filter(cl.client, cl.namespace)
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			if len(pods) == 0 {
0000000000000000000000000000000000000000;;				Failf("No pods matched the filter.")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("ForEach: Found %v pods from the filter.  Now looping through them.", len(pods))
0000000000000000000000000000000000000000;;			for _, p := range pods {
0000000000000000000000000000000000000000;;				podFunc(p)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			Logf("ForEach: Something went wrong when filtering pods to execute against: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetLogToFileFunc is a convenience function that returns a function that have the same interface as
0000000000000000000000000000000000000000;;	// Logf, but writes to a specified file.
0000000000000000000000000000000000000000;;	func GetLogToFileFunc(file *os.File) func(format string, args ...interface{}) {
0000000000000000000000000000000000000000;;		return func(format string, args ...interface{}) {
0000000000000000000000000000000000000000;;			writer := bufio.NewWriter(file)
0000000000000000000000000000000000000000;;			if _, err := fmt.Fprintf(writer, format, args...); err != nil {
0000000000000000000000000000000000000000;;				Logf("Failed to write file %v with test performance data: %v", file.Name(), err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			writer.Flush()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
