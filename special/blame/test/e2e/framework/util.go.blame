0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2014 The Kubernetes Authors.
7378a0cb38d21d9c5b6304002e679b9e0c9d364d;test/e2e/util.go[test/e2e/util.go][test/e2e/framework/util.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package framework
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"bytes"
0000000000000000000000000000000000000000;;		"context"
0000000000000000000000000000000000000000;;		"encoding/json"
0000000000000000000000000000000000000000;;		"errors"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"io"
0000000000000000000000000000000000000000;;		"io/ioutil"
0000000000000000000000000000000000000000;;		"math/rand"
0000000000000000000000000000000000000000;;		"net"
0000000000000000000000000000000000000000;;		"net/http"
0000000000000000000000000000000000000000;;		"net/url"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"os/exec"
0000000000000000000000000000000000000000;;		"path"
0000000000000000000000000000000000000000;;		"path/filepath"
0000000000000000000000000000000000000000;;		"regexp"
0000000000000000000000000000000000000000;;		goruntime "runtime"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"syscall"
0000000000000000000000000000000000000000;;		"text/tabwriter"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"golang.org/x/crypto/ssh"
0000000000000000000000000000000000000000;;		"golang.org/x/net/websocket"
0000000000000000000000000000000000000000;;		"google.golang.org/api/googleapi"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;		gomegatypes "github.com/onsi/gomega/types"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		batch "k8s.io/api/batch/v1"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		extensions "k8s.io/api/extensions/v1beta1"
0000000000000000000000000000000000000000;;		apierrs "k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/fields"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime/schema"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/watch"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/discovery"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/dynamic"
0000000000000000000000000000000000000000;;		restclient "k8s.io/client-go/rest"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/clientcmd"
0000000000000000000000000000000000000000;;		clientcmdapi "k8s.io/client-go/tools/clientcmd/api"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		v1helper "k8s.io/kubernetes/pkg/api/v1/helper"
0000000000000000000000000000000000000000;;		nodeutil "k8s.io/kubernetes/pkg/api/v1/node"
0000000000000000000000000000000000000000;;		podutil "k8s.io/kubernetes/pkg/api/v1/pod"
0000000000000000000000000000000000000000;;		batchinternal "k8s.io/kubernetes/pkg/apis/batch"
0000000000000000000000000000000000000000;;		extensionsinternal "k8s.io/kubernetes/pkg/apis/extensions"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/conditions"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/cloudprovider/providers/azure"
0000000000000000000000000000000000000000;;		gcecloud "k8s.io/kubernetes/pkg/cloudprovider/providers/gce"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/controller"
0000000000000000000000000000000000000000;;		nodectlr "k8s.io/kubernetes/pkg/controller/node"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubectl"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/format"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/master/ports"
0000000000000000000000000000000000000000;;		sshutil "k8s.io/kubernetes/pkg/ssh"
0000000000000000000000000000000000000000;;		uexec "k8s.io/kubernetes/pkg/util/exec"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/system"
0000000000000000000000000000000000000000;;		utilversion "k8s.io/kubernetes/pkg/util/version"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm/predicates"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/schedulercache"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework/ginkgowrapper"
0000000000000000000000000000000000000000;;		testutil "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// How long to wait for the pod to be listable
0000000000000000000000000000000000000000;;		PodListTimeout = time.Minute
0000000000000000000000000000000000000000;;		// Initial pod start can be delayed O(minutes) by slow docker pulls
0000000000000000000000000000000000000000;;		// TODO: Make this 30 seconds once #4566 is resolved.
0000000000000000000000000000000000000000;;		PodStartTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If there are any orphaned namespaces to clean up, this test is running
0000000000000000000000000000000000000000;;		// on a long lived cluster. A long wait here is preferably to spurious test
0000000000000000000000000000000000000000;;		// failures caused by leaked resources from a previous test run.
0000000000000000000000000000000000000000;;		NamespaceCleanupTimeout = 15 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Some pods can take much longer to get ready due to volume attach/detach latency.
0000000000000000000000000000000000000000;;		slowPodStartTimeout = 15 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long to wait for a service endpoint to be resolvable.
0000000000000000000000000000000000000000;;		ServiceStartTimeout = 1 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How often to Poll pods, nodes and claims.
0000000000000000000000000000000000000000;;		Poll = 2 * time.Second
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pollShortTimeout = 1 * time.Minute
0000000000000000000000000000000000000000;;		pollLongTimeout  = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// service accounts are provisioned after namespace creation
0000000000000000000000000000000000000000;;		// a service account is required to support pod creation in a namespace as part of admission control
0000000000000000000000000000000000000000;;		ServiceAccountProvisionTimeout = 2 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long to try single API calls (like 'get' or 'list'). Used to prevent
0000000000000000000000000000000000000000;;		// transient failures from failing tests.
0000000000000000000000000000000000000000;;		// TODO: client should not apply this timeout to Watch calls. Increased from 30s until that is fixed.
0000000000000000000000000000000000000000;;		SingleCallTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long nodes have to be "ready" when a test begins. They should already
0000000000000000000000000000000000000000;;		// be "ready" before the test starts, so this is small.
0000000000000000000000000000000000000000;;		NodeReadyInitialTimeout = 20 * time.Second
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long pods have to be "ready" when a test begins.
0000000000000000000000000000000000000000;;		PodReadyBeforeTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long pods have to become scheduled onto nodes
0000000000000000000000000000000000000000;;		podScheduledBeforeTimeout = PodListTimeout + (20 * time.Second)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podRespondingTimeout     = 15 * time.Minute
0000000000000000000000000000000000000000;;		ServiceRespondingTimeout = 2 * time.Minute
0000000000000000000000000000000000000000;;		EndpointRegisterTimeout  = time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long claims have to become dynamically provisioned
0000000000000000000000000000000000000000;;		ClaimProvisionTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// When these values are updated, also update cmd/kubelet/app/options/options.go
0000000000000000000000000000000000000000;;		currentPodInfraContainerImageName    = "gcr.io/google_containers/pause"
0000000000000000000000000000000000000000;;		currentPodInfraContainerImageVersion = "3.0"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long each node is given during a process that restarts all nodes
0000000000000000000000000000000000000000;;		// before the test is considered failed. (Note that the total time to
0000000000000000000000000000000000000000;;		// restart all nodes will be this number times the number of nodes.)
0000000000000000000000000000000000000000;;		RestartPerNodeTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How often to Poll the statues of a restart.
0000000000000000000000000000000000000000;;		RestartPoll = 20 * time.Second
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long a node is allowed to become "Ready" after it is restarted before
0000000000000000000000000000000000000000;;		// the test is considered failed.
0000000000000000000000000000000000000000;;		RestartNodeReadyAgainTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long a pod is allowed to become "running" and "ready" after a node
0000000000000000000000000000000000000000;;		// restart before test is considered failed.
0000000000000000000000000000000000000000;;		RestartPodReadyAgainTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Number of objects that gc can delete in a second.
0000000000000000000000000000000000000000;;		// GC issues 2 requestes for single delete.
0000000000000000000000000000000000000000;;		gcThroughput = 10
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Minimal number of nodes for the cluster to be considered large.
0000000000000000000000000000000000000000;;		largeClusterThreshold = 100
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO(justinsb): Avoid hardcoding this.
0000000000000000000000000000000000000000;;		awsMasterIP = "172.20.0.9"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Serve hostname image name
0000000000000000000000000000000000000000;;		ServeHostnameImage = "gcr.io/google_containers/serve_hostname:v1.4"
0000000000000000000000000000000000000000;;		// ssh port
0000000000000000000000000000000000000000;;		sshPort = "22"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var (
0000000000000000000000000000000000000000;;		// Label allocated to the image puller static pod that runs on each node
0000000000000000000000000000000000000000;;		// before e2es.
0000000000000000000000000000000000000000;;		ImagePullerLabels = map[string]string{"name": "e2e-image-puller"}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// For parsing Kubectl version for version-skewed testing.
0000000000000000000000000000000000000000;;		gitVersionRegexp = regexp.MustCompile("GitVersion:\"(v.+?)\"")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Slice of regexps for names of pods that have to be running to consider a Node "healthy"
0000000000000000000000000000000000000000;;		requiredPerNodePods = []*regexp.Regexp{
0000000000000000000000000000000000000000;;			regexp.MustCompile(".*kube-proxy.*"),
0000000000000000000000000000000000000000;;			regexp.MustCompile(".*fluentd-elasticsearch.*"),
0000000000000000000000000000000000000000;;			regexp.MustCompile(".*node-problem-detector.*"),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type Address struct {
0000000000000000000000000000000000000000;;		internalIP string
0000000000000000000000000000000000000000;;		externalIP string
0000000000000000000000000000000000000000;;		hostname   string
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetServerArchitecture fetches the architecture of the cluster's apiserver.
0000000000000000000000000000000000000000;;	func GetServerArchitecture(c clientset.Interface) string {
0000000000000000000000000000000000000000;;		arch := ""
0000000000000000000000000000000000000000;;		sVer, err := c.Discovery().ServerVersion()
0000000000000000000000000000000000000000;;		if err != nil || sVer.Platform == "" {
0000000000000000000000000000000000000000;;			// If we failed to get the server version for some reason, default to amd64.
0000000000000000000000000000000000000000;;			arch = "amd64"
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// Split the platform string into OS and Arch separately.
0000000000000000000000000000000000000000;;			// The platform string may for example be "linux/amd64", "linux/arm" or "windows/amd64".
0000000000000000000000000000000000000000;;			osArchArray := strings.Split(sVer.Platform, "/")
0000000000000000000000000000000000000000;;			arch = osArchArray[1]
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return arch
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetPauseImageName fetches the pause image name for the same architecture as the apiserver.
0000000000000000000000000000000000000000;;	func GetPauseImageName(c clientset.Interface) string {
0000000000000000000000000000000000000000;;		return currentPodInfraContainerImageName + "-" + GetServerArchitecture(c) + ":" + currentPodInfraContainerImageVersion
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetPauseImageNameForHostArch fetches the pause image name for the same architecture the test is running on.
0000000000000000000000000000000000000000;;	// TODO: move this function to the test/utils
0000000000000000000000000000000000000000;;	func GetPauseImageNameForHostArch() string {
0000000000000000000000000000000000000000;;		return currentPodInfraContainerImageName + "-" + goruntime.GOARCH + ":" + currentPodInfraContainerImageVersion
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// SubResource proxy should have been functional in v1.0.0, but SubResource
0000000000000000000000000000000000000000;;	// proxy via tunneling is known to be broken in v1.0.  See
0000000000000000000000000000000000000000;;	// https://github.com/kubernetes/kubernetes/pull/15224#issuecomment-146769463
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// TODO(ihmccreery): remove once we don't care about v1.0 anymore, (tentatively
0000000000000000000000000000000000000000;;	// in v1.3).
0000000000000000000000000000000000000000;;	var SubResourcePodProxyVersion = utilversion.MustParseSemantic("v1.1.0")
0000000000000000000000000000000000000000;;	var SubResourceServiceAndNodeProxyVersion = utilversion.MustParseSemantic("v1.2.0")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func GetServicesProxyRequest(c clientset.Interface, request *restclient.Request) (*restclient.Request, error) {
0000000000000000000000000000000000000000;;		subResourceProxyAvailable, err := ServerVersionGTE(SubResourceServiceAndNodeProxyVersion, c.Discovery())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if subResourceProxyAvailable {
0000000000000000000000000000000000000000;;			return request.Resource("services").SubResource("proxy"), nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return request.Prefix("proxy").Resource("services"), nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// unique identifier of the e2e run
0000000000000000000000000000000000000000;;	var RunId = uuid.NewUUID()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type CreateTestingNSFn func(baseName string, c clientset.Interface, labels map[string]string) (*v1.Namespace, error)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type ContainerFailures struct {
0000000000000000000000000000000000000000;;		status   *v1.ContainerStateTerminated
0000000000000000000000000000000000000000;;		Restarts int
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func GetMasterHost() string {
0000000000000000000000000000000000000000;;		masterUrl, err := url.Parse(TestContext.Host)
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;		return masterUrl.Host
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func nowStamp() string {
0000000000000000000000000000000000000000;;		return time.Now().Format(time.StampMilli)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func log(level string, format string, args ...interface{}) {
0000000000000000000000000000000000000000;;		fmt.Fprintf(GinkgoWriter, nowStamp()+": "+level+": "+format+"\n", args...)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func Logf(format string, args ...interface{}) {
0000000000000000000000000000000000000000;;		log("INFO", format, args...)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func Failf(format string, args ...interface{}) {
0000000000000000000000000000000000000000;;		FailfWithOffset(1, format, args...)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// FailfWithOffset calls "Fail" and logs the error at "offset" levels above its caller
0000000000000000000000000000000000000000;;	// (for example, for call chain f -> g -> FailfWithOffset(1, ...) error would be logged for "f").
0000000000000000000000000000000000000000;;	func FailfWithOffset(offset int, format string, args ...interface{}) {
0000000000000000000000000000000000000000;;		msg := fmt.Sprintf(format, args...)
0000000000000000000000000000000000000000;;		log("INFO", msg)
0000000000000000000000000000000000000000;;		ginkgowrapper.Fail(nowStamp()+": "+msg, 1+offset)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func Skipf(format string, args ...interface{}) {
0000000000000000000000000000000000000000;;		msg := fmt.Sprintf(format, args...)
0000000000000000000000000000000000000000;;		log("INFO", msg)
0000000000000000000000000000000000000000;;		ginkgowrapper.Skip(nowStamp() + ": " + msg)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipUnlessNodeCountIsAtLeast(minNodeCount int) {
0000000000000000000000000000000000000000;;		if TestContext.CloudConfig.NumNodes < minNodeCount {
0000000000000000000000000000000000000000;;			Skipf("Requires at least %d nodes (not %d)", minNodeCount, TestContext.CloudConfig.NumNodes)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipUnlessNodeCountIsAtMost(maxNodeCount int) {
0000000000000000000000000000000000000000;;		if TestContext.CloudConfig.NumNodes > maxNodeCount {
0000000000000000000000000000000000000000;;			Skipf("Requires at most %d nodes (not %d)", maxNodeCount, TestContext.CloudConfig.NumNodes)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipUnlessAtLeast(value int, minValue int, message string) {
0000000000000000000000000000000000000000;;		if value < minValue {
0000000000000000000000000000000000000000;;			Skipf(message)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipIfProviderIs(unsupportedProviders ...string) {
0000000000000000000000000000000000000000;;		if ProviderIs(unsupportedProviders...) {
0000000000000000000000000000000000000000;;			Skipf("Not supported for providers %v (found %s)", unsupportedProviders, TestContext.Provider)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipUnlessSSHKeyPresent() {
0000000000000000000000000000000000000000;;		if _, err := GetSigner(TestContext.Provider); err != nil {
0000000000000000000000000000000000000000;;			Skipf("No SSH Key for provider %s: '%v'", TestContext.Provider, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipUnlessProviderIs(supportedProviders ...string) {
0000000000000000000000000000000000000000;;		if !ProviderIs(supportedProviders...) {
0000000000000000000000000000000000000000;;			Skipf("Only supported for providers %v (not %s)", supportedProviders, TestContext.Provider)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipUnlessNodeOSDistroIs(supportedNodeOsDistros ...string) {
0000000000000000000000000000000000000000;;		if !NodeOSDistroIs(supportedNodeOsDistros...) {
0000000000000000000000000000000000000000;;			Skipf("Only supported for node OS distro %v (not %s)", supportedNodeOsDistros, TestContext.NodeOSDistro)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipIfContainerRuntimeIs(runtimes ...string) {
0000000000000000000000000000000000000000;;		for _, runtime := range runtimes {
0000000000000000000000000000000000000000;;			if runtime == TestContext.ContainerRuntime {
0000000000000000000000000000000000000000;;				Skipf("Not supported under container runtime %s", runtime)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ProviderIs(providers ...string) bool {
0000000000000000000000000000000000000000;;		for _, provider := range providers {
0000000000000000000000000000000000000000;;			if strings.ToLower(provider) == strings.ToLower(TestContext.Provider) {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NodeOSDistroIs(supportedNodeOsDistros ...string) bool {
0000000000000000000000000000000000000000;;		for _, distro := range supportedNodeOsDistros {
0000000000000000000000000000000000000000;;			if strings.ToLower(distro) == strings.ToLower(TestContext.NodeOSDistro) {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ProxyMode(f *Framework) (string, error) {
0000000000000000000000000000000000000000;;		pod := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:      "kube-proxy-mode-detector",
0000000000000000000000000000000000000000;;				Namespace: f.Namespace.Name,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				HostNetwork: true,
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:    "detector",
0000000000000000000000000000000000000000;;						Image:   "gcr.io/google_containers/e2e-net-amd64:1.0",
0000000000000000000000000000000000000000;;						Command: []string{"/bin/sleep", "3600"},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		f.PodClient().CreateSync(pod)
0000000000000000000000000000000000000000;;		defer f.PodClient().DeleteSync(pod.Name, &metav1.DeleteOptions{}, DefaultPodDeletionTimeout)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cmd := "curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode"
0000000000000000000000000000000000000000;;		stdout, err := RunHostCmd(pod.Namespace, pod.Name, cmd)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("ProxyMode: %s", stdout)
0000000000000000000000000000000000000000;;		return stdout, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipUnlessServerVersionGTE(v *utilversion.Version, c discovery.ServerVersionInterface) {
0000000000000000000000000000000000000000;;		gte, err := ServerVersionGTE(v, c)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Failf("Failed to get server version: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !gte {
0000000000000000000000000000000000000000;;			Skipf("Not supported for server versions before %q", v)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipIfMissingResource(clientPool dynamic.ClientPool, gvr schema.GroupVersionResource, namespace string) {
0000000000000000000000000000000000000000;;		dynamicClient, err := clientPool.ClientForGroupVersionResource(gvr)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Failf("Unexpected error getting dynamic client for %v: %v", gvr.GroupVersion(), err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		apiResource := metav1.APIResource{Name: gvr.Resource, Namespaced: true}
0000000000000000000000000000000000000000;;		_, err = dynamicClient.Resource(&apiResource, namespace).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			// not all resources support list, so we ignore those
0000000000000000000000000000000000000000;;			if apierrs.IsMethodNotSupported(err) || apierrs.IsNotFound(err) || apierrs.IsForbidden(err) {
0000000000000000000000000000000000000000;;				Skipf("Could not find %s resource, skipping test: %#v", gvr, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Failf("Unexpected error getting %v: %v", gvr, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ProvidersWithSSH are those providers where each node is accessible with SSH
0000000000000000000000000000000000000000;;	var ProvidersWithSSH = []string{"gce", "gke", "aws", "local"}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type podCondition func(pod *v1.Pod) (bool, error)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// logPodStates logs basic info of provided pods for debugging.
0000000000000000000000000000000000000000;;	func logPodStates(pods []v1.Pod) {
0000000000000000000000000000000000000000;;		// Find maximum widths for pod, node, and phase strings for column printing.
0000000000000000000000000000000000000000;;		maxPodW, maxNodeW, maxPhaseW, maxGraceW := len("POD"), len("NODE"), len("PHASE"), len("GRACE")
0000000000000000000000000000000000000000;;		for i := range pods {
0000000000000000000000000000000000000000;;			pod := &pods[i]
0000000000000000000000000000000000000000;;			if len(pod.ObjectMeta.Name) > maxPodW {
0000000000000000000000000000000000000000;;				maxPodW = len(pod.ObjectMeta.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(pod.Spec.NodeName) > maxNodeW {
0000000000000000000000000000000000000000;;				maxNodeW = len(pod.Spec.NodeName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(pod.Status.Phase) > maxPhaseW {
0000000000000000000000000000000000000000;;				maxPhaseW = len(pod.Status.Phase)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Increase widths by one to separate by a single space.
0000000000000000000000000000000000000000;;		maxPodW++
0000000000000000000000000000000000000000;;		maxNodeW++
0000000000000000000000000000000000000000;;		maxPhaseW++
0000000000000000000000000000000000000000;;		maxGraceW++
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Log pod info. * does space padding, - makes them left-aligned.
0000000000000000000000000000000000000000;;		Logf("%-[1]*[2]s %-[3]*[4]s %-[5]*[6]s %-[7]*[8]s %[9]s",
0000000000000000000000000000000000000000;;			maxPodW, "POD", maxNodeW, "NODE", maxPhaseW, "PHASE", maxGraceW, "GRACE", "CONDITIONS")
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			grace := ""
0000000000000000000000000000000000000000;;			if pod.DeletionGracePeriodSeconds != nil {
0000000000000000000000000000000000000000;;				grace = fmt.Sprintf("%ds", *pod.DeletionGracePeriodSeconds)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("%-[1]*[2]s %-[3]*[4]s %-[5]*[6]s %-[7]*[8]s %[9]s",
0000000000000000000000000000000000000000;;				maxPodW, pod.ObjectMeta.Name, maxNodeW, pod.Spec.NodeName, maxPhaseW, pod.Status.Phase, maxGraceW, grace, pod.Status.Conditions)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("") // Final empty line helps for readability.
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// errorBadPodsStates create error message of basic info of bad pods for debugging.
0000000000000000000000000000000000000000;;	func errorBadPodsStates(badPods []v1.Pod, desiredPods int, ns, desiredState string, timeout time.Duration) string {
0000000000000000000000000000000000000000;;		errStr := fmt.Sprintf("%d / %d pods in namespace %q are NOT in %s state in %v\n", len(badPods), desiredPods, ns, desiredState, timeout)
0000000000000000000000000000000000000000;;		// Pirnt bad pods info only if there are fewer than 10 bad pods
0000000000000000000000000000000000000000;;		if len(badPods) > 10 {
0000000000000000000000000000000000000000;;			return errStr + "There are too many bad pods. Please check log for details."
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		buf := bytes.NewBuffer(nil)
0000000000000000000000000000000000000000;;		w := tabwriter.NewWriter(buf, 0, 0, 1, ' ', 0)
0000000000000000000000000000000000000000;;		fmt.Fprintln(w, "POD\tNODE\tPHASE\tGRACE\tCONDITIONS")
0000000000000000000000000000000000000000;;		for _, badPod := range badPods {
0000000000000000000000000000000000000000;;			grace := ""
0000000000000000000000000000000000000000;;			if badPod.DeletionGracePeriodSeconds != nil {
0000000000000000000000000000000000000000;;				grace = fmt.Sprintf("%ds", *badPod.DeletionGracePeriodSeconds)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			podInfo := fmt.Sprintf("%s\t%s\t%s\t%s\t%s",
0000000000000000000000000000000000000000;;				badPod.ObjectMeta.Name, badPod.Spec.NodeName, badPod.Status.Phase, grace, badPod.Status.Conditions)
0000000000000000000000000000000000000000;;			fmt.Fprintln(w, podInfo)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		w.Flush()
0000000000000000000000000000000000000000;;		return errStr + buf.String()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPodsSuccess waits till all labels matching the given selector enter
0000000000000000000000000000000000000000;;	// the Success state. The caller is expected to only invoke this method once the
0000000000000000000000000000000000000000;;	// pods have been created.
0000000000000000000000000000000000000000;;	func WaitForPodsSuccess(c clientset.Interface, ns string, successPodLabels map[string]string, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		successPodSelector := labels.SelectorFromSet(successPodLabels)
0000000000000000000000000000000000000000;;		start, badPods, desiredPods := time.Now(), []v1.Pod{}, 0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if wait.PollImmediate(30*time.Second, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			podList, err := c.Core().Pods(ns).List(metav1.ListOptions{LabelSelector: successPodSelector.String()})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Error getting pods in namespace %q: %v", ns, err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(podList.Items) == 0 {
0000000000000000000000000000000000000000;;				Logf("Waiting for pods to enter Success, but no pods in %q match label %v", ns, successPodLabels)
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			badPods = []v1.Pod{}
0000000000000000000000000000000000000000;;			desiredPods = len(podList.Items)
0000000000000000000000000000000000000000;;			for _, pod := range podList.Items {
0000000000000000000000000000000000000000;;				if pod.Status.Phase != v1.PodSucceeded {
0000000000000000000000000000000000000000;;					badPods = append(badPods, pod)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			successPods := len(podList.Items) - len(badPods)
0000000000000000000000000000000000000000;;			Logf("%d / %d pods in namespace %q are in Success state (%d seconds elapsed)",
0000000000000000000000000000000000000000;;				successPods, len(podList.Items), ns, int(time.Since(start).Seconds()))
0000000000000000000000000000000000000000;;			if len(badPods) == 0 {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}) != nil {
0000000000000000000000000000000000000000;;			logPodStates(badPods)
0000000000000000000000000000000000000000;;			LogPodsWithLabels(c, ns, successPodLabels, Logf)
0000000000000000000000000000000000000000;;			return errors.New(errorBadPodsStates(badPods, desiredPods, ns, "SUCCESS", timeout))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPodsRunningReady waits up to timeout to ensure that all pods in
0000000000000000000000000000000000000000;;	// namespace ns are either running and ready, or failed but controlled by a
0000000000000000000000000000000000000000;;	// controller. Also, it ensures that at least minPods are running and
0000000000000000000000000000000000000000;;	// ready. It has separate behavior from other 'wait for' pods functions in
0000000000000000000000000000000000000000;;	// that it requests the list of pods on every iteration. This is useful, for
0000000000000000000000000000000000000000;;	// example, in cluster startup, because the number of pods increases while
0000000000000000000000000000000000000000;;	// waiting. All pods that are in SUCCESS state are not counted.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// If ignoreLabels is not empty, pods matching this selector are ignored.
0000000000000000000000000000000000000000;;	func WaitForPodsRunningReady(c clientset.Interface, ns string, minPods, allowedNotReadyPods int32, timeout time.Duration, ignoreLabels map[string]string) error {
0000000000000000000000000000000000000000;;		ignoreSelector := labels.SelectorFromSet(ignoreLabels)
0000000000000000000000000000000000000000;;		start := time.Now()
0000000000000000000000000000000000000000;;		Logf("Waiting up to %v for all pods (need at least %d) in namespace '%s' to be running and ready",
0000000000000000000000000000000000000000;;			timeout, minPods, ns)
0000000000000000000000000000000000000000;;		wg := sync.WaitGroup{}
0000000000000000000000000000000000000000;;		wg.Add(1)
0000000000000000000000000000000000000000;;		var ignoreNotReady bool
0000000000000000000000000000000000000000;;		badPods := []v1.Pod{}
0000000000000000000000000000000000000000;;		desiredPods := 0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if wait.PollImmediate(Poll, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			// We get the new list of pods, replication controllers, and
0000000000000000000000000000000000000000;;			// replica sets in every iteration because more pods come
0000000000000000000000000000000000000000;;			// online during startup and we want to ensure they are also
0000000000000000000000000000000000000000;;			// checked.
0000000000000000000000000000000000000000;;			replicas, replicaOk := int32(0), int32(0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			rcList, err := c.Core().ReplicationControllers(ns).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Error getting replication controllers in namespace '%s': %v", ns, err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, rc := range rcList.Items {
0000000000000000000000000000000000000000;;				replicas += *rc.Spec.Replicas
0000000000000000000000000000000000000000;;				replicaOk += rc.Status.ReadyReplicas
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			rsList, err := c.Extensions().ReplicaSets(ns).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Error getting replication sets in namespace %q: %v", ns, err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, rs := range rsList.Items {
0000000000000000000000000000000000000000;;				replicas += *rs.Spec.Replicas
0000000000000000000000000000000000000000;;				replicaOk += rs.Status.ReadyReplicas
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			podList, err := c.Core().Pods(ns).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Error getting pods in namespace '%s': %v", ns, err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			nOk := int32(0)
0000000000000000000000000000000000000000;;			notReady := int32(0)
0000000000000000000000000000000000000000;;			badPods = []v1.Pod{}
0000000000000000000000000000000000000000;;			desiredPods = len(podList.Items)
0000000000000000000000000000000000000000;;			for _, pod := range podList.Items {
0000000000000000000000000000000000000000;;				if len(ignoreLabels) != 0 && ignoreSelector.Matches(labels.Set(pod.Labels)) {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				res, err := testutil.PodRunningReady(&pod)
0000000000000000000000000000000000000000;;				switch {
0000000000000000000000000000000000000000;;				case res && err == nil:
0000000000000000000000000000000000000000;;					nOk++
0000000000000000000000000000000000000000;;				case pod.Status.Phase == v1.PodSucceeded:
0000000000000000000000000000000000000000;;					Logf("The status of Pod %s is Succeeded which is unexpected", pod.ObjectMeta.Name)
0000000000000000000000000000000000000000;;					badPods = append(badPods, pod)
0000000000000000000000000000000000000000;;					// it doesn't make sense to wait for this pod
0000000000000000000000000000000000000000;;					return false, errors.New("unexpected Succeeded pod state")
0000000000000000000000000000000000000000;;				case pod.Status.Phase != v1.PodFailed:
0000000000000000000000000000000000000000;;					Logf("The status of Pod %s is %s (Ready = false), waiting for it to be either Running (with Ready = true) or Failed", pod.ObjectMeta.Name, pod.Status.Phase)
0000000000000000000000000000000000000000;;					notReady++
0000000000000000000000000000000000000000;;					badPods = append(badPods, pod)
0000000000000000000000000000000000000000;;				default:
0000000000000000000000000000000000000000;;					if controller.GetControllerOf(&pod) == nil {
0000000000000000000000000000000000000000;;						Logf("Pod %s is Failed, but it's not controlled by a controller", pod.ObjectMeta.Name)
0000000000000000000000000000000000000000;;						badPods = append(badPods, pod)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					//ignore failed pods that are controlled by some controller
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			Logf("%d / %d pods in namespace '%s' are running and ready (%d seconds elapsed)",
0000000000000000000000000000000000000000;;				nOk, len(podList.Items), ns, int(time.Since(start).Seconds()))
0000000000000000000000000000000000000000;;			Logf("expected %d pod replicas in namespace '%s', %d are Running and Ready.", replicas, ns, replicaOk)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if replicaOk == replicas && nOk >= minPods && len(badPods) == 0 {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			ignoreNotReady = (notReady <= allowedNotReadyPods)
0000000000000000000000000000000000000000;;			logPodStates(badPods)
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}) != nil {
0000000000000000000000000000000000000000;;			if !ignoreNotReady {
0000000000000000000000000000000000000000;;				return errors.New(errorBadPodsStates(badPods, desiredPods, ns, "RUNNING and READY", timeout))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("Number of not-ready pods is allowed.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func kubectlLogPod(c clientset.Interface, pod v1.Pod, containerNameSubstr string, logFunc func(ftm string, args ...interface{})) {
0000000000000000000000000000000000000000;;		for _, container := range pod.Spec.Containers {
0000000000000000000000000000000000000000;;			if strings.Contains(container.Name, containerNameSubstr) {
0000000000000000000000000000000000000000;;				// Contains() matches all strings if substr is empty
0000000000000000000000000000000000000000;;				logs, err := GetPodLogs(c, pod.Namespace, pod.Name, container.Name)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					logs, err = getPreviousPodLogs(c, pod.Namespace, pod.Name, container.Name)
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						logFunc("Failed to get logs of pod %v, container %v, err: %v", pod.Name, container.Name, err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				logFunc("Logs of %v/%v:%v on node %v", pod.Namespace, pod.Name, container.Name, pod.Spec.NodeName)
0000000000000000000000000000000000000000;;				logFunc("%s : STARTLOG\n%s\nENDLOG for container %v:%v:%v", containerNameSubstr, logs, pod.Namespace, pod.Name, container.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func LogFailedContainers(c clientset.Interface, ns string, logFunc func(ftm string, args ...interface{})) {
0000000000000000000000000000000000000000;;		podList, err := c.Core().Pods(ns).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			logFunc("Error getting pods in namespace '%s': %v", ns, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		logFunc("Running kubectl logs on non-ready containers in %v", ns)
0000000000000000000000000000000000000000;;		for _, pod := range podList.Items {
0000000000000000000000000000000000000000;;			if res, err := testutil.PodRunningReady(&pod); !res || err != nil {
0000000000000000000000000000000000000000;;				kubectlLogPod(c, pod, "", Logf)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func LogPodsWithLabels(c clientset.Interface, ns string, match map[string]string, logFunc func(ftm string, args ...interface{})) {
0000000000000000000000000000000000000000;;		podList, err := c.Core().Pods(ns).List(metav1.ListOptions{LabelSelector: labels.SelectorFromSet(match).String()})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			logFunc("Error getting pods in namespace %q: %v", ns, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		logFunc("Running kubectl logs on pods with labels %v in %v", match, ns)
0000000000000000000000000000000000000000;;		for _, pod := range podList.Items {
0000000000000000000000000000000000000000;;			kubectlLogPod(c, pod, "", logFunc)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func LogContainersInPodsWithLabels(c clientset.Interface, ns string, match map[string]string, containerSubstr string, logFunc func(ftm string, args ...interface{})) {
0000000000000000000000000000000000000000;;		podList, err := c.Core().Pods(ns).List(metav1.ListOptions{LabelSelector: labels.SelectorFromSet(match).String()})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("Error getting pods in namespace %q: %v", ns, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pod := range podList.Items {
0000000000000000000000000000000000000000;;			kubectlLogPod(c, pod, containerSubstr, logFunc)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// DeleteNamespaces deletes all namespaces that match the given delete and skip filters.
0000000000000000000000000000000000000000;;	// Filter is by simple strings.Contains; first skip filter, then delete filter.
0000000000000000000000000000000000000000;;	// Returns the list of deleted namespaces or an error.
0000000000000000000000000000000000000000;;	func DeleteNamespaces(c clientset.Interface, deleteFilter, skipFilter []string) ([]string, error) {
0000000000000000000000000000000000000000;;		By("Deleting namespaces")
0000000000000000000000000000000000000000;;		nsList, err := c.Core().Namespaces().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		var deleted []string
0000000000000000000000000000000000000000;;		var wg sync.WaitGroup
0000000000000000000000000000000000000000;;	OUTER:
0000000000000000000000000000000000000000;;		for _, item := range nsList.Items {
0000000000000000000000000000000000000000;;			if skipFilter != nil {
0000000000000000000000000000000000000000;;				for _, pattern := range skipFilter {
0000000000000000000000000000000000000000;;					if strings.Contains(item.Name, pattern) {
0000000000000000000000000000000000000000;;						continue OUTER
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if deleteFilter != nil {
0000000000000000000000000000000000000000;;				var shouldDelete bool
0000000000000000000000000000000000000000;;				for _, pattern := range deleteFilter {
0000000000000000000000000000000000000000;;					if strings.Contains(item.Name, pattern) {
0000000000000000000000000000000000000000;;						shouldDelete = true
0000000000000000000000000000000000000000;;						break
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if !shouldDelete {
0000000000000000000000000000000000000000;;					continue OUTER
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			wg.Add(1)
0000000000000000000000000000000000000000;;			deleted = append(deleted, item.Name)
0000000000000000000000000000000000000000;;			go func(nsName string) {
0000000000000000000000000000000000000000;;				defer wg.Done()
0000000000000000000000000000000000000000;;				defer GinkgoRecover()
0000000000000000000000000000000000000000;;				Expect(c.Core().Namespaces().Delete(nsName, nil)).To(Succeed())
0000000000000000000000000000000000000000;;				Logf("namespace : %v api call to delete is complete ", nsName)
0000000000000000000000000000000000000000;;			}(item.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		wg.Wait()
0000000000000000000000000000000000000000;;		return deleted, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func WaitForNamespacesDeleted(c clientset.Interface, namespaces []string, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		By("Waiting for namespaces to vanish")
0000000000000000000000000000000000000000;;		nsMap := map[string]bool{}
0000000000000000000000000000000000000000;;		for _, ns := range namespaces {
0000000000000000000000000000000000000000;;			nsMap[ns] = true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		//Now POLL until all namespaces have been eradicated.
0000000000000000000000000000000000000000;;		return wait.Poll(2*time.Second, timeout,
0000000000000000000000000000000000000000;;			func() (bool, error) {
0000000000000000000000000000000000000000;;				nsList, err := c.Core().Namespaces().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return false, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for _, item := range nsList.Items {
0000000000000000000000000000000000000000;;					if _, ok := nsMap[item.Name]; ok {
0000000000000000000000000000000000000000;;						return false, nil
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func waitForServiceAccountInNamespace(c clientset.Interface, ns, serviceAccountName string, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		w, err := c.Core().ServiceAccounts(ns).Watch(metav1.SingleObject(metav1.ObjectMeta{Name: serviceAccountName}))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, err = watch.Until(timeout, w, conditions.ServiceAccountHasSecrets)
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func WaitForPodCondition(c clientset.Interface, ns, podName, desc string, timeout time.Duration, condition podCondition) error {
0000000000000000000000000000000000000000;;		Logf("Waiting up to %[1]v for pod %[2]s status to be %[3]s", timeout, podName, desc)
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(Poll) {
0000000000000000000000000000000000000000;;			pod, err := c.Core().Pods(ns).Get(podName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				if apierrs.IsNotFound(err) {
0000000000000000000000000000000000000000;;					Logf("Pod %q in namespace %q disappeared. Error: %v", podName, ns, err)
0000000000000000000000000000000000000000;;					return err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// Aligning this text makes it much more readable
0000000000000000000000000000000000000000;;				Logf("Get pod %[1]s in namespace '%[2]s' failed, ignoring for %[3]v. Error: %[4]v",
0000000000000000000000000000000000000000;;					podName, ns, Poll, err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			done, err := condition(pod)
0000000000000000000000000000000000000000;;			if done {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("Waiting for pod %[1]s in namespace '%[2]s' status to be '%[3]s'"+
0000000000000000000000000000000000000000;;				"(found phase: %[4]q, readiness: %[5]t) (%[6]v elapsed)",
0000000000000000000000000000000000000000;;				podName, ns, desc, pod.Status.Phase, testutil.PodReady(pod), time.Since(start))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("gave up waiting for pod '%s' to be '%s' after %v", podName, desc, timeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForMatchPodsCondition finds match pods based on the input ListOptions.
0000000000000000000000000000000000000000;;	// waits and checks if all match pods are in the given podCondition
0000000000000000000000000000000000000000;;	func WaitForMatchPodsCondition(c clientset.Interface, opts metav1.ListOptions, desc string, timeout time.Duration, condition podCondition) error {
0000000000000000000000000000000000000000;;		Logf("Waiting up to %v for matching pods' status to be %s", timeout, desc)
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(Poll) {
0000000000000000000000000000000000000000;;			pods, err := c.Core().Pods(metav1.NamespaceAll).List(opts)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			conditionNotMatch := []string{}
0000000000000000000000000000000000000000;;			for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;				done, err := condition(&pod)
0000000000000000000000000000000000000000;;				if done && err != nil {
0000000000000000000000000000000000000000;;					return fmt.Errorf("Unexpected error: %v", err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if !done {
0000000000000000000000000000000000000000;;					conditionNotMatch = append(conditionNotMatch, format.Pod(&pod))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(conditionNotMatch) <= 0 {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("%d pods are not %s: %v", len(conditionNotMatch), desc, conditionNotMatch)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("gave up waiting for matching pods to be '%s' after %v", desc, timeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForDefaultServiceAccountInNamespace waits for the default service account to be provisioned
0000000000000000000000000000000000000000;;	// the default service account is what is associated with pods when they do not specify a service account
0000000000000000000000000000000000000000;;	// as a result, pods are not able to be provisioned in a namespace until the service account is provisioned
0000000000000000000000000000000000000000;;	func WaitForDefaultServiceAccountInNamespace(c clientset.Interface, namespace string) error {
0000000000000000000000000000000000000000;;		return waitForServiceAccountInNamespace(c, namespace, "default", ServiceAccountProvisionTimeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPersistentVolumePhase waits for a PersistentVolume to be in a specific phase or until timeout occurs, whichever comes first.
0000000000000000000000000000000000000000;;	func WaitForPersistentVolumePhase(phase v1.PersistentVolumePhase, c clientset.Interface, pvName string, Poll, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		Logf("Waiting up to %v for PersistentVolume %s to have phase %s", timeout, pvName, phase)
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(Poll) {
0000000000000000000000000000000000000000;;			pv, err := c.Core().PersistentVolumes().Get(pvName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Get persistent volume %s in failed, ignoring for %v: %v", pvName, Poll, err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				if pv.Status.Phase == phase {
0000000000000000000000000000000000000000;;					Logf("PersistentVolume %s found and phase=%s (%v)", pvName, phase, time.Since(start))
0000000000000000000000000000000000000000;;					return nil
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					Logf("PersistentVolume %s found but phase is %s instead of %s.", pvName, pv.Status.Phase, phase)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("PersistentVolume %s not in phase %s within %v", pvName, phase, timeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPersistentVolumeDeleted waits for a PersistentVolume to get deleted or until timeout occurs, whichever comes first.
0000000000000000000000000000000000000000;;	func WaitForPersistentVolumeDeleted(c clientset.Interface, pvName string, Poll, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		Logf("Waiting up to %v for PersistentVolume %s to get deleted", timeout, pvName)
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(Poll) {
0000000000000000000000000000000000000000;;			pv, err := c.Core().PersistentVolumes().Get(pvName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				Logf("PersistentVolume %s found and phase=%s (%v)", pvName, pv.Status.Phase, time.Since(start))
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				if apierrs.IsNotFound(err) {
0000000000000000000000000000000000000000;;					Logf("PersistentVolume %s was removed", pvName)
0000000000000000000000000000000000000000;;					return nil
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					Logf("Get persistent volume %s in failed, ignoring for %v: %v", pvName, Poll, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("PersistentVolume %s still exists within %v", pvName, timeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPersistentVolumeClaimPhase waits for a PersistentVolumeClaim to be in a specific phase or until timeout occurs, whichever comes first.
0000000000000000000000000000000000000000;;	func WaitForPersistentVolumeClaimPhase(phase v1.PersistentVolumeClaimPhase, c clientset.Interface, ns string, pvcName string, Poll, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		Logf("Waiting up to %v for PersistentVolumeClaim %s to have phase %s", timeout, pvcName, phase)
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(Poll) {
0000000000000000000000000000000000000000;;			pvc, err := c.Core().PersistentVolumeClaims(ns).Get(pvcName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Failed to get claim %q, retrying in %v. Error: %v", pvcName, Poll, err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				if pvc.Status.Phase == phase {
0000000000000000000000000000000000000000;;					Logf("PersistentVolumeClaim %s found and phase=%s (%v)", pvcName, phase, time.Since(start))
0000000000000000000000000000000000000000;;					return nil
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					Logf("PersistentVolumeClaim %s found but phase is %s instead of %s.", pvcName, pvc.Status.Phase, phase)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("PersistentVolumeClaim %s not in phase %s within %v", pvcName, phase, timeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CreateTestingNS should be used by every test, note that we append a common prefix to the provided test name.
0000000000000000000000000000000000000000;;	// Please see NewFramework instead of using this directly.
0000000000000000000000000000000000000000;;	func CreateTestingNS(baseName string, c clientset.Interface, labels map[string]string) (*v1.Namespace, error) {
0000000000000000000000000000000000000000;;		if labels == nil {
0000000000000000000000000000000000000000;;			labels = map[string]string{}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		labels["e2e-run"] = string(RunId)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		namespaceObj := &v1.Namespace{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				GenerateName: fmt.Sprintf("e2e-tests-%v-", baseName),
0000000000000000000000000000000000000000;;				Namespace:    "",
0000000000000000000000000000000000000000;;				Labels:       labels,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Status: v1.NamespaceStatus{},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Be robust about making the namespace creation call.
0000000000000000000000000000000000000000;;		var got *v1.Namespace
0000000000000000000000000000000000000000;;		if err := wait.PollImmediate(Poll, 30*time.Second, func() (bool, error) {
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			got, err = c.Core().Namespaces().Create(namespaceObj)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Unexpected error while creating namespace: %v", err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		}); err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if TestContext.VerifyServiceAccount {
0000000000000000000000000000000000000000;;			if err := WaitForDefaultServiceAccountInNamespace(c, got.Name); err != nil {
0000000000000000000000000000000000000000;;				// Even if we fail to create serviceAccount in the namespace,
0000000000000000000000000000000000000000;;				// we have successfully create a namespace.
0000000000000000000000000000000000000000;;				// So, return the created namespace.
0000000000000000000000000000000000000000;;				return got, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return got, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CheckTestingNSDeletedExcept checks whether all e2e based existing namespaces are in the Terminating state
0000000000000000000000000000000000000000;;	// and waits until they are finally deleted. It ignores namespace skip.
0000000000000000000000000000000000000000;;	func CheckTestingNSDeletedExcept(c clientset.Interface, skip string) error {
0000000000000000000000000000000000000000;;		// TODO: Since we don't have support for bulk resource deletion in the API,
0000000000000000000000000000000000000000;;		// while deleting a namespace we are deleting all objects from that namespace
0000000000000000000000000000000000000000;;		// one by one (one deletion == one API call). This basically exposes us to
0000000000000000000000000000000000000000;;		// throttling - currently controller-manager has a limit of max 20 QPS.
0000000000000000000000000000000000000000;;		// Once #10217 is implemented and used in namespace-controller, deleting all
0000000000000000000000000000000000000000;;		// object from a given namespace should be much faster and we will be able
0000000000000000000000000000000000000000;;		// to lower this timeout.
0000000000000000000000000000000000000000;;		// However, now Density test is producing ~26000 events and Load capacity test
0000000000000000000000000000000000000000;;		// is producing ~35000 events, thus assuming there are no other requests it will
0000000000000000000000000000000000000000;;		// take ~30 minutes to fully delete the namespace. Thus I'm setting it to 60
0000000000000000000000000000000000000000;;		// minutes to avoid any timeouts here.
0000000000000000000000000000000000000000;;		timeout := 60 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Logf("Waiting for terminating namespaces to be deleted...")
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(15 * time.Second) {
0000000000000000000000000000000000000000;;			namespaces, err := c.Core().Namespaces().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Listing namespaces failed: %v", err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			terminating := 0
0000000000000000000000000000000000000000;;			for _, ns := range namespaces.Items {
0000000000000000000000000000000000000000;;				if strings.HasPrefix(ns.ObjectMeta.Name, "e2e-tests-") && ns.ObjectMeta.Name != skip {
0000000000000000000000000000000000000000;;					if ns.Status.Phase == v1.NamespaceActive {
0000000000000000000000000000000000000000;;						return fmt.Errorf("Namespace %s is active", ns.ObjectMeta.Name)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					terminating++
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if terminating == 0 {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("Waiting for terminating namespaces to be deleted timed out")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// deleteNS deletes the provided namespace, waits for it to be completely deleted, and then checks
0000000000000000000000000000000000000000;;	// whether there are any pods remaining in a non-terminating state.
0000000000000000000000000000000000000000;;	func deleteNS(c clientset.Interface, clientPool dynamic.ClientPool, namespace string, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		startTime := time.Now()
0000000000000000000000000000000000000000;;		if err := c.Core().Namespaces().Delete(namespace, nil); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// wait for namespace to delete or timeout.
0000000000000000000000000000000000000000;;		err := wait.PollImmediate(2*time.Second, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			if _, err := c.Core().Namespaces().Get(namespace, metav1.GetOptions{}); err != nil {
0000000000000000000000000000000000000000;;				if apierrs.IsNotFound(err) {
0000000000000000000000000000000000000000;;					return true, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				Logf("Error while waiting for namespace to be terminated: %v", err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// verify there is no more remaining content in the namespace
0000000000000000000000000000000000000000;;		remainingContent, cerr := hasRemainingContent(c, clientPool, namespace)
0000000000000000000000000000000000000000;;		if cerr != nil {
0000000000000000000000000000000000000000;;			return cerr
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// if content remains, let's dump information about the namespace, and system for flake debugging.
0000000000000000000000000000000000000000;;		remainingPods := 0
0000000000000000000000000000000000000000;;		missingTimestamp := 0
0000000000000000000000000000000000000000;;		if remainingContent {
0000000000000000000000000000000000000000;;			// log information about namespace, and set of namespaces in api server to help flake detection
0000000000000000000000000000000000000000;;			logNamespace(c, namespace)
0000000000000000000000000000000000000000;;			logNamespaces(c, namespace)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// if we can, check if there were pods remaining with no timestamp.
0000000000000000000000000000000000000000;;			remainingPods, missingTimestamp, _ = countRemainingPods(c, namespace)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// a timeout waiting for namespace deletion happened!
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			// some content remains in the namespace
0000000000000000000000000000000000000000;;			if remainingContent {
0000000000000000000000000000000000000000;;				// pods remain
0000000000000000000000000000000000000000;;				if remainingPods > 0 {
0000000000000000000000000000000000000000;;					if missingTimestamp != 0 {
0000000000000000000000000000000000000000;;						// pods remained, but were not undergoing deletion (namespace controller is probably culprit)
0000000000000000000000000000000000000000;;						return fmt.Errorf("namespace %v was not deleted with limit: %v, pods remaining: %v, pods missing deletion timestamp: %v", namespace, err, remainingPods, missingTimestamp)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					// but they were all undergoing deletion (kubelet is probably culprit, check NodeLost)
0000000000000000000000000000000000000000;;					return fmt.Errorf("namespace %v was not deleted with limit: %v, pods remaining: %v", namespace, err, remainingPods)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// other content remains (namespace controller is probably screwed up)
0000000000000000000000000000000000000000;;				return fmt.Errorf("namespace %v was not deleted with limit: %v, namespaced content other than pods remain", namespace, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// no remaining content, but namespace was not deleted (namespace controller is probably wedged)
0000000000000000000000000000000000000000;;			return fmt.Errorf("namespace %v was not deleted with limit: %v, namespace is empty but is not yet removed", namespace, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("namespace %v deletion completed in %s", namespace, time.Now().Sub(startTime))
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// logNamespaces logs the number of namespaces by phase
0000000000000000000000000000000000000000;;	// namespace is the namespace the test was operating against that failed to delete so it can be grepped in logs
0000000000000000000000000000000000000000;;	func logNamespaces(c clientset.Interface, namespace string) {
0000000000000000000000000000000000000000;;		namespaceList, err := c.Core().Namespaces().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("namespace: %v, unable to list namespaces: %v", namespace, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		numActive := 0
0000000000000000000000000000000000000000;;		numTerminating := 0
0000000000000000000000000000000000000000;;		for _, namespace := range namespaceList.Items {
0000000000000000000000000000000000000000;;			if namespace.Status.Phase == v1.NamespaceActive {
0000000000000000000000000000000000000000;;				numActive++
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				numTerminating++
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("namespace: %v, total namespaces: %v, active: %v, terminating: %v", namespace, len(namespaceList.Items), numActive, numTerminating)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// logNamespace logs detail about a namespace
0000000000000000000000000000000000000000;;	func logNamespace(c clientset.Interface, namespace string) {
0000000000000000000000000000000000000000;;		ns, err := c.Core().Namespaces().Get(namespace, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			if apierrs.IsNotFound(err) {
0000000000000000000000000000000000000000;;				Logf("namespace: %v no longer exists", namespace)
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("namespace: %v, unable to get namespace due to error: %v", namespace, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("namespace: %v, DeletionTimetamp: %v, Finalizers: %v, Phase: %v", ns.Name, ns.DeletionTimestamp, ns.Spec.Finalizers, ns.Status.Phase)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// countRemainingPods queries the server to count number of remaining pods, and number of pods that had a missing deletion timestamp.
0000000000000000000000000000000000000000;;	func countRemainingPods(c clientset.Interface, namespace string) (int, int, error) {
0000000000000000000000000000000000000000;;		// check for remaining pods
0000000000000000000000000000000000000000;;		pods, err := c.Core().Pods(namespace).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return 0, 0, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// nothing remains!
0000000000000000000000000000000000000000;;		if len(pods.Items) == 0 {
0000000000000000000000000000000000000000;;			return 0, 0, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// stuff remains, log about it
0000000000000000000000000000000000000000;;		logPodStates(pods.Items)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// check if there were any pods with missing deletion timestamp
0000000000000000000000000000000000000000;;		numPods := len(pods.Items)
0000000000000000000000000000000000000000;;		missingTimestamp := 0
0000000000000000000000000000000000000000;;		for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;			if pod.DeletionTimestamp == nil {
0000000000000000000000000000000000000000;;				missingTimestamp++
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return numPods, missingTimestamp, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// hasRemainingContent checks if there is remaining content in the namespace via API discovery
0000000000000000000000000000000000000000;;	func hasRemainingContent(c clientset.Interface, clientPool dynamic.ClientPool, namespace string) (bool, error) {
0000000000000000000000000000000000000000;;		// some tests generate their own framework.Client rather than the default
0000000000000000000000000000000000000000;;		// TODO: ensure every test call has a configured clientPool
0000000000000000000000000000000000000000;;		if clientPool == nil {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// find out what content is supported on the server
0000000000000000000000000000000000000000;;		resources, err := c.Discovery().ServerPreferredNamespacedResources()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		groupVersionResources, err := discovery.GroupVersionResources(resources)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: temporary hack for https://github.com/kubernetes/kubernetes/issues/31798
0000000000000000000000000000000000000000;;		ignoredResources := sets.NewString("bindings")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		contentRemaining := false
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// dump how many of resource type is on the server in a log.
0000000000000000000000000000000000000000;;		for gvr := range groupVersionResources {
0000000000000000000000000000000000000000;;			// get a client for this group version...
0000000000000000000000000000000000000000;;			dynamicClient, err := clientPool.ClientForGroupVersionResource(gvr)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				// not all resource types support list, so some errors here are normal depending on the resource type.
0000000000000000000000000000000000000000;;				Logf("namespace: %s, unable to get client - gvr: %v, error: %v", namespace, gvr, err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// get the api resource
0000000000000000000000000000000000000000;;			apiResource := metav1.APIResource{Name: gvr.Resource, Namespaced: true}
0000000000000000000000000000000000000000;;			// TODO: temporary hack for https://github.com/kubernetes/kubernetes/issues/31798
0000000000000000000000000000000000000000;;			if ignoredResources.Has(apiResource.Name) {
0000000000000000000000000000000000000000;;				Logf("namespace: %s, resource: %s, ignored listing per whitelist", namespace, apiResource.Name)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			obj, err := dynamicClient.Resource(&apiResource, namespace).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				// not all resources support list, so we ignore those
0000000000000000000000000000000000000000;;				if apierrs.IsMethodNotSupported(err) || apierrs.IsNotFound(err) || apierrs.IsForbidden(err) {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			unstructuredList, ok := obj.(*unstructured.UnstructuredList)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				return false, fmt.Errorf("namespace: %s, resource: %s, expected *unstructured.UnstructuredList, got %#v", namespace, apiResource.Name, obj)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(unstructuredList.Items) > 0 {
0000000000000000000000000000000000000000;;				Logf("namespace: %s, resource: %s, items remaining: %v", namespace, apiResource.Name, len(unstructuredList.Items))
0000000000000000000000000000000000000000;;				contentRemaining = true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return contentRemaining, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ContainerInitInvariant(older, newer runtime.Object) error {
0000000000000000000000000000000000000000;;		oldPod := older.(*v1.Pod)
0000000000000000000000000000000000000000;;		newPod := newer.(*v1.Pod)
0000000000000000000000000000000000000000;;		if len(oldPod.Spec.InitContainers) == 0 {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(oldPod.Spec.InitContainers) != len(newPod.Spec.InitContainers) {
0000000000000000000000000000000000000000;;			return fmt.Errorf("init container list changed")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if oldPod.UID != newPod.UID {
0000000000000000000000000000000000000000;;			return fmt.Errorf("two different pods exist in the condition: %s vs %s", oldPod.UID, newPod.UID)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := initContainersInvariants(oldPod); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := initContainersInvariants(newPod); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		oldInit, _, _ := podInitialized(oldPod)
0000000000000000000000000000000000000000;;		newInit, _, _ := podInitialized(newPod)
0000000000000000000000000000000000000000;;		if oldInit && !newInit {
0000000000000000000000000000000000000000;;			// TODO: we may in the future enable resetting PodInitialized = false if the kubelet needs to restart it
0000000000000000000000000000000000000000;;			// from scratch
0000000000000000000000000000000000000000;;			return fmt.Errorf("pod cannot be initialized and then regress to not being initialized")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podInitialized(pod *v1.Pod) (ok bool, failed bool, err error) {
0000000000000000000000000000000000000000;;		allInit := true
0000000000000000000000000000000000000000;;		initFailed := false
0000000000000000000000000000000000000000;;		for _, s := range pod.Status.InitContainerStatuses {
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case initFailed && s.State.Waiting == nil:
0000000000000000000000000000000000000000;;				return allInit, initFailed, fmt.Errorf("container %s is after a failed container but isn't waiting", s.Name)
0000000000000000000000000000000000000000;;			case allInit && s.State.Waiting == nil:
0000000000000000000000000000000000000000;;				return allInit, initFailed, fmt.Errorf("container %s is after an initializing container but isn't waiting", s.Name)
0000000000000000000000000000000000000000;;			case s.State.Terminated == nil:
0000000000000000000000000000000000000000;;				allInit = false
0000000000000000000000000000000000000000;;			case s.State.Terminated.ExitCode != 0:
0000000000000000000000000000000000000000;;				allInit = false
0000000000000000000000000000000000000000;;				initFailed = true
0000000000000000000000000000000000000000;;			case !s.Ready:
0000000000000000000000000000000000000000;;				return allInit, initFailed, fmt.Errorf("container %s initialized but isn't marked as ready", s.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return allInit, initFailed, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func initContainersInvariants(pod *v1.Pod) error {
0000000000000000000000000000000000000000;;		allInit, initFailed, err := podInitialized(pod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !allInit || initFailed {
0000000000000000000000000000000000000000;;			for _, s := range pod.Status.ContainerStatuses {
0000000000000000000000000000000000000000;;				if s.State.Waiting == nil || s.RestartCount != 0 {
0000000000000000000000000000000000000000;;					return fmt.Errorf("container %s is not waiting but initialization not complete", s.Name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if s.State.Waiting.Reason != "PodInitializing" {
0000000000000000000000000000000000000000;;					return fmt.Errorf("container %s should have reason PodInitializing: %s", s.Name, s.State.Waiting.Reason)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, c := podutil.GetPodCondition(&pod.Status, v1.PodInitialized)
0000000000000000000000000000000000000000;;		if c == nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("pod does not have initialized condition")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if c.LastTransitionTime.IsZero() {
0000000000000000000000000000000000000000;;			return fmt.Errorf("PodInitialized condition should always have a transition time")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		switch {
0000000000000000000000000000000000000000;;		case c.Status == v1.ConditionUnknown:
0000000000000000000000000000000000000000;;			return fmt.Errorf("PodInitialized condition should never be Unknown")
0000000000000000000000000000000000000000;;		case c.Status == v1.ConditionTrue && (initFailed || !allInit):
0000000000000000000000000000000000000000;;			return fmt.Errorf("PodInitialized condition was True but all not all containers initialized")
0000000000000000000000000000000000000000;;		case c.Status == v1.ConditionFalse && (!initFailed && allInit):
0000000000000000000000000000000000000000;;			return fmt.Errorf("PodInitialized condition was False but all containers initialized")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type InvariantFunc func(older, newer runtime.Object) error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func CheckInvariants(events []watch.Event, fns ...InvariantFunc) error {
0000000000000000000000000000000000000000;;		errs := sets.NewString()
0000000000000000000000000000000000000000;;		for i := range events {
0000000000000000000000000000000000000000;;			j := i + 1
0000000000000000000000000000000000000000;;			if j >= len(events) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, fn := range fns {
0000000000000000000000000000000000000000;;				if err := fn(events[i].Object, events[j].Object); err != nil {
0000000000000000000000000000000000000000;;					errs.Insert(err.Error())
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if errs.Len() > 0 {
0000000000000000000000000000000000000000;;			return fmt.Errorf("invariants violated:\n* %s", strings.Join(errs.List(), "\n* "))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Waits default amount of time (PodStartTimeout) for the specified pod to become running.
0000000000000000000000000000000000000000;;	// Returns an error if timeout occurs first, or pod goes in to failed state.
0000000000000000000000000000000000000000;;	func WaitForPodRunningInNamespace(c clientset.Interface, pod *v1.Pod) error {
0000000000000000000000000000000000000000;;		if pod.Status.Phase == v1.PodRunning {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return waitTimeoutForPodRunningInNamespace(c, pod.Name, pod.Namespace, PodStartTimeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Waits default amount of time (PodStartTimeout) for the specified pod to become running.
0000000000000000000000000000000000000000;;	// Returns an error if timeout occurs first, or pod goes in to failed state.
0000000000000000000000000000000000000000;;	func WaitForPodNameRunningInNamespace(c clientset.Interface, podName, namespace string) error {
0000000000000000000000000000000000000000;;		return waitTimeoutForPodRunningInNamespace(c, podName, namespace, PodStartTimeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Waits an extended amount of time (slowPodStartTimeout) for the specified pod to become running.
0000000000000000000000000000000000000000;;	// The resourceVersion is used when Watching object changes, it tells since when we care
0000000000000000000000000000000000000000;;	// about changes to the pod. Returns an error if timeout occurs first, or pod goes in to failed state.
0000000000000000000000000000000000000000;;	func waitForPodRunningInNamespaceSlow(c clientset.Interface, podName, namespace string) error {
0000000000000000000000000000000000000000;;		return waitTimeoutForPodRunningInNamespace(c, podName, namespace, slowPodStartTimeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func waitTimeoutForPodRunningInNamespace(c clientset.Interface, podName, namespace string, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		return wait.PollImmediate(Poll, timeout, podRunning(c, podName, namespace))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podRunning(c clientset.Interface, podName, namespace string) wait.ConditionFunc {
0000000000000000000000000000000000000000;;		return func() (bool, error) {
0000000000000000000000000000000000000000;;			pod, err := c.Core().Pods(namespace).Get(podName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			switch pod.Status.Phase {
0000000000000000000000000000000000000000;;			case v1.PodRunning:
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			case v1.PodFailed, v1.PodSucceeded:
0000000000000000000000000000000000000000;;				return false, conditions.ErrPodCompleted
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Waits default amount of time (DefaultPodDeletionTimeout) for the specified pod to stop running.
0000000000000000000000000000000000000000;;	// Returns an error if timeout occurs first.
0000000000000000000000000000000000000000;;	func WaitForPodNoLongerRunningInNamespace(c clientset.Interface, podName, namespace string) error {
0000000000000000000000000000000000000000;;		return WaitTimeoutForPodNoLongerRunningInNamespace(c, podName, namespace, DefaultPodDeletionTimeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func WaitTimeoutForPodNoLongerRunningInNamespace(c clientset.Interface, podName, namespace string, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		return wait.PollImmediate(Poll, timeout, podCompleted(c, podName, namespace))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podCompleted(c clientset.Interface, podName, namespace string) wait.ConditionFunc {
0000000000000000000000000000000000000000;;		return func() (bool, error) {
0000000000000000000000000000000000000000;;			pod, err := c.Core().Pods(namespace).Get(podName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			switch pod.Status.Phase {
0000000000000000000000000000000000000000;;			case v1.PodFailed, v1.PodSucceeded:
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func waitTimeoutForPodReadyInNamespace(c clientset.Interface, podName, namespace string, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		return wait.PollImmediate(Poll, timeout, podRunningAndReady(c, podName, namespace))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podRunningAndReady(c clientset.Interface, podName, namespace string) wait.ConditionFunc {
0000000000000000000000000000000000000000;;		return func() (bool, error) {
0000000000000000000000000000000000000000;;			pod, err := c.Core().Pods(namespace).Get(podName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			switch pod.Status.Phase {
0000000000000000000000000000000000000000;;			case v1.PodFailed, v1.PodSucceeded:
0000000000000000000000000000000000000000;;				return false, conditions.ErrPodCompleted
0000000000000000000000000000000000000000;;			case v1.PodRunning:
0000000000000000000000000000000000000000;;				return podutil.IsPodReady(pod), nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPodNotPending returns an error if it took too long for the pod to go out of pending state.
0000000000000000000000000000000000000000;;	// The resourceVersion is used when Watching object changes, it tells since when we care
0000000000000000000000000000000000000000;;	// about changes to the pod.
0000000000000000000000000000000000000000;;	func WaitForPodNotPending(c clientset.Interface, ns, podName string) error {
0000000000000000000000000000000000000000;;		return wait.PollImmediate(Poll, PodStartTimeout, podNotPending(c, podName, ns))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podNotPending(c clientset.Interface, podName, namespace string) wait.ConditionFunc {
0000000000000000000000000000000000000000;;		return func() (bool, error) {
0000000000000000000000000000000000000000;;			pod, err := c.Core().Pods(namespace).Get(podName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			switch pod.Status.Phase {
0000000000000000000000000000000000000000;;			case v1.PodPending:
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// waitForPodTerminatedInNamespace returns an error if it took too long for the pod
0000000000000000000000000000000000000000;;	// to terminate or if the pod terminated with an unexpected reason.
0000000000000000000000000000000000000000;;	func waitForPodTerminatedInNamespace(c clientset.Interface, podName, reason, namespace string) error {
0000000000000000000000000000000000000000;;		return WaitForPodCondition(c, namespace, podName, "terminated due to deadline exceeded", PodStartTimeout, func(pod *v1.Pod) (bool, error) {
0000000000000000000000000000000000000000;;			if pod.Status.Phase == v1.PodFailed {
0000000000000000000000000000000000000000;;				if pod.Status.Reason == reason {
0000000000000000000000000000000000000000;;					return true, nil
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					return true, fmt.Errorf("Expected pod %v in namespace %v to be terminated with reason %v, got reason: %v", podName, namespace, reason, pod.Status.Reason)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// waitForPodSuccessInNamespaceTimeout returns nil if the pod reached state success, or an error if it reached failure or ran too long.
0000000000000000000000000000000000000000;;	func waitForPodSuccessInNamespaceTimeout(c clientset.Interface, podName string, namespace string, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		return WaitForPodCondition(c, namespace, podName, "success or failure", timeout, func(pod *v1.Pod) (bool, error) {
0000000000000000000000000000000000000000;;			if pod.Spec.RestartPolicy == v1.RestartPolicyAlways {
0000000000000000000000000000000000000000;;				return true, fmt.Errorf("pod %q will never terminate with a succeeded state since its restart policy is Always", podName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			switch pod.Status.Phase {
0000000000000000000000000000000000000000;;			case v1.PodSucceeded:
0000000000000000000000000000000000000000;;				By("Saw pod success")
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			case v1.PodFailed:
0000000000000000000000000000000000000000;;				return true, fmt.Errorf("pod %q failed with status: %+v", podName, pod.Status)
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPodSuccessInNamespace returns nil if the pod reached state success, or an error if it reached failure or until podStartupTimeout.
0000000000000000000000000000000000000000;;	func WaitForPodSuccessInNamespace(c clientset.Interface, podName string, namespace string) error {
0000000000000000000000000000000000000000;;		return waitForPodSuccessInNamespaceTimeout(c, podName, namespace, PodStartTimeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForPodSuccessInNamespaceSlow returns nil if the pod reached state success, or an error if it reached failure or until slowPodStartupTimeout.
0000000000000000000000000000000000000000;;	func WaitForPodSuccessInNamespaceSlow(c clientset.Interface, podName string, namespace string) error {
0000000000000000000000000000000000000000;;		return waitForPodSuccessInNamespaceTimeout(c, podName, namespace, slowPodStartTimeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForRCToStabilize waits till the RC has a matching generation/replica count between spec and status.
0000000000000000000000000000000000000000;;	func WaitForRCToStabilize(c clientset.Interface, ns, name string, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{FieldSelector: fields.Set{
0000000000000000000000000000000000000000;;			"metadata.name":      name,
0000000000000000000000000000000000000000;;			"metadata.namespace": ns,
0000000000000000000000000000000000000000;;		}.AsSelector().String()}
0000000000000000000000000000000000000000;;		w, err := c.Core().ReplicationControllers(ns).Watch(options)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, err = watch.Until(timeout, w, func(event watch.Event) (bool, error) {
0000000000000000000000000000000000000000;;			switch event.Type {
0000000000000000000000000000000000000000;;			case watch.Deleted:
0000000000000000000000000000000000000000;;				return false, apierrs.NewNotFound(schema.GroupResource{Resource: "replicationcontrollers"}, "")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			switch rc := event.Object.(type) {
0000000000000000000000000000000000000000;;			case *v1.ReplicationController:
0000000000000000000000000000000000000000;;				if rc.Name == name && rc.Namespace == ns &&
0000000000000000000000000000000000000000;;					rc.Generation <= rc.Status.ObservedGeneration &&
0000000000000000000000000000000000000000;;					*(rc.Spec.Replicas) == rc.Status.Replicas {
0000000000000000000000000000000000000000;;					return true, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				Logf("Waiting for rc %s to stabilize, generation %v observed generation %v spec.replicas %d status.replicas %d",
0000000000000000000000000000000000000000;;					name, rc.Generation, rc.Status.ObservedGeneration, *(rc.Spec.Replicas), rc.Status.Replicas)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func WaitForPodToDisappear(c clientset.Interface, ns, podName string, label labels.Selector, interval, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		return wait.PollImmediate(interval, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			Logf("Waiting for pod %s to disappear", podName)
0000000000000000000000000000000000000000;;			options := metav1.ListOptions{LabelSelector: label.String()}
0000000000000000000000000000000000000000;;			pods, err := c.Core().Pods(ns).List(options)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			found := false
0000000000000000000000000000000000000000;;			for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;				if pod.Name == podName {
0000000000000000000000000000000000000000;;					Logf("Pod %s still exists", podName)
0000000000000000000000000000000000000000;;					found = true
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if !found {
0000000000000000000000000000000000000000;;				Logf("Pod %s no longer exists", podName)
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForService waits until the service appears (exist == true), or disappears (exist == false)
0000000000000000000000000000000000000000;;	func WaitForService(c clientset.Interface, namespace, name string, exist bool, interval, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		err := wait.PollImmediate(interval, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			_, err := c.Core().Services(namespace).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case err == nil:
0000000000000000000000000000000000000000;;				Logf("Service %s in namespace %s found.", name, namespace)
0000000000000000000000000000000000000000;;				return exist, nil
0000000000000000000000000000000000000000;;			case apierrs.IsNotFound(err):
0000000000000000000000000000000000000000;;				Logf("Service %s in namespace %s disappeared.", name, namespace)
0000000000000000000000000000000000000000;;				return !exist, nil
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				Logf("Get service %s in namespace %s failed: %v", name, namespace, err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			stateMsg := map[bool]string{true: "to appear", false: "to disappear"}
0000000000000000000000000000000000000000;;			return fmt.Errorf("error waiting for service %s/%s %s: %v", namespace, name, stateMsg[exist], err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForServiceWithSelector waits until any service with given selector appears (exist == true), or disappears (exist == false)
0000000000000000000000000000000000000000;;	func WaitForServiceWithSelector(c clientset.Interface, namespace string, selector labels.Selector, exist bool, interval,
0000000000000000000000000000000000000000;;		timeout time.Duration) error {
0000000000000000000000000000000000000000;;		err := wait.PollImmediate(interval, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			services, err := c.Core().Services(namespace).List(metav1.ListOptions{LabelSelector: selector.String()})
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case len(services.Items) != 0:
0000000000000000000000000000000000000000;;				Logf("Service with %s in namespace %s found.", selector.String(), namespace)
0000000000000000000000000000000000000000;;				return exist, nil
0000000000000000000000000000000000000000;;			case len(services.Items) == 0:
0000000000000000000000000000000000000000;;				Logf("Service with %s in namespace %s disappeared.", selector.String(), namespace)
0000000000000000000000000000000000000000;;				return !exist, nil
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				Logf("List service with %s in namespace %s failed: %v", selector.String(), namespace, err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			stateMsg := map[bool]string{true: "to appear", false: "to disappear"}
0000000000000000000000000000000000000000;;			return fmt.Errorf("error waiting for service with %s in namespace %s %s: %v", selector.String(), namespace, stateMsg[exist], err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	//WaitForServiceEndpointsNum waits until the amount of endpoints that implement service to expectNum.
0000000000000000000000000000000000000000;;	func WaitForServiceEndpointsNum(c clientset.Interface, namespace, serviceName string, expectNum int, interval, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		return wait.Poll(interval, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			Logf("Waiting for amount of service:%s endpoints to be %d", serviceName, expectNum)
0000000000000000000000000000000000000000;;			list, err := c.Core().Endpoints(namespace).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, e := range list.Items {
0000000000000000000000000000000000000000;;				if e.Name == serviceName && countEndpointsNum(&e) == expectNum {
0000000000000000000000000000000000000000;;					return true, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func countEndpointsNum(e *v1.Endpoints) int {
0000000000000000000000000000000000000000;;		num := 0
0000000000000000000000000000000000000000;;		for _, sub := range e.Subsets {
0000000000000000000000000000000000000000;;			num += len(sub.Addresses)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return num
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func WaitForEndpoint(c clientset.Interface, ns, name string) error {
0000000000000000000000000000000000000000;;		for t := time.Now(); time.Since(t) < EndpointRegisterTimeout; time.Sleep(Poll) {
0000000000000000000000000000000000000000;;			endpoint, err := c.Core().Endpoints(ns).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if apierrs.IsNotFound(err) {
0000000000000000000000000000000000000000;;				Logf("Endpoint %s/%s is not ready yet", ns, name)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			if len(endpoint.Subsets) == 0 || len(endpoint.Subsets[0].Addresses) == 0 {
0000000000000000000000000000000000000000;;				Logf("Endpoint %s/%s is not ready yet", ns, name)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("Failed to get endpoints for %s/%s", ns, name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Context for checking pods responses by issuing GETs to them (via the API
0000000000000000000000000000000000000000;;	// proxy) and verifying that they answer with ther own pod name.
0000000000000000000000000000000000000000;;	type podProxyResponseChecker struct {
0000000000000000000000000000000000000000;;		c              clientset.Interface
0000000000000000000000000000000000000000;;		ns             string
0000000000000000000000000000000000000000;;		label          labels.Selector
0000000000000000000000000000000000000000;;		controllerName string
0000000000000000000000000000000000000000;;		respondName    bool // Whether the pod should respond with its own name.
0000000000000000000000000000000000000000;;		pods           *v1.PodList
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func PodProxyResponseChecker(c clientset.Interface, ns string, label labels.Selector, controllerName string, respondName bool, pods *v1.PodList) podProxyResponseChecker {
0000000000000000000000000000000000000000;;		return podProxyResponseChecker{c, ns, label, controllerName, respondName, pods}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CheckAllResponses issues GETs to all pods in the context and verify they
0000000000000000000000000000000000000000;;	// reply with their own pod name.
0000000000000000000000000000000000000000;;	func (r podProxyResponseChecker) CheckAllResponses() (done bool, err error) {
0000000000000000000000000000000000000000;;		successes := 0
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{LabelSelector: r.label.String()}
0000000000000000000000000000000000000000;;		currentPods, err := r.c.Core().Pods(r.ns).List(options)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		for i, pod := range r.pods.Items {
0000000000000000000000000000000000000000;;			// Check that the replica list remains unchanged, otherwise we have problems.
0000000000000000000000000000000000000000;;			if !isElementOf(pod.UID, currentPods) {
0000000000000000000000000000000000000000;;				return false, fmt.Errorf("pod with UID %s is no longer a member of the replica set.  Must have been restarted for some reason.  Current replica set: %v", pod.UID, currentPods)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			subResourceProxyAvailable, err := ServerVersionGTE(SubResourcePodProxyVersion, r.c.Discovery())
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			ctx, cancel := context.WithTimeout(context.Background(), SingleCallTimeout)
0000000000000000000000000000000000000000;;			defer cancel()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			var body []byte
0000000000000000000000000000000000000000;;			if subResourceProxyAvailable {
0000000000000000000000000000000000000000;;				body, err = r.c.Core().RESTClient().Get().
0000000000000000000000000000000000000000;;					Context(ctx).
0000000000000000000000000000000000000000;;					Namespace(r.ns).
0000000000000000000000000000000000000000;;					Resource("pods").
0000000000000000000000000000000000000000;;					SubResource("proxy").
0000000000000000000000000000000000000000;;					Name(string(pod.Name)).
0000000000000000000000000000000000000000;;					Do().
0000000000000000000000000000000000000000;;					Raw()
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				body, err = r.c.Core().RESTClient().Get().
0000000000000000000000000000000000000000;;					Context(ctx).
0000000000000000000000000000000000000000;;					Prefix("proxy").
0000000000000000000000000000000000000000;;					Namespace(r.ns).
0000000000000000000000000000000000000000;;					Resource("pods").
0000000000000000000000000000000000000000;;					Name(string(pod.Name)).
0000000000000000000000000000000000000000;;					Do().
0000000000000000000000000000000000000000;;					Raw()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				if ctx.Err() != nil {
0000000000000000000000000000000000000000;;					// We may encounter errors here because of a race between the pod readiness and apiserver
0000000000000000000000000000000000000000;;					// proxy. So, we log the error and retry if this occurs.
0000000000000000000000000000000000000000;;					Logf("Controller %s: Failed to Get from replica %d [%s]: %v\n pod status: %#v", r.controllerName, i+1, pod.Name, err, pod.Status)
0000000000000000000000000000000000000000;;					return false, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				Logf("Controller %s: Failed to GET from replica %d [%s]: %v\npod status: %#v", r.controllerName, i+1, pod.Name, err, pod.Status)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// The response checker expects the pod's name unless !respondName, in
0000000000000000000000000000000000000000;;			// which case it just checks for a non-empty response.
0000000000000000000000000000000000000000;;			got := string(body)
0000000000000000000000000000000000000000;;			what := ""
0000000000000000000000000000000000000000;;			if r.respondName {
0000000000000000000000000000000000000000;;				what = "expected"
0000000000000000000000000000000000000000;;				want := pod.Name
0000000000000000000000000000000000000000;;				if got != want {
0000000000000000000000000000000000000000;;					Logf("Controller %s: Replica %d [%s] expected response %q but got %q",
0000000000000000000000000000000000000000;;						r.controllerName, i+1, pod.Name, want, got)
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				what = "non-empty"
0000000000000000000000000000000000000000;;				if len(got) == 0 {
0000000000000000000000000000000000000000;;					Logf("Controller %s: Replica %d [%s] expected non-empty response",
0000000000000000000000000000000000000000;;						r.controllerName, i+1, pod.Name)
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			successes++
0000000000000000000000000000000000000000;;			Logf("Controller %s: Got %s result from replica %d [%s]: %q, %d of %d required successes so far",
0000000000000000000000000000000000000000;;				r.controllerName, what, i+1, pod.Name, got, successes, len(r.pods.Items))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if successes < len(r.pods.Items) {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ServerVersionGTE returns true if v is greater than or equal to the server
0000000000000000000000000000000000000000;;	// version.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// TODO(18726): This should be incorporated into client.VersionInterface.
0000000000000000000000000000000000000000;;	func ServerVersionGTE(v *utilversion.Version, c discovery.ServerVersionInterface) (bool, error) {
0000000000000000000000000000000000000000;;		serverVersion, err := c.ServerVersion()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, fmt.Errorf("Unable to get server version: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		sv, err := utilversion.ParseSemantic(serverVersion.GitVersion)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, fmt.Errorf("Unable to parse server version %q: %v", serverVersion.GitVersion, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return sv.AtLeast(v), nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SkipUnlessKubectlVersionGTE(v *utilversion.Version) {
0000000000000000000000000000000000000000;;		gte, err := KubectlVersionGTE(v)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Failf("Failed to get kubectl version: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !gte {
0000000000000000000000000000000000000000;;			Skipf("Not supported for kubectl versions before %q", v)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// KubectlVersionGTE returns true if the kubectl version is greater than or
0000000000000000000000000000000000000000;;	// equal to v.
0000000000000000000000000000000000000000;;	func KubectlVersionGTE(v *utilversion.Version) (bool, error) {
0000000000000000000000000000000000000000;;		kv, err := KubectlVersion()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return kv.AtLeast(v), nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// KubectlVersion gets the version of kubectl that's currently being used (see
0000000000000000000000000000000000000000;;	// --kubectl-path in e2e.go to use an alternate kubectl).
0000000000000000000000000000000000000000;;	func KubectlVersion() (*utilversion.Version, error) {
0000000000000000000000000000000000000000;;		output := RunKubectlOrDie("version", "--client")
0000000000000000000000000000000000000000;;		matches := gitVersionRegexp.FindStringSubmatch(output)
0000000000000000000000000000000000000000;;		if len(matches) != 2 {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("Could not find kubectl version in output %v", output)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Don't use the full match, as it contains "GitVersion:\"" and a
0000000000000000000000000000000000000000;;		// trailing "\"".  Just use the submatch.
0000000000000000000000000000000000000000;;		return utilversion.ParseSemantic(matches[1])
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func PodsResponding(c clientset.Interface, ns, name string, wantName bool, pods *v1.PodList) error {
0000000000000000000000000000000000000000;;		By("trying to dial each unique pod")
0000000000000000000000000000000000000000;;		label := labels.SelectorFromSet(labels.Set(map[string]string{"name": name}))
0000000000000000000000000000000000000000;;		return wait.PollImmediate(Poll, podRespondingTimeout, PodProxyResponseChecker(c, ns, label, name, wantName, pods).CheckAllResponses)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func PodsCreated(c clientset.Interface, ns, name string, replicas int32) (*v1.PodList, error) {
0000000000000000000000000000000000000000;;		label := labels.SelectorFromSet(labels.Set(map[string]string{"name": name}))
0000000000000000000000000000000000000000;;		return PodsCreatedByLabel(c, ns, name, replicas, label)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func PodsCreatedByLabel(c clientset.Interface, ns, name string, replicas int32, label labels.Selector) (*v1.PodList, error) {
0000000000000000000000000000000000000000;;		timeout := 2 * time.Minute
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(5 * time.Second) {
0000000000000000000000000000000000000000;;			options := metav1.ListOptions{LabelSelector: label.String()}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// List the pods, making sure we observe all the replicas.
0000000000000000000000000000000000000000;;			pods, err := c.Core().Pods(ns).List(options)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			created := []v1.Pod{}
0000000000000000000000000000000000000000;;			for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;				if pod.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				created = append(created, pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("Pod name %s: Found %d pods out of %d", name, len(created), replicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if int32(len(created)) == replicas {
0000000000000000000000000000000000000000;;				pods.Items = created
0000000000000000000000000000000000000000;;				return pods, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil, fmt.Errorf("Pod name %s: Gave up waiting %v for %d pods to come up", name, timeout, replicas)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podsRunning(c clientset.Interface, pods *v1.PodList) []error {
0000000000000000000000000000000000000000;;		// Wait for the pods to enter the running state. Waiting loops until the pods
0000000000000000000000000000000000000000;;		// are running so non-running pods cause a timeout for this test.
0000000000000000000000000000000000000000;;		By("ensuring each pod is running")
0000000000000000000000000000000000000000;;		e := []error{}
0000000000000000000000000000000000000000;;		error_chan := make(chan error)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;			go func(p v1.Pod) {
0000000000000000000000000000000000000000;;				error_chan <- WaitForPodRunningInNamespace(c, &p)
0000000000000000000000000000000000000000;;			}(pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for range pods.Items {
0000000000000000000000000000000000000000;;			err := <-error_chan
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				e = append(e, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return e
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func VerifyPods(c clientset.Interface, ns, name string, wantName bool, replicas int32) error {
0000000000000000000000000000000000000000;;		return podRunningMaybeResponding(c, ns, name, wantName, replicas, true)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func VerifyPodsRunning(c clientset.Interface, ns, name string, wantName bool, replicas int32) error {
0000000000000000000000000000000000000000;;		return podRunningMaybeResponding(c, ns, name, wantName, replicas, false)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podRunningMaybeResponding(c clientset.Interface, ns, name string, wantName bool, replicas int32, checkResponding bool) error {
0000000000000000000000000000000000000000;;		pods, err := PodsCreated(c, ns, name, replicas)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		e := podsRunning(c, pods)
0000000000000000000000000000000000000000;;		if len(e) > 0 {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to wait for pods running: %v", e)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if checkResponding {
0000000000000000000000000000000000000000;;			err = PodsResponding(c, ns, name, wantName, pods)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("failed to wait for pods responding: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ServiceResponding(c clientset.Interface, ns, name string) error {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("trying to dial the service %s.%s via the proxy", ns, name))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return wait.PollImmediate(Poll, ServiceRespondingTimeout, func() (done bool, err error) {
0000000000000000000000000000000000000000;;			proxyRequest, errProxy := GetServicesProxyRequest(c, c.Core().RESTClient().Get())
0000000000000000000000000000000000000000;;			if errProxy != nil {
0000000000000000000000000000000000000000;;				Logf("Failed to get services proxy request: %v:", errProxy)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			ctx, cancel := context.WithTimeout(context.Background(), SingleCallTimeout)
0000000000000000000000000000000000000000;;			defer cancel()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			body, err := proxyRequest.Namespace(ns).
0000000000000000000000000000000000000000;;				Context(ctx).
0000000000000000000000000000000000000000;;				Name(name).
0000000000000000000000000000000000000000;;				Do().
0000000000000000000000000000000000000000;;				Raw()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				if ctx.Err() != nil {
0000000000000000000000000000000000000000;;					Failf("Failed to GET from service %s: %v", name, err)
0000000000000000000000000000000000000000;;					return true, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				Logf("Failed to GET from service %s: %v:", name, err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			got := string(body)
0000000000000000000000000000000000000000;;			if len(got) == 0 {
0000000000000000000000000000000000000000;;				Logf("Service %s: expected non-empty response", name)
0000000000000000000000000000000000000000;;				return false, err // stop polling
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("Service %s: found nonempty answer: %s", name, got)
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func RestclientConfig(kubeContext string) (*clientcmdapi.Config, error) {
0000000000000000000000000000000000000000;;		Logf(">>> kubeConfig: %s", TestContext.KubeConfig)
0000000000000000000000000000000000000000;;		if TestContext.KubeConfig == "" {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("KubeConfig must be specified to load client config")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		c, err := clientcmd.LoadFromFile(TestContext.KubeConfig)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("error loading KubeConfig: %v", err.Error())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kubeContext != "" {
0000000000000000000000000000000000000000;;			Logf(">>> kubeContext: %s", kubeContext)
0000000000000000000000000000000000000000;;			c.CurrentContext = kubeContext
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return c, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type ClientConfigGetter func() (*restclient.Config, error)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func LoadConfig() (*restclient.Config, error) {
0000000000000000000000000000000000000000;;		if TestContext.NodeE2E {
0000000000000000000000000000000000000000;;			// This is a node e2e test, apply the node e2e configuration
0000000000000000000000000000000000000000;;			return &restclient.Config{Host: TestContext.Host}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		c, err := RestclientConfig(TestContext.KubeContext)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			if TestContext.KubeConfig == "" {
0000000000000000000000000000000000000000;;				return restclient.InClusterConfig()
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return clientcmd.NewDefaultClientConfig(*c, &clientcmd.ConfigOverrides{ClusterInfo: clientcmdapi.Cluster{Server: TestContext.Host}}).ClientConfig()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	func LoadInternalClientset() (*internalclientset.Clientset, error) {
0000000000000000000000000000000000000000;;		config, err := LoadConfig()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("error creating client: %v", err.Error())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return internalclientset.NewForConfig(config)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func LoadClientset() (*clientset.Clientset, error) {
0000000000000000000000000000000000000000;;		config, err := LoadConfig()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("error creating client: %v", err.Error())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return clientset.NewForConfig(config)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// randomSuffix provides a random string to append to pods,services,rcs.
0000000000000000000000000000000000000000;;	// TODO: Allow service names to have the same form as names
0000000000000000000000000000000000000000;;	//       for pods and replication controllers so we don't
0000000000000000000000000000000000000000;;	//       need to use such a function and can instead
0000000000000000000000000000000000000000;;	//       use the UUID utility function.
0000000000000000000000000000000000000000;;	func randomSuffix() string {
0000000000000000000000000000000000000000;;		r := rand.New(rand.NewSource(time.Now().UnixNano()))
0000000000000000000000000000000000000000;;		return strconv.Itoa(r.Int() % 10000)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ExpectNoError(err error, explain ...interface{}) {
0000000000000000000000000000000000000000;;		ExpectNoErrorWithOffset(1, err, explain...)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ExpectNoErrorWithOffset checks if "err" is set, and if so, fails assertion while logging the error at "offset" levels above its caller
0000000000000000000000000000000000000000;;	// (for example, for call chain f -> g -> ExpectNoErrorWithOffset(1, ...) error would be logged for "f").
0000000000000000000000000000000000000000;;	func ExpectNoErrorWithOffset(offset int, err error, explain ...interface{}) {
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("Unexpected error occurred: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ExpectWithOffset(1+offset, err).NotTo(HaveOccurred(), explain...)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ExpectNoErrorWithRetries(fn func() error, maxRetries int, explain ...interface{}) {
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		for i := 0; i < maxRetries; i++ {
0000000000000000000000000000000000000000;;			err = fn()
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("(Attempt %d of %d) Unexpected error occurred: %v", i+1, maxRetries, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ExpectWithOffset(1, err).NotTo(HaveOccurred(), explain...)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Stops everything from filePath from namespace ns and checks if everything matching selectors from the given namespace is correctly stopped.
0000000000000000000000000000000000000000;;	func Cleanup(filePath, ns string, selectors ...string) {
0000000000000000000000000000000000000000;;		By("using delete to clean up resources")
0000000000000000000000000000000000000000;;		var nsArg string
0000000000000000000000000000000000000000;;		if ns != "" {
0000000000000000000000000000000000000000;;			nsArg = fmt.Sprintf("--namespace=%s", ns)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		RunKubectlOrDie("delete", "--grace-period=0", "-f", filePath, nsArg)
0000000000000000000000000000000000000000;;		AssertCleanup(ns, selectors...)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Asserts that cleanup of a namespace wrt selectors occurred.
0000000000000000000000000000000000000000;;	func AssertCleanup(ns string, selectors ...string) {
0000000000000000000000000000000000000000;;		var nsArg string
0000000000000000000000000000000000000000;;		if ns != "" {
0000000000000000000000000000000000000000;;			nsArg = fmt.Sprintf("--namespace=%s", ns)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, selector := range selectors {
0000000000000000000000000000000000000000;;			resources := RunKubectlOrDie("get", "rc,svc", "-l", selector, "--no-headers", nsArg)
0000000000000000000000000000000000000000;;			if resources != "" {
0000000000000000000000000000000000000000;;				Failf("Resources left running after stop:\n%s", resources)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pods := RunKubectlOrDie("get", "pods", "-l", selector, nsArg, "-o", "go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ \"\\n\" }}{{ end }}{{ end }}")
0000000000000000000000000000000000000000;;			if pods != "" {
0000000000000000000000000000000000000000;;				Failf("Pods left unterminated after stop:\n%s", pods)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// KubectlCmd runs the kubectl executable through the wrapper script.
0000000000000000000000000000000000000000;;	func KubectlCmd(args ...string) *exec.Cmd {
0000000000000000000000000000000000000000;;		defaultArgs := []string{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Reference a --server option so tests can run anywhere.
0000000000000000000000000000000000000000;;		if TestContext.Host != "" {
0000000000000000000000000000000000000000;;			defaultArgs = append(defaultArgs, "--"+clientcmd.FlagAPIServer+"="+TestContext.Host)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if TestContext.KubeConfig != "" {
0000000000000000000000000000000000000000;;			defaultArgs = append(defaultArgs, "--"+clientcmd.RecommendedConfigPathFlag+"="+TestContext.KubeConfig)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Reference the KubeContext
0000000000000000000000000000000000000000;;			if TestContext.KubeContext != "" {
0000000000000000000000000000000000000000;;				defaultArgs = append(defaultArgs, "--"+clientcmd.FlagContext+"="+TestContext.KubeContext)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			if TestContext.CertDir != "" {
0000000000000000000000000000000000000000;;				defaultArgs = append(defaultArgs,
0000000000000000000000000000000000000000;;					fmt.Sprintf("--certificate-authority=%s", filepath.Join(TestContext.CertDir, "ca.crt")),
0000000000000000000000000000000000000000;;					fmt.Sprintf("--client-certificate=%s", filepath.Join(TestContext.CertDir, "kubecfg.crt")),
0000000000000000000000000000000000000000;;					fmt.Sprintf("--client-key=%s", filepath.Join(TestContext.CertDir, "kubecfg.key")))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kubectlArgs := append(defaultArgs, args...)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		//We allow users to specify path to kubectl, so you can test either "kubectl" or "cluster/kubectl.sh"
0000000000000000000000000000000000000000;;		//and so on.
0000000000000000000000000000000000000000;;		cmd := exec.Command(TestContext.KubectlPath, kubectlArgs...)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		//caller will invoke this and wait on it.
0000000000000000000000000000000000000000;;		return cmd
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// kubectlBuilder is used to build, customize and execute a kubectl Command.
0000000000000000000000000000000000000000;;	// Add more functions to customize the builder as needed.
0000000000000000000000000000000000000000;;	type kubectlBuilder struct {
0000000000000000000000000000000000000000;;		cmd     *exec.Cmd
0000000000000000000000000000000000000000;;		timeout <-chan time.Time
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewKubectlCommand(args ...string) *kubectlBuilder {
0000000000000000000000000000000000000000;;		b := new(kubectlBuilder)
0000000000000000000000000000000000000000;;		b.cmd = KubectlCmd(args...)
0000000000000000000000000000000000000000;;		return b
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (b *kubectlBuilder) WithEnv(env []string) *kubectlBuilder {
0000000000000000000000000000000000000000;;		b.cmd.Env = env
0000000000000000000000000000000000000000;;		return b
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (b *kubectlBuilder) WithTimeout(t <-chan time.Time) *kubectlBuilder {
0000000000000000000000000000000000000000;;		b.timeout = t
0000000000000000000000000000000000000000;;		return b
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (b kubectlBuilder) WithStdinData(data string) *kubectlBuilder {
0000000000000000000000000000000000000000;;		b.cmd.Stdin = strings.NewReader(data)
0000000000000000000000000000000000000000;;		return &b
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (b kubectlBuilder) WithStdinReader(reader io.Reader) *kubectlBuilder {
0000000000000000000000000000000000000000;;		b.cmd.Stdin = reader
0000000000000000000000000000000000000000;;		return &b
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (b kubectlBuilder) ExecOrDie() string {
0000000000000000000000000000000000000000;;		str, err := b.Exec()
0000000000000000000000000000000000000000;;		Logf("stdout: %q", str)
0000000000000000000000000000000000000000;;		// In case of i/o timeout error, try talking to the apiserver again after 2s before dying.
0000000000000000000000000000000000000000;;		// Note that we're still dying after retrying so that we can get visibility to triage it further.
0000000000000000000000000000000000000000;;		if isTimeout(err) {
0000000000000000000000000000000000000000;;			Logf("Hit i/o timeout error, talking to the server 2s later to see if it's temporary.")
0000000000000000000000000000000000000000;;			time.Sleep(2 * time.Second)
0000000000000000000000000000000000000000;;			retryStr, retryErr := RunKubectl("version")
0000000000000000000000000000000000000000;;			Logf("stdout: %q", retryStr)
0000000000000000000000000000000000000000;;			Logf("err: %v", retryErr)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		return str
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func isTimeout(err error) bool {
0000000000000000000000000000000000000000;;		switch err := err.(type) {
0000000000000000000000000000000000000000;;		case net.Error:
0000000000000000000000000000000000000000;;			if err.Timeout() {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		case *url.Error:
0000000000000000000000000000000000000000;;			if err, ok := err.Err.(net.Error); ok && err.Timeout() {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (b kubectlBuilder) Exec() (string, error) {
0000000000000000000000000000000000000000;;		var stdout, stderr bytes.Buffer
0000000000000000000000000000000000000000;;		cmd := b.cmd
0000000000000000000000000000000000000000;;		cmd.Stdout, cmd.Stderr = &stdout, &stderr
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Logf("Running '%s %s'", cmd.Path, strings.Join(cmd.Args[1:], " ")) // skip arg[0] as it is printed separately
0000000000000000000000000000000000000000;;		if err := cmd.Start(); err != nil {
0000000000000000000000000000000000000000;;			return "", fmt.Errorf("error starting %v:\nCommand stdout:\n%v\nstderr:\n%v\nerror:\n%v\n", cmd, cmd.Stdout, cmd.Stderr, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		errCh := make(chan error, 1)
0000000000000000000000000000000000000000;;		go func() {
0000000000000000000000000000000000000000;;			errCh <- cmd.Wait()
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;		select {
0000000000000000000000000000000000000000;;		case err := <-errCh:
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				var rc int = 127
0000000000000000000000000000000000000000;;				if ee, ok := err.(*exec.ExitError); ok {
0000000000000000000000000000000000000000;;					Logf("rc: %d", rc)
0000000000000000000000000000000000000000;;					rc = int(ee.Sys().(syscall.WaitStatus).ExitStatus())
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return "", uexec.CodeExitError{
0000000000000000000000000000000000000000;;					Err:  fmt.Errorf("error running %v:\nCommand stdout:\n%v\nstderr:\n%v\nerror:\n%v\n", cmd, cmd.Stdout, cmd.Stderr, err),
0000000000000000000000000000000000000000;;					Code: rc,
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		case <-b.timeout:
0000000000000000000000000000000000000000;;			b.cmd.Process.Kill()
0000000000000000000000000000000000000000;;			return "", fmt.Errorf("timed out waiting for command %v:\nCommand stdout:\n%v\nstderr:\n%v\n", cmd, cmd.Stdout, cmd.Stderr)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("stderr: %q", stderr.String())
0000000000000000000000000000000000000000;;		return stdout.String(), nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunKubectlOrDie is a convenience wrapper over kubectlBuilder
0000000000000000000000000000000000000000;;	func RunKubectlOrDie(args ...string) string {
0000000000000000000000000000000000000000;;		return NewKubectlCommand(args...).ExecOrDie()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunKubectl is a convenience wrapper over kubectlBuilder
0000000000000000000000000000000000000000;;	func RunKubectl(args ...string) (string, error) {
0000000000000000000000000000000000000000;;		return NewKubectlCommand(args...).Exec()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunKubectlOrDieInput is a convenience wrapper over kubectlBuilder that takes input to stdin
0000000000000000000000000000000000000000;;	func RunKubectlOrDieInput(data string, args ...string) string {
0000000000000000000000000000000000000000;;		return NewKubectlCommand(args...).WithStdinData(data).ExecOrDie()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func StartCmdAndStreamOutput(cmd *exec.Cmd) (stdout, stderr io.ReadCloser, err error) {
0000000000000000000000000000000000000000;;		stdout, err = cmd.StdoutPipe()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		stderr, err = cmd.StderrPipe()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("Asynchronously running '%s %s'", cmd.Path, strings.Join(cmd.Args, " "))
0000000000000000000000000000000000000000;;		err = cmd.Start()
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Rough equivalent of ctrl+c for cleaning up processes. Intended to be run in defer.
0000000000000000000000000000000000000000;;	func TryKill(cmd *exec.Cmd) {
0000000000000000000000000000000000000000;;		if err := cmd.Process.Kill(); err != nil {
0000000000000000000000000000000000000000;;			Logf("ERROR failed to kill command %v! The process may leak", cmd)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// testContainerOutputMatcher runs the given pod in the given namespace and waits
0000000000000000000000000000000000000000;;	// for all of the containers in the podSpec to move into the 'Success' status, and tests
0000000000000000000000000000000000000000;;	// the specified container log against the given expected output using the given matcher.
0000000000000000000000000000000000000000;;	func (f *Framework) testContainerOutputMatcher(scenarioName string,
0000000000000000000000000000000000000000;;		pod *v1.Pod,
0000000000000000000000000000000000000000;;		containerIndex int,
0000000000000000000000000000000000000000;;		expectedOutput []string,
0000000000000000000000000000000000000000;;		matcher func(string, ...interface{}) gomegatypes.GomegaMatcher) {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Creating a pod to test %v", scenarioName))
0000000000000000000000000000000000000000;;		if containerIndex < 0 || containerIndex >= len(pod.Spec.Containers) {
0000000000000000000000000000000000000000;;			Failf("Invalid container index: %d", containerIndex)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ExpectNoError(f.MatchContainerOutput(pod, pod.Spec.Containers[containerIndex].Name, expectedOutput, matcher))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// MatchContainerOutput creates a pod and waits for all it's containers to exit with success.
0000000000000000000000000000000000000000;;	// It then tests that the matcher with each expectedOutput matches the output of the specified container.
0000000000000000000000000000000000000000;;	func (f *Framework) MatchContainerOutput(
0000000000000000000000000000000000000000;;		pod *v1.Pod,
0000000000000000000000000000000000000000;;		containerName string,
0000000000000000000000000000000000000000;;		expectedOutput []string,
0000000000000000000000000000000000000000;;		matcher func(string, ...interface{}) gomegatypes.GomegaMatcher) error {
0000000000000000000000000000000000000000;;		ns := pod.ObjectMeta.Namespace
0000000000000000000000000000000000000000;;		if ns == "" {
0000000000000000000000000000000000000000;;			ns = f.Namespace.Name
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podClient := f.PodClientNS(ns)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		createdPod := podClient.Create(pod)
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			By("delete the pod")
0000000000000000000000000000000000000000;;			podClient.DeleteSync(createdPod.Name, &metav1.DeleteOptions{}, DefaultPodDeletionTimeout)
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Wait for client pod to complete.
0000000000000000000000000000000000000000;;		podErr := WaitForPodSuccessInNamespace(f.ClientSet, createdPod.Name, ns)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Grab its logs.  Get host first.
0000000000000000000000000000000000000000;;		podStatus, err := podClient.Get(createdPod.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to get pod status: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if podErr != nil {
0000000000000000000000000000000000000000;;			// Pod failed. Dump all logs from all containers to see what's wrong
0000000000000000000000000000000000000000;;			for _, container := range podStatus.Spec.Containers {
0000000000000000000000000000000000000000;;				logs, err := GetPodLogs(f.ClientSet, ns, podStatus.Name, container.Name)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					Logf("Failed to get logs from node %q pod %q container %q: %v",
0000000000000000000000000000000000000000;;						podStatus.Spec.NodeName, podStatus.Name, container.Name, err)
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				Logf("Output of node %q pod %q container %q: %s", podStatus.Spec.NodeName, podStatus.Name, container.Name, logs)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return fmt.Errorf("expected pod %q success: %v", createdPod.Name, podErr)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Logf("Trying to get logs from node %s pod %s container %s: %v",
0000000000000000000000000000000000000000;;			podStatus.Spec.NodeName, podStatus.Name, containerName, err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Sometimes the actual containers take a second to get started, try to get logs for 60s
0000000000000000000000000000000000000000;;		logs, err := GetPodLogs(f.ClientSet, ns, podStatus.Name, containerName)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("Failed to get logs from node %q pod %q container %q. %v",
0000000000000000000000000000000000000000;;				podStatus.Spec.NodeName, podStatus.Name, containerName, err)
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to get logs from %s for %s: %v", podStatus.Name, containerName, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, expected := range expectedOutput {
0000000000000000000000000000000000000000;;			m := matcher(expected)
0000000000000000000000000000000000000000;;			matches, err := m.Match(logs)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("expected %q in container output: %v", expected, err)
0000000000000000000000000000000000000000;;			} else if !matches {
0000000000000000000000000000000000000000;;				return fmt.Errorf("expected %q in container output: %s", expected, m.FailureMessage(logs))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type EventsLister func(opts metav1.ListOptions, ns string) (*v1.EventList, error)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func DumpEventsInNamespace(eventsLister EventsLister, namespace string) {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Collecting events from namespace %q.", namespace))
0000000000000000000000000000000000000000;;		events, err := eventsLister(metav1.ListOptions{}, namespace)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Found %d events.", len(events.Items)))
0000000000000000000000000000000000000000;;		// Sort events by their first timestamp
0000000000000000000000000000000000000000;;		sortedEvents := events.Items
0000000000000000000000000000000000000000;;		if len(sortedEvents) > 1 {
0000000000000000000000000000000000000000;;			sort.Sort(byFirstTimestamp(sortedEvents))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, e := range sortedEvents {
0000000000000000000000000000000000000000;;			Logf("At %v - event for %v: %v %v: %v", e.FirstTimestamp, e.InvolvedObject.Name, e.Source, e.Reason, e.Message)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Note that we don't wait for any Cleanup to propagate, which means
0000000000000000000000000000000000000000;;		// that if you delete a bunch of pods right before ending your test,
0000000000000000000000000000000000000000;;		// you may or may not see the killing/deletion/Cleanup events.
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func DumpAllNamespaceInfo(c clientset.Interface, namespace string) {
0000000000000000000000000000000000000000;;		DumpEventsInNamespace(func(opts metav1.ListOptions, ns string) (*v1.EventList, error) {
0000000000000000000000000000000000000000;;			return c.Core().Events(ns).List(opts)
0000000000000000000000000000000000000000;;		}, namespace)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If cluster is large, then the following logs are basically useless, because:
0000000000000000000000000000000000000000;;		// 1. it takes tens of minutes or hours to grab all of them
0000000000000000000000000000000000000000;;		// 2. there are so many of them that working with them are mostly impossible
0000000000000000000000000000000000000000;;		// So we dump them only if the cluster is relatively small.
0000000000000000000000000000000000000000;;		maxNodesForDump := 20
0000000000000000000000000000000000000000;;		if nodes, err := c.Core().Nodes().List(metav1.ListOptions{}); err == nil {
0000000000000000000000000000000000000000;;			if len(nodes.Items) <= maxNodesForDump {
0000000000000000000000000000000000000000;;				dumpAllPodInfo(c)
0000000000000000000000000000000000000000;;				dumpAllNodeInfo(c)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				Logf("skipping dumping cluster info - cluster too large")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			Logf("unable to fetch node list: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// byFirstTimestamp sorts a slice of events by first timestamp, using their involvedObject's name as a tie breaker.
0000000000000000000000000000000000000000;;	type byFirstTimestamp []v1.Event
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (o byFirstTimestamp) Len() int      { return len(o) }
0000000000000000000000000000000000000000;;	func (o byFirstTimestamp) Swap(i, j int) { o[i], o[j] = o[j], o[i] }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (o byFirstTimestamp) Less(i, j int) bool {
0000000000000000000000000000000000000000;;		if o[i].FirstTimestamp.Equal(o[j].FirstTimestamp) {
0000000000000000000000000000000000000000;;			return o[i].InvolvedObject.Name < o[j].InvolvedObject.Name
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return o[i].FirstTimestamp.Before(o[j].FirstTimestamp)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func dumpAllPodInfo(c clientset.Interface) {
0000000000000000000000000000000000000000;;		pods, err := c.Core().Pods("").List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("unable to fetch pod debug info: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		logPodStates(pods.Items)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func dumpAllNodeInfo(c clientset.Interface) {
0000000000000000000000000000000000000000;;		// It should be OK to list unschedulable Nodes here.
0000000000000000000000000000000000000000;;		nodes, err := c.Core().Nodes().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("unable to fetch node list: %v", err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		names := make([]string, len(nodes.Items))
0000000000000000000000000000000000000000;;		for ix := range nodes.Items {
0000000000000000000000000000000000000000;;			names[ix] = nodes.Items[ix].Name
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		DumpNodeDebugInfo(c, names, Logf)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func DumpNodeDebugInfo(c clientset.Interface, nodeNames []string, logFunc func(fmt string, args ...interface{})) {
0000000000000000000000000000000000000000;;		for _, n := range nodeNames {
0000000000000000000000000000000000000000;;			logFunc("\nLogging node info for node %v", n)
0000000000000000000000000000000000000000;;			node, err := c.Core().Nodes().Get(n, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				logFunc("Error getting node info %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			logFunc("Node Info: %v", node)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			logFunc("\nLogging kubelet events for node %v", n)
0000000000000000000000000000000000000000;;			for _, e := range getNodeEvents(c, n) {
0000000000000000000000000000000000000000;;				logFunc("source %v type %v message %v reason %v first ts %v last ts %v, involved obj %+v",
0000000000000000000000000000000000000000;;					e.Source, e.Type, e.Message, e.Reason, e.FirstTimestamp, e.LastTimestamp, e.InvolvedObject)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			logFunc("\nLogging pods the kubelet thinks is on node %v", n)
0000000000000000000000000000000000000000;;			podList, err := GetKubeletPods(c, n)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				logFunc("Unable to retrieve kubelet pods for node %v", n)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, p := range podList.Items {
0000000000000000000000000000000000000000;;				logFunc("%v started at %v (%d+%d container statuses recorded)", p.Name, p.Status.StartTime, len(p.Status.InitContainerStatuses), len(p.Status.ContainerStatuses))
0000000000000000000000000000000000000000;;				for _, c := range p.Status.InitContainerStatuses {
0000000000000000000000000000000000000000;;					logFunc("\tInit container %v ready: %v, restart count %v",
0000000000000000000000000000000000000000;;						c.Name, c.Ready, c.RestartCount)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for _, c := range p.Status.ContainerStatuses {
0000000000000000000000000000000000000000;;					logFunc("\tContainer %v ready: %v, restart count %v",
0000000000000000000000000000000000000000;;						c.Name, c.Ready, c.RestartCount)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			HighLatencyKubeletOperations(c, 10*time.Second, n, logFunc)
0000000000000000000000000000000000000000;;			// TODO: Log node resource info
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// logNodeEvents logs kubelet events from the given node. This includes kubelet
0000000000000000000000000000000000000000;;	// restart and node unhealthy events. Note that listing events like this will mess
0000000000000000000000000000000000000000;;	// with latency metrics, beware of calling it during a test.
0000000000000000000000000000000000000000;;	func getNodeEvents(c clientset.Interface, nodeName string) []v1.Event {
0000000000000000000000000000000000000000;;		selector := fields.Set{
0000000000000000000000000000000000000000;;			"involvedObject.kind":      "Node",
0000000000000000000000000000000000000000;;			"involvedObject.name":      nodeName,
0000000000000000000000000000000000000000;;			"involvedObject.namespace": metav1.NamespaceAll,
0000000000000000000000000000000000000000;;			"source":                   "kubelet",
0000000000000000000000000000000000000000;;		}.AsSelector().String()
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{FieldSelector: selector}
0000000000000000000000000000000000000000;;		events, err := c.Core().Events(metav1.NamespaceSystem).List(options)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("Unexpected error retrieving node events %v", err)
0000000000000000000000000000000000000000;;			return []v1.Event{}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return events.Items
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// waitListSchedulableNodesOrDie is a wrapper around listing nodes supporting retries.
0000000000000000000000000000000000000000;;	func waitListSchedulableNodesOrDie(c clientset.Interface) *v1.NodeList {
0000000000000000000000000000000000000000;;		var nodes *v1.NodeList
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		if wait.PollImmediate(Poll, SingleCallTimeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			nodes, err = c.Core().Nodes().List(metav1.ListOptions{FieldSelector: fields.Set{
0000000000000000000000000000000000000000;;				"spec.unschedulable": "false",
0000000000000000000000000000000000000000;;			}.AsSelector().String()})
0000000000000000000000000000000000000000;;			return err == nil, nil
0000000000000000000000000000000000000000;;		}) != nil {
0000000000000000000000000000000000000000;;			ExpectNoError(err, "Timed out while listing nodes for e2e cluster.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nodes
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Node is schedulable if:
0000000000000000000000000000000000000000;;	// 1) doesn't have "unschedulable" field set
0000000000000000000000000000000000000000;;	// 2) it's Ready condition is set to true
0000000000000000000000000000000000000000;;	// 3) doesn't have NetworkUnavailable condition set to true
0000000000000000000000000000000000000000;;	func isNodeSchedulable(node *v1.Node) bool {
0000000000000000000000000000000000000000;;		nodeReady := IsNodeConditionSetAsExpected(node, v1.NodeReady, true)
0000000000000000000000000000000000000000;;		networkReady := IsNodeConditionUnset(node, v1.NodeNetworkUnavailable) ||
0000000000000000000000000000000000000000;;			IsNodeConditionSetAsExpectedSilent(node, v1.NodeNetworkUnavailable, false)
0000000000000000000000000000000000000000;;		return !node.Spec.Unschedulable && nodeReady && networkReady
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Test whether a fake pod can be scheduled on "node", given its current taints.
0000000000000000000000000000000000000000;;	func isNodeUntainted(node *v1.Node) bool {
0000000000000000000000000000000000000000;;		fakePod := &v1.Pod{
0000000000000000000000000000000000000000;;			TypeMeta: metav1.TypeMeta{
0000000000000000000000000000000000000000;;				Kind:       "Pod",
0000000000000000000000000000000000000000;;				APIVersion: api.Registry.GroupOrDie(v1.GroupName).GroupVersion.String(),
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:      "fake-not-scheduled",
0000000000000000000000000000000000000000;;				Namespace: "fake-not-scheduled",
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  "fake-not-scheduled",
0000000000000000000000000000000000000000;;						Image: "fake-not-scheduled",
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		nodeInfo := schedulercache.NewNodeInfo()
0000000000000000000000000000000000000000;;		nodeInfo.SetNode(node)
0000000000000000000000000000000000000000;;		fit, _, err := predicates.PodToleratesNodeTaints(fakePod, nil, nodeInfo)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Failf("Can't test predicates for node %s: %v", node.Name, err)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetReadySchedulableNodesOrDie addresses the common use case of getting nodes you can do work on.
0000000000000000000000000000000000000000;;	// 1) Needs to be schedulable.
0000000000000000000000000000000000000000;;	// 2) Needs to be ready.
0000000000000000000000000000000000000000;;	// If EITHER 1 or 2 is not true, most tests will want to ignore the node entirely.
0000000000000000000000000000000000000000;;	func GetReadySchedulableNodesOrDie(c clientset.Interface) (nodes *v1.NodeList) {
0000000000000000000000000000000000000000;;		nodes = waitListSchedulableNodesOrDie(c)
0000000000000000000000000000000000000000;;		// previous tests may have cause failures of some nodes. Let's skip
0000000000000000000000000000000000000000;;		// 'Not Ready' nodes, just in case (there is no need to fail the test).
0000000000000000000000000000000000000000;;		FilterNodes(nodes, func(node v1.Node) bool {
0000000000000000000000000000000000000000;;			return isNodeSchedulable(&node) && isNodeUntainted(&node)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		return nodes
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func WaitForAllNodesSchedulable(c clientset.Interface, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		Logf("Waiting up to %v for all (but %d) nodes to be schedulable", timeout, TestContext.AllowedNotReadyNodes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var notSchedulable []*v1.Node
0000000000000000000000000000000000000000;;		attempt := 0
0000000000000000000000000000000000000000;;		return wait.PollImmediate(30*time.Second, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			attempt++
0000000000000000000000000000000000000000;;			notSchedulable = nil
0000000000000000000000000000000000000000;;			opts := metav1.ListOptions{
0000000000000000000000000000000000000000;;				ResourceVersion: "0",
0000000000000000000000000000000000000000;;				FieldSelector:   fields.Set{"spec.unschedulable": "false"}.AsSelector().String(),
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			nodes, err := c.Core().Nodes().List(opts)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Unexpected error listing nodes: %v", err)
0000000000000000000000000000000000000000;;				// Ignore the error here - it will be retried.
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for i := range nodes.Items {
0000000000000000000000000000000000000000;;				node := &nodes.Items[i]
0000000000000000000000000000000000000000;;				if !isNodeSchedulable(node) {
0000000000000000000000000000000000000000;;					notSchedulable = append(notSchedulable, node)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Framework allows for <TestContext.AllowedNotReadyNodes> nodes to be non-ready,
0000000000000000000000000000000000000000;;			// to make it possible e.g. for incorrect deployment of some small percentage
0000000000000000000000000000000000000000;;			// of nodes (which we allow in cluster validation). Some nodes that are not
0000000000000000000000000000000000000000;;			// provisioned correctly at startup will never become ready (e.g. when something
0000000000000000000000000000000000000000;;			// won't install correctly), so we can't expect them to be ready at any point.
0000000000000000000000000000000000000000;;			//
0000000000000000000000000000000000000000;;			// However, we only allow non-ready nodes with some specific reasons.
0000000000000000000000000000000000000000;;			if len(notSchedulable) > 0 {
0000000000000000000000000000000000000000;;				// In large clusters, log them only every 10th pass.
0000000000000000000000000000000000000000;;				if len(nodes.Items) >= largeClusterThreshold && attempt%10 == 0 {
0000000000000000000000000000000000000000;;					Logf("Unschedulable nodes:")
0000000000000000000000000000000000000000;;					for i := range notSchedulable {
0000000000000000000000000000000000000000;;						Logf("-> %s Ready=%t Network=%t",
0000000000000000000000000000000000000000;;							notSchedulable[i].Name,
0000000000000000000000000000000000000000;;							IsNodeConditionSetAsExpectedSilent(notSchedulable[i], v1.NodeReady, true),
0000000000000000000000000000000000000000;;							IsNodeConditionSetAsExpectedSilent(notSchedulable[i], v1.NodeNetworkUnavailable, false))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					Logf("================================")
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(notSchedulable) > TestContext.AllowedNotReadyNodes {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return allowedNotReadyReasons(notSchedulable), nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func GetPodSecretUpdateTimeout(c clientset.Interface) time.Duration {
0000000000000000000000000000000000000000;;		// With SecretManager(ConfigMapManager), we may have to wait up to full sync period +
0000000000000000000000000000000000000000;;		// TTL of secret(configmap) to elapse before the Kubelet projects the update into the
0000000000000000000000000000000000000000;;		// volume and the container picks it up.
0000000000000000000000000000000000000000;;		// So this timeout is based on default Kubelet sync period (1 minute) + maximum TTL for
0000000000000000000000000000000000000000;;		// secret(configmap) that's based on cluster size + additional time as a fudge factor.
0000000000000000000000000000000000000000;;		secretTTL, err := GetNodeTTLAnnotationValue(c)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Logf("Couldn't get node TTL annotation (using default value of 0): %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podLogTimeout := 240*time.Second + secretTTL
0000000000000000000000000000000000000000;;		return podLogTimeout
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func GetNodeTTLAnnotationValue(c clientset.Interface) (time.Duration, error) {
0000000000000000000000000000000000000000;;		nodes, err := c.Core().Nodes().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil || len(nodes.Items) == 0 {
0000000000000000000000000000000000000000;;			return time.Duration(0), fmt.Errorf("Couldn't list any nodes to get TTL annotation: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Since TTL the kubelet is using is stored in node object, for the timeout
0000000000000000000000000000000000000000;;		// purpose we take it from the first node (all of them should be the same).
0000000000000000000000000000000000000000;;		node := &nodes.Items[0]
0000000000000000000000000000000000000000;;		if node.Annotations == nil {
0000000000000000000000000000000000000000;;			return time.Duration(0), fmt.Errorf("No annotations found on the node")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		value, ok := node.Annotations[v1.ObjectTTLAnnotationKey]
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			return time.Duration(0), fmt.Errorf("No TTL annotation found on the node")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		intValue, err := strconv.Atoi(value)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return time.Duration(0), fmt.Errorf("Cannot convert TTL annotation from %#v to int", *node)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return time.Duration(intValue) * time.Second, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func AddOrUpdateLabelOnNode(c clientset.Interface, nodeName string, labelKey, labelValue string) {
0000000000000000000000000000000000000000;;		ExpectNoError(testutil.AddLabelsToNode(c, nodeName, map[string]string{labelKey: labelValue}))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ExpectNodeHasLabel(c clientset.Interface, nodeName string, labelKey string, labelValue string) {
0000000000000000000000000000000000000000;;		By("verifying the node has the label " + labelKey + " " + labelValue)
0000000000000000000000000000000000000000;;		node, err := c.Core().Nodes().Get(nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;		Expect(node.Labels[labelKey]).To(Equal(labelValue))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func RemoveTaintOffNode(c clientset.Interface, nodeName string, taint v1.Taint) {
0000000000000000000000000000000000000000;;		ExpectNoError(controller.RemoveTaintOffNode(c, nodeName, &taint, nil))
0000000000000000000000000000000000000000;;		VerifyThatTaintIsGone(c, nodeName, &taint)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func AddOrUpdateTaintOnNode(c clientset.Interface, nodeName string, taint v1.Taint) {
0000000000000000000000000000000000000000;;		ExpectNoError(controller.AddOrUpdateTaintOnNode(c, nodeName, &taint))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RemoveLabelOffNode is for cleaning up labels temporarily added to node,
0000000000000000000000000000000000000000;;	// won't fail if target label doesn't exist or has been removed.
0000000000000000000000000000000000000000;;	func RemoveLabelOffNode(c clientset.Interface, nodeName string, labelKey string) {
0000000000000000000000000000000000000000;;		By("removing the label " + labelKey + " off the node " + nodeName)
0000000000000000000000000000000000000000;;		ExpectNoError(testutil.RemoveLabelOffNode(c, nodeName, []string{labelKey}))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("verifying the node doesn't have the label " + labelKey)
0000000000000000000000000000000000000000;;		ExpectNoError(testutil.VerifyLabelsRemoved(c, nodeName, []string{labelKey}))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func VerifyThatTaintIsGone(c clientset.Interface, nodeName string, taint *v1.Taint) {
0000000000000000000000000000000000000000;;		By("verifying the node doesn't have the taint " + taint.ToString())
0000000000000000000000000000000000000000;;		nodeUpdated, err := c.Core().Nodes().Get(nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;		if v1helper.TaintExists(nodeUpdated.Spec.Taints, taint) {
0000000000000000000000000000000000000000;;			Failf("Failed removing taint " + taint.ToString() + " of the node " + nodeName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ExpectNodeHasTaint(c clientset.Interface, nodeName string, taint *v1.Taint) {
0000000000000000000000000000000000000000;;		By("verifying the node has the taint " + taint.ToString())
0000000000000000000000000000000000000000;;		if has, err := NodeHasTaint(c, nodeName, taint); !has {
0000000000000000000000000000000000000000;;			ExpectNoError(err)
0000000000000000000000000000000000000000;;			Failf("Failed to find taint %s on node %s", taint.ToString(), nodeName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NodeHasTaint(c clientset.Interface, nodeName string, taint *v1.Taint) (bool, error) {
0000000000000000000000000000000000000000;;		node, err := c.Core().Nodes().Get(nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nodeTaints := node.Spec.Taints
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(nodeTaints) == 0 || !v1helper.TaintExists(nodeTaints, taint) {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	//AddOrUpdateAvoidPodOnNode adds avoidPods annotations to node, will override if it exists
0000000000000000000000000000000000000000;;	func AddOrUpdateAvoidPodOnNode(c clientset.Interface, nodeName string, avoidPods v1.AvoidPods) {
0000000000000000000000000000000000000000;;		err := wait.PollImmediate(Poll, SingleCallTimeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			node, err := c.CoreV1().Nodes().Get(nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			taintsData, err := json.Marshal(avoidPods)
0000000000000000000000000000000000000000;;			ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if node.Annotations == nil {
0000000000000000000000000000000000000000;;				node.Annotations = make(map[string]string)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			node.Annotations[v1.PreferAvoidPodsAnnotationKey] = string(taintsData)
0000000000000000000000000000000000000000;;			_, err = c.CoreV1().Nodes().Update(node)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				if !apierrs.IsConflict(err) {
0000000000000000000000000000000000000000;;					ExpectNoError(err)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					Logf("Conflict when trying to add/update avoidPonds %v to %v", avoidPods, nodeName)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	//RemoveAnnotationOffNode removes AvoidPods annotations from the node. It does not fail if no such annotation exists.
0000000000000000000000000000000000000000;;	func RemoveAvoidPodsOffNode(c clientset.Interface, nodeName string) {
0000000000000000000000000000000000000000;;		err := wait.PollImmediate(Poll, SingleCallTimeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			node, err := c.CoreV1().Nodes().Get(nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if node.Annotations == nil {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			delete(node.Annotations, v1.PreferAvoidPodsAnnotationKey)
0000000000000000000000000000000000000000;;			_, err = c.CoreV1().Nodes().Update(node)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				if !apierrs.IsConflict(err) {
0000000000000000000000000000000000000000;;					ExpectNoError(err)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					Logf("Conflict when trying to remove avoidPods to %v", nodeName)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getScalerForKind(internalClientset internalclientset.Interface, kind schema.GroupKind) (kubectl.Scaler, error) {
0000000000000000000000000000000000000000;;		return kubectl.ScalerFor(kind, internalClientset)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ScaleResource(
0000000000000000000000000000000000000000;;		clientset clientset.Interface,
0000000000000000000000000000000000000000;;		internalClientset internalclientset.Interface,
0000000000000000000000000000000000000000;;		ns, name string,
0000000000000000000000000000000000000000;;		size uint,
0000000000000000000000000000000000000000;;		wait bool,
0000000000000000000000000000000000000000;;		kind schema.GroupKind,
0000000000000000000000000000000000000000;;	) error {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Scaling %v %s in namespace %s to %d", kind, name, ns, size))
0000000000000000000000000000000000000000;;		scaler, err := getScalerForKind(internalClientset, kind)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		waitForScale := kubectl.NewRetryParams(5*time.Second, 1*time.Minute)
0000000000000000000000000000000000000000;;		waitForReplicas := kubectl.NewRetryParams(5*time.Second, 5*time.Minute)
0000000000000000000000000000000000000000;;		if err = scaler.Scale(ns, name, size, nil, waitForScale, waitForReplicas); err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error while scaling RC %s to %d replicas: %v", name, size, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !wait {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return WaitForControlledPodsRunning(clientset, ns, name, kind)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Wait up to 10 minutes for pods to become Running.
0000000000000000000000000000000000000000;;	func WaitForControlledPodsRunning(c clientset.Interface, ns, name string, kind schema.GroupKind) error {
0000000000000000000000000000000000000000;;		rtObject, err := getRuntimeObjectForKind(c, kind, ns, name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		selector, err := getSelectorFromRuntimeObject(rtObject)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err = testutil.WaitForPodsWithLabelRunning(c, ns, selector)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Error while waiting for replication controller %s pods to be running: %v", name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Returns true if all the specified pods are scheduled, else returns false.
0000000000000000000000000000000000000000;;	func podsWithLabelScheduled(c clientset.Interface, ns string, label labels.Selector) (bool, error) {
0000000000000000000000000000000000000000;;		PodStore := testutil.NewPodStore(c, ns, label, fields.Everything())
0000000000000000000000000000000000000000;;		defer PodStore.Stop()
0000000000000000000000000000000000000000;;		pods := PodStore.List()
0000000000000000000000000000000000000000;;		if len(pods) == 0 {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			if pod.Spec.NodeName == "" {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Wait for all matching pods to become scheduled and at least one
0000000000000000000000000000000000000000;;	// matching pod exists.  Return the list of matching pods.
0000000000000000000000000000000000000000;;	func WaitForPodsWithLabelScheduled(c clientset.Interface, ns string, label labels.Selector) (pods *v1.PodList, err error) {
0000000000000000000000000000000000000000;;		err = wait.PollImmediate(Poll, podScheduledBeforeTimeout,
0000000000000000000000000000000000000000;;			func() (bool, error) {
0000000000000000000000000000000000000000;;				pods, err = WaitForPodsWithLabel(c, ns, label)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return false, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;					if pod.Spec.NodeName == "" {
0000000000000000000000000000000000000000;;						return false, nil
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		return pods, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Wait up to PodListTimeout for getting pods with certain label
0000000000000000000000000000000000000000;;	func WaitForPodsWithLabel(c clientset.Interface, ns string, label labels.Selector) (pods *v1.PodList, err error) {
0000000000000000000000000000000000000000;;		for t := time.Now(); time.Since(t) < PodListTimeout; time.Sleep(Poll) {
0000000000000000000000000000000000000000;;			options := metav1.ListOptions{LabelSelector: label.String()}
0000000000000000000000000000000000000000;;			pods, err = c.Core().Pods(ns).List(options)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			if len(pods.Items) > 0 {
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if pods == nil || len(pods.Items) == 0 {
0000000000000000000000000000000000000000;;			err = fmt.Errorf("Timeout while waiting for pods with label %v", label)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Wait for exact amount of matching pods to become running and ready.
0000000000000000000000000000000000000000;;	// Return the list of matching pods.
0000000000000000000000000000000000000000;;	func WaitForPodsWithLabelRunningReady(c clientset.Interface, ns string, label labels.Selector, num int, timeout time.Duration) (pods *v1.PodList, err error) {
0000000000000000000000000000000000000000;;		var current int
0000000000000000000000000000000000000000;;		err = wait.Poll(Poll, timeout,
0000000000000000000000000000000000000000;;			func() (bool, error) {
0000000000000000000000000000000000000000;;				pods, err := WaitForPodsWithLabel(c, ns, label)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					Logf("Failed to list pods: %v", err)
0000000000000000000000000000000000000000;;					return false, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				current = 0
0000000000000000000000000000000000000000;;				for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;					if flag, err := testutil.PodRunningReady(&pod); err == nil && flag == true {
0000000000000000000000000000000000000000;;						current++
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if current != num {
0000000000000000000000000000000000000000;;					Logf("Got %v pods running and ready, expect: %v", current, num)
0000000000000000000000000000000000000000;;					return false, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		return pods, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getRuntimeObjectForKind(c clientset.Interface, kind schema.GroupKind, ns, name string) (runtime.Object, error) {
0000000000000000000000000000000000000000;;		switch kind {
0000000000000000000000000000000000000000;;		case api.Kind("ReplicationController"):
0000000000000000000000000000000000000000;;			return c.Core().ReplicationControllers(ns).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		case extensionsinternal.Kind("ReplicaSet"):
0000000000000000000000000000000000000000;;			return c.Extensions().ReplicaSets(ns).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		case extensionsinternal.Kind("Deployment"):
0000000000000000000000000000000000000000;;			return c.Extensions().Deployments(ns).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		case extensionsinternal.Kind("DaemonSet"):
0000000000000000000000000000000000000000;;			return c.Extensions().DaemonSets(ns).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		case batchinternal.Kind("Job"):
0000000000000000000000000000000000000000;;			return c.Batch().Jobs(ns).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("Unsupported kind when getting runtime object: %v", kind)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func deleteResource(c clientset.Interface, kind schema.GroupKind, ns, name string, deleteOption *metav1.DeleteOptions) error {
0000000000000000000000000000000000000000;;		switch kind {
0000000000000000000000000000000000000000;;		case api.Kind("ReplicationController"):
0000000000000000000000000000000000000000;;			return c.Core().ReplicationControllers(ns).Delete(name, deleteOption)
0000000000000000000000000000000000000000;;		case extensionsinternal.Kind("ReplicaSet"):
0000000000000000000000000000000000000000;;			return c.Extensions().ReplicaSets(ns).Delete(name, deleteOption)
0000000000000000000000000000000000000000;;		case extensionsinternal.Kind("Deployment"):
0000000000000000000000000000000000000000;;			return c.Extensions().Deployments(ns).Delete(name, deleteOption)
0000000000000000000000000000000000000000;;		case extensionsinternal.Kind("DaemonSet"):
0000000000000000000000000000000000000000;;			return c.Extensions().DaemonSets(ns).Delete(name, deleteOption)
0000000000000000000000000000000000000000;;		case batchinternal.Kind("Job"):
0000000000000000000000000000000000000000;;			return c.Batch().Jobs(ns).Delete(name, deleteOption)
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			return fmt.Errorf("Unsupported kind when deleting: %v", kind)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getSelectorFromRuntimeObject(obj runtime.Object) (labels.Selector, error) {
0000000000000000000000000000000000000000;;		switch typed := obj.(type) {
0000000000000000000000000000000000000000;;		case *v1.ReplicationController:
0000000000000000000000000000000000000000;;			return labels.SelectorFromSet(typed.Spec.Selector), nil
0000000000000000000000000000000000000000;;		case *extensions.ReplicaSet:
0000000000000000000000000000000000000000;;			return metav1.LabelSelectorAsSelector(typed.Spec.Selector)
0000000000000000000000000000000000000000;;		case *extensions.Deployment:
0000000000000000000000000000000000000000;;			return metav1.LabelSelectorAsSelector(typed.Spec.Selector)
0000000000000000000000000000000000000000;;		case *extensions.DaemonSet:
0000000000000000000000000000000000000000;;			return metav1.LabelSelectorAsSelector(typed.Spec.Selector)
0000000000000000000000000000000000000000;;		case *batch.Job:
0000000000000000000000000000000000000000;;			return metav1.LabelSelectorAsSelector(typed.Spec.Selector)
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("Unsupported kind when getting selector: %v", obj)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getReplicasFromRuntimeObject(obj runtime.Object) (int32, error) {
0000000000000000000000000000000000000000;;		switch typed := obj.(type) {
0000000000000000000000000000000000000000;;		case *v1.ReplicationController:
0000000000000000000000000000000000000000;;			if typed.Spec.Replicas != nil {
0000000000000000000000000000000000000000;;				return *typed.Spec.Replicas, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return 0, nil
0000000000000000000000000000000000000000;;		case *extensions.ReplicaSet:
0000000000000000000000000000000000000000;;			if typed.Spec.Replicas != nil {
0000000000000000000000000000000000000000;;				return *typed.Spec.Replicas, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return 0, nil
0000000000000000000000000000000000000000;;		case *extensions.Deployment:
0000000000000000000000000000000000000000;;			if typed.Spec.Replicas != nil {
0000000000000000000000000000000000000000;;				return *typed.Spec.Replicas, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return 0, nil
0000000000000000000000000000000000000000;;		case *batch.Job:
0000000000000000000000000000000000000000;;			// TODO: currently we use pause pods so that's OK. When we'll want to switch to Pods
0000000000000000000000000000000000000000;;			// that actually finish we need a better way to do this.
0000000000000000000000000000000000000000;;			if typed.Spec.Parallelism != nil {
0000000000000000000000000000000000000000;;				return *typed.Spec.Parallelism, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return 0, nil
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			return -1, fmt.Errorf("Unsupported kind when getting number of replicas: %v", obj)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getReaperForKind(internalClientset internalclientset.Interface, kind schema.GroupKind) (kubectl.Reaper, error) {
0000000000000000000000000000000000000000;;		return kubectl.ReaperFor(kind, internalClientset)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// DeleteResourceAndPods deletes a given resource and all pods it spawned
0000000000000000000000000000000000000000;;	func DeleteResourceAndPods(clientset clientset.Interface, internalClientset internalclientset.Interface, kind schema.GroupKind, ns, name string) error {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("deleting %v %s in namespace %s", kind, name, ns))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rtObject, err := getRuntimeObjectForKind(clientset, kind, ns, name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			if apierrs.IsNotFound(err) {
0000000000000000000000000000000000000000;;				Logf("%v %s not found: %v", kind, name, err)
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		selector, err := getSelectorFromRuntimeObject(rtObject)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		reaper, err := getReaperForKind(internalClientset, kind)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ps, err := podStoreForSelector(clientset, ns, selector)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer ps.Stop()
0000000000000000000000000000000000000000;;		startTime := time.Now()
0000000000000000000000000000000000000000;;		err = reaper.Stop(ns, name, 0, nil)
0000000000000000000000000000000000000000;;		if apierrs.IsNotFound(err) {
0000000000000000000000000000000000000000;;			Logf("%v %s was already deleted: %v", kind, name, err)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error while stopping %v: %s: %v", kind, name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		deleteTime := time.Now().Sub(startTime)
0000000000000000000000000000000000000000;;		Logf("Deleting %v %s took: %v", kind, name, deleteTime)
0000000000000000000000000000000000000000;;		err = waitForPodsInactive(ps, 100*time.Millisecond, 10*time.Minute)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error while waiting for pods to become inactive %s: %v", name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		terminatePodTime := time.Now().Sub(startTime) - deleteTime
0000000000000000000000000000000000000000;;		Logf("Terminating %v %s pods took: %v", kind, name, terminatePodTime)
0000000000000000000000000000000000000000;;		// this is to relieve namespace controller's pressure when deleting the
0000000000000000000000000000000000000000;;		// namespace after a test.
0000000000000000000000000000000000000000;;		err = waitForPodsGone(ps, 100*time.Millisecond, 10*time.Minute)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error while waiting for pods gone %s: %v", name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		gcPodTime := time.Now().Sub(startTime) - terminatePodTime
0000000000000000000000000000000000000000;;		Logf("Garbage collecting %v %s pods took: %v", kind, name, gcPodTime)
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// DeleteResourceAndWaitForGC deletes only given resource and waits for GC to delete the pods.
0000000000000000000000000000000000000000;;	func DeleteResourceAndWaitForGC(c clientset.Interface, kind schema.GroupKind, ns, name string) error {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("deleting %v %s in namespace %s, will wait for the garbage collector to delete the pods", kind, name, ns))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rtObject, err := getRuntimeObjectForKind(c, kind, ns, name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			if apierrs.IsNotFound(err) {
0000000000000000000000000000000000000000;;				Logf("%v %s not found: %v", kind, name, err)
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		selector, err := getSelectorFromRuntimeObject(rtObject)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		replicas, err := getReplicasFromRuntimeObject(rtObject)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ps, err := podStoreForSelector(c, ns, selector)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		defer ps.Stop()
0000000000000000000000000000000000000000;;		startTime := time.Now()
0000000000000000000000000000000000000000;;		falseVar := false
0000000000000000000000000000000000000000;;		deleteOption := &metav1.DeleteOptions{OrphanDependents: &falseVar}
0000000000000000000000000000000000000000;;		err = deleteResource(c, kind, ns, name, deleteOption)
0000000000000000000000000000000000000000;;		if err != nil && apierrs.IsNotFound(err) {
0000000000000000000000000000000000000000;;			Logf("%v %s was already deleted: %v", kind, name, err)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		deleteTime := time.Now().Sub(startTime)
0000000000000000000000000000000000000000;;		Logf("Deleting %v %s took: %v", kind, name, deleteTime)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var interval, timeout time.Duration
0000000000000000000000000000000000000000;;		switch {
0000000000000000000000000000000000000000;;		case replicas < 100:
0000000000000000000000000000000000000000;;			interval = 100 * time.Millisecond
0000000000000000000000000000000000000000;;		case replicas < 1000:
0000000000000000000000000000000000000000;;			interval = 1 * time.Second
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			interval = 10 * time.Second
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if replicas < 5000 {
0000000000000000000000000000000000000000;;			timeout = 10 * time.Minute
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			timeout = time.Duration(replicas/gcThroughput) * time.Second
0000000000000000000000000000000000000000;;			// gcThroughput is pretty strict now, add a bit more to it
0000000000000000000000000000000000000000;;			timeout = timeout + 3*time.Minute
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err = waitForPodsInactive(ps, interval, timeout)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error while waiting for pods to become inactive %s: %v", name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		terminatePodTime := time.Now().Sub(startTime) - deleteTime
0000000000000000000000000000000000000000;;		Logf("Terminating %v %s pods took: %v", kind, name, terminatePodTime)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err = waitForPodsGone(ps, interval, 10*time.Minute)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error while waiting for pods gone %s: %v", name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// podStoreForSelector creates a PodStore that monitors pods from given namespace matching given selector.
0000000000000000000000000000000000000000;;	// It waits until the reflector does a List() before returning.
0000000000000000000000000000000000000000;;	func podStoreForSelector(c clientset.Interface, ns string, selector labels.Selector) (*testutil.PodStore, error) {
0000000000000000000000000000000000000000;;		ps := testutil.NewPodStore(c, ns, selector, fields.Everything())
0000000000000000000000000000000000000000;;		err := wait.Poll(100*time.Millisecond, 2*time.Minute, func() (bool, error) {
0000000000000000000000000000000000000000;;			if len(ps.Reflector.LastSyncResourceVersion()) != 0 {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		return ps, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// waitForPodsInactive waits until there are no active pods left in the PodStore.
0000000000000000000000000000000000000000;;	// This is to make a fair comparison of deletion time between DeleteRCAndPods
0000000000000000000000000000000000000000;;	// and DeleteRCAndWaitForGC, because the RC controller decreases status.replicas
0000000000000000000000000000000000000000;;	// when the pod is inactvie.
0000000000000000000000000000000000000000;;	func waitForPodsInactive(ps *testutil.PodStore, interval, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		return wait.PollImmediate(interval, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			pods := ps.List()
0000000000000000000000000000000000000000;;			for _, pod := range pods {
0000000000000000000000000000000000000000;;				if controller.IsPodActive(pod) {
0000000000000000000000000000000000000000;;					return false, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// waitForPodsGone waits until there are no pods left in the PodStore.
0000000000000000000000000000000000000000;;	func waitForPodsGone(ps *testutil.PodStore, interval, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		return wait.PollImmediate(interval, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			if pods := ps.List(); len(pods) == 0 {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func WaitForPodsReady(c clientset.Interface, ns, name string, minReadySeconds int) error {
0000000000000000000000000000000000000000;;		label := labels.SelectorFromSet(labels.Set(map[string]string{"name": name}))
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{LabelSelector: label.String()}
0000000000000000000000000000000000000000;;		return wait.Poll(Poll, 5*time.Minute, func() (bool, error) {
0000000000000000000000000000000000000000;;			pods, err := c.Core().Pods(ns).List(options)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;				if !podutil.IsPodAvailable(&pod, int32(minReadySeconds), metav1.Now()) {
0000000000000000000000000000000000000000;;					return false, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Waits for the number of events on the given object to reach a desired count.
0000000000000000000000000000000000000000;;	func WaitForEvents(c clientset.Interface, ns string, objOrRef runtime.Object, desiredEventsCount int) error {
0000000000000000000000000000000000000000;;		return wait.Poll(Poll, 5*time.Minute, func() (bool, error) {
0000000000000000000000000000000000000000;;			events, err := c.Core().Events(ns).Search(api.Scheme, objOrRef)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, fmt.Errorf("error in listing events: %s", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			eventsCount := len(events.Items)
0000000000000000000000000000000000000000;;			if eventsCount == desiredEventsCount {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if eventsCount < desiredEventsCount {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Number of events has exceeded the desired count.
0000000000000000000000000000000000000000;;			return false, fmt.Errorf("number of events has exceeded the desired count, eventsCount: %d, desiredCount: %d", eventsCount, desiredEventsCount)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Waits for the number of events on the given object to be at least a desired count.
0000000000000000000000000000000000000000;;	func WaitForPartialEvents(c clientset.Interface, ns string, objOrRef runtime.Object, atLeastEventsCount int) error {
0000000000000000000000000000000000000000;;		return wait.Poll(Poll, 5*time.Minute, func() (bool, error) {
0000000000000000000000000000000000000000;;			events, err := c.Core().Events(ns).Search(api.Scheme, objOrRef)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, fmt.Errorf("error in listing events: %s", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			eventsCount := len(events.Items)
0000000000000000000000000000000000000000;;			if eventsCount >= atLeastEventsCount {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type updateDSFunc func(*extensions.DaemonSet)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func UpdateDaemonSetWithRetries(c clientset.Interface, namespace, name string, applyUpdate updateDSFunc) (ds *extensions.DaemonSet, err error) {
0000000000000000000000000000000000000000;;		daemonsets := c.ExtensionsV1beta1().DaemonSets(namespace)
0000000000000000000000000000000000000000;;		var updateErr error
0000000000000000000000000000000000000000;;		pollErr := wait.PollImmediate(10*time.Millisecond, 1*time.Minute, func() (bool, error) {
0000000000000000000000000000000000000000;;			if ds, err = daemonsets.Get(name, metav1.GetOptions{}); err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Apply the update, then attempt to push it to the apiserver.
0000000000000000000000000000000000000000;;			applyUpdate(ds)
0000000000000000000000000000000000000000;;			if ds, err = daemonsets.Update(ds); err == nil {
0000000000000000000000000000000000000000;;				Logf("Updating DaemonSet %s", name)
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			updateErr = err
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if pollErr == wait.ErrWaitTimeout {
0000000000000000000000000000000000000000;;			pollErr = fmt.Errorf("couldn't apply the provided updated to DaemonSet %q: %v", name, updateErr)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return ds, pollErr
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NodeAddresses returns the first address of the given type of each node.
0000000000000000000000000000000000000000;;	func NodeAddresses(nodelist *v1.NodeList, addrType v1.NodeAddressType) []string {
0000000000000000000000000000000000000000;;		hosts := []string{}
0000000000000000000000000000000000000000;;		for _, n := range nodelist.Items {
0000000000000000000000000000000000000000;;			for _, addr := range n.Status.Addresses {
0000000000000000000000000000000000000000;;				// Use the first external IP address we find on the node, and
0000000000000000000000000000000000000000;;				// use at most one per node.
0000000000000000000000000000000000000000;;				// TODO(roberthbailey): Use the "preferred" address for the node, once
0000000000000000000000000000000000000000;;				// such a thing is defined (#2462).
0000000000000000000000000000000000000000;;				if addr.Type == addrType {
0000000000000000000000000000000000000000;;					hosts = append(hosts, addr.Address)
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return hosts
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NodeSSHHosts returns SSH-able host names for all schedulable nodes - this excludes master node.
0000000000000000000000000000000000000000;;	// It returns an error if it can't find an external IP for every node, though it still returns all
0000000000000000000000000000000000000000;;	// hosts that it found in that case.
0000000000000000000000000000000000000000;;	func NodeSSHHosts(c clientset.Interface) ([]string, error) {
0000000000000000000000000000000000000000;;		nodelist := waitListSchedulableNodesOrDie(c)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO(roberthbailey): Use the "preferred" address for the node, once such a thing is defined (#2462).
0000000000000000000000000000000000000000;;		hosts := NodeAddresses(nodelist, v1.NodeExternalIP)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Error if any node didn't have an external IP.
0000000000000000000000000000000000000000;;		if len(hosts) != len(nodelist.Items) {
0000000000000000000000000000000000000000;;			return hosts, fmt.Errorf(
0000000000000000000000000000000000000000;;				"only found %d external IPs on nodes, but found %d nodes. Nodelist: %v",
0000000000000000000000000000000000000000;;				len(hosts), len(nodelist.Items), nodelist)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sshHosts := make([]string, 0, len(hosts))
0000000000000000000000000000000000000000;;		for _, h := range hosts {
0000000000000000000000000000000000000000;;			sshHosts = append(sshHosts, net.JoinHostPort(h, sshPort))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return sshHosts, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type SSHResult struct {
0000000000000000000000000000000000000000;;		User   string
0000000000000000000000000000000000000000;;		Host   string
0000000000000000000000000000000000000000;;		Cmd    string
0000000000000000000000000000000000000000;;		Stdout string
0000000000000000000000000000000000000000;;		Stderr string
0000000000000000000000000000000000000000;;		Code   int
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NodeExec execs the given cmd on node via SSH. Note that the nodeName is an sshable name,
0000000000000000000000000000000000000000;;	// eg: the name returned by framework.GetMasterHost(). This is also not guaranteed to work across
0000000000000000000000000000000000000000;;	// cloud providers since it involves ssh.
0000000000000000000000000000000000000000;;	func NodeExec(nodeName, cmd string) (SSHResult, error) {
0000000000000000000000000000000000000000;;		return SSH(cmd, net.JoinHostPort(nodeName, sshPort), TestContext.Provider)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// SSH synchronously SSHs to a node running on provider and runs cmd. If there
0000000000000000000000000000000000000000;;	// is no error performing the SSH, the stdout, stderr, and exit code are
0000000000000000000000000000000000000000;;	// returned.
0000000000000000000000000000000000000000;;	func SSH(cmd, host, provider string) (SSHResult, error) {
0000000000000000000000000000000000000000;;		result := SSHResult{Host: host, Cmd: cmd}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Get a signer for the provider.
0000000000000000000000000000000000000000;;		signer, err := GetSigner(provider)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return result, fmt.Errorf("error getting signer for provider %s: '%v'", provider, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// RunSSHCommand will default to Getenv("USER") if user == "", but we're
0000000000000000000000000000000000000000;;		// defaulting here as well for logging clarity.
0000000000000000000000000000000000000000;;		result.User = os.Getenv("KUBE_SSH_USER")
0000000000000000000000000000000000000000;;		if result.User == "" {
0000000000000000000000000000000000000000;;			result.User = os.Getenv("USER")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		stdout, stderr, code, err := sshutil.RunSSHCommand(cmd, result.User, host, signer)
0000000000000000000000000000000000000000;;		result.Stdout = stdout
0000000000000000000000000000000000000000;;		result.Stderr = stderr
0000000000000000000000000000000000000000;;		result.Code = code
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return result, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func LogSSHResult(result SSHResult) {
0000000000000000000000000000000000000000;;		remote := fmt.Sprintf("%s@%s", result.User, result.Host)
0000000000000000000000000000000000000000;;		Logf("ssh %s: command:   %s", remote, result.Cmd)
0000000000000000000000000000000000000000;;		Logf("ssh %s: stdout:    %q", remote, result.Stdout)
0000000000000000000000000000000000000000;;		Logf("ssh %s: stderr:    %q", remote, result.Stderr)
0000000000000000000000000000000000000000;;		Logf("ssh %s: exit code: %d", remote, result.Code)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func IssueSSHCommandWithResult(cmd, provider string, node *v1.Node) (*SSHResult, error) {
0000000000000000000000000000000000000000;;		Logf("Getting external IP address for %s", node.Name)
0000000000000000000000000000000000000000;;		host := ""
0000000000000000000000000000000000000000;;		for _, a := range node.Status.Addresses {
0000000000000000000000000000000000000000;;			if a.Type == v1.NodeExternalIP {
0000000000000000000000000000000000000000;;				host = net.JoinHostPort(a.Address, sshPort)
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if host == "" {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("couldn't find external IP address for node %s", node.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Logf("SSH %q on %s(%s)", cmd, node.Name, host)
0000000000000000000000000000000000000000;;		result, err := SSH(cmd, host, provider)
0000000000000000000000000000000000000000;;		LogSSHResult(result)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if result.Code != 0 || err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed running %q: %v (exit code %d)",
0000000000000000000000000000000000000000;;				cmd, err, result.Code)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return &result, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func IssueSSHCommand(cmd, provider string, node *v1.Node) error {
0000000000000000000000000000000000000000;;		_, err := IssueSSHCommandWithResult(cmd, provider, node)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewHostExecPodSpec returns the pod spec of hostexec pod
0000000000000000000000000000000000000000;;	func NewHostExecPodSpec(ns, name string) *v1.Pod {
0000000000000000000000000000000000000000;;		immediate := int64(0)
0000000000000000000000000000000000000000;;		pod := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:      name,
0000000000000000000000000000000000000000;;				Namespace: ns,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:            "hostexec",
0000000000000000000000000000000000000000;;						Image:           "gcr.io/google_containers/hostexec:1.2",
0000000000000000000000000000000000000000;;						ImagePullPolicy: v1.PullIfNotPresent,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				HostNetwork:                   true,
0000000000000000000000000000000000000000;;				SecurityContext:               &v1.PodSecurityContext{},
0000000000000000000000000000000000000000;;				TerminationGracePeriodSeconds: &immediate,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunHostCmd runs the given cmd in the context of the given pod using `kubectl exec`
0000000000000000000000000000000000000000;;	// inside of a shell.
0000000000000000000000000000000000000000;;	func RunHostCmd(ns, name, cmd string) (string, error) {
0000000000000000000000000000000000000000;;		return RunKubectl("exec", fmt.Sprintf("--namespace=%v", ns), name, "--", "/bin/sh", "-c", cmd)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunHostCmdOrDie calls RunHostCmd and dies on error.
0000000000000000000000000000000000000000;;	func RunHostCmdOrDie(ns, name, cmd string) string {
0000000000000000000000000000000000000000;;		stdout, err := RunHostCmd(ns, name, cmd)
0000000000000000000000000000000000000000;;		Logf("stdout: %v", stdout)
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;		return stdout
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// LaunchHostExecPod launches a hostexec pod in the given namespace and waits
0000000000000000000000000000000000000000;;	// until it's Running
0000000000000000000000000000000000000000;;	func LaunchHostExecPod(client clientset.Interface, ns, name string) *v1.Pod {
0000000000000000000000000000000000000000;;		hostExecPod := NewHostExecPodSpec(ns, name)
0000000000000000000000000000000000000000;;		pod, err := client.Core().Pods(ns).Create(hostExecPod)
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;		err = WaitForPodRunningInNamespace(client, pod)
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;		return pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// newExecPodSpec returns the pod spec of exec pod
0000000000000000000000000000000000000000;;	func newExecPodSpec(ns, generateName string) *v1.Pod {
0000000000000000000000000000000000000000;;		immediate := int64(0)
0000000000000000000000000000000000000000;;		pod := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				GenerateName: generateName,
0000000000000000000000000000000000000000;;				Namespace:    ns,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				TerminationGracePeriodSeconds: &immediate,
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:    "exec",
0000000000000000000000000000000000000000;;						Image:   "gcr.io/google_containers/busybox:1.24",
0000000000000000000000000000000000000000;;						Command: []string{"sh", "-c", "while true; do sleep 5; done"},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CreateExecPodOrFail creates a simple busybox pod in a sleep loop used as a
0000000000000000000000000000000000000000;;	// vessel for kubectl exec commands.
0000000000000000000000000000000000000000;;	// Returns the name of the created pod.
0000000000000000000000000000000000000000;;	func CreateExecPodOrFail(client clientset.Interface, ns, generateName string, tweak func(*v1.Pod)) string {
0000000000000000000000000000000000000000;;		Logf("Creating new exec pod")
0000000000000000000000000000000000000000;;		execPod := newExecPodSpec(ns, generateName)
0000000000000000000000000000000000000000;;		if tweak != nil {
0000000000000000000000000000000000000000;;			tweak(execPod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		created, err := client.Core().Pods(ns).Create(execPod)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		err = wait.PollImmediate(Poll, 5*time.Minute, func() (bool, error) {
0000000000000000000000000000000000000000;;			retrievedPod, err := client.Core().Pods(execPod.Namespace).Get(created.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return retrievedPod.Status.Phase == v1.PodRunning, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		return created.Name
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func CreatePodOrFail(c clientset.Interface, ns, name string, labels map[string]string, containerPorts []v1.ContainerPort) {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Creating pod %s in namespace %s", name, ns))
0000000000000000000000000000000000000000;;		pod := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:   name,
0000000000000000000000000000000000000000;;				Labels: labels,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  "pause",
0000000000000000000000000000000000000000;;						Image: GetPauseImageName(c),
0000000000000000000000000000000000000000;;						Ports: containerPorts,
0000000000000000000000000000000000000000;;						// Add a dummy environment variable to work around a docker issue.
0000000000000000000000000000000000000000;;						// https://github.com/docker/docker/issues/14203
0000000000000000000000000000000000000000;;						Env: []v1.EnvVar{{Name: "FOO", Value: " "}},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, err := c.Core().Pods(ns).Create(pod)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func DeletePodOrFail(c clientset.Interface, ns, name string) {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Deleting pod %s in namespace %s", name, ns))
0000000000000000000000000000000000000000;;		err := c.Core().Pods(ns).Delete(name, nil)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetSigner returns an ssh.Signer for the provider ("gce", etc.) that can be
0000000000000000000000000000000000000000;;	// used to SSH to their nodes.
0000000000000000000000000000000000000000;;	func GetSigner(provider string) (ssh.Signer, error) {
0000000000000000000000000000000000000000;;		// Get the directory in which SSH keys are located.
0000000000000000000000000000000000000000;;		keydir := filepath.Join(os.Getenv("HOME"), ".ssh")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Select the key itself to use. When implementing more providers here,
0000000000000000000000000000000000000000;;		// please also add them to any SSH tests that are disabled because of signer
0000000000000000000000000000000000000000;;		// support.
0000000000000000000000000000000000000000;;		keyfile := ""
0000000000000000000000000000000000000000;;		key := ""
0000000000000000000000000000000000000000;;		switch provider {
0000000000000000000000000000000000000000;;		case "gce", "gke", "kubemark":
0000000000000000000000000000000000000000;;			keyfile = "google_compute_engine"
0000000000000000000000000000000000000000;;		case "aws":
0000000000000000000000000000000000000000;;			// If there is an env. variable override, use that.
0000000000000000000000000000000000000000;;			aws_keyfile := os.Getenv("AWS_SSH_KEY")
0000000000000000000000000000000000000000;;			if len(aws_keyfile) != 0 {
0000000000000000000000000000000000000000;;				return sshutil.MakePrivateKeySignerFromFile(aws_keyfile)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Otherwise revert to home dir
0000000000000000000000000000000000000000;;			keyfile = "kube_aws_rsa"
0000000000000000000000000000000000000000;;		case "vagrant":
0000000000000000000000000000000000000000;;			keyfile = os.Getenv("VAGRANT_SSH_KEY")
0000000000000000000000000000000000000000;;			if len(keyfile) != 0 {
0000000000000000000000000000000000000000;;				return sshutil.MakePrivateKeySignerFromFile(keyfile)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("VAGRANT_SSH_KEY env variable should be provided")
0000000000000000000000000000000000000000;;		case "local", "vsphere":
0000000000000000000000000000000000000000;;			keyfile = os.Getenv("LOCAL_SSH_KEY") // maybe?
0000000000000000000000000000000000000000;;			if len(keyfile) == 0 {
0000000000000000000000000000000000000000;;				keyfile = "id_rsa"
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("GetSigner(...) not implemented for %s", provider)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(key) == 0 {
0000000000000000000000000000000000000000;;			key = filepath.Join(keydir, keyfile)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return sshutil.MakePrivateKeySignerFromFile(key)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CheckPodsRunningReady returns whether all pods whose names are listed in
0000000000000000000000000000000000000000;;	// podNames in namespace ns are running and ready, using c and waiting at most
0000000000000000000000000000000000000000;;	// timeout.
0000000000000000000000000000000000000000;;	func CheckPodsRunningReady(c clientset.Interface, ns string, podNames []string, timeout time.Duration) bool {
0000000000000000000000000000000000000000;;		return CheckPodsCondition(c, ns, podNames, timeout, testutil.PodRunningReady, "running and ready")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CheckPodsRunningReadyOrSucceeded returns whether all pods whose names are
0000000000000000000000000000000000000000;;	// listed in podNames in namespace ns are running and ready, or succeeded; use
0000000000000000000000000000000000000000;;	// c and waiting at most timeout.
0000000000000000000000000000000000000000;;	func CheckPodsRunningReadyOrSucceeded(c clientset.Interface, ns string, podNames []string, timeout time.Duration) bool {
0000000000000000000000000000000000000000;;		return CheckPodsCondition(c, ns, podNames, timeout, testutil.PodRunningReadyOrSucceeded, "running and ready, or succeeded")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CheckPodsCondition returns whether all pods whose names are listed in podNames
0000000000000000000000000000000000000000;;	// in namespace ns are in the condition, using c and waiting at most timeout.
0000000000000000000000000000000000000000;;	func CheckPodsCondition(c clientset.Interface, ns string, podNames []string, timeout time.Duration, condition podCondition, desc string) bool {
0000000000000000000000000000000000000000;;		np := len(podNames)
0000000000000000000000000000000000000000;;		Logf("Waiting up to %v for %d pods to be %s: %s", timeout, np, desc, podNames)
0000000000000000000000000000000000000000;;		type waitPodResult struct {
0000000000000000000000000000000000000000;;			success bool
0000000000000000000000000000000000000000;;			podName string
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		result := make(chan waitPodResult, len(podNames))
0000000000000000000000000000000000000000;;		for _, podName := range podNames {
0000000000000000000000000000000000000000;;			// Launch off pod readiness checkers.
0000000000000000000000000000000000000000;;			go func(name string) {
0000000000000000000000000000000000000000;;				err := WaitForPodCondition(c, ns, name, desc, timeout, condition)
0000000000000000000000000000000000000000;;				result <- waitPodResult{err == nil, name}
0000000000000000000000000000000000000000;;			}(podName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Wait for them all to finish.
0000000000000000000000000000000000000000;;		success := true
0000000000000000000000000000000000000000;;		for range podNames {
0000000000000000000000000000000000000000;;			res := <-result
0000000000000000000000000000000000000000;;			if !res.success {
0000000000000000000000000000000000000000;;				Logf("Pod %[1]s failed to be %[2]s.", res.podName, desc)
0000000000000000000000000000000000000000;;				success = false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("Wanted all %d pods to be %s. Result: %t. Pods: %v", np, desc, success, podNames)
0000000000000000000000000000000000000000;;		return success
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForNodeToBeReady returns whether node name is ready within timeout.
0000000000000000000000000000000000000000;;	func WaitForNodeToBeReady(c clientset.Interface, name string, timeout time.Duration) bool {
0000000000000000000000000000000000000000;;		return WaitForNodeToBe(c, name, v1.NodeReady, true, timeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForNodeToBeNotReady returns whether node name is not ready (i.e. the
0000000000000000000000000000000000000000;;	// readiness condition is anything but ready, e.g false or unknown) within
0000000000000000000000000000000000000000;;	// timeout.
0000000000000000000000000000000000000000;;	func WaitForNodeToBeNotReady(c clientset.Interface, name string, timeout time.Duration) bool {
0000000000000000000000000000000000000000;;		return WaitForNodeToBe(c, name, v1.NodeReady, false, timeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func isNodeConditionSetAsExpected(node *v1.Node, conditionType v1.NodeConditionType, wantTrue, silent bool) bool {
0000000000000000000000000000000000000000;;		// Check the node readiness condition (logging all).
0000000000000000000000000000000000000000;;		for _, cond := range node.Status.Conditions {
0000000000000000000000000000000000000000;;			// Ensure that the condition type and the status matches as desired.
0000000000000000000000000000000000000000;;			if cond.Type == conditionType {
0000000000000000000000000000000000000000;;				// For NodeReady condition we need to check Taints as well
0000000000000000000000000000000000000000;;				if cond.Type == v1.NodeReady {
0000000000000000000000000000000000000000;;					hasNodeControllerTaints := false
0000000000000000000000000000000000000000;;					// For NodeReady we need to check if Taints are gone as well
0000000000000000000000000000000000000000;;					taints := node.Spec.Taints
0000000000000000000000000000000000000000;;					for _, taint := range taints {
0000000000000000000000000000000000000000;;						if taint.MatchTaint(nodectlr.UnreachableTaintTemplate) || taint.MatchTaint(nodectlr.NotReadyTaintTemplate) {
0000000000000000000000000000000000000000;;							hasNodeControllerTaints = true
0000000000000000000000000000000000000000;;							break
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if wantTrue {
0000000000000000000000000000000000000000;;						if (cond.Status == v1.ConditionTrue) && !hasNodeControllerTaints {
0000000000000000000000000000000000000000;;							return true
0000000000000000000000000000000000000000;;						} else {
0000000000000000000000000000000000000000;;							msg := ""
0000000000000000000000000000000000000000;;							if !hasNodeControllerTaints {
0000000000000000000000000000000000000000;;								msg = fmt.Sprintf("Condition %s of node %s is %v instead of %t. Reason: %v, message: %v",
0000000000000000000000000000000000000000;;									conditionType, node.Name, cond.Status == v1.ConditionTrue, wantTrue, cond.Reason, cond.Message)
0000000000000000000000000000000000000000;;							} else {
0000000000000000000000000000000000000000;;								msg = fmt.Sprintf("Condition %s of node %s is %v, but Node is tainted by NodeController with %v. Failure",
0000000000000000000000000000000000000000;;									conditionType, node.Name, cond.Status == v1.ConditionTrue, taints)
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							if !silent {
0000000000000000000000000000000000000000;;								Logf(msg)
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							return false
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					} else {
0000000000000000000000000000000000000000;;						// TODO: check if the Node is tainted once we enable NC notReady/unreachable taints by default
0000000000000000000000000000000000000000;;						if cond.Status != v1.ConditionTrue {
0000000000000000000000000000000000000000;;							return true
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						if !silent {
0000000000000000000000000000000000000000;;							Logf("Condition %s of node %s is %v instead of %t. Reason: %v, message: %v",
0000000000000000000000000000000000000000;;								conditionType, node.Name, cond.Status == v1.ConditionTrue, wantTrue, cond.Reason, cond.Message)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						return false
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if (wantTrue && (cond.Status == v1.ConditionTrue)) || (!wantTrue && (cond.Status != v1.ConditionTrue)) {
0000000000000000000000000000000000000000;;					return true
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					if !silent {
0000000000000000000000000000000000000000;;						Logf("Condition %s of node %s is %v instead of %t. Reason: %v, message: %v",
0000000000000000000000000000000000000000;;							conditionType, node.Name, cond.Status == v1.ConditionTrue, wantTrue, cond.Reason, cond.Message)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					return false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !silent {
0000000000000000000000000000000000000000;;			Logf("Couldn't find condition %v on node %v", conditionType, node.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func IsNodeConditionSetAsExpected(node *v1.Node, conditionType v1.NodeConditionType, wantTrue bool) bool {
0000000000000000000000000000000000000000;;		return isNodeConditionSetAsExpected(node, conditionType, wantTrue, false)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func IsNodeConditionSetAsExpectedSilent(node *v1.Node, conditionType v1.NodeConditionType, wantTrue bool) bool {
0000000000000000000000000000000000000000;;		return isNodeConditionSetAsExpected(node, conditionType, wantTrue, true)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func IsNodeConditionUnset(node *v1.Node, conditionType v1.NodeConditionType) bool {
0000000000000000000000000000000000000000;;		for _, cond := range node.Status.Conditions {
0000000000000000000000000000000000000000;;			if cond.Type == conditionType {
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForNodeToBe returns whether node "name's" condition state matches wantTrue
0000000000000000000000000000000000000000;;	// within timeout. If wantTrue is true, it will ensure the node condition status
0000000000000000000000000000000000000000;;	// is ConditionTrue; if it's false, it ensures the node condition is in any state
0000000000000000000000000000000000000000;;	// other than ConditionTrue (e.g. not true or unknown).
0000000000000000000000000000000000000000;;	func WaitForNodeToBe(c clientset.Interface, name string, conditionType v1.NodeConditionType, wantTrue bool, timeout time.Duration) bool {
0000000000000000000000000000000000000000;;		Logf("Waiting up to %v for node %s condition %s to be %t", timeout, name, conditionType, wantTrue)
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(Poll) {
0000000000000000000000000000000000000000;;			node, err := c.Core().Nodes().Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Couldn't get node %s", name)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if IsNodeConditionSetAsExpected(node, conditionType, wantTrue) {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("Node %s didn't reach desired %s condition status (%t) within %v", name, conditionType, wantTrue, timeout)
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Checks whether not-ready nodes can be ignored while checking if all nodes are
0000000000000000000000000000000000000000;;	// ready (we allow e.g. for incorrect provisioning of some small percentage of nodes
0000000000000000000000000000000000000000;;	// while validating cluster, and those nodes may never become healthy).
0000000000000000000000000000000000000000;;	// Currently we allow only for:
0000000000000000000000000000000000000000;;	// - not present CNI plugins on node
0000000000000000000000000000000000000000;;	// TODO: we should extend it for other reasons.
0000000000000000000000000000000000000000;;	func allowedNotReadyReasons(nodes []*v1.Node) bool {
0000000000000000000000000000000000000000;;		for _, node := range nodes {
0000000000000000000000000000000000000000;;			index, condition := nodeutil.GetNodeCondition(&node.Status, v1.NodeReady)
0000000000000000000000000000000000000000;;			if index == -1 ||
0000000000000000000000000000000000000000;;				!strings.Contains(condition.Message, "could not locate kubenet required CNI plugins") {
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Checks whether all registered nodes are ready.
0000000000000000000000000000000000000000;;	// TODO: we should change the AllNodesReady call in AfterEach to WaitForAllNodesHealthy,
0000000000000000000000000000000000000000;;	// and figure out how to do it in a configurable way, as we can't expect all setups to run
0000000000000000000000000000000000000000;;	// default test add-ons.
0000000000000000000000000000000000000000;;	func AllNodesReady(c clientset.Interface, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		Logf("Waiting up to %v for all (but %d) nodes to be ready", timeout, TestContext.AllowedNotReadyNodes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var notReady []*v1.Node
0000000000000000000000000000000000000000;;		err := wait.PollImmediate(Poll, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			notReady = nil
0000000000000000000000000000000000000000;;			// It should be OK to list unschedulable Nodes here.
0000000000000000000000000000000000000000;;			nodes, err := c.Core().Nodes().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for i := range nodes.Items {
0000000000000000000000000000000000000000;;				node := &nodes.Items[i]
0000000000000000000000000000000000000000;;				if !IsNodeConditionSetAsExpected(node, v1.NodeReady, true) {
0000000000000000000000000000000000000000;;					notReady = append(notReady, node)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Framework allows for <TestContext.AllowedNotReadyNodes> nodes to be non-ready,
0000000000000000000000000000000000000000;;			// to make it possible e.g. for incorrect deployment of some small percentage
0000000000000000000000000000000000000000;;			// of nodes (which we allow in cluster validation). Some nodes that are not
0000000000000000000000000000000000000000;;			// provisioned correctly at startup will never become ready (e.g. when something
0000000000000000000000000000000000000000;;			// won't install correctly), so we can't expect them to be ready at any point.
0000000000000000000000000000000000000000;;			//
0000000000000000000000000000000000000000;;			// However, we only allow non-ready nodes with some specific reasons.
0000000000000000000000000000000000000000;;			if len(notReady) > TestContext.AllowedNotReadyNodes {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return allowedNotReadyReasons(notReady), nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err != nil && err != wait.ErrWaitTimeout {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(notReady) > TestContext.AllowedNotReadyNodes || !allowedNotReadyReasons(notReady) {
0000000000000000000000000000000000000000;;			msg := ""
0000000000000000000000000000000000000000;;			for _, node := range notReady {
0000000000000000000000000000000000000000;;				msg = fmt.Sprintf("%s, %s", msg, node.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return fmt.Errorf("Not ready nodes: %#v", msg)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// checks whether all registered nodes are ready and all required Pods are running on them.
0000000000000000000000000000000000000000;;	func WaitForAllNodesHealthy(c clientset.Interface, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		Logf("Waiting up to %v for all nodes to be ready", timeout)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var notReady []v1.Node
0000000000000000000000000000000000000000;;		var missingPodsPerNode map[string][]string
0000000000000000000000000000000000000000;;		err := wait.PollImmediate(Poll, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			notReady = nil
0000000000000000000000000000000000000000;;			// It should be OK to list unschedulable Nodes here.
0000000000000000000000000000000000000000;;			nodes, err := c.Core().Nodes().List(metav1.ListOptions{ResourceVersion: "0"})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, node := range nodes.Items {
0000000000000000000000000000000000000000;;				if !IsNodeConditionSetAsExpected(&node, v1.NodeReady, true) {
0000000000000000000000000000000000000000;;					notReady = append(notReady, node)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pods, err := c.Core().Pods(metav1.NamespaceAll).List(metav1.ListOptions{ResourceVersion: "0"})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			systemPodsPerNode := make(map[string][]string)
0000000000000000000000000000000000000000;;			for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;				if pod.Namespace == metav1.NamespaceSystem && pod.Status.Phase == v1.PodRunning {
0000000000000000000000000000000000000000;;					if pod.Spec.NodeName != "" {
0000000000000000000000000000000000000000;;						systemPodsPerNode[pod.Spec.NodeName] = append(systemPodsPerNode[pod.Spec.NodeName], pod.Name)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			missingPodsPerNode = make(map[string][]string)
0000000000000000000000000000000000000000;;			for _, node := range nodes.Items {
0000000000000000000000000000000000000000;;				if !system.IsMasterNode(node.Name) {
0000000000000000000000000000000000000000;;					for _, requiredPod := range requiredPerNodePods {
0000000000000000000000000000000000000000;;						foundRequired := false
0000000000000000000000000000000000000000;;						for _, presentPod := range systemPodsPerNode[node.Name] {
0000000000000000000000000000000000000000;;							if requiredPod.MatchString(presentPod) {
0000000000000000000000000000000000000000;;								foundRequired = true
0000000000000000000000000000000000000000;;								break
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						if !foundRequired {
0000000000000000000000000000000000000000;;							missingPodsPerNode[node.Name] = append(missingPodsPerNode[node.Name], requiredPod.String())
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return len(notReady) == 0 && len(missingPodsPerNode) == 0, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err != nil && err != wait.ErrWaitTimeout {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(notReady) > 0 {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Not ready nodes: %v", notReady)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(missingPodsPerNode) > 0 {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Not running system Pods: %v", missingPodsPerNode)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Filters nodes in NodeList in place, removing nodes that do not
0000000000000000000000000000000000000000;;	// satisfy the given condition
0000000000000000000000000000000000000000;;	// TODO: consider merging with pkg/client/cache.NodeLister
0000000000000000000000000000000000000000;;	func FilterNodes(nodeList *v1.NodeList, fn func(node v1.Node) bool) {
0000000000000000000000000000000000000000;;		var l []v1.Node
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, node := range nodeList.Items {
0000000000000000000000000000000000000000;;			if fn(node) {
0000000000000000000000000000000000000000;;				l = append(l, node)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		nodeList.Items = l
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ParseKVLines parses output that looks like lines containing "<key>: <val>"
0000000000000000000000000000000000000000;;	// and returns <val> if <key> is found. Otherwise, it returns the empty string.
0000000000000000000000000000000000000000;;	func ParseKVLines(output, key string) string {
0000000000000000000000000000000000000000;;		delim := ":"
0000000000000000000000000000000000000000;;		key = key + delim
0000000000000000000000000000000000000000;;		for _, line := range strings.Split(output, "\n") {
0000000000000000000000000000000000000000;;			pieces := strings.SplitAfterN(line, delim, 2)
0000000000000000000000000000000000000000;;			if len(pieces) != 2 {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			k, v := pieces[0], pieces[1]
0000000000000000000000000000000000000000;;			if k == key {
0000000000000000000000000000000000000000;;				return strings.TrimSpace(v)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return ""
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func RestartKubeProxy(host string) error {
0000000000000000000000000000000000000000;;		// TODO: Make it work for all providers.
0000000000000000000000000000000000000000;;		if !ProviderIs("gce", "gke", "aws") {
0000000000000000000000000000000000000000;;			return fmt.Errorf("unsupported provider: %s", TestContext.Provider)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// kubelet will restart the kube-proxy since it's running in a static pod
0000000000000000000000000000000000000000;;		Logf("Killing kube-proxy on node %v", host)
0000000000000000000000000000000000000000;;		result, err := SSH("sudo pkill kube-proxy", host, TestContext.Provider)
0000000000000000000000000000000000000000;;		if err != nil || result.Code != 0 {
0000000000000000000000000000000000000000;;			LogSSHResult(result)
0000000000000000000000000000000000000000;;			return fmt.Errorf("couldn't restart kube-proxy: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// wait for kube-proxy to come back up
0000000000000000000000000000000000000000;;		sshCmd := "sudo /bin/sh -c 'pgrep kube-proxy | wc -l'"
0000000000000000000000000000000000000000;;		err = wait.Poll(5*time.Second, 60*time.Second, func() (bool, error) {
0000000000000000000000000000000000000000;;			Logf("Waiting for kubeproxy to come back up with %v on %v", sshCmd, host)
0000000000000000000000000000000000000000;;			result, err := SSH(sshCmd, host, TestContext.Provider)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if result.Code != 0 {
0000000000000000000000000000000000000000;;				LogSSHResult(result)
0000000000000000000000000000000000000000;;				return false, fmt.Errorf("failed to run command, exited %d", result.Code)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if result.Stdout == "0\n" {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("kube-proxy is back up.")
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("kube-proxy didn't recover: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func RestartApiserver(c discovery.ServerVersionInterface) error {
0000000000000000000000000000000000000000;;		// TODO: Make it work for all providers.
0000000000000000000000000000000000000000;;		if !ProviderIs("gce", "gke", "aws") {
0000000000000000000000000000000000000000;;			return fmt.Errorf("unsupported provider: %s", TestContext.Provider)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if ProviderIs("gce", "aws") {
0000000000000000000000000000000000000000;;			return sshRestartMaster()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// GKE doesn't allow ssh access, so use a same-version master
0000000000000000000000000000000000000000;;		// upgrade to teardown/recreate master.
0000000000000000000000000000000000000000;;		v, err := c.ServerVersion()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return masterUpgradeGKE(v.GitVersion[1:]) // strip leading 'v'
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func sshRestartMaster() error {
0000000000000000000000000000000000000000;;		if !ProviderIs("gce", "aws") {
0000000000000000000000000000000000000000;;			return fmt.Errorf("unsupported provider: %s", TestContext.Provider)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var command string
0000000000000000000000000000000000000000;;		if ProviderIs("gce") {
0000000000000000000000000000000000000000;;			// `kube-apiserver_kube-apiserver` matches the name of the apiserver
0000000000000000000000000000000000000000;;			// container.
0000000000000000000000000000000000000000;;			command = "sudo docker ps | grep kube-apiserver_kube-apiserver | cut -d ' ' -f 1 | xargs sudo docker kill"
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			command = "sudo /etc/init.d/kube-apiserver restart"
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Logf("Restarting master via ssh, running: %v", command)
0000000000000000000000000000000000000000;;		result, err := SSH(command, net.JoinHostPort(GetMasterHost(), sshPort), TestContext.Provider)
0000000000000000000000000000000000000000;;		if err != nil || result.Code != 0 {
0000000000000000000000000000000000000000;;			LogSSHResult(result)
0000000000000000000000000000000000000000;;			return fmt.Errorf("couldn't restart apiserver: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func WaitForApiserverUp(c clientset.Interface) error {
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < time.Minute; time.Sleep(5 * time.Second) {
0000000000000000000000000000000000000000;;			body, err := c.Core().RESTClient().Get().AbsPath("/healthz").Do().Raw()
0000000000000000000000000000000000000000;;			if err == nil && string(body) == "ok" {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("waiting for apiserver timed out")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForClusterSize waits until the cluster has desired size and there is no not-ready nodes in it.
0000000000000000000000000000000000000000;;	// By cluster size we mean number of Nodes excluding Master Node.
0000000000000000000000000000000000000000;;	func WaitForClusterSize(c clientset.Interface, size int, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(20 * time.Second) {
0000000000000000000000000000000000000000;;			nodes, err := c.Core().Nodes().List(metav1.ListOptions{FieldSelector: fields.Set{
0000000000000000000000000000000000000000;;				"spec.unschedulable": "false",
0000000000000000000000000000000000000000;;			}.AsSelector().String()})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Failed to list nodes: %v", err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			numNodes := len(nodes.Items)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Filter out not-ready nodes.
0000000000000000000000000000000000000000;;			FilterNodes(nodes, func(node v1.Node) bool {
0000000000000000000000000000000000000000;;				return IsNodeConditionSetAsExpected(&node, v1.NodeReady, true)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			numReady := len(nodes.Items)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if numNodes == size && numReady == size {
0000000000000000000000000000000000000000;;				Logf("Cluster has reached the desired size %d", size)
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("Waiting for cluster size %d, current size %d, not ready nodes %d", size, numNodes, numNodes-numReady)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("timeout waiting %v for cluster size to be %d", timeout, size)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func GenerateMasterRegexp(prefix string) string {
0000000000000000000000000000000000000000;;		return prefix + "(-...)?"
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// waitForMasters waits until the cluster has the desired number of ready masters in it.
0000000000000000000000000000000000000000;;	func WaitForMasters(masterPrefix string, c clientset.Interface, size int, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(20 * time.Second) {
0000000000000000000000000000000000000000;;			nodes, err := c.Core().Nodes().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Failed to list nodes: %v", err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Filter out nodes that are not master replicas
0000000000000000000000000000000000000000;;			FilterNodes(nodes, func(node v1.Node) bool {
0000000000000000000000000000000000000000;;				res, err := regexp.Match(GenerateMasterRegexp(masterPrefix), ([]byte)(node.Name))
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					Logf("Failed to match regexp to node name: %v", err)
0000000000000000000000000000000000000000;;					return false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return res
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			numNodes := len(nodes.Items)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Filter out not-ready nodes.
0000000000000000000000000000000000000000;;			FilterNodes(nodes, func(node v1.Node) bool {
0000000000000000000000000000000000000000;;				return IsNodeConditionSetAsExpected(&node, v1.NodeReady, true)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			numReady := len(nodes.Items)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if numNodes == size && numReady == size {
0000000000000000000000000000000000000000;;				Logf("Cluster has reached the desired number of masters %d", size)
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Logf("Waiting for the number of masters %d, current %d, not ready master nodes %d", size, numNodes, numNodes-numReady)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("timeout waiting %v for the number of masters to be %d", timeout, size)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetHostExternalAddress gets the node for a pod and returns the first External
0000000000000000000000000000000000000000;;	// address. Returns an error if the node the pod is on doesn't have an External
0000000000000000000000000000000000000000;;	// address.
0000000000000000000000000000000000000000;;	func GetHostExternalAddress(client clientset.Interface, p *v1.Pod) (externalAddress string, err error) {
0000000000000000000000000000000000000000;;		node, err := client.Core().Nodes().Get(p.Spec.NodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, address := range node.Status.Addresses {
0000000000000000000000000000000000000000;;			if address.Type == v1.NodeExternalIP {
0000000000000000000000000000000000000000;;				if address.Address != "" {
0000000000000000000000000000000000000000;;					externalAddress = address.Address
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if externalAddress == "" {
0000000000000000000000000000000000000000;;			err = fmt.Errorf("No external address for pod %v on node %v",
0000000000000000000000000000000000000000;;				p.Name, p.Spec.NodeName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type extractRT struct {
0000000000000000000000000000000000000000;;		http.Header
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (rt *extractRT) RoundTrip(req *http.Request) (*http.Response, error) {
0000000000000000000000000000000000000000;;		rt.Header = req.Header
0000000000000000000000000000000000000000;;		return &http.Response{}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// headersForConfig extracts any http client logic necessary for the provided
0000000000000000000000000000000000000000;;	// config.
0000000000000000000000000000000000000000;;	func headersForConfig(c *restclient.Config) (http.Header, error) {
0000000000000000000000000000000000000000;;		extract := &extractRT{}
0000000000000000000000000000000000000000;;		rt, err := restclient.HTTPWrappersForConfig(c, extract)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if _, err := rt.RoundTrip(&http.Request{}); err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return extract.Header, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// OpenWebSocketForURL constructs a websocket connection to the provided URL, using the client
0000000000000000000000000000000000000000;;	// config, with the specified protocols.
0000000000000000000000000000000000000000;;	func OpenWebSocketForURL(url *url.URL, config *restclient.Config, protocols []string) (*websocket.Conn, error) {
0000000000000000000000000000000000000000;;		tlsConfig, err := restclient.TLSConfigFor(config)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed to create tls config: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if tlsConfig != nil {
0000000000000000000000000000000000000000;;			url.Scheme = "wss"
0000000000000000000000000000000000000000;;			if !strings.Contains(url.Host, ":") {
0000000000000000000000000000000000000000;;				url.Host += ":443"
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			url.Scheme = "ws"
0000000000000000000000000000000000000000;;			if !strings.Contains(url.Host, ":") {
0000000000000000000000000000000000000000;;				url.Host += ":80"
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		headers, err := headersForConfig(config)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed to load http headers: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		cfg, err := websocket.NewConfig(url.String(), "http://localhost")
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed to create websocket config: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		cfg.Header = headers
0000000000000000000000000000000000000000;;		cfg.TlsConfig = tlsConfig
0000000000000000000000000000000000000000;;		cfg.Protocol = protocols
0000000000000000000000000000000000000000;;		return websocket.DialConfig(cfg)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getIngressAddress returns the ips/hostnames associated with the Ingress.
0000000000000000000000000000000000000000;;	func getIngressAddress(client clientset.Interface, ns, name string) ([]string, error) {
0000000000000000000000000000000000000000;;		ing, err := client.Extensions().Ingresses(ns).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		addresses := []string{}
0000000000000000000000000000000000000000;;		for _, a := range ing.Status.LoadBalancer.Ingress {
0000000000000000000000000000000000000000;;			if a.IP != "" {
0000000000000000000000000000000000000000;;				addresses = append(addresses, a.IP)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if a.Hostname != "" {
0000000000000000000000000000000000000000;;				addresses = append(addresses, a.Hostname)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return addresses, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForIngressAddress waits for the Ingress to acquire an address.
0000000000000000000000000000000000000000;;	func WaitForIngressAddress(c clientset.Interface, ns, ingName string, timeout time.Duration) (string, error) {
0000000000000000000000000000000000000000;;		var address string
0000000000000000000000000000000000000000;;		err := wait.PollImmediate(10*time.Second, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			ipOrNameList, err := getIngressAddress(c, ns, ingName)
0000000000000000000000000000000000000000;;			if err != nil || len(ipOrNameList) == 0 {
0000000000000000000000000000000000000000;;				Logf("Waiting for Ingress %v to acquire IP, error %v", ingName, err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			address = ipOrNameList[0]
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		return address, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Looks for the given string in the log of a specific pod container
0000000000000000000000000000000000000000;;	func LookForStringInLog(ns, podName, container, expectedString string, timeout time.Duration) (result string, err error) {
0000000000000000000000000000000000000000;;		return LookForString(expectedString, timeout, func() string {
0000000000000000000000000000000000000000;;			return RunKubectlOrDie("logs", podName, container, fmt.Sprintf("--namespace=%v", ns))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Looks for the given string in a file in a specific pod container
0000000000000000000000000000000000000000;;	func LookForStringInFile(ns, podName, container, file, expectedString string, timeout time.Duration) (result string, err error) {
0000000000000000000000000000000000000000;;		return LookForString(expectedString, timeout, func() string {
0000000000000000000000000000000000000000;;			return RunKubectlOrDie("exec", podName, "-c", container, fmt.Sprintf("--namespace=%v", ns), "--", "cat", file)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Looks for the given string in the output of a command executed in a specific pod container
0000000000000000000000000000000000000000;;	func LookForStringInPodExec(ns, podName string, command []string, expectedString string, timeout time.Duration) (result string, err error) {
0000000000000000000000000000000000000000;;		return LookForString(expectedString, timeout, func() string {
0000000000000000000000000000000000000000;;			// use the first container
0000000000000000000000000000000000000000;;			args := []string{"exec", podName, fmt.Sprintf("--namespace=%v", ns), "--"}
0000000000000000000000000000000000000000;;			args = append(args, command...)
0000000000000000000000000000000000000000;;			return RunKubectlOrDie(args...)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Looks for the given string in the output of fn, repeatedly calling fn until
0000000000000000000000000000000000000000;;	// the timeout is reached or the string is found. Returns last log and possibly
0000000000000000000000000000000000000000;;	// error if the string was not found.
0000000000000000000000000000000000000000;;	func LookForString(expectedString string, timeout time.Duration, fn func() string) (result string, err error) {
0000000000000000000000000000000000000000;;		for t := time.Now(); time.Since(t) < timeout; time.Sleep(Poll) {
0000000000000000000000000000000000000000;;			result = fn()
0000000000000000000000000000000000000000;;			if strings.Contains(result, expectedString) {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err = fmt.Errorf("Failed to find \"%s\", last result: \"%s\"", expectedString, result)
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getSvcNodePort returns the node port for the given service:port.
0000000000000000000000000000000000000000;;	func getSvcNodePort(client clientset.Interface, ns, name string, svcPort int) (int, error) {
0000000000000000000000000000000000000000;;		svc, err := client.Core().Services(ns).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return 0, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, p := range svc.Spec.Ports {
0000000000000000000000000000000000000000;;			if p.Port == int32(svcPort) {
0000000000000000000000000000000000000000;;				if p.NodePort != 0 {
0000000000000000000000000000000000000000;;					return int(p.NodePort), nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return 0, fmt.Errorf(
0000000000000000000000000000000000000000;;			"No node port found for service %v, port %v", name, svcPort)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetNodePortURL returns the url to a nodeport Service.
0000000000000000000000000000000000000000;;	func GetNodePortURL(client clientset.Interface, ns, name string, svcPort int) (string, error) {
0000000000000000000000000000000000000000;;		nodePort, err := getSvcNodePort(client, ns, name, svcPort)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// This list of nodes must not include the master, which is marked
0000000000000000000000000000000000000000;;		// unschedulable, since the master doesn't run kube-proxy. Without
0000000000000000000000000000000000000000;;		// kube-proxy NodePorts won't work.
0000000000000000000000000000000000000000;;		var nodes *v1.NodeList
0000000000000000000000000000000000000000;;		if wait.PollImmediate(Poll, SingleCallTimeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			nodes, err = client.Core().Nodes().List(metav1.ListOptions{FieldSelector: fields.Set{
0000000000000000000000000000000000000000;;				"spec.unschedulable": "false",
0000000000000000000000000000000000000000;;			}.AsSelector().String()})
0000000000000000000000000000000000000000;;			return err == nil, nil
0000000000000000000000000000000000000000;;		}) != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(nodes.Items) == 0 {
0000000000000000000000000000000000000000;;			return "", fmt.Errorf("Unable to list nodes in cluster.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, node := range nodes.Items {
0000000000000000000000000000000000000000;;			for _, address := range node.Status.Addresses {
0000000000000000000000000000000000000000;;				if address.Type == v1.NodeExternalIP {
0000000000000000000000000000000000000000;;					if address.Address != "" {
0000000000000000000000000000000000000000;;						return fmt.Sprintf("http://%v:%v", address.Address, nodePort), nil
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return "", fmt.Errorf("Failed to find external address for service %v", name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO(random-liu): Change this to be a member function of the framework.
0000000000000000000000000000000000000000;;	func GetPodLogs(c clientset.Interface, namespace, podName, containerName string) (string, error) {
0000000000000000000000000000000000000000;;		return getPodLogsInternal(c, namespace, podName, containerName, false)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getPreviousPodLogs(c clientset.Interface, namespace, podName, containerName string) (string, error) {
0000000000000000000000000000000000000000;;		return getPodLogsInternal(c, namespace, podName, containerName, true)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// utility function for gomega Eventually
0000000000000000000000000000000000000000;;	func getPodLogsInternal(c clientset.Interface, namespace, podName, containerName string, previous bool) (string, error) {
0000000000000000000000000000000000000000;;		logs, err := c.Core().RESTClient().Get().
0000000000000000000000000000000000000000;;			Resource("pods").
0000000000000000000000000000000000000000;;			Namespace(namespace).
0000000000000000000000000000000000000000;;			Name(podName).SubResource("log").
0000000000000000000000000000000000000000;;			Param("container", containerName).
0000000000000000000000000000000000000000;;			Param("previous", strconv.FormatBool(previous)).
0000000000000000000000000000000000000000;;			Do().
0000000000000000000000000000000000000000;;			Raw()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err == nil && strings.Contains(string(logs), "Internal Error") {
0000000000000000000000000000000000000000;;			return "", fmt.Errorf("Fetched log contains \"Internal Error\": %q.", string(logs))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return string(logs), err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func GetGCECloud() (*gcecloud.GCECloud, error) {
0000000000000000000000000000000000000000;;		gceCloud, ok := TestContext.CloudConfig.Provider.(*gcecloud.GCECloud)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed to convert CloudConfig.Provider to GCECloud: %#v", TestContext.CloudConfig.Provider)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return gceCloud, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// EnsureLoadBalancerResourcesDeleted ensures that cloud load balancer resources that were created
0000000000000000000000000000000000000000;;	// are actually cleaned up.  Currently only implemented for GCE/GKE.
0000000000000000000000000000000000000000;;	func EnsureLoadBalancerResourcesDeleted(ip, portRange string) error {
0000000000000000000000000000000000000000;;		if TestContext.Provider == "gce" || TestContext.Provider == "gke" {
0000000000000000000000000000000000000000;;			return ensureGCELoadBalancerResourcesDeleted(ip, portRange)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ensureGCELoadBalancerResourcesDeleted(ip, portRange string) error {
0000000000000000000000000000000000000000;;		gceCloud, err := GetGCECloud()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		project := TestContext.CloudConfig.ProjectID
0000000000000000000000000000000000000000;;		region, err := gcecloud.GetGCERegion(TestContext.CloudConfig.Zone)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("could not get region for zone %q: %v", TestContext.CloudConfig.Zone, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return wait.Poll(10*time.Second, 5*time.Minute, func() (bool, error) {
0000000000000000000000000000000000000000;;			service := gceCloud.GetComputeService()
0000000000000000000000000000000000000000;;			list, err := service.ForwardingRules.List(project, region).Do()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, item := range list.Items {
0000000000000000000000000000000000000000;;				if item.PortRange == portRange && item.IPAddress == ip {
0000000000000000000000000000000000000000;;					Logf("found a load balancer: %v", item)
0000000000000000000000000000000000000000;;					return false, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// The following helper functions can block/unblock network from source
0000000000000000000000000000000000000000;;	// host to destination host by manipulating iptable rules.
0000000000000000000000000000000000000000;;	// This function assumes it can ssh to the source host.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Caution:
0000000000000000000000000000000000000000;;	// Recommend to input IP instead of hostnames. Using hostnames will cause iptables to
0000000000000000000000000000000000000000;;	// do a DNS lookup to resolve the name to an IP address, which will
0000000000000000000000000000000000000000;;	// slow down the test and cause it to fail if DNS is absent or broken.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Suggested usage pattern:
0000000000000000000000000000000000000000;;	// func foo() {
0000000000000000000000000000000000000000;;	//	...
0000000000000000000000000000000000000000;;	//	defer UnblockNetwork(from, to)
0000000000000000000000000000000000000000;;	//	BlockNetwork(from, to)
0000000000000000000000000000000000000000;;	//	...
0000000000000000000000000000000000000000;;	// }
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	func BlockNetwork(from string, to string) {
0000000000000000000000000000000000000000;;		Logf("block network traffic from %s to %s", from, to)
0000000000000000000000000000000000000000;;		iptablesRule := fmt.Sprintf("OUTPUT --destination %s --jump REJECT", to)
0000000000000000000000000000000000000000;;		dropCmd := fmt.Sprintf("sudo iptables --insert %s", iptablesRule)
0000000000000000000000000000000000000000;;		if result, err := SSH(dropCmd, from, TestContext.Provider); result.Code != 0 || err != nil {
0000000000000000000000000000000000000000;;			LogSSHResult(result)
0000000000000000000000000000000000000000;;			Failf("Unexpected error: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func UnblockNetwork(from string, to string) {
0000000000000000000000000000000000000000;;		Logf("Unblock network traffic from %s to %s", from, to)
0000000000000000000000000000000000000000;;		iptablesRule := fmt.Sprintf("OUTPUT --destination %s --jump REJECT", to)
0000000000000000000000000000000000000000;;		undropCmd := fmt.Sprintf("sudo iptables --delete %s", iptablesRule)
0000000000000000000000000000000000000000;;		// Undrop command may fail if the rule has never been created.
0000000000000000000000000000000000000000;;		// In such case we just lose 30 seconds, but the cluster is healthy.
0000000000000000000000000000000000000000;;		// But if the rule had been created and removing it failed, the node is broken and
0000000000000000000000000000000000000000;;		// not coming back. Subsequent tests will run or fewer nodes (some of the tests
0000000000000000000000000000000000000000;;		// may fail). Manual intervention is required in such case (recreating the
0000000000000000000000000000000000000000;;		// cluster solves the problem too).
0000000000000000000000000000000000000000;;		err := wait.Poll(time.Millisecond*100, time.Second*30, func() (bool, error) {
0000000000000000000000000000000000000000;;			result, err := SSH(undropCmd, from, TestContext.Provider)
0000000000000000000000000000000000000000;;			if result.Code == 0 && err == nil {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			LogSSHResult(result)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Unexpected error: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Failf("Failed to remove the iptable REJECT rule. Manual intervention is "+
0000000000000000000000000000000000000000;;				"required on host %s: remove rule %s, if exists", from, iptablesRule)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func isElementOf(podUID types.UID, pods *v1.PodList) bool {
0000000000000000000000000000000000000000;;		for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;			if pod.UID == podUID {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func CheckRSHashLabel(rs *extensions.ReplicaSet) error {
0000000000000000000000000000000000000000;;		if len(rs.Labels[extensions.DefaultDeploymentUniqueLabelKey]) == 0 ||
0000000000000000000000000000000000000000;;			len(rs.Spec.Selector.MatchLabels[extensions.DefaultDeploymentUniqueLabelKey]) == 0 ||
0000000000000000000000000000000000000000;;			len(rs.Spec.Template.Labels[extensions.DefaultDeploymentUniqueLabelKey]) == 0 {
0000000000000000000000000000000000000000;;			return fmt.Errorf("unexpected RS missing required pod-hash-template: %+v, selector = %+v, template = %+v", rs, rs.Spec.Selector, rs.Spec.Template)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func CheckPodHashLabel(pods *v1.PodList) error {
0000000000000000000000000000000000000000;;		invalidPod := ""
0000000000000000000000000000000000000000;;		for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;			if len(pod.Labels[extensions.DefaultDeploymentUniqueLabelKey]) == 0 {
0000000000000000000000000000000000000000;;				if len(invalidPod) == 0 {
0000000000000000000000000000000000000000;;					invalidPod = "unexpected pods missing required pod-hash-template:"
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				invalidPod = fmt.Sprintf("%s %+v;", invalidPod, pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(invalidPod) > 0 {
0000000000000000000000000000000000000000;;			return fmt.Errorf("%s", invalidPod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// timeout for proxy requests.
0000000000000000000000000000000000000000;;	const proxyTimeout = 2 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NodeProxyRequest performs a get on a node proxy endpoint given the nodename and rest client.
0000000000000000000000000000000000000000;;	func NodeProxyRequest(c clientset.Interface, node, endpoint string) (restclient.Result, error) {
0000000000000000000000000000000000000000;;		// proxy tends to hang in some cases when Node is not ready. Add an artificial timeout for this call.
0000000000000000000000000000000000000000;;		// This will leak a goroutine if proxy hangs. #22165
0000000000000000000000000000000000000000;;		subResourceProxyAvailable, err := ServerVersionGTE(SubResourceServiceAndNodeProxyVersion, c.Discovery())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return restclient.Result{}, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var result restclient.Result
0000000000000000000000000000000000000000;;		finished := make(chan struct{})
0000000000000000000000000000000000000000;;		go func() {
0000000000000000000000000000000000000000;;			if subResourceProxyAvailable {
0000000000000000000000000000000000000000;;				result = c.Core().RESTClient().Get().
0000000000000000000000000000000000000000;;					Resource("nodes").
0000000000000000000000000000000000000000;;					SubResource("proxy").
0000000000000000000000000000000000000000;;					Name(fmt.Sprintf("%v:%v", node, ports.KubeletPort)).
0000000000000000000000000000000000000000;;					Suffix(endpoint).
0000000000000000000000000000000000000000;;					Do()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				result = c.Core().RESTClient().Get().
0000000000000000000000000000000000000000;;					Prefix("proxy").
0000000000000000000000000000000000000000;;					Resource("nodes").
0000000000000000000000000000000000000000;;					Name(fmt.Sprintf("%v:%v", node, ports.KubeletPort)).
0000000000000000000000000000000000000000;;					Suffix(endpoint).
0000000000000000000000000000000000000000;;					Do()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			finished <- struct{}{}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;		select {
0000000000000000000000000000000000000000;;		case <-finished:
0000000000000000000000000000000000000000;;			return result, nil
0000000000000000000000000000000000000000;;		case <-time.After(proxyTimeout):
0000000000000000000000000000000000000000;;			return restclient.Result{}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetKubeletPods retrieves the list of pods on the kubelet
0000000000000000000000000000000000000000;;	func GetKubeletPods(c clientset.Interface, node string) (*v1.PodList, error) {
0000000000000000000000000000000000000000;;		return getKubeletPods(c, node, "pods")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetKubeletRunningPods retrieves the list of running pods on the kubelet. The pods
0000000000000000000000000000000000000000;;	// includes necessary information (e.g., UID, name, namespace for
0000000000000000000000000000000000000000;;	// pods/containers), but do not contain the full spec.
0000000000000000000000000000000000000000;;	func GetKubeletRunningPods(c clientset.Interface, node string) (*v1.PodList, error) {
0000000000000000000000000000000000000000;;		return getKubeletPods(c, node, "runningpods")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getKubeletPods(c clientset.Interface, node, resource string) (*v1.PodList, error) {
0000000000000000000000000000000000000000;;		result := &v1.PodList{}
0000000000000000000000000000000000000000;;		client, err := NodeProxyRequest(c, node, resource)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return &v1.PodList{}, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err = client.Into(result); err != nil {
0000000000000000000000000000000000000000;;			return &v1.PodList{}, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// LaunchWebserverPod launches a pod serving http on port 8080 to act
0000000000000000000000000000000000000000;;	// as the target for networking connectivity checks.  The ip address
0000000000000000000000000000000000000000;;	// of the created pod will be returned if the pod is launched
0000000000000000000000000000000000000000;;	// successfully.
0000000000000000000000000000000000000000;;	func LaunchWebserverPod(f *Framework, podName, nodeName string) (ip string) {
0000000000000000000000000000000000000000;;		containerName := fmt.Sprintf("%s-container", podName)
0000000000000000000000000000000000000000;;		port := 8080
0000000000000000000000000000000000000000;;		pod := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name: podName,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  containerName,
0000000000000000000000000000000000000000;;						Image: "gcr.io/google_containers/porter:4524579c0eb935c056c8e75563b4e1eda31587e0",
0000000000000000000000000000000000000000;;						Env:   []v1.EnvVar{{Name: fmt.Sprintf("SERVE_PORT_%d", port), Value: "foo"}},
0000000000000000000000000000000000000000;;						Ports: []v1.ContainerPort{{ContainerPort: int32(port)}},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				NodeName:      nodeName,
0000000000000000000000000000000000000000;;				RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podClient := f.ClientSet.Core().Pods(f.Namespace.Name)
0000000000000000000000000000000000000000;;		_, err := podClient.Create(pod)
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;		ExpectNoError(f.WaitForPodRunning(podName))
0000000000000000000000000000000000000000;;		createdPod, err := podClient.Get(podName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;		ip = fmt.Sprintf("%s:%d", createdPod.Status.PodIP, port)
0000000000000000000000000000000000000000;;		Logf("Target pod IP:port is %s", ip)
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CheckConnectivityToHost launches a pod to test connectivity to the specified
0000000000000000000000000000000000000000;;	// host. An error will be returned if the host is not reachable from the pod.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// An empty nodeName will use the schedule to choose where the pod is executed.
0000000000000000000000000000000000000000;;	func CheckConnectivityToHost(f *Framework, nodeName, podName, host string, timeout int) error {
0000000000000000000000000000000000000000;;		contName := fmt.Sprintf("%s-container", podName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		command := []string{
0000000000000000000000000000000000000000;;			"ping",
0000000000000000000000000000000000000000;;			"-c", "3", // send 3 pings
0000000000000000000000000000000000000000;;			"-W", "2", // wait at most 2 seconds for a reply
0000000000000000000000000000000000000000;;			"-w", strconv.Itoa(timeout),
0000000000000000000000000000000000000000;;			host,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pod := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name: podName,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:    contName,
0000000000000000000000000000000000000000;;						Image:   "gcr.io/google_containers/busybox:1.24",
0000000000000000000000000000000000000000;;						Command: command,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				NodeName:      nodeName,
0000000000000000000000000000000000000000;;				RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podClient := f.ClientSet.Core().Pods(f.Namespace.Name)
0000000000000000000000000000000000000000;;		_, err := podClient.Create(pod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err = WaitForPodSuccessInNamespace(f.ClientSet, podName, f.Namespace.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			logs, logErr := GetPodLogs(f.ClientSet, f.Namespace.Name, pod.Name, contName)
0000000000000000000000000000000000000000;;			if logErr != nil {
0000000000000000000000000000000000000000;;				Logf("Warning: Failed to get logs from pod %q: %v", pod.Name, logErr)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				Logf("pod %s/%s logs:\n%s", f.Namespace.Name, pod.Name, logs)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CoreDump SSHs to the master and all nodes and dumps their logs into dir.
0000000000000000000000000000000000000000;;	// It shells out to cluster/log-dump/log-dump.sh to accomplish this.
0000000000000000000000000000000000000000;;	func CoreDump(dir string) {
0000000000000000000000000000000000000000;;		if TestContext.DisableLogDump {
0000000000000000000000000000000000000000;;			Logf("Skipping dumping logs from cluster")
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		cmd := exec.Command(path.Join(TestContext.RepoRoot, "cluster", "log-dump", "log-dump.sh"), dir)
0000000000000000000000000000000000000000;;		cmd.Stdout = os.Stdout
0000000000000000000000000000000000000000;;		cmd.Stderr = os.Stderr
0000000000000000000000000000000000000000;;		if err := cmd.Run(); err != nil {
0000000000000000000000000000000000000000;;			Logf("Error running cluster/log-dump/log-dump.sh: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func UpdatePodWithRetries(client clientset.Interface, ns, name string, update func(*v1.Pod)) (*v1.Pod, error) {
0000000000000000000000000000000000000000;;		for i := 0; i < 3; i++ {
0000000000000000000000000000000000000000;;			pod, err := client.Core().Pods(ns).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("Failed to get pod %q: %v", name, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			update(pod)
0000000000000000000000000000000000000000;;			pod, err = client.Core().Pods(ns).Update(pod)
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				return pod, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if !apierrs.IsConflict(err) && !apierrs.IsServerTimeout(err) {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("Failed to update pod %q: %v", name, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil, fmt.Errorf("Too many retries updating Pod %q", name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func GetPodsInNamespace(c clientset.Interface, ns string, ignoreLabels map[string]string) ([]*v1.Pod, error) {
0000000000000000000000000000000000000000;;		pods, err := c.Core().Pods(ns).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return []*v1.Pod{}, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ignoreSelector := labels.SelectorFromSet(ignoreLabels)
0000000000000000000000000000000000000000;;		filtered := []*v1.Pod{}
0000000000000000000000000000000000000000;;		for _, p := range pods.Items {
0000000000000000000000000000000000000000;;			if len(ignoreLabels) != 0 && ignoreSelector.Matches(labels.Set(p.Labels)) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			filtered = append(filtered, &p)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return filtered, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunCmd runs cmd using args and returns its stdout and stderr. It also outputs
0000000000000000000000000000000000000000;;	// cmd's stdout and stderr to their respective OS streams.
0000000000000000000000000000000000000000;;	func RunCmd(command string, args ...string) (string, string, error) {
0000000000000000000000000000000000000000;;		return RunCmdEnv(nil, command, args...)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunCmdEnv runs cmd with the provided environment and args and
0000000000000000000000000000000000000000;;	// returns its stdout and stderr. It also outputs cmd's stdout and
0000000000000000000000000000000000000000;;	// stderr to their respective OS streams.
0000000000000000000000000000000000000000;;	func RunCmdEnv(env []string, command string, args ...string) (string, string, error) {
0000000000000000000000000000000000000000;;		Logf("Running %s %v", command, args)
0000000000000000000000000000000000000000;;		var bout, berr bytes.Buffer
0000000000000000000000000000000000000000;;		cmd := exec.Command(command, args...)
0000000000000000000000000000000000000000;;		// We also output to the OS stdout/stderr to aid in debugging in case cmd
0000000000000000000000000000000000000000;;		// hangs and never returns before the test gets killed.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// This creates some ugly output because gcloud doesn't always provide
0000000000000000000000000000000000000000;;		// newlines.
0000000000000000000000000000000000000000;;		cmd.Stdout = io.MultiWriter(os.Stdout, &bout)
0000000000000000000000000000000000000000;;		cmd.Stderr = io.MultiWriter(os.Stderr, &berr)
0000000000000000000000000000000000000000;;		cmd.Env = env
0000000000000000000000000000000000000000;;		err := cmd.Run()
0000000000000000000000000000000000000000;;		stdout, stderr := bout.String(), berr.String()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", "", fmt.Errorf("error running %s %v; got error %v, stdout %q, stderr %q",
0000000000000000000000000000000000000000;;				command, args, err, stdout, stderr)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return stdout, stderr, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// retryCmd runs cmd using args and retries it for up to SingleCallTimeout if
0000000000000000000000000000000000000000;;	// it returns an error. It returns stdout and stderr.
0000000000000000000000000000000000000000;;	func retryCmd(command string, args ...string) (string, string, error) {
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		stdout, stderr := "", ""
0000000000000000000000000000000000000000;;		wait.Poll(Poll, SingleCallTimeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			stdout, stderr, err = RunCmd(command, args...)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("Got %v", err)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		return stdout, stderr, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetPodsScheduled returns a number of currently scheduled and not scheduled Pods.
0000000000000000000000000000000000000000;;	func GetPodsScheduled(masterNodes sets.String, pods *v1.PodList) (scheduledPods, notScheduledPods []v1.Pod) {
0000000000000000000000000000000000000000;;		for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;			if !masterNodes.Has(pod.Spec.NodeName) {
0000000000000000000000000000000000000000;;				if pod.Spec.NodeName != "" {
0000000000000000000000000000000000000000;;					_, scheduledCondition := podutil.GetPodCondition(&pod.Status, v1.PodScheduled)
0000000000000000000000000000000000000000;;					Expect(scheduledCondition != nil).To(Equal(true))
0000000000000000000000000000000000000000;;					Expect(scheduledCondition.Status).To(Equal(v1.ConditionTrue))
0000000000000000000000000000000000000000;;					scheduledPods = append(scheduledPods, pod)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					_, scheduledCondition := podutil.GetPodCondition(&pod.Status, v1.PodScheduled)
0000000000000000000000000000000000000000;;					Expect(scheduledCondition != nil).To(Equal(true))
0000000000000000000000000000000000000000;;					Expect(scheduledCondition.Status).To(Equal(v1.ConditionFalse))
0000000000000000000000000000000000000000;;					if scheduledCondition.Reason == "Unschedulable" {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						notScheduledPods = append(notScheduledPods, pod)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForStableCluster waits until all existing pods are scheduled and returns their amount.
0000000000000000000000000000000000000000;;	func WaitForStableCluster(c clientset.Interface, masterNodes sets.String) int {
0000000000000000000000000000000000000000;;		timeout := 10 * time.Minute
0000000000000000000000000000000000000000;;		startTime := time.Now()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		allPods, err := c.Core().Pods(metav1.NamespaceAll).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		ExpectNoError(err)
0000000000000000000000000000000000000000;;		// API server returns also Pods that succeeded. We need to filter them out.
0000000000000000000000000000000000000000;;		currentPods := make([]v1.Pod, 0, len(allPods.Items))
0000000000000000000000000000000000000000;;		for _, pod := range allPods.Items {
0000000000000000000000000000000000000000;;			if pod.Status.Phase != v1.PodSucceeded && pod.Status.Phase != v1.PodFailed {
0000000000000000000000000000000000000000;;				currentPods = append(currentPods, pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		allPods.Items = currentPods
0000000000000000000000000000000000000000;;		scheduledPods, currentlyNotScheduledPods := GetPodsScheduled(masterNodes, allPods)
0000000000000000000000000000000000000000;;		for len(currentlyNotScheduledPods) != 0 {
0000000000000000000000000000000000000000;;			time.Sleep(2 * time.Second)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			allPods, err := c.Core().Pods(metav1.NamespaceAll).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			ExpectNoError(err)
0000000000000000000000000000000000000000;;			scheduledPods, currentlyNotScheduledPods = GetPodsScheduled(masterNodes, allPods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if startTime.Add(timeout).Before(time.Now()) {
0000000000000000000000000000000000000000;;				Failf("Timed out after %v waiting for stable cluster.", timeout)
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return len(scheduledPods)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetMasterAndWorkerNodesOrDie will return a list masters and schedulable worker nodes
0000000000000000000000000000000000000000;;	func GetMasterAndWorkerNodesOrDie(c clientset.Interface) (sets.String, *v1.NodeList) {
0000000000000000000000000000000000000000;;		nodes := &v1.NodeList{}
0000000000000000000000000000000000000000;;		masters := sets.NewString()
0000000000000000000000000000000000000000;;		all, _ := c.Core().Nodes().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		for _, n := range all.Items {
0000000000000000000000000000000000000000;;			if system.IsMasterNode(n.Name) {
0000000000000000000000000000000000000000;;				masters.Insert(n.Name)
0000000000000000000000000000000000000000;;			} else if isNodeSchedulable(&n) && isNodeUntainted(&n) {
0000000000000000000000000000000000000000;;				nodes.Items = append(nodes.Items, n)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return masters, nodes
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ListNamespaceEvents(c clientset.Interface, ns string) error {
0000000000000000000000000000000000000000;;		ls, err := c.Core().Events(ns).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, event := range ls.Items {
0000000000000000000000000000000000000000;;			glog.Infof("Event(%#v): type: '%v' reason: '%v' %v", event.InvolvedObject, event.Type, event.Reason, event.Message)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// E2ETestNodePreparer implements testutil.TestNodePreparer interface, which is used
0000000000000000000000000000000000000000;;	// to create/modify Nodes before running a test.
0000000000000000000000000000000000000000;;	type E2ETestNodePreparer struct {
0000000000000000000000000000000000000000;;		client clientset.Interface
0000000000000000000000000000000000000000;;		// Specifies how many nodes should be modified using the given strategy.
0000000000000000000000000000000000000000;;		// Only one strategy can be applied to a single Node, so there needs to
0000000000000000000000000000000000000000;;		// be at least <sum_of_keys> Nodes in the cluster.
0000000000000000000000000000000000000000;;		countToStrategy       []testutil.CountToStrategy
0000000000000000000000000000000000000000;;		nodeToAppliedStrategy map[string]testutil.PrepareNodeStrategy
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewE2ETestNodePreparer(client clientset.Interface, countToStrategy []testutil.CountToStrategy) testutil.TestNodePreparer {
0000000000000000000000000000000000000000;;		return &E2ETestNodePreparer{
0000000000000000000000000000000000000000;;			client:                client,
0000000000000000000000000000000000000000;;			countToStrategy:       countToStrategy,
0000000000000000000000000000000000000000;;			nodeToAppliedStrategy: make(map[string]testutil.PrepareNodeStrategy),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *E2ETestNodePreparer) PrepareNodes() error {
0000000000000000000000000000000000000000;;		nodes := GetReadySchedulableNodesOrDie(p.client)
0000000000000000000000000000000000000000;;		numTemplates := 0
0000000000000000000000000000000000000000;;		for k := range p.countToStrategy {
0000000000000000000000000000000000000000;;			numTemplates += k
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if numTemplates > len(nodes.Items) {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Can't prepare Nodes. Got more templates than existing Nodes.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		index := 0
0000000000000000000000000000000000000000;;		sum := 0
0000000000000000000000000000000000000000;;		for _, v := range p.countToStrategy {
0000000000000000000000000000000000000000;;			sum += v.Count
0000000000000000000000000000000000000000;;			for ; index < sum; index++ {
0000000000000000000000000000000000000000;;				if err := testutil.DoPrepareNode(p.client, &nodes.Items[index], v.Strategy); err != nil {
0000000000000000000000000000000000000000;;					glog.Errorf("Aborting node preparation: %v", err)
0000000000000000000000000000000000000000;;					return err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				p.nodeToAppliedStrategy[nodes.Items[index].Name] = v.Strategy
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *E2ETestNodePreparer) CleanupNodes() error {
0000000000000000000000000000000000000000;;		var encounteredError error
0000000000000000000000000000000000000000;;		nodes := GetReadySchedulableNodesOrDie(p.client)
0000000000000000000000000000000000000000;;		for i := range nodes.Items {
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			name := nodes.Items[i].Name
0000000000000000000000000000000000000000;;			strategy, found := p.nodeToAppliedStrategy[name]
0000000000000000000000000000000000000000;;			if found {
0000000000000000000000000000000000000000;;				if err = testutil.DoCleanupNode(p.client, name, strategy); err != nil {
0000000000000000000000000000000000000000;;					glog.Errorf("Skipping cleanup of Node: failed update of %v: %v", name, err)
0000000000000000000000000000000000000000;;					encounteredError = err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return encounteredError
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CleanupGCEResources cleans up GCE Service Type=LoadBalancer resources with
0000000000000000000000000000000000000000;;	// the given name. The name is usually the UUID of the Service prefixed with an
0000000000000000000000000000000000000000;;	// alpha-numeric character ('a') to work around cloudprovider rules.
0000000000000000000000000000000000000000;;	func CleanupGCEResources(c clientset.Interface, loadBalancerName, zone string) (retErr error) {
0000000000000000000000000000000000000000;;		gceCloud, err := GetGCECloud()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		region, err := gcecloud.GetGCERegion(zone)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error parsing GCE/GKE region from zone %q: %v", zone, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := gceCloud.DeleteFirewall(loadBalancerName); err != nil &&
0000000000000000000000000000000000000000;;			!IsGoogleAPIHTTPErrorCode(err, http.StatusNotFound) {
0000000000000000000000000000000000000000;;			retErr = err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := gceCloud.DeleteRegionForwardingRule(loadBalancerName, region); err != nil &&
0000000000000000000000000000000000000000;;			!IsGoogleAPIHTTPErrorCode(err, http.StatusNotFound) {
0000000000000000000000000000000000000000;;			retErr = fmt.Errorf("%v\n%v", retErr, err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := gceCloud.DeleteRegionAddress(loadBalancerName, region); err != nil &&
0000000000000000000000000000000000000000;;			!IsGoogleAPIHTTPErrorCode(err, http.StatusNotFound) {
0000000000000000000000000000000000000000;;			retErr = fmt.Errorf("%v\n%v", retErr, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var hcNames []string
0000000000000000000000000000000000000000;;		hc, getErr := gceCloud.GetHttpHealthCheck(loadBalancerName)
0000000000000000000000000000000000000000;;		if getErr != nil && !IsGoogleAPIHTTPErrorCode(getErr, http.StatusNotFound) {
0000000000000000000000000000000000000000;;			retErr = fmt.Errorf("%v\n%v", retErr, getErr)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if hc != nil {
0000000000000000000000000000000000000000;;			hcNames = append(hcNames, hc.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		clusterID, err := GetClusterID(c)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			retErr = fmt.Errorf("%v\n%v", retErr, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := gceCloud.DeleteExternalTargetPoolAndChecks(loadBalancerName, region, clusterID, hcNames...); err != nil &&
0000000000000000000000000000000000000000;;			!IsGoogleAPIHTTPErrorCode(err, http.StatusNotFound) {
0000000000000000000000000000000000000000;;			retErr = fmt.Errorf("%v\n%v", retErr, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// IsHTTPErrorCode returns true if the error is a google api
0000000000000000000000000000000000000000;;	// error matching the corresponding HTTP error code.
0000000000000000000000000000000000000000;;	func IsGoogleAPIHTTPErrorCode(err error, code int) bool {
0000000000000000000000000000000000000000;;		apiErr, ok := err.(*googleapi.Error)
0000000000000000000000000000000000000000;;		return ok && apiErr.Code == code
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getMaster populates the externalIP, internalIP and hostname fields of the master.
0000000000000000000000000000000000000000;;	// If any of these is unavailable, it is set to "".
0000000000000000000000000000000000000000;;	func getMaster(c clientset.Interface) Address {
0000000000000000000000000000000000000000;;		master := Address{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Populate the internal IP.
0000000000000000000000000000000000000000;;		eps, err := c.Core().Endpoints(metav1.NamespaceDefault).Get("kubernetes", metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Failf("Failed to get kubernetes endpoints: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(eps.Subsets) != 1 || len(eps.Subsets[0].Addresses) != 1 {
0000000000000000000000000000000000000000;;			Failf("There are more than 1 endpoints for kubernetes service: %+v", eps)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		master.internalIP = eps.Subsets[0].Addresses[0].IP
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Populate the external IP/hostname.
0000000000000000000000000000000000000000;;		url, err := url.Parse(TestContext.Host)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			Failf("Failed to parse hostname: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if net.ParseIP(url.Host) != nil {
0000000000000000000000000000000000000000;;			// TODO: Check that it is external IP (not having a reserved IP address as per RFC1918).
0000000000000000000000000000000000000000;;			master.externalIP = url.Host
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			master.hostname = url.Host
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return master
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetMasterAddress returns the hostname/external IP/internal IP as appropriate for e2e tests on a particular provider
0000000000000000000000000000000000000000;;	// which is the address of the interface used for communication with the kubelet.
0000000000000000000000000000000000000000;;	func GetMasterAddress(c clientset.Interface) string {
0000000000000000000000000000000000000000;;		master := getMaster(c)
0000000000000000000000000000000000000000;;		switch TestContext.Provider {
0000000000000000000000000000000000000000;;		case "gce", "gke":
0000000000000000000000000000000000000000;;			return master.externalIP
0000000000000000000000000000000000000000;;		case "aws":
0000000000000000000000000000000000000000;;			return awsMasterIP
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			Failf("This test is not supported for provider %s and should be disabled", TestContext.Provider)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return ""
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetNodeExternalIP returns node external IP concatenated with port 22 for ssh
0000000000000000000000000000000000000000;;	// e.g. 1.2.3.4:22
0000000000000000000000000000000000000000;;	func GetNodeExternalIP(node *v1.Node) string {
0000000000000000000000000000000000000000;;		Logf("Getting external IP address for %s", node.Name)
0000000000000000000000000000000000000000;;		host := ""
0000000000000000000000000000000000000000;;		for _, a := range node.Status.Addresses {
0000000000000000000000000000000000000000;;			if a.Type == v1.NodeExternalIP {
0000000000000000000000000000000000000000;;				host = net.JoinHostPort(a.Address, sshPort)
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if host == "" {
0000000000000000000000000000000000000000;;			Failf("Couldn't get the external IP of host %s with addresses %v", node.Name, node.Status.Addresses)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return host
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// SimpleGET executes a get on the given url, returns error if non-200 returned.
0000000000000000000000000000000000000000;;	func SimpleGET(c *http.Client, url, host string) (string, error) {
0000000000000000000000000000000000000000;;		req, err := http.NewRequest("GET", url, nil)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		req.Host = host
0000000000000000000000000000000000000000;;		res, err := c.Do(req)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer res.Body.Close()
0000000000000000000000000000000000000000;;		rawBody, err := ioutil.ReadAll(res.Body)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		body := string(rawBody)
0000000000000000000000000000000000000000;;		if res.StatusCode != http.StatusOK {
0000000000000000000000000000000000000000;;			err = fmt.Errorf(
0000000000000000000000000000000000000000;;				"GET returned http error %v", res.StatusCode)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return body, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PollURL polls till the url responds with a healthy http code. If
0000000000000000000000000000000000000000;;	// expectUnreachable is true, it breaks on first non-healthy http code instead.
0000000000000000000000000000000000000000;;	func PollURL(route, host string, timeout time.Duration, interval time.Duration, httpClient *http.Client, expectUnreachable bool) error {
0000000000000000000000000000000000000000;;		var lastBody string
0000000000000000000000000000000000000000;;		pollErr := wait.PollImmediate(interval, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			lastBody, err = SimpleGET(httpClient, route, host)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				Logf("host %v path %v: %v unreachable", host, route, err)
0000000000000000000000000000000000000000;;				return expectUnreachable, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return !expectUnreachable, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if pollErr != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Failed to execute a successful GET within %v, Last response body for %v, host %v:\n%v\n\n%v\n",
0000000000000000000000000000000000000000;;				timeout, route, host, lastBody, pollErr)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func DescribeIng(ns string) {
0000000000000000000000000000000000000000;;		Logf("\nOutput of kubectl describe ing:\n")
0000000000000000000000000000000000000000;;		desc, _ := RunKubectl(
0000000000000000000000000000000000000000;;			"describe", "ing", fmt.Sprintf("--namespace=%v", ns))
0000000000000000000000000000000000000000;;		Logf(desc)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewTestPod returns a pod that has the specified requests and limits
0000000000000000000000000000000000000000;;	func (f *Framework) NewTestPod(name string, requests v1.ResourceList, limits v1.ResourceList) *v1.Pod {
0000000000000000000000000000000000000000;;		return &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name: name,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  "pause",
0000000000000000000000000000000000000000;;						Image: GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;						Resources: v1.ResourceRequirements{
0000000000000000000000000000000000000000;;							Requests: requests,
0000000000000000000000000000000000000000;;							Limits:   limits,
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// create empty file at given path on the pod.
0000000000000000000000000000000000000000;;	func CreateEmptyFileOnPod(namespace string, podName string, filePath string) error {
0000000000000000000000000000000000000000;;		_, err := RunKubectl("exec", fmt.Sprintf("--namespace=%s", namespace), podName, "--", "/bin/sh", "-c", fmt.Sprintf("touch %s", filePath))
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetAzureCloud returns azure cloud provider
0000000000000000000000000000000000000000;;	func GetAzureCloud() (*azure.Cloud, error) {
0000000000000000000000000000000000000000;;		cloud, ok := TestContext.CloudConfig.Provider.(*azure.Cloud)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed to convert CloudConfig.Provider to Azure: %#v", TestContext.CloudConfig.Provider)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return cloud, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func PrintSummaries(summaries []TestDataSummary, testBaseName string) {
0000000000000000000000000000000000000000;;		now := time.Now()
0000000000000000000000000000000000000000;;		for i := range summaries {
0000000000000000000000000000000000000000;;			Logf("Printing summary: %v", summaries[i].SummaryKind())
0000000000000000000000000000000000000000;;			switch TestContext.OutputPrintType {
0000000000000000000000000000000000000000;;			case "hr":
0000000000000000000000000000000000000000;;				if TestContext.ReportDir == "" {
0000000000000000000000000000000000000000;;					Logf(summaries[i].PrintHumanReadable())
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					// TODO: learn to extract test name and append it to the kind instead of timestamp.
0000000000000000000000000000000000000000;;					filePath := path.Join(TestContext.ReportDir, summaries[i].SummaryKind()+"_"+testBaseName+"_"+now.Format(time.RFC3339)+".txt")
0000000000000000000000000000000000000000;;					if err := ioutil.WriteFile(filePath, []byte(summaries[i].PrintHumanReadable()), 0644); err != nil {
0000000000000000000000000000000000000000;;						Logf("Failed to write file %v with test performance data: %v", filePath, err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case "json":
0000000000000000000000000000000000000000;;				fallthrough
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				if TestContext.OutputPrintType != "json" {
0000000000000000000000000000000000000000;;					Logf("Unknown output type: %v. Printing JSON", TestContext.OutputPrintType)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if TestContext.ReportDir == "" {
0000000000000000000000000000000000000000;;					Logf("%v JSON\n%v", summaries[i].SummaryKind(), summaries[i].PrintJSON())
0000000000000000000000000000000000000000;;					Logf("Finished")
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					// TODO: learn to extract test name and append it to the kind instead of timestamp.
0000000000000000000000000000000000000000;;					filePath := path.Join(TestContext.ReportDir, summaries[i].SummaryKind()+"_"+testBaseName+"_"+now.Format(time.RFC3339)+".json")
0000000000000000000000000000000000000000;;					if err := ioutil.WriteFile(filePath, []byte(summaries[i].PrintJSON()), 0644); err != nil {
0000000000000000000000000000000000000000;;						Logf("Failed to write file %v with test performance data: %v", filePath, err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func DumpDebugInfo(c clientset.Interface, ns string) {
0000000000000000000000000000000000000000;;		sl, _ := c.Core().Pods(ns).List(metav1.ListOptions{LabelSelector: labels.Everything().String()})
0000000000000000000000000000000000000000;;		for _, s := range sl.Items {
0000000000000000000000000000000000000000;;			desc, _ := RunKubectl("describe", "po", s.Name, fmt.Sprintf("--namespace=%v", ns))
0000000000000000000000000000000000000000;;			Logf("\nOutput of kubectl describe %v:\n%v", s.Name, desc)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			l, _ := RunKubectl("logs", s.Name, fmt.Sprintf("--namespace=%v", ns), "--tail=100")
0000000000000000000000000000000000000000;;			Logf("\nLast 100 log lines of %v:\n%v", s.Name, l)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
