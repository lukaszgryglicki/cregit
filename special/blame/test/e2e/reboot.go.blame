0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
e8805bae668c0d7e1e88e0a000c35742b0b24a09;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/fields"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// How long a node is allowed to go from "Ready" to "NotReady" after a
0000000000000000000000000000000000000000;;		// reboot is issued before the test is considered failed.
0000000000000000000000000000000000000000;;		rebootNodeNotReadyTimeout = 2 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long a node is allowed to go from "NotReady" to "Ready" after a
0000000000000000000000000000000000000000;;		// reboot is issued and it is found to be "NotReady" before the test is
0000000000000000000000000000000000000000;;		// considered failed.
0000000000000000000000000000000000000000;;		rebootNodeReadyAgainTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long pods have to be "ready" after the reboot.
0000000000000000000000000000000000000000;;		rebootPodReadyAgainTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Reboot [Disruptive] [Feature:Reboot]", func() {
0000000000000000000000000000000000000000;;		var f *framework.Framework
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			// These tests requires SSH to nodes, so the provider check should be identical to there
0000000000000000000000000000000000000000;;			// (the limiting factor is the implementation of util.go's framework.GetSigner(...)).
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Cluster must support node reboot
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs(framework.ProvidersWithSSH...)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			if CurrentGinkgoTestDescription().Failed {
0000000000000000000000000000000000000000;;				// Most of the reboot tests just make sure that addon/system pods are running, so dump
0000000000000000000000000000000000000000;;				// events for the kube-system namespace on failures
0000000000000000000000000000000000000000;;				namespaceName := metav1.NamespaceSystem
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("Collecting events from namespace %q.", namespaceName))
0000000000000000000000000000000000000000;;				events, err := f.ClientSet.Core().Events(namespaceName).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				for _, e := range events.Items {
0000000000000000000000000000000000000000;;					framework.Logf("event for %v: %v %v: %v", e.InvolvedObject.Name, e.Source, e.Reason, e.Message)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// In GKE, our current tunneling setup has the potential to hold on to a broken tunnel (from a
0000000000000000000000000000000000000000;;			// rebooted/deleted node) for up to 5 minutes before all tunnels are dropped and recreated.  Most tests
0000000000000000000000000000000000000000;;			// make use of some proxy feature to verify functionality. So, if a reboot test runs right before a test
0000000000000000000000000000000000000000;;			// that tries to get logs, for example, we may get unlucky and try to use a closed tunnel to a node that
0000000000000000000000000000000000000000;;			// was recently rebooted. There's no good way to framework.Poll for proxies being closed, so we sleep.
0000000000000000000000000000000000000000;;			//
0000000000000000000000000000000000000000;;			// TODO(cjcullen) reduce this sleep (#19314)
0000000000000000000000000000000000000000;;			if framework.ProviderIs("gke") {
0000000000000000000000000000000000000000;;				By("waiting 5 minutes for all dead tunnels to be dropped")
0000000000000000000000000000000000000000;;				time.Sleep(5 * time.Minute)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		f = framework.NewDefaultFramework("reboot")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("each node by ordering clean reboot and ensure they function upon restart", func() {
0000000000000000000000000000000000000000;;			// clean shutdown and restart
0000000000000000000000000000000000000000;;			// We sleep 10 seconds to give some time for ssh command to cleanly finish before the node is rebooted.
0000000000000000000000000000000000000000;;			testReboot(f.ClientSet, "nohup sh -c 'sleep 10 && sudo reboot' >/dev/null 2>&1 &", nil)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("each node by ordering unclean reboot and ensure they function upon restart", func() {
0000000000000000000000000000000000000000;;			// unclean shutdown and restart
0000000000000000000000000000000000000000;;			// We sleep 10 seconds to give some time for ssh command to cleanly finish before the node is shutdown.
0000000000000000000000000000000000000000;;			testReboot(f.ClientSet, "nohup sh -c 'sleep 10 && echo b | sudo tee /proc/sysrq-trigger' >/dev/null 2>&1 &", nil)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("each node by triggering kernel panic and ensure they function upon restart", func() {
0000000000000000000000000000000000000000;;			// kernel panic
0000000000000000000000000000000000000000;;			// We sleep 10 seconds to give some time for ssh command to cleanly finish before kernel panic is triggered.
0000000000000000000000000000000000000000;;			testReboot(f.ClientSet, "nohup sh -c 'sleep 10 && echo c | sudo tee /proc/sysrq-trigger' >/dev/null 2>&1 &", nil)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("each node by switching off the network interface and ensure they function upon switch on", func() {
0000000000000000000000000000000000000000;;			// switch the network interface off for a while to simulate a network outage
0000000000000000000000000000000000000000;;			// We sleep 10 seconds to give some time for ssh command to cleanly finish before network is down.
0000000000000000000000000000000000000000;;			testReboot(f.ClientSet, "nohup sh -c 'sleep 10 && (sudo ifdown eth0 || sudo ip link set eth0 down) && sleep 120 && (sudo ifup eth0 || sudo ip link set eth0 up)' >/dev/null 2>&1 &", nil)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("each node by dropping all inbound packets for a while and ensure they function afterwards", func() {
0000000000000000000000000000000000000000;;			// tell the firewall to drop all inbound packets for a while
0000000000000000000000000000000000000000;;			// We sleep 10 seconds to give some time for ssh command to cleanly finish before starting dropping inbound packets.
0000000000000000000000000000000000000000;;			// We still accept packages send from localhost to prevent monit from restarting kubelet.
0000000000000000000000000000000000000000;;			tmpLogPath := "/tmp/drop-inbound.log"
0000000000000000000000000000000000000000;;			testReboot(f.ClientSet, dropPacketsScript("INPUT", tmpLogPath), catLogHook(tmpLogPath))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("each node by dropping all outbound packets for a while and ensure they function afterwards", func() {
0000000000000000000000000000000000000000;;			// tell the firewall to drop all outbound packets for a while
0000000000000000000000000000000000000000;;			// We sleep 10 seconds to give some time for ssh command to cleanly finish before starting dropping outbound packets.
0000000000000000000000000000000000000000;;			// We still accept packages send to localhost to prevent monit from restarting kubelet.
0000000000000000000000000000000000000000;;			tmpLogPath := "/tmp/drop-outbound.log"
0000000000000000000000000000000000000000;;			testReboot(f.ClientSet, dropPacketsScript("OUTPUT", tmpLogPath), catLogHook(tmpLogPath))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func testReboot(c clientset.Interface, rebootCmd string, hook terminationHook) {
0000000000000000000000000000000000000000;;		// Get all nodes, and kick off the test on each.
0000000000000000000000000000000000000000;;		nodelist := framework.GetReadySchedulableNodesOrDie(c)
0000000000000000000000000000000000000000;;		if hook != nil {
0000000000000000000000000000000000000000;;			defer func() {
0000000000000000000000000000000000000000;;				framework.Logf("Executing termination hook on nodes")
0000000000000000000000000000000000000000;;				hook(framework.TestContext.Provider, nodelist)
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		result := make([]bool, len(nodelist.Items))
0000000000000000000000000000000000000000;;		wg := sync.WaitGroup{}
0000000000000000000000000000000000000000;;		wg.Add(len(nodelist.Items))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		failed := false
0000000000000000000000000000000000000000;;		for ix := range nodelist.Items {
0000000000000000000000000000000000000000;;			go func(ix int) {
0000000000000000000000000000000000000000;;				defer wg.Done()
0000000000000000000000000000000000000000;;				n := nodelist.Items[ix]
0000000000000000000000000000000000000000;;				result[ix] = rebootNode(c, framework.TestContext.Provider, n.ObjectMeta.Name, rebootCmd)
0000000000000000000000000000000000000000;;				if !result[ix] {
0000000000000000000000000000000000000000;;					failed = true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}(ix)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Wait for all to finish and check the final result.
0000000000000000000000000000000000000000;;		wg.Wait()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if failed {
0000000000000000000000000000000000000000;;			for ix := range nodelist.Items {
0000000000000000000000000000000000000000;;				n := nodelist.Items[ix]
0000000000000000000000000000000000000000;;				if !result[ix] {
0000000000000000000000000000000000000000;;					framework.Logf("Node %s failed reboot test.", n.ObjectMeta.Name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.Failf("Test failed; at least one node failed to reboot in the time given.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func printStatusAndLogsForNotReadyPods(c clientset.Interface, ns string, podNames []string, pods []*v1.Pod) {
0000000000000000000000000000000000000000;;		printFn := func(id, log string, err error, previous bool) {
0000000000000000000000000000000000000000;;			prefix := "Retrieving log for container"
0000000000000000000000000000000000000000;;			if previous {
0000000000000000000000000000000000000000;;				prefix = "Retrieving log for the last terminated container"
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				framework.Logf("%s %s, err: %v:\n%s\n", prefix, id, err, log)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				framework.Logf("%s %s:\n%s\n", prefix, id, log)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podNameSet := sets.NewString(podNames...)
0000000000000000000000000000000000000000;;		for _, p := range pods {
0000000000000000000000000000000000000000;;			if p.Namespace != ns {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if !podNameSet.Has(p.Name) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if ok, _ := testutils.PodRunningReady(p); ok {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.Logf("Status for not ready pod %s/%s: %+v", p.Namespace, p.Name, p.Status)
0000000000000000000000000000000000000000;;			// Print the log of the containers if pod is not running and ready.
0000000000000000000000000000000000000000;;			for _, container := range p.Status.ContainerStatuses {
0000000000000000000000000000000000000000;;				cIdentifer := fmt.Sprintf("%s/%s/%s", p.Namespace, p.Name, container.Name)
0000000000000000000000000000000000000000;;				log, err := framework.GetPodLogs(c, p.Namespace, p.Name, container.Name)
0000000000000000000000000000000000000000;;				printFn(cIdentifer, log, err, false)
0000000000000000000000000000000000000000;;				// Get log from the previous container.
0000000000000000000000000000000000000000;;				if container.RestartCount > 0 {
0000000000000000000000000000000000000000;;					printFn(cIdentifer, log, err, true)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// rebootNode takes node name on provider through the following steps using c:
0000000000000000000000000000000000000000;;	//  - ensures the node is ready
0000000000000000000000000000000000000000;;	//  - ensures all pods on the node are running and ready
0000000000000000000000000000000000000000;;	//  - reboots the node (by executing rebootCmd over ssh)
0000000000000000000000000000000000000000;;	//  - ensures the node reaches some non-ready state
0000000000000000000000000000000000000000;;	//  - ensures the node becomes ready again
0000000000000000000000000000000000000000;;	//  - ensures all pods on the node become running and ready again
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// It returns true through result only if all of the steps pass; at the first
0000000000000000000000000000000000000000;;	// failed step, it will return false through result and not run the rest.
0000000000000000000000000000000000000000;;	func rebootNode(c clientset.Interface, provider, name, rebootCmd string) bool {
0000000000000000000000000000000000000000;;		// Setup
0000000000000000000000000000000000000000;;		ns := metav1.NamespaceSystem
0000000000000000000000000000000000000000;;		ps := testutils.NewPodStore(c, ns, labels.Everything(), fields.OneTermEqualSelector(api.PodHostField, name))
0000000000000000000000000000000000000000;;		defer ps.Stop()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Get the node initially.
0000000000000000000000000000000000000000;;		framework.Logf("Getting %s", name)
0000000000000000000000000000000000000000;;		node, err := c.Core().Nodes().Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			framework.Logf("Couldn't get node %s", name)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Node sanity check: ensure it is "ready".
0000000000000000000000000000000000000000;;		if !framework.WaitForNodeToBeReady(c, name, framework.NodeReadyInitialTimeout) {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Get all the pods on the node that don't have liveness probe set.
0000000000000000000000000000000000000000;;		// Liveness probe may cause restart of a pod during node reboot, and the pod may not be running.
0000000000000000000000000000000000000000;;		pods := ps.List()
0000000000000000000000000000000000000000;;		podNames := []string{}
0000000000000000000000000000000000000000;;		for _, p := range pods {
0000000000000000000000000000000000000000;;			probe := false
0000000000000000000000000000000000000000;;			for _, c := range p.Spec.Containers {
0000000000000000000000000000000000000000;;				if c.LivenessProbe != nil {
0000000000000000000000000000000000000000;;					probe = true
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if !probe {
0000000000000000000000000000000000000000;;				podNames = append(podNames, p.ObjectMeta.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		framework.Logf("Node %s has %d assigned pods with no liveness probes: %v", name, len(podNames), podNames)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// For each pod, we do a sanity check to ensure it's running / healthy
0000000000000000000000000000000000000000;;		// or succeeded now, as that's what we'll be checking later.
0000000000000000000000000000000000000000;;		if !framework.CheckPodsRunningReadyOrSucceeded(c, ns, podNames, framework.PodReadyBeforeTimeout) {
0000000000000000000000000000000000000000;;			printStatusAndLogsForNotReadyPods(c, ns, podNames, pods)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Reboot the node.
0000000000000000000000000000000000000000;;		if err = framework.IssueSSHCommand(rebootCmd, provider, node); err != nil {
0000000000000000000000000000000000000000;;			framework.Logf("Error while issuing ssh command: %v", err)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Wait for some kind of "not ready" status.
0000000000000000000000000000000000000000;;		if !framework.WaitForNodeToBeNotReady(c, name, rebootNodeNotReadyTimeout) {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Wait for some kind of "ready" status.
0000000000000000000000000000000000000000;;		if !framework.WaitForNodeToBeReady(c, name, rebootNodeReadyAgainTimeout) {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Ensure all of the pods that we found on this node before the reboot are
0000000000000000000000000000000000000000;;		// running / healthy, or succeeded.
0000000000000000000000000000000000000000;;		if !framework.CheckPodsRunningReadyOrSucceeded(c, ns, podNames, rebootPodReadyAgainTimeout) {
0000000000000000000000000000000000000000;;			newPods := ps.List()
0000000000000000000000000000000000000000;;			printStatusAndLogsForNotReadyPods(c, ns, podNames, newPods)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.Logf("Reboot successful on node %s", name)
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type terminationHook func(provider string, nodes *v1.NodeList)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func catLogHook(logPath string) terminationHook {
0000000000000000000000000000000000000000;;		return func(provider string, nodes *v1.NodeList) {
0000000000000000000000000000000000000000;;			for _, n := range nodes.Items {
0000000000000000000000000000000000000000;;				cmd := fmt.Sprintf("cat %v && rm %v", logPath, logPath)
0000000000000000000000000000000000000000;;				if _, err := framework.IssueSSHCommandWithResult(cmd, provider, &n); err != nil {
0000000000000000000000000000000000000000;;					framework.Logf("Error while issuing ssh command: %v", err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func dropPacketsScript(chainName, logPath string) string {
0000000000000000000000000000000000000000;;		return strings.Replace(fmt.Sprintf(`
0000000000000000000000000000000000000000;;			nohup sh -c '
0000000000000000000000000000000000000000;;				set -x
0000000000000000000000000000000000000000;;				sleep 10
0000000000000000000000000000000000000000;;				while true; do sudo iptables -I ${CHAIN} 1 -s 127.0.0.1 -j ACCEPT && break; done
0000000000000000000000000000000000000000;;				while true; do sudo iptables -I ${CHAIN} 2 -j DROP && break; done
0000000000000000000000000000000000000000;;				date
0000000000000000000000000000000000000000;;				sleep 120
0000000000000000000000000000000000000000;;				while true; do sudo iptables -D ${CHAIN} -j DROP && break; done
0000000000000000000000000000000000000000;;				while true; do sudo iptables -D ${CHAIN} -s 127.0.0.1 -j ACCEPT && break; done
0000000000000000000000000000000000000000;;			' >%v 2>&1 &
0000000000000000000000000000000000000000;;			`, logPath), "${CHAIN}", chainName, -1)
0000000000000000000000000000000000000000;;	}
