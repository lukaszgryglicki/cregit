0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
ad26e2b1dadff34b9133457b5f1a740abe783cb9;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"encoding/json"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cadvisorapi "github.com/google/cadvisor/info/v1"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/fields"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		mb = 1024 * 1024
0000000000000000000000000000000000000000;;		gb = 1024 * mb
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO(madhusudancs): find a way to query kubelet's disk space manager to obtain this value. 256MB
0000000000000000000000000000000000000000;;		// is the default that is set today. This test might break if the default value changes. This value
0000000000000000000000000000000000000000;;		// can be configured by setting the "low-diskspace-threshold-mb" flag while starting a kubelet.
0000000000000000000000000000000000000000;;		// However, kubelets are started as part of the cluster start up, once, before any e2e test is run,
0000000000000000000000000000000000000000;;		// and remain unchanged until all the tests are run and the cluster is brought down. Changing the
0000000000000000000000000000000000000000;;		// flag value affects all the e2e tests. So we are hard-coding this value for now.
0000000000000000000000000000000000000000;;		lowDiskSpaceThreshold uint64 = 256 * mb
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nodeOODTimeOut = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		numNodeOODPods = 3
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Plan:
0000000000000000000000000000000000000000;;	// 1. Fill disk space on all nodes except one. One node is left out so that we can schedule pods
0000000000000000000000000000000000000000;;	//    on that node. Arbitrarily choose that node to be node with index 0.  This makes this a disruptive test.
0000000000000000000000000000000000000000;;	// 2. Get the CPU capacity on unfilled node.
0000000000000000000000000000000000000000;;	// 3. Divide the available CPU into one less than the number of pods we want to schedule. We want
0000000000000000000000000000000000000000;;	//    to schedule 3 pods, so divide CPU capacity by 2.
0000000000000000000000000000000000000000;;	// 4. Request the divided CPU for each pod.
0000000000000000000000000000000000000000;;	// 5. Observe that 2 of the pods schedule onto the node whose disk is not full, and the remaining
0000000000000000000000000000000000000000;;	//    pod stays pending and does not schedule onto the nodes whose disks are full nor the node
0000000000000000000000000000000000000000;;	//    with the other two pods, since there is not enough free CPU capacity there.
0000000000000000000000000000000000000000;;	// 6. Recover disk space from one of the nodes whose disk space was previously filled. Arbritrarily
0000000000000000000000000000000000000000;;	//    choose that node to be node with index 1.
0000000000000000000000000000000000000000;;	// 7. Observe that the pod in pending status schedules on that node.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Flaky issue #20015.  We have no clear path for how to test this functionality in a non-flaky way.
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("NodeOutOfDisk [Serial] [Flaky] [Disruptive]", func() {
0000000000000000000000000000000000000000;;		var c clientset.Interface
0000000000000000000000000000000000000000;;		var unfilledNodeName, recoveredNodeName string
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("node-outofdisk")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			c = f.ClientSet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.Skipf("test is broken. #40249")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodelist := framework.GetReadySchedulableNodesOrDie(c)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Skip this test on small clusters.  No need to fail since it is not a use
0000000000000000000000000000000000000000;;			// case that any cluster of small size needs to support.
0000000000000000000000000000000000000000;;			framework.SkipUnlessNodeCountIsAtLeast(2)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			unfilledNodeName = nodelist.Items[0].Name
0000000000000000000000000000000000000000;;			for _, node := range nodelist.Items[1:] {
0000000000000000000000000000000000000000;;				fillDiskSpace(c, &node)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodelist := framework.GetReadySchedulableNodesOrDie(c)
0000000000000000000000000000000000000000;;			Expect(len(nodelist.Items)).ToNot(BeZero())
0000000000000000000000000000000000000000;;			for _, node := range nodelist.Items {
0000000000000000000000000000000000000000;;				if unfilledNodeName == node.Name || recoveredNodeName == node.Name {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				recoverDiskSpace(c, &node)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("runs out of disk space", func() {
0000000000000000000000000000000000000000;;			unfilledNode, err := c.Core().Nodes().Get(unfilledNodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Calculating CPU availability on node %s", unfilledNode.Name))
0000000000000000000000000000000000000000;;			milliCpu, err := availCpu(c, unfilledNode)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Per pod CPU should be just enough to fit only (numNodeOODPods - 1) pods on the given
0000000000000000000000000000000000000000;;			// node. We compute this value by dividing the available CPU capacity on the node by
0000000000000000000000000000000000000000;;			// (numNodeOODPods - 1) and subtracting ϵ from it. We arbitrarily choose ϵ to be 1%
0000000000000000000000000000000000000000;;			// of the available CPU per pod, i.e. 0.01 * milliCpu/(numNodeOODPods-1). Instead of
0000000000000000000000000000000000000000;;			// subtracting 1% from the value, we directly use 0.99 as the multiplier.
0000000000000000000000000000000000000000;;			podCPU := int64(float64(milliCpu/(numNodeOODPods-1)) * 0.99)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			ns := f.Namespace.Name
0000000000000000000000000000000000000000;;			podClient := c.Core().Pods(ns)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Creating pods and waiting for all but one pods to be scheduled")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for i := 0; i < numNodeOODPods-1; i++ {
0000000000000000000000000000000000000000;;				name := fmt.Sprintf("pod-node-outofdisk-%d", i)
0000000000000000000000000000000000000000;;				createOutOfDiskPod(c, ns, name, podCPU)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				framework.ExpectNoError(f.WaitForPodRunning(name))
0000000000000000000000000000000000000000;;				pod, err := podClient.Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;				Expect(pod.Spec.NodeName).To(Equal(unfilledNodeName))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			pendingPodName := fmt.Sprintf("pod-node-outofdisk-%d", numNodeOODPods-1)
0000000000000000000000000000000000000000;;			createOutOfDiskPod(c, ns, pendingPodName, podCPU)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Finding a failed scheduler event for pod %s", pendingPodName))
0000000000000000000000000000000000000000;;			wait.Poll(2*time.Second, 5*time.Minute, func() (bool, error) {
0000000000000000000000000000000000000000;;				selector := fields.Set{
0000000000000000000000000000000000000000;;					"involvedObject.kind":      "Pod",
0000000000000000000000000000000000000000;;					"involvedObject.name":      pendingPodName,
0000000000000000000000000000000000000000;;					"involvedObject.namespace": ns,
0000000000000000000000000000000000000000;;					"source":                   v1.DefaultSchedulerName,
0000000000000000000000000000000000000000;;					"reason":                   "FailedScheduling",
0000000000000000000000000000000000000000;;				}.AsSelector().String()
0000000000000000000000000000000000000000;;				options := metav1.ListOptions{FieldSelector: selector}
0000000000000000000000000000000000000000;;				schedEvents, err := c.Core().Events(ns).List(options)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if len(schedEvents.Items) > 0 {
0000000000000000000000000000000000000000;;					return true, nil
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					return false, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodelist := framework.GetReadySchedulableNodesOrDie(c)
0000000000000000000000000000000000000000;;			Expect(len(nodelist.Items)).To(BeNumerically(">", 1))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodeToRecover := nodelist.Items[1]
0000000000000000000000000000000000000000;;			Expect(nodeToRecover.Name).ToNot(Equal(unfilledNodeName))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			recoverDiskSpace(c, &nodeToRecover)
0000000000000000000000000000000000000000;;			recoveredNodeName = nodeToRecover.Name
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Verifying that pod %s schedules on node %s", pendingPodName, recoveredNodeName))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(f.WaitForPodRunning(pendingPodName))
0000000000000000000000000000000000000000;;			pendingPod, err := podClient.Get(pendingPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			Expect(pendingPod.Spec.NodeName).To(Equal(recoveredNodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// createOutOfDiskPod creates a pod in the given namespace with the requested amount of CPU.
0000000000000000000000000000000000000000;;	func createOutOfDiskPod(c clientset.Interface, ns, name string, milliCPU int64) {
0000000000000000000000000000000000000000;;		podClient := c.Core().Pods(ns)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pod := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name: name,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  "pause",
0000000000000000000000000000000000000000;;						Image: framework.GetPauseImageName(c),
0000000000000000000000000000000000000000;;						Resources: v1.ResourceRequirements{
0000000000000000000000000000000000000000;;							Requests: v1.ResourceList{
0000000000000000000000000000000000000000;;								// Request enough CPU to fit only two pods on a given node.
0000000000000000000000000000000000000000;;								v1.ResourceCPU: *resource.NewMilliQuantity(milliCPU, resource.DecimalSI),
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		_, err := podClient.Create(pod)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// availCpu calculates the available CPU on a given node by subtracting the CPU requested by
0000000000000000000000000000000000000000;;	// all the pods from the total available CPU capacity on the node.
0000000000000000000000000000000000000000;;	func availCpu(c clientset.Interface, node *v1.Node) (int64, error) {
0000000000000000000000000000000000000000;;		podClient := c.Core().Pods(metav1.NamespaceAll)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		selector := fields.Set{"spec.nodeName": node.Name}.AsSelector().String()
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{FieldSelector: selector}
0000000000000000000000000000000000000000;;		pods, err := podClient.List(options)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return 0, fmt.Errorf("failed to retrieve all the pods on node %s: %v", node.Name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		avail := node.Status.Capacity.Cpu().MilliValue()
0000000000000000000000000000000000000000;;		for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;			for _, cont := range pod.Spec.Containers {
0000000000000000000000000000000000000000;;				avail -= cont.Resources.Requests.Cpu().MilliValue()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return avail, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// availSize returns the available disk space on a given node by querying node stats which
0000000000000000000000000000000000000000;;	// is in turn obtained internally from cadvisor.
0000000000000000000000000000000000000000;;	func availSize(c clientset.Interface, node *v1.Node) (uint64, error) {
0000000000000000000000000000000000000000;;		statsResource := fmt.Sprintf("api/v1/proxy/nodes/%s/stats/", node.Name)
0000000000000000000000000000000000000000;;		framework.Logf("Querying stats for node %s using url %s", node.Name, statsResource)
0000000000000000000000000000000000000000;;		res, err := c.Core().RESTClient().Get().AbsPath(statsResource).Timeout(time.Minute).Do().Raw()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return 0, fmt.Errorf("error querying cAdvisor API: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ci := cadvisorapi.ContainerInfo{}
0000000000000000000000000000000000000000;;		err = json.Unmarshal(res, &ci)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return 0, fmt.Errorf("couldn't unmarshal container info: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return ci.Stats[len(ci.Stats)-1].Filesystem[0].Available, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// fillDiskSpace fills the available disk space on a given node by creating a large file. The disk
0000000000000000000000000000000000000000;;	// space on the node is filled in such a way that the available space after filling the disk is just
0000000000000000000000000000000000000000;;	// below the lowDiskSpaceThreshold mark.
0000000000000000000000000000000000000000;;	func fillDiskSpace(c clientset.Interface, node *v1.Node) {
0000000000000000000000000000000000000000;;		avail, err := availSize(c, node)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err, "Node %s: couldn't obtain available disk size %v", node.Name, err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fillSize := (avail - lowDiskSpaceThreshold + (100 * mb))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.Logf("Node %s: disk space available %d bytes", node.Name, avail)
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Node %s: creating a file of size %d bytes to fill the available disk space", node.Name, fillSize))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cmd := fmt.Sprintf("fallocate -l %d test.img", fillSize)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(framework.IssueSSHCommand(cmd, framework.TestContext.Provider, node))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ood := framework.WaitForNodeToBe(c, node.Name, v1.NodeOutOfDisk, true, nodeOODTimeOut)
0000000000000000000000000000000000000000;;		Expect(ood).To(BeTrue(), "Node %s did not run out of disk within %v", node.Name, nodeOODTimeOut)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		avail, err = availSize(c, node)
0000000000000000000000000000000000000000;;		framework.Logf("Node %s: disk space available %d bytes", node.Name, avail)
0000000000000000000000000000000000000000;;		Expect(avail < lowDiskSpaceThreshold).To(BeTrue())
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// recoverDiskSpace recovers disk space, filled by creating a large file, on a given node.
0000000000000000000000000000000000000000;;	func recoverDiskSpace(c clientset.Interface, node *v1.Node) {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Recovering disk space on node %s", node.Name))
0000000000000000000000000000000000000000;;		cmd := "rm -f test.img"
0000000000000000000000000000000000000000;;		framework.ExpectNoError(framework.IssueSSHCommand(cmd, framework.TestContext.Provider, node))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ood := framework.WaitForNodeToBe(c, node.Name, v1.NodeOutOfDisk, false, nodeOODTimeOut)
0000000000000000000000000000000000000000;;		Expect(ood).To(BeTrue(), "Node %s's out of disk condition status did not change to false within %v", node.Name, nodeOODTimeOut)
0000000000000000000000000000000000000000;;	}
