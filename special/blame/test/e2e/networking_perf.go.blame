0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
9f2ee1995756fb7d9e2f78c143e508eec9946495;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Tests network performance using iperf or other containers.
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"math"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// empirically derived as a baseline for expectations from running this test using kube-up.sh.
0000000000000000000000000000000000000000;;		gceBandwidthBitsEstimate = int64(30000000000)
0000000000000000000000000000000000000000;;		// on 4 node clusters, we found this test passes very quickly, generally in less then 100 seconds.
0000000000000000000000000000000000000000;;		smallClusterTimeout = 200 * time.Second
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Declared as Flakey since it has not been proven to run in parallel on small nodes or slow networks in CI
0000000000000000000000000000000000000000;;	// TODO jayunit100 : Retag this test according to semantics from #22401
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Networking IPerf [Experimental] [Slow] [Feature:Networking-Performance]", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("network-perf")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// A few simple bandwidth tests which are capped by nodes.
0000000000000000000000000000000000000000;;		// TODO replace the 1 with the scale option implementation
0000000000000000000000000000000000000000;;		// TODO: Make this a function parameter, once we distribute iperf endpoints, possibly via session affinity.
0000000000000000000000000000000000000000;;		numClient := 1
0000000000000000000000000000000000000000;;		numServer := 1
0000000000000000000000000000000000000000;;		maxBandwidthBits := gceBandwidthBitsEstimate
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It(fmt.Sprintf("should transfer ~ 1GB onto the service endpoint %v servers (maximum of %v clients)", numServer, numClient), func() {
0000000000000000000000000000000000000000;;			nodes := framework.GetReadySchedulableNodesOrDie(f.ClientSet)
0000000000000000000000000000000000000000;;			totalPods := len(nodes.Items)
0000000000000000000000000000000000000000;;			// for a single service, we expect to divide bandwidth between the network.  Very crude estimate.
0000000000000000000000000000000000000000;;			expectedBandwidth := int(float64(maxBandwidthBits) / float64(totalPods))
0000000000000000000000000000000000000000;;			Expect(totalPods).NotTo(Equal(0))
0000000000000000000000000000000000000000;;			appName := "iperf-e2e"
0000000000000000000000000000000000000000;;			err, _ := f.CreateServiceForSimpleAppWithPods(
0000000000000000000000000000000000000000;;				8001,
0000000000000000000000000000000000000000;;				8002,
0000000000000000000000000000000000000000;;				appName,
0000000000000000000000000000000000000000;;				func(n v1.Node) v1.PodSpec {
0000000000000000000000000000000000000000;;					return v1.PodSpec{
0000000000000000000000000000000000000000;;						Containers: []v1.Container{{
0000000000000000000000000000000000000000;;							Name:  "iperf-server",
0000000000000000000000000000000000000000;;							Image: "gcr.io/google_containers/iperf:e2e",
0000000000000000000000000000000000000000;;							Args: []string{
0000000000000000000000000000000000000000;;								"/bin/sh",
0000000000000000000000000000000000000000;;								"-c",
0000000000000000000000000000000000000000;;								"/usr/local/bin/iperf -s -p 8001 ",
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;							Ports: []v1.ContainerPort{{ContainerPort: 8001}},
0000000000000000000000000000000000000000;;						}},
0000000000000000000000000000000000000000;;						NodeName:      n.Name,
0000000000000000000000000000000000000000;;						RestartPolicy: v1.RestartPolicyOnFailure,
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				// this will be used to generate the -service name which all iperf clients point at.
0000000000000000000000000000000000000000;;				numServer, // Generally should be 1 server unless we do affinity or use a version of iperf that supports LB
0000000000000000000000000000000000000000;;				true,      // Make sure we wait, otherwise all the clients will die and need to restart.
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				framework.Failf("Fatal error waiting for iperf server endpoint : %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			iperfClientPodLabels := f.CreatePodsPerNodeForSimpleApp(
0000000000000000000000000000000000000000;;				"iperf-e2e-cli",
0000000000000000000000000000000000000000;;				func(n v1.Node) v1.PodSpec {
0000000000000000000000000000000000000000;;					return v1.PodSpec{
0000000000000000000000000000000000000000;;						Containers: []v1.Container{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Name:  "iperf-client",
0000000000000000000000000000000000000000;;								Image: "gcr.io/google_containers/iperf:e2e",
0000000000000000000000000000000000000000;;								Args: []string{
0000000000000000000000000000000000000000;;									"/bin/sh",
0000000000000000000000000000000000000000;;									"-c",
0000000000000000000000000000000000000000;;									"/usr/local/bin/iperf -c service-for-" + appName + " -p 8002 --reportstyle C && sleep 5",
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						RestartPolicy: v1.RestartPolicyOnFailure, // let them successfully die.
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				numClient,
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.Logf("Reading all perf results to stdout.")
0000000000000000000000000000000000000000;;			framework.Logf("date,cli,cliPort,server,serverPort,id,interval,transferBits,bandwidthBits")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Calculate expected number of clients based on total nodes.
0000000000000000000000000000000000000000;;			expectedCli := func() int {
0000000000000000000000000000000000000000;;				nodes := framework.GetReadySchedulableNodesOrDie(f.ClientSet)
0000000000000000000000000000000000000000;;				return int(math.Min(float64(len(nodes.Items)), float64(numClient)))
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Extra 1/10 second per client.
0000000000000000000000000000000000000000;;			iperfTimeout := smallClusterTimeout + (time.Duration(expectedCli/10) * time.Second)
0000000000000000000000000000000000000000;;			iperfResults := &IPerfResults{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			iperfClusterVerification := f.NewClusterVerification(
0000000000000000000000000000000000000000;;				f.Namespace,
0000000000000000000000000000000000000000;;				framework.PodStateVerification{
0000000000000000000000000000000000000000;;					Selectors:   iperfClientPodLabels,
0000000000000000000000000000000000000000;;					ValidPhases: []v1.PodPhase{v1.PodSucceeded},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			pods, err2 := iperfClusterVerification.WaitFor(expectedCli, iperfTimeout)
0000000000000000000000000000000000000000;;			if err2 != nil {
0000000000000000000000000000000000000000;;				framework.Failf("Error in wait...")
0000000000000000000000000000000000000000;;			} else if len(pods) < expectedCli {
0000000000000000000000000000000000000000;;				framework.Failf("IPerf restuls : Only got %v out of %v, after waiting %v", len(pods), expectedCli, iperfTimeout)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				// For each builds up a collection of IPerfRecords
0000000000000000000000000000000000000000;;				iperfClusterVerification.ForEach(
0000000000000000000000000000000000000000;;					func(p v1.Pod) {
0000000000000000000000000000000000000000;;						resultS, err := framework.LookForStringInLog(f.Namespace.Name, p.Name, "iperf-client", "0-", 1*time.Second)
0000000000000000000000000000000000000000;;						if err == nil {
0000000000000000000000000000000000000000;;							framework.Logf(resultS)
0000000000000000000000000000000000000000;;							iperfResults.Add(NewIPerf(resultS))
0000000000000000000000000000000000000000;;						} else {
0000000000000000000000000000000000000000;;							framework.Failf("Unexpected error, %v when running forEach on the pods.", err)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			fmt.Println("[begin] Node,Bandwith CSV")
0000000000000000000000000000000000000000;;			fmt.Println(iperfResults.ToTSV())
0000000000000000000000000000000000000000;;			fmt.Println("[end] Node,Bandwith CSV")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for ipClient, bandwidth := range iperfResults.BandwidthMap {
0000000000000000000000000000000000000000;;				framework.Logf("%v had bandwidth %v.  Ratio to expected (%v) was %f", ipClient, bandwidth, expectedBandwidth, float64(bandwidth)/float64(expectedBandwidth))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
