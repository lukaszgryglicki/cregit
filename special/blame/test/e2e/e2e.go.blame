0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
77b9ed9c123b4e65742a3d7721e1dd2d3bb31ed0;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"io/ioutil"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"path"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"testing"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		"github.com/onsi/ginkgo/config"
0000000000000000000000000000000000000000;;		"github.com/onsi/ginkgo/reporters"
0000000000000000000000000000000000000000;;		"github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		runtimeutils "k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/cloudprovider/providers/azure"
0000000000000000000000000000000000000000;;		gcecloud "k8s.io/kubernetes/pkg/cloudprovider/providers/gce"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/logs"
0000000000000000000000000000000000000000;;		commontest "k8s.io/kubernetes/test/e2e/common"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework/ginkgowrapper"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/manifest"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/metrics"
0000000000000000000000000000000000000000;;		federationtest "k8s.io/kubernetes/test/e2e_federation"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// imagePrePullingTimeout is the time we wait for the e2e-image-puller
0000000000000000000000000000000000000000;;		// static pods to pull the list of seeded images. If they don't pull
0000000000000000000000000000000000000000;;		// images within this time we simply log their output and carry on
0000000000000000000000000000000000000000;;		// with the tests.
0000000000000000000000000000000000000000;;		imagePrePullingTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Delete this once all the tests that depend upon it are moved out of test/e2e and into subdirs
0000000000000000000000000000000000000000;;		podName = "pfpod"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var (
0000000000000000000000000000000000000000;;		cloudConfig = &framework.TestContext.CloudConfig
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// setupProviderConfig validates and sets up cloudConfig based on framework.TestContext.Provider.
0000000000000000000000000000000000000000;;	func setupProviderConfig() error {
0000000000000000000000000000000000000000;;		switch framework.TestContext.Provider {
0000000000000000000000000000000000000000;;		case "":
0000000000000000000000000000000000000000;;			glog.Info("The --provider flag is not set.  Treating as a conformance test.  Some tests may not be run.")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case "gce", "gke":
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			framework.Logf("Fetching cloud provider for %q\r\n", framework.TestContext.Provider)
0000000000000000000000000000000000000000;;			zone := framework.TestContext.CloudConfig.Zone
0000000000000000000000000000000000000000;;			region, err := gcecloud.GetGCERegion(zone)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("error parsing GCE/GKE region from zone %q: %v", zone, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			managedZones := []string{} // Manage all zones in the region
0000000000000000000000000000000000000000;;			if !framework.TestContext.CloudConfig.MultiZone {
0000000000000000000000000000000000000000;;				managedZones = []string{zone}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			cloudConfig.Provider, err = gcecloud.CreateGCECloud(framework.TestContext.CloudConfig.ApiEndpoint,
0000000000000000000000000000000000000000;;				framework.TestContext.CloudConfig.ProjectID,
0000000000000000000000000000000000000000;;				region, zone, managedZones, "" /* networkUrl */, "" /* subnetworkUrl */, nil, /* nodeTags */
0000000000000000000000000000000000000000;;				"" /* nodeInstancePerfix */, nil /* tokenSource */, false /* useMetadataServer */)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("Error building GCE/GKE provider: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case "aws":
0000000000000000000000000000000000000000;;			if cloudConfig.Zone == "" {
0000000000000000000000000000000000000000;;				return fmt.Errorf("gce-zone must be specified for AWS")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		case "azure":
0000000000000000000000000000000000000000;;			if cloudConfig.ConfigFile == "" {
0000000000000000000000000000000000000000;;				return fmt.Errorf("config-file must be specified for Azure")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			config, err := os.Open(cloudConfig.ConfigFile)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				framework.Logf("Couldn't open cloud provider configuration %s: %#v",
0000000000000000000000000000000000000000;;					cloudConfig.ConfigFile, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			defer config.Close()
0000000000000000000000000000000000000000;;			cloudConfig.Provider, err = azure.NewCloud(config)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// There are certain operations we only want to run once per overall test invocation
0000000000000000000000000000000000000000;;	// (such as deleting old namespaces, or verifying that all system pods are running.
0000000000000000000000000000000000000000;;	// Because of the way Ginkgo runs tests in parallel, we must use SynchronizedBeforeSuite
0000000000000000000000000000000000000000;;	// to ensure that these operations only run on the first parallel Ginkgo node.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// This function takes two parameters: one function which runs on only the first Ginkgo node,
0000000000000000000000000000000000000000;;	// returning an opaque byte array, and then a second function which runs on all Ginkgo nodes,
0000000000000000000000000000000000000000;;	// accepting the byte array.
0000000000000000000000000000000000000000;;	var _ = ginkgo.SynchronizedBeforeSuite(func() []byte {
0000000000000000000000000000000000000000;;		// Run only on Ginkgo node 1
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err := setupProviderConfig(); err != nil {
0000000000000000000000000000000000000000;;			framework.Failf("Failed to setup provider config: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		switch framework.TestContext.Provider {
0000000000000000000000000000000000000000;;		case "gce", "gke":
0000000000000000000000000000000000000000;;			framework.LogClusterImageSources()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		c, err := framework.LoadClientset()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Fatal("Error loading client: ", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Delete any namespaces except those created by the system. This ensures no
0000000000000000000000000000000000000000;;		// lingering resources are left over from a previous test run.
0000000000000000000000000000000000000000;;		if framework.TestContext.CleanStart {
0000000000000000000000000000000000000000;;			deleted, err := framework.DeleteNamespaces(c, nil, /* deleteFilter */
0000000000000000000000000000000000000000;;				[]string{
0000000000000000000000000000000000000000;;					metav1.NamespaceSystem,
0000000000000000000000000000000000000000;;					metav1.NamespaceDefault,
0000000000000000000000000000000000000000;;					metav1.NamespacePublic,
0000000000000000000000000000000000000000;;					framework.FederationSystemNamespace(),
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				framework.Failf("Error deleting orphaned namespaces: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("Waiting for deletion of the following namespaces: %v", deleted)
0000000000000000000000000000000000000000;;			if err := framework.WaitForNamespacesDeleted(c, deleted, framework.NamespaceCleanupTimeout); err != nil {
0000000000000000000000000000000000000000;;				framework.Failf("Failed to delete orphaned namespaces %v: %v", deleted, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// In large clusters we may get to this point but still have a bunch
0000000000000000000000000000000000000000;;		// of nodes without Routes created. Since this would make a node
0000000000000000000000000000000000000000;;		// unschedulable, we need to wait until all of them are schedulable.
0000000000000000000000000000000000000000;;		framework.ExpectNoError(framework.WaitForAllNodesSchedulable(c, framework.TestContext.NodeSchedulableTimeout))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Ensure all pods are running and ready before starting tests (otherwise,
0000000000000000000000000000000000000000;;		// cluster infrastructure pods that are being pulled or started can block
0000000000000000000000000000000000000000;;		// test pods from running, and tests that ensure all pods are running and
0000000000000000000000000000000000000000;;		// ready will fail).
0000000000000000000000000000000000000000;;		podStartupTimeout := framework.TestContext.SystemPodsStartupTimeout
0000000000000000000000000000000000000000;;		// TODO: In large clusters, we often observe a non-starting pods due to
0000000000000000000000000000000000000000;;		// #41007. To avoid those pods preventing the whole test runs (and just
0000000000000000000000000000000000000000;;		// wasting the whole run), we allow for some not-ready pods (with the
0000000000000000000000000000000000000000;;		// number equal to the number of allowed not-ready nodes).
0000000000000000000000000000000000000000;;		if err := framework.WaitForPodsRunningReady(c, metav1.NamespaceSystem, int32(framework.TestContext.MinStartupPods), int32(framework.TestContext.AllowedNotReadyNodes), podStartupTimeout, framework.ImagePullerLabels); err != nil {
0000000000000000000000000000000000000000;;			framework.DumpAllNamespaceInfo(c, metav1.NamespaceSystem)
0000000000000000000000000000000000000000;;			framework.LogFailedContainers(c, metav1.NamespaceSystem, framework.Logf)
0000000000000000000000000000000000000000;;			runKubernetesServiceTestContainer(c, metav1.NamespaceDefault)
0000000000000000000000000000000000000000;;			framework.Failf("Error waiting for all pods to be running and ready: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err := framework.WaitForPodsSuccess(c, metav1.NamespaceSystem, framework.ImagePullerLabels, imagePrePullingTimeout); err != nil {
0000000000000000000000000000000000000000;;			// There is no guarantee that the image pulling will succeed in 3 minutes
0000000000000000000000000000000000000000;;			// and we don't even run the image puller on all platforms (including GKE).
0000000000000000000000000000000000000000;;			// We wait for it so we get an indication of failures in the logs, and to
0000000000000000000000000000000000000000;;			// maximize benefit of image pre-pulling.
0000000000000000000000000000000000000000;;			framework.Logf("WARNING: Image pulling pods failed to enter success in %v: %v", imagePrePullingTimeout, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Dump the output of the nethealth containers only once per run
0000000000000000000000000000000000000000;;		if framework.TestContext.DumpLogsOnFailure {
0000000000000000000000000000000000000000;;			logFunc := framework.Logf
0000000000000000000000000000000000000000;;			if framework.TestContext.ReportDir != "" {
0000000000000000000000000000000000000000;;				filePath := path.Join(framework.TestContext.ReportDir, "nethealth.txt")
0000000000000000000000000000000000000000;;				file, err := os.Create(filePath)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					framework.Logf("Failed to create a file with network health data %v: %v\nPrinting to stdout", filePath, err)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					defer file.Close()
0000000000000000000000000000000000000000;;					if err = file.Chmod(0644); err != nil {
0000000000000000000000000000000000000000;;						framework.Logf("Failed to chmod to 644 of %v: %v", filePath, err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					logFunc = framework.GetLogToFileFunc(file)
0000000000000000000000000000000000000000;;					framework.Logf("Dumping network health container logs from all nodes to file %v", filePath)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				framework.Logf("Dumping network health container logs from all nodes...")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.LogContainersInPodsWithLabels(c, metav1.NamespaceSystem, framework.ImagePullerLabels, "nethealth", logFunc)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Reference common test to make the import valid.
0000000000000000000000000000000000000000;;		commontest.CurrentSuite = commontest.E2E
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Reference federation test to make the import valid.
0000000000000000000000000000000000000000;;		federationtest.FederationSuite = commontest.FederationE2E
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	}, func(data []byte) {
0000000000000000000000000000000000000000;;		// Run on all Ginkgo nodes
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if cloudConfig.Provider == nil {
0000000000000000000000000000000000000000;;			if err := setupProviderConfig(); err != nil {
0000000000000000000000000000000000000000;;				framework.Failf("Failed to setup provider config: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type CleanupActionHandle *int
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var cleanupActionsLock sync.Mutex
0000000000000000000000000000000000000000;;	var cleanupActions = map[CleanupActionHandle]func(){}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// AddCleanupAction installs a function that will be called in the event of the
0000000000000000000000000000000000000000;;	// whole test being terminated.  This allows arbitrary pieces of the overall
0000000000000000000000000000000000000000;;	// test to hook into SynchronizedAfterSuite().
0000000000000000000000000000000000000000;;	func AddCleanupAction(fn func()) CleanupActionHandle {
0000000000000000000000000000000000000000;;		p := CleanupActionHandle(new(int))
0000000000000000000000000000000000000000;;		cleanupActionsLock.Lock()
0000000000000000000000000000000000000000;;		defer cleanupActionsLock.Unlock()
0000000000000000000000000000000000000000;;		cleanupActions[p] = fn
0000000000000000000000000000000000000000;;		return p
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RemoveCleanupAction removes a function that was installed by
0000000000000000000000000000000000000000;;	// AddCleanupAction.
0000000000000000000000000000000000000000;;	func RemoveCleanupAction(p CleanupActionHandle) {
0000000000000000000000000000000000000000;;		cleanupActionsLock.Lock()
0000000000000000000000000000000000000000;;		defer cleanupActionsLock.Unlock()
0000000000000000000000000000000000000000;;		delete(cleanupActions, p)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunCleanupActions runs all functions installed by AddCleanupAction.  It does
0000000000000000000000000000000000000000;;	// not remove them (see RemoveCleanupAction) but it does run unlocked, so they
0000000000000000000000000000000000000000;;	// may remove themselves.
0000000000000000000000000000000000000000;;	func RunCleanupActions() {
0000000000000000000000000000000000000000;;		list := []func(){}
0000000000000000000000000000000000000000;;		func() {
0000000000000000000000000000000000000000;;			cleanupActionsLock.Lock()
0000000000000000000000000000000000000000;;			defer cleanupActionsLock.Unlock()
0000000000000000000000000000000000000000;;			for _, fn := range cleanupActions {
0000000000000000000000000000000000000000;;				list = append(list, fn)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;		// Run unlocked.
0000000000000000000000000000000000000000;;		for _, fn := range list {
0000000000000000000000000000000000000000;;			fn()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Similar to SynchornizedBeforeSuite, we want to run some operations only once (such as collecting cluster logs).
0000000000000000000000000000000000000000;;	// Here, the order of functions is reversed; first, the function which runs everywhere,
0000000000000000000000000000000000000000;;	// and then the function that only runs on the first Ginkgo node.
0000000000000000000000000000000000000000;;	var _ = ginkgo.SynchronizedAfterSuite(func() {
0000000000000000000000000000000000000000;;		// Run on all Ginkgo nodes
0000000000000000000000000000000000000000;;		framework.Logf("Running AfterSuite actions on all node")
0000000000000000000000000000000000000000;;		RunCleanupActions()
0000000000000000000000000000000000000000;;	}, func() {
0000000000000000000000000000000000000000;;		// Run only Ginkgo on node 1
0000000000000000000000000000000000000000;;		framework.Logf("Running AfterSuite actions on node 1")
0000000000000000000000000000000000000000;;		if framework.TestContext.ReportDir != "" {
0000000000000000000000000000000000000000;;			framework.CoreDump(framework.TestContext.ReportDir)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if framework.TestContext.GatherSuiteMetricsAfterTest {
0000000000000000000000000000000000000000;;			if err := gatherTestSuiteMetrics(); err != nil {
0000000000000000000000000000000000000000;;				framework.Logf("Error gathering metrics: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func gatherTestSuiteMetrics() error {
0000000000000000000000000000000000000000;;		framework.Logf("Gathering metrics")
0000000000000000000000000000000000000000;;		c, err := framework.LoadClientset()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error loading client: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Grab metrics for apiserver, scheduler, controller-manager, kubelet (for non-kubemark case).
0000000000000000000000000000000000000000;;		grabber, err := metrics.NewMetricsGrabber(c, !framework.ProviderIs("kubemark"), true, true, true)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to create MetricsGrabber: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		received, err := grabber.Grab()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to grab metrics: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		metricsForE2E := (*framework.MetricsForE2E)(&received)
0000000000000000000000000000000000000000;;		metricsJson := metricsForE2E.PrintJSON()
0000000000000000000000000000000000000000;;		if framework.TestContext.ReportDir != "" {
0000000000000000000000000000000000000000;;			filePath := path.Join(framework.TestContext.ReportDir, "MetricsForE2ESuite_"+time.Now().Format(time.RFC3339)+".json")
0000000000000000000000000000000000000000;;			if err := ioutil.WriteFile(filePath, []byte(metricsJson), 0644); err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("error writing to %q: %v", filePath, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			framework.Logf("\n\nTest Suite Metrics:\n%s\n\n", metricsJson)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestE2E checks configuration parameters (specified through flags) and then runs
0000000000000000000000000000000000000000;;	// E2E tests using the Ginkgo runner.
0000000000000000000000000000000000000000;;	// If a "report directory" is specified, one or more JUnit test reports will be
0000000000000000000000000000000000000000;;	// generated in this directory, and cluster logs will also be saved.
0000000000000000000000000000000000000000;;	// This function is called on each Ginkgo node in parallel mode.
0000000000000000000000000000000000000000;;	func RunE2ETests(t *testing.T) {
0000000000000000000000000000000000000000;;		runtimeutils.ReallyCrash = true
0000000000000000000000000000000000000000;;		logs.InitLogs()
0000000000000000000000000000000000000000;;		defer logs.FlushLogs()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		gomega.RegisterFailHandler(ginkgowrapper.Fail)
0000000000000000000000000000000000000000;;		// Disable skipped tests unless they are explicitly requested.
0000000000000000000000000000000000000000;;		if config.GinkgoConfig.FocusString == "" && config.GinkgoConfig.SkipString == "" {
0000000000000000000000000000000000000000;;			config.GinkgoConfig.SkipString = `\[Flaky\]|\[Feature:.+\]`
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Run tests through the Ginkgo runner with output to console + JUnit for Jenkins
0000000000000000000000000000000000000000;;		var r []ginkgo.Reporter
0000000000000000000000000000000000000000;;		if framework.TestContext.ReportDir != "" {
0000000000000000000000000000000000000000;;			// TODO: we should probably only be trying to create this directory once
0000000000000000000000000000000000000000;;			// rather than once-per-Ginkgo-node.
0000000000000000000000000000000000000000;;			if err := os.MkdirAll(framework.TestContext.ReportDir, 0755); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Failed creating report directory: %v", err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				r = append(r, reporters.NewJUnitReporter(path.Join(framework.TestContext.ReportDir, fmt.Sprintf("junit_%v%02d.xml", framework.TestContext.ReportPrefix, config.GinkgoConfig.ParallelNode))))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.Infof("Starting e2e run %q on Ginkgo node %d", framework.RunId, config.GinkgoConfig.ParallelNode)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ginkgo.RunSpecsWithDefaultAndCustomReporters(t, "Kubernetes e2e suite", r)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Run a test container to try and contact the Kubernetes api-server from a pod, wait for it
0000000000000000000000000000000000000000;;	// to flip to Ready, log its output and delete it.
0000000000000000000000000000000000000000;;	func runKubernetesServiceTestContainer(c clientset.Interface, ns string) {
0000000000000000000000000000000000000000;;		path := "test/images/clusterapi-tester/pod.yaml"
0000000000000000000000000000000000000000;;		framework.Logf("Parsing pod from %v", path)
0000000000000000000000000000000000000000;;		p, err := manifest.PodFromManifest(path)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			framework.Logf("Failed to parse clusterapi-tester from manifest %v: %v", path, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		p.Namespace = ns
0000000000000000000000000000000000000000;;		if _, err := c.Core().Pods(ns).Create(p); err != nil {
0000000000000000000000000000000000000000;;			framework.Logf("Failed to create %v: %v", p.Name, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			if err := c.Core().Pods(ns).Delete(p.Name, nil); err != nil {
0000000000000000000000000000000000000000;;				framework.Logf("Failed to delete pod %v: %v", p.Name, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;		timeout := 5 * time.Minute
0000000000000000000000000000000000000000;;		if err := framework.WaitForPodCondition(c, ns, p.Name, "clusterapi-tester", timeout, testutils.PodRunningReady); err != nil {
0000000000000000000000000000000000000000;;			framework.Logf("Pod %v took longer than %v to enter running/ready: %v", p.Name, timeout, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		logs, err := framework.GetPodLogs(c, ns, p.Name, p.Spec.Containers[0].Name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			framework.Logf("Failed to retrieve logs from %v: %v", p.Name, err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			framework.Logf("Output of clusterapi-tester:\n%v", logs)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
