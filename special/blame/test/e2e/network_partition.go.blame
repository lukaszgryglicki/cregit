0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
3ccd0711a68f22eefd47446712ce2cd829903035;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/fields"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/watch"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/cache"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		nodepkg "k8s.io/kubernetes/pkg/controller/node"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		timeout = 60 * time.Second
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func expectNodeReadiness(isReady bool, newNode chan *v1.Node) {
0000000000000000000000000000000000000000;;		timeout := false
0000000000000000000000000000000000000000;;		expected := false
0000000000000000000000000000000000000000;;		timer := time.After(nodeReadinessTimeout)
0000000000000000000000000000000000000000;;		for !expected && !timeout {
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case n := <-newNode:
0000000000000000000000000000000000000000;;				if framework.IsNodeConditionSetAsExpected(n, v1.NodeReady, isReady) {
0000000000000000000000000000000000000000;;					expected = true
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					framework.Logf("Observed node ready status is NOT %v as expected", isReady)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case <-timer:
0000000000000000000000000000000000000000;;				timeout = true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !expected {
0000000000000000000000000000000000000000;;			framework.Failf("Failed to observe node ready status change to %v", isReady)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podOnNode(podName, nodeName string, image string) *v1.Pod {
0000000000000000000000000000000000000000;;		return &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name: podName,
0000000000000000000000000000000000000000;;				Labels: map[string]string{
0000000000000000000000000000000000000000;;					"name": podName,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  podName,
0000000000000000000000000000000000000000;;						Image: image,
0000000000000000000000000000000000000000;;						Ports: []v1.ContainerPort{{ContainerPort: 9376}},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				NodeName:      nodeName,
0000000000000000000000000000000000000000;;				RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newPodOnNode(c clientset.Interface, namespace, podName, nodeName string) error {
0000000000000000000000000000000000000000;;		pod, err := c.Core().Pods(namespace).Create(podOnNode(podName, nodeName, framework.ServeHostnameImage))
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			framework.Logf("Created pod %s on node %s", pod.ObjectMeta.Name, nodeName)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			framework.Logf("Failed to create pod %s on node %s: %v", podName, nodeName, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("[sig-apps] Network Partition [Disruptive] [Slow]", func() {
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("network-partition")
0000000000000000000000000000000000000000;;		var systemPodsNo int32
0000000000000000000000000000000000000000;;		var c clientset.Interface
0000000000000000000000000000000000000000;;		var ns string
0000000000000000000000000000000000000000;;		ignoreLabels := framework.ImagePullerLabels
0000000000000000000000000000000000000000;;		var group string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			c = f.ClientSet
0000000000000000000000000000000000000000;;			ns = f.Namespace.Name
0000000000000000000000000000000000000000;;			systemPods, err := framework.GetPodsInNamespace(c, ns, ignoreLabels)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			systemPodsNo = int32(len(systemPods))
0000000000000000000000000000000000000000;;			if strings.Index(framework.TestContext.CloudConfig.NodeInstanceGroup, ",") >= 0 {
0000000000000000000000000000000000000000;;				framework.Failf("Test dose not support cluster setup with more than one MIG: %s", framework.TestContext.CloudConfig.NodeInstanceGroup)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				group = framework.TestContext.CloudConfig.NodeInstanceGroup
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.KubeDescribe("Pods", func() {
0000000000000000000000000000000000000000;;			Context("should return to running and ready state after network partition is healed", func() {
0000000000000000000000000000000000000000;;				BeforeEach(func() {
0000000000000000000000000000000000000000;;					framework.SkipUnlessProviderIs("gce", "gke", "aws")
0000000000000000000000000000000000000000;;					framework.SkipUnlessNodeCountIsAtLeast(2)
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// What happens in this test:
0000000000000000000000000000000000000000;;				//	Network traffic from a node to master is cut off to simulate network partition
0000000000000000000000000000000000000000;;				// Expect to observe:
0000000000000000000000000000000000000000;;				// 1. Node is marked NotReady after timeout by nodecontroller (40seconds)
0000000000000000000000000000000000000000;;				// 2. All pods on node are marked NotReady shortly after #1
0000000000000000000000000000000000000000;;				// 3. Node and pods return to Ready after connectivivty recovers
0000000000000000000000000000000000000000;;				It("All pods on the unreachable node should be marked as NotReady upon the node turn NotReady "+
0000000000000000000000000000000000000000;;					"AND all pods should be mark back to Ready when the node get back to Ready before pod eviction timeout", func() {
0000000000000000000000000000000000000000;;					By("choose a node - we will block all network traffic on this node")
0000000000000000000000000000000000000000;;					var podOpts metav1.ListOptions
0000000000000000000000000000000000000000;;					nodeOpts := metav1.ListOptions{}
0000000000000000000000000000000000000000;;					nodes, err := c.Core().Nodes().List(nodeOpts)
0000000000000000000000000000000000000000;;					Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;					framework.FilterNodes(nodes, func(node v1.Node) bool {
0000000000000000000000000000000000000000;;						if !framework.IsNodeConditionSetAsExpected(&node, v1.NodeReady, true) {
0000000000000000000000000000000000000000;;							return false
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						podOpts = metav1.ListOptions{FieldSelector: fields.OneTermEqualSelector(api.PodHostField, node.Name).String()}
0000000000000000000000000000000000000000;;						pods, err := c.Core().Pods(metav1.NamespaceAll).List(podOpts)
0000000000000000000000000000000000000000;;						if err != nil || len(pods.Items) <= 0 {
0000000000000000000000000000000000000000;;							return false
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						return true
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;					if len(nodes.Items) <= 0 {
0000000000000000000000000000000000000000;;						framework.Failf("No eligible node were found: %d", len(nodes.Items))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					node := nodes.Items[0]
0000000000000000000000000000000000000000;;					podOpts = metav1.ListOptions{FieldSelector: fields.OneTermEqualSelector(api.PodHostField, node.Name).String()}
0000000000000000000000000000000000000000;;					if err = framework.WaitForMatchPodsCondition(c, podOpts, "Running and Ready", podReadyTimeout, testutils.PodRunningReady); err != nil {
0000000000000000000000000000000000000000;;						framework.Failf("Pods on node %s are not ready and running within %v: %v", node.Name, podReadyTimeout, err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By("Set up watch on node status")
0000000000000000000000000000000000000000;;					nodeSelector := fields.OneTermEqualSelector("metadata.name", node.Name)
0000000000000000000000000000000000000000;;					stopCh := make(chan struct{})
0000000000000000000000000000000000000000;;					newNode := make(chan *v1.Node)
0000000000000000000000000000000000000000;;					var controller cache.Controller
0000000000000000000000000000000000000000;;					_, controller = cache.NewInformer(
0000000000000000000000000000000000000000;;						&cache.ListWatch{
0000000000000000000000000000000000000000;;							ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
0000000000000000000000000000000000000000;;								options.FieldSelector = nodeSelector.String()
0000000000000000000000000000000000000000;;								obj, err := f.ClientSet.Core().Nodes().List(options)
0000000000000000000000000000000000000000;;								return runtime.Object(obj), err
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;							WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
0000000000000000000000000000000000000000;;								options.FieldSelector = nodeSelector.String()
0000000000000000000000000000000000000000;;								return f.ClientSet.Core().Nodes().Watch(options)
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						&v1.Node{},
0000000000000000000000000000000000000000;;						0,
0000000000000000000000000000000000000000;;						cache.ResourceEventHandlerFuncs{
0000000000000000000000000000000000000000;;							UpdateFunc: func(oldObj, newObj interface{}) {
0000000000000000000000000000000000000000;;								n, ok := newObj.(*v1.Node)
0000000000000000000000000000000000000000;;								Expect(ok).To(Equal(true))
0000000000000000000000000000000000000000;;								newNode <- n
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					defer func() {
0000000000000000000000000000000000000000;;						// Will not explicitly close newNode channel here due to
0000000000000000000000000000000000000000;;						// race condition where stopCh and newNode are closed but informer onUpdate still executes.
0000000000000000000000000000000000000000;;						close(stopCh)
0000000000000000000000000000000000000000;;					}()
0000000000000000000000000000000000000000;;					go controller.Run(stopCh)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By(fmt.Sprintf("Block traffic from node %s to the master", node.Name))
0000000000000000000000000000000000000000;;					host := framework.GetNodeExternalIP(&node)
0000000000000000000000000000000000000000;;					master := framework.GetMasterAddress(c)
0000000000000000000000000000000000000000;;					defer func() {
0000000000000000000000000000000000000000;;						By(fmt.Sprintf("Unblock traffic from node %s to the master", node.Name))
0000000000000000000000000000000000000000;;						framework.UnblockNetwork(host, master)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						if CurrentGinkgoTestDescription().Failed {
0000000000000000000000000000000000000000;;							return
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						By("Expect to observe node and pod status change from NotReady to Ready after network connectivity recovers")
0000000000000000000000000000000000000000;;						expectNodeReadiness(true, newNode)
0000000000000000000000000000000000000000;;						if err = framework.WaitForMatchPodsCondition(c, podOpts, "Running and Ready", podReadyTimeout, testutils.PodRunningReady); err != nil {
0000000000000000000000000000000000000000;;							framework.Failf("Pods on node %s did not become ready and running within %v: %v", node.Name, podReadyTimeout, err)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					framework.BlockNetwork(host, master)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By("Expect to observe node and pod status change from Ready to NotReady after network partition")
0000000000000000000000000000000000000000;;					expectNodeReadiness(false, newNode)
0000000000000000000000000000000000000000;;					if err = framework.WaitForMatchPodsCondition(c, podOpts, "NotReady", podNotReadyTimeout, testutils.PodNotReady); err != nil {
0000000000000000000000000000000000000000;;						framework.Failf("Pods on node %s did not become NotReady within %v: %v", node.Name, podNotReadyTimeout, err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.KubeDescribe("[ReplicationController]", func() {
0000000000000000000000000000000000000000;;			It("should recreate pods scheduled on the unreachable node "+
0000000000000000000000000000000000000000;;				"AND allow scheduling of pods on a node after it rejoins the cluster", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Create a replication controller for a service that serves its hostname.
0000000000000000000000000000000000000000;;				// The source for the Docker container kubernetes/serve_hostname is in contrib/for-demos/serve_hostname
0000000000000000000000000000000000000000;;				name := "my-hostname-net"
0000000000000000000000000000000000000000;;				newSVCByName(c, ns, name)
0000000000000000000000000000000000000000;;				replicas := int32(framework.TestContext.CloudConfig.NumNodes)
0000000000000000000000000000000000000000;;				newRCByName(c, ns, name, replicas, nil)
0000000000000000000000000000000000000000;;				err := framework.VerifyPods(c, ns, name, true, replicas)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred(), "Each pod should start running and responding")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				By("choose a node with at least one pod - we will block some network traffic on this node")
0000000000000000000000000000000000000000;;				label := labels.SelectorFromSet(labels.Set(map[string]string{"name": name}))
0000000000000000000000000000000000000000;;				options := metav1.ListOptions{LabelSelector: label.String()}
0000000000000000000000000000000000000000;;				pods, err := c.Core().Pods(ns).List(options) // list pods after all have been scheduled
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;				nodeName := pods.Items[0].Spec.NodeName
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				node, err := c.Core().Nodes().Get(nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// This creates a temporary network partition, verifies that 'podNameToDisappear',
0000000000000000000000000000000000000000;;				// that belongs to replication controller 'rcName', really disappeared (because its
0000000000000000000000000000000000000000;;				// grace period is set to 0).
0000000000000000000000000000000000000000;;				// Finally, it checks that the replication controller recreates the
0000000000000000000000000000000000000000;;				// pods on another node and that now the number of replicas is equal 'replicas'.
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("blocking network traffic from node %s", node.Name))
0000000000000000000000000000000000000000;;				framework.TestUnderTemporaryNetworkFailure(c, ns, node, func() {
0000000000000000000000000000000000000000;;					framework.Logf("Waiting for pod %s to be removed", pods.Items[0].Name)
0000000000000000000000000000000000000000;;					err := framework.WaitForRCPodToDisappear(c, ns, name, pods.Items[0].Name)
0000000000000000000000000000000000000000;;					Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By("verifying whether the pod from the unreachable node is recreated")
0000000000000000000000000000000000000000;;					err = framework.VerifyPods(c, ns, name, true, replicas)
0000000000000000000000000000000000000000;;					Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				framework.Logf("Waiting %v for node %s to be ready once temporary network failure ends", resizeNodeReadyTimeout, node.Name)
0000000000000000000000000000000000000000;;				if !framework.WaitForNodeToBeReady(c, node.Name, resizeNodeReadyTimeout) {
0000000000000000000000000000000000000000;;					framework.Failf("Node %s did not become ready within %v", node.Name, resizeNodeReadyTimeout)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// sleep a bit, to allow Watch in NodeController to catch up.
0000000000000000000000000000000000000000;;				time.Sleep(5 * time.Second)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				By("verify whether new pods can be created on the re-attached node")
0000000000000000000000000000000000000000;;				// increasing the RC size is not a valid way to test this
0000000000000000000000000000000000000000;;				// since we have no guarantees the pod will be scheduled on our node.
0000000000000000000000000000000000000000;;				additionalPod := "additionalpod"
0000000000000000000000000000000000000000;;				err = newPodOnNode(c, ns, additionalPod, node.Name)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;				err = framework.VerifyPods(c, ns, additionalPod, true, 1)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// verify that it is really on the requested node
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					pod, err := c.Core().Pods(ns).Get(additionalPod, metav1.GetOptions{})
0000000000000000000000000000000000000000;;					Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;					if pod.Spec.NodeName != node.Name {
0000000000000000000000000000000000000000;;						framework.Logf("Pod %s found on invalid node: %s instead of %s", pod.Name, pod.Spec.NodeName, node.Name)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			It("should eagerly create replacement pod during network partition when termination grace is non-zero", func() {
0000000000000000000000000000000000000000;;				// Create a replication controller for a service that serves its hostname.
0000000000000000000000000000000000000000;;				// The source for the Docker container kubernetes/serve_hostname is in contrib/for-demos/serve_hostname
0000000000000000000000000000000000000000;;				name := "my-hostname-net"
0000000000000000000000000000000000000000;;				gracePeriod := int64(30)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				newSVCByName(c, ns, name)
0000000000000000000000000000000000000000;;				replicas := int32(framework.TestContext.CloudConfig.NumNodes)
0000000000000000000000000000000000000000;;				newRCByName(c, ns, name, replicas, &gracePeriod)
0000000000000000000000000000000000000000;;				err := framework.VerifyPods(c, ns, name, true, replicas)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred(), "Each pod should start running and responding")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				By("choose a node with at least one pod - we will block some network traffic on this node")
0000000000000000000000000000000000000000;;				label := labels.SelectorFromSet(labels.Set(map[string]string{"name": name}))
0000000000000000000000000000000000000000;;				options := metav1.ListOptions{LabelSelector: label.String()}
0000000000000000000000000000000000000000;;				pods, err := c.Core().Pods(ns).List(options) // list pods after all have been scheduled
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;				nodeName := pods.Items[0].Spec.NodeName
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				node, err := c.Core().Nodes().Get(nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// This creates a temporary network partition, verifies that 'podNameToDisappear',
0000000000000000000000000000000000000000;;				// that belongs to replication controller 'rcName', did not disappear (because its
0000000000000000000000000000000000000000;;				// grace period is set to 30).
0000000000000000000000000000000000000000;;				// Finally, it checks that the replication controller recreates the
0000000000000000000000000000000000000000;;				// pods on another node and that now the number of replicas is equal 'replicas + 1'.
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("blocking network traffic from node %s", node.Name))
0000000000000000000000000000000000000000;;				framework.TestUnderTemporaryNetworkFailure(c, ns, node, func() {
0000000000000000000000000000000000000000;;					framework.Logf("Waiting for pod %s to be removed", pods.Items[0].Name)
0000000000000000000000000000000000000000;;					err := framework.WaitForRCPodToDisappear(c, ns, name, pods.Items[0].Name)
0000000000000000000000000000000000000000;;					Expect(err).To(Equal(wait.ErrWaitTimeout), "Pod was not deleted during network partition.")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By(fmt.Sprintf("verifying that there are %v running pods during partition", replicas))
0000000000000000000000000000000000000000;;					_, err = framework.PodsCreated(c, ns, name, replicas)
0000000000000000000000000000000000000000;;					Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				framework.Logf("Waiting %v for node %s to be ready once temporary network failure ends", resizeNodeReadyTimeout, node.Name)
0000000000000000000000000000000000000000;;				if !framework.WaitForNodeToBeReady(c, node.Name, resizeNodeReadyTimeout) {
0000000000000000000000000000000000000000;;					framework.Failf("Node %s did not become ready within %v", node.Name, resizeNodeReadyTimeout)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.KubeDescribe("[StatefulSet]", func() {
0000000000000000000000000000000000000000;;			psName := "ss"
0000000000000000000000000000000000000000;;			labels := map[string]string{
0000000000000000000000000000000000000000;;				"foo": "bar",
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			headlessSvcName := "test"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			BeforeEach(func() {
0000000000000000000000000000000000000000;;				framework.SkipUnlessProviderIs("gce", "gke")
0000000000000000000000000000000000000000;;				By("creating service " + headlessSvcName + " in namespace " + f.Namespace.Name)
0000000000000000000000000000000000000000;;				headlessService := framework.CreateServiceSpec(headlessSvcName, "", true, labels)
0000000000000000000000000000000000000000;;				_, err := f.ClientSet.Core().Services(f.Namespace.Name).Create(headlessService)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;				c = f.ClientSet
0000000000000000000000000000000000000000;;				ns = f.Namespace.Name
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			AfterEach(func() {
0000000000000000000000000000000000000000;;				if CurrentGinkgoTestDescription().Failed {
0000000000000000000000000000000000000000;;					framework.DumpDebugInfo(c, ns)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				framework.Logf("Deleting all stateful set in ns %v", ns)
0000000000000000000000000000000000000000;;				framework.DeleteAllStatefulSets(c, ns)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			It("should come back up if node goes down [Slow] [Disruptive]", func() {
0000000000000000000000000000000000000000;;				petMounts := []v1.VolumeMount{{Name: "datadir", MountPath: "/data/"}}
0000000000000000000000000000000000000000;;				podMounts := []v1.VolumeMount{{Name: "home", MountPath: "/home"}}
0000000000000000000000000000000000000000;;				ps := framework.NewStatefulSet(psName, ns, headlessSvcName, 3, petMounts, podMounts, labels)
0000000000000000000000000000000000000000;;				_, err := c.Apps().StatefulSets(ns).Create(ps)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				pst := framework.NewStatefulSetTester(c)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				nn := framework.TestContext.CloudConfig.NumNodes
0000000000000000000000000000000000000000;;				nodeNames, err := framework.CheckNodesReady(f.ClientSet, framework.NodeReadyInitialTimeout, nn)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;				restartNodes(f, nodeNames)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				By("waiting for pods to be running again")
0000000000000000000000000000000000000000;;				pst.WaitForRunningAndReady(*ps.Spec.Replicas, ps)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			It("should not reschedule stateful pods if there is a network partition [Slow] [Disruptive]", func() {
0000000000000000000000000000000000000000;;				ps := framework.NewStatefulSet(psName, ns, headlessSvcName, 3, []v1.VolumeMount{}, []v1.VolumeMount{}, labels)
0000000000000000000000000000000000000000;;				_, err := c.Apps().StatefulSets(ns).Create(ps)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				pst := framework.NewStatefulSetTester(c)
0000000000000000000000000000000000000000;;				pst.WaitForRunningAndReady(*ps.Spec.Replicas, ps)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				pod := pst.GetPodList(ps).Items[0]
0000000000000000000000000000000000000000;;				node, err := c.Core().Nodes().Get(pod.Spec.NodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Blocks outgoing network traffic on 'node'. Then verifies that 'podNameToDisappear',
0000000000000000000000000000000000000000;;				// that belongs to StatefulSet 'statefulSetName', **does not** disappear due to forced deletion from the apiserver.
0000000000000000000000000000000000000000;;				// The grace period on the stateful pods is set to a value > 0.
0000000000000000000000000000000000000000;;				framework.TestUnderTemporaryNetworkFailure(c, ns, node, func() {
0000000000000000000000000000000000000000;;					framework.Logf("Checking that the NodeController does not force delete stateful pods %v", pod.Name)
0000000000000000000000000000000000000000;;					err := framework.WaitTimeoutForPodNoLongerRunningInNamespace(c, pod.Name, ns, 10*time.Minute)
0000000000000000000000000000000000000000;;					Expect(err).To(Equal(wait.ErrWaitTimeout), "Pod was not deleted during network partition.")
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				framework.Logf("Waiting %v for node %s to be ready once temporary network failure ends", resizeNodeReadyTimeout, node.Name)
0000000000000000000000000000000000000000;;				if !framework.WaitForNodeToBeReady(c, node.Name, resizeNodeReadyTimeout) {
0000000000000000000000000000000000000000;;					framework.Failf("Node %s did not become ready within %v", node.Name, resizeNodeReadyTimeout)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				By("waiting for pods to be running again")
0000000000000000000000000000000000000000;;				pst.WaitForRunningAndReady(*ps.Spec.Replicas, ps)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.KubeDescribe("[Job]", func() {
0000000000000000000000000000000000000000;;			It("should create new pods when node is partitioned", func() {
0000000000000000000000000000000000000000;;				parallelism := int32(2)
0000000000000000000000000000000000000000;;				completions := int32(4)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				job := framework.NewTestJob("notTerminate", "network-partition", v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;					parallelism, completions)
0000000000000000000000000000000000000000;;				job, err := framework.CreateJob(f.ClientSet, f.Namespace.Name, job)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;				label := labels.SelectorFromSet(labels.Set(map[string]string{framework.JobSelectorKey: job.Name}))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("verifying that there are now %v running pods", parallelism))
0000000000000000000000000000000000000000;;				_, err = framework.PodsCreatedByLabel(c, ns, job.Name, parallelism, label)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				By("choose a node with at least one pod - we will block some network traffic on this node")
0000000000000000000000000000000000000000;;				options := metav1.ListOptions{LabelSelector: label.String()}
0000000000000000000000000000000000000000;;				pods, err := c.Core().Pods(ns).List(options) // list pods after all have been scheduled
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;				nodeName := pods.Items[0].Spec.NodeName
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				node, err := c.Core().Nodes().Get(nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// This creates a temporary network partition, verifies that the job has 'parallelism' number of
0000000000000000000000000000000000000000;;				// running pods after the node-controller detects node unreachable.
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("blocking network traffic from node %s", node.Name))
0000000000000000000000000000000000000000;;				framework.TestUnderTemporaryNetworkFailure(c, ns, node, func() {
0000000000000000000000000000000000000000;;					framework.Logf("Waiting for pod %s to be removed", pods.Items[0].Name)
0000000000000000000000000000000000000000;;					err := framework.WaitForPodToDisappear(c, ns, pods.Items[0].Name, label, 20*time.Second, 10*time.Minute)
0000000000000000000000000000000000000000;;					Expect(err).To(Equal(wait.ErrWaitTimeout), "Pod was not deleted during network partition.")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By(fmt.Sprintf("verifying that there are now %v running pods", parallelism))
0000000000000000000000000000000000000000;;					_, err = framework.PodsCreatedByLabel(c, ns, job.Name, parallelism, label)
0000000000000000000000000000000000000000;;					Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				framework.Logf("Waiting %v for node %s to be ready once temporary network failure ends", resizeNodeReadyTimeout, node.Name)
0000000000000000000000000000000000000000;;				if !framework.WaitForNodeToBeReady(c, node.Name, resizeNodeReadyTimeout) {
0000000000000000000000000000000000000000;;					framework.Failf("Node %s did not become ready within %v", node.Name, resizeNodeReadyTimeout)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.KubeDescribe("Pods", func() {
0000000000000000000000000000000000000000;;			Context("should be evicted from unready Node", func() {
0000000000000000000000000000000000000000;;				BeforeEach(func() {
0000000000000000000000000000000000000000;;					framework.SkipUnlessProviderIs("gce", "gke", "aws")
0000000000000000000000000000000000000000;;					framework.SkipUnlessNodeCountIsAtLeast(2)
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// What happens in this test:
0000000000000000000000000000000000000000;;				//	Network traffic from a node to master is cut off to simulate network partition
0000000000000000000000000000000000000000;;				// Expect to observe:
0000000000000000000000000000000000000000;;				// 1. Node is marked NotReady after timeout by nodecontroller (40seconds)
0000000000000000000000000000000000000000;;				// 2. All pods on node are marked NotReady shortly after #1
0000000000000000000000000000000000000000;;				// 3. After enough time passess all Pods are evicted from the given Node
0000000000000000000000000000000000000000;;				It("[Feature:TaintEviction] All pods on the unreachable node should be marked as NotReady upon the node turn NotReady "+
0000000000000000000000000000000000000000;;					"AND all pods should be evicted after eviction timeout passes", func() {
0000000000000000000000000000000000000000;;					By("choose a node - we will block all network traffic on this node")
0000000000000000000000000000000000000000;;					var podOpts metav1.ListOptions
0000000000000000000000000000000000000000;;					nodes := framework.GetReadySchedulableNodesOrDie(c)
0000000000000000000000000000000000000000;;					framework.FilterNodes(nodes, func(node v1.Node) bool {
0000000000000000000000000000000000000000;;						if !framework.IsNodeConditionSetAsExpected(&node, v1.NodeReady, true) {
0000000000000000000000000000000000000000;;							return false
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						podOpts = metav1.ListOptions{FieldSelector: fields.OneTermEqualSelector(api.PodHostField, node.Name).String()}
0000000000000000000000000000000000000000;;						pods, err := c.Core().Pods(metav1.NamespaceAll).List(podOpts)
0000000000000000000000000000000000000000;;						if err != nil || len(pods.Items) <= 0 {
0000000000000000000000000000000000000000;;							return false
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						return true
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;					if len(nodes.Items) <= 0 {
0000000000000000000000000000000000000000;;						framework.Failf("No eligible node were found: %d", len(nodes.Items))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					node := nodes.Items[0]
0000000000000000000000000000000000000000;;					podOpts = metav1.ListOptions{FieldSelector: fields.OneTermEqualSelector(api.PodHostField, node.Name).String()}
0000000000000000000000000000000000000000;;					if err := framework.WaitForMatchPodsCondition(c, podOpts, "Running and Ready", podReadyTimeout, testutils.PodRunningReadyOrSucceeded); err != nil {
0000000000000000000000000000000000000000;;						framework.Failf("Pods on node %s are not ready and running within %v: %v", node.Name, podReadyTimeout, err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					pods, err := c.Core().Pods(metav1.NamespaceAll).List(podOpts)
0000000000000000000000000000000000000000;;					framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;					podTolerationTimes := map[string]time.Duration{}
0000000000000000000000000000000000000000;;					// This test doesn't add tolerations by itself, but because they may be present in the cluster
0000000000000000000000000000000000000000;;					// it needs to account for that.
0000000000000000000000000000000000000000;;					for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;						namespacedName := fmt.Sprintf("%v/%v", pod.Namespace, pod.Name)
0000000000000000000000000000000000000000;;						tolerations := pod.Spec.Tolerations
0000000000000000000000000000000000000000;;						framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;						for _, toleration := range tolerations {
0000000000000000000000000000000000000000;;							if toleration.ToleratesTaint(nodepkg.UnreachableTaintTemplate) {
0000000000000000000000000000000000000000;;								if toleration.TolerationSeconds != nil {
0000000000000000000000000000000000000000;;									podTolerationTimes[namespacedName] = time.Duration(*toleration.TolerationSeconds) * time.Second
0000000000000000000000000000000000000000;;									break
0000000000000000000000000000000000000000;;								} else {
0000000000000000000000000000000000000000;;									podTolerationTimes[namespacedName] = -1
0000000000000000000000000000000000000000;;								}
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						if _, ok := podTolerationTimes[namespacedName]; !ok {
0000000000000000000000000000000000000000;;							podTolerationTimes[namespacedName] = 0
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					neverEvictedPods := []string{}
0000000000000000000000000000000000000000;;					maxTolerationTime := time.Duration(0)
0000000000000000000000000000000000000000;;					for podName, tolerationTime := range podTolerationTimes {
0000000000000000000000000000000000000000;;						if tolerationTime < 0 {
0000000000000000000000000000000000000000;;							neverEvictedPods = append(neverEvictedPods, podName)
0000000000000000000000000000000000000000;;						} else {
0000000000000000000000000000000000000000;;							if tolerationTime > maxTolerationTime {
0000000000000000000000000000000000000000;;								maxTolerationTime = tolerationTime
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					framework.Logf(
0000000000000000000000000000000000000000;;						"Only %v should be running after partition. Maximum TolerationSeconds among other Pods is %v",
0000000000000000000000000000000000000000;;						neverEvictedPods,
0000000000000000000000000000000000000000;;						maxTolerationTime,
0000000000000000000000000000000000000000;;					)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By("Set up watch on node status")
0000000000000000000000000000000000000000;;					nodeSelector := fields.OneTermEqualSelector("metadata.name", node.Name)
0000000000000000000000000000000000000000;;					stopCh := make(chan struct{})
0000000000000000000000000000000000000000;;					newNode := make(chan *v1.Node)
0000000000000000000000000000000000000000;;					var controller cache.Controller
0000000000000000000000000000000000000000;;					_, controller = cache.NewInformer(
0000000000000000000000000000000000000000;;						&cache.ListWatch{
0000000000000000000000000000000000000000;;							ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
0000000000000000000000000000000000000000;;								options.FieldSelector = nodeSelector.String()
0000000000000000000000000000000000000000;;								obj, err := f.ClientSet.Core().Nodes().List(options)
0000000000000000000000000000000000000000;;								return runtime.Object(obj), err
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;							WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
0000000000000000000000000000000000000000;;								options.FieldSelector = nodeSelector.String()
0000000000000000000000000000000000000000;;								return f.ClientSet.Core().Nodes().Watch(options)
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						&v1.Node{},
0000000000000000000000000000000000000000;;						0,
0000000000000000000000000000000000000000;;						cache.ResourceEventHandlerFuncs{
0000000000000000000000000000000000000000;;							UpdateFunc: func(oldObj, newObj interface{}) {
0000000000000000000000000000000000000000;;								n, ok := newObj.(*v1.Node)
0000000000000000000000000000000000000000;;								Expect(ok).To(Equal(true))
0000000000000000000000000000000000000000;;								newNode <- n
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					defer func() {
0000000000000000000000000000000000000000;;						// Will not explicitly close newNode channel here due to
0000000000000000000000000000000000000000;;						// race condition where stopCh and newNode are closed but informer onUpdate still executes.
0000000000000000000000000000000000000000;;						close(stopCh)
0000000000000000000000000000000000000000;;					}()
0000000000000000000000000000000000000000;;					go controller.Run(stopCh)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By(fmt.Sprintf("Block traffic from node %s to the master", node.Name))
0000000000000000000000000000000000000000;;					host := framework.GetNodeExternalIP(&node)
0000000000000000000000000000000000000000;;					master := framework.GetMasterAddress(c)
0000000000000000000000000000000000000000;;					defer func() {
0000000000000000000000000000000000000000;;						By(fmt.Sprintf("Unblock traffic from node %s to the master", node.Name))
0000000000000000000000000000000000000000;;						framework.UnblockNetwork(host, master)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						if CurrentGinkgoTestDescription().Failed {
0000000000000000000000000000000000000000;;							return
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						By("Expect to observe node status change from NotReady to Ready after network connectivity recovers")
0000000000000000000000000000000000000000;;						expectNodeReadiness(true, newNode)
0000000000000000000000000000000000000000;;					}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					framework.BlockNetwork(host, master)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By("Expect to observe node and pod status change from Ready to NotReady after network partition")
0000000000000000000000000000000000000000;;					expectNodeReadiness(false, newNode)
0000000000000000000000000000000000000000;;					framework.ExpectNoError(wait.Poll(1*time.Second, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;						return framework.NodeHasTaint(c, node.Name, nodepkg.UnreachableTaintTemplate)
0000000000000000000000000000000000000000;;					}))
0000000000000000000000000000000000000000;;					if err = framework.WaitForMatchPodsCondition(c, podOpts, "NotReady", podNotReadyTimeout, testutils.PodNotReady); err != nil {
0000000000000000000000000000000000000000;;						framework.Failf("Pods on node %s did not become NotReady within %v: %v", node.Name, podNotReadyTimeout, err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					sleepTime := maxTolerationTime + 20*time.Second
0000000000000000000000000000000000000000;;					By(fmt.Sprintf("Sleeping for %v and checking if all Pods were evicted", sleepTime))
0000000000000000000000000000000000000000;;					time.Sleep(sleepTime)
0000000000000000000000000000000000000000;;					pods, err = c.Core().Pods(v1.NamespaceAll).List(podOpts)
0000000000000000000000000000000000000000;;					framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;					seenRunning := []string{}
0000000000000000000000000000000000000000;;					for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;						namespacedName := fmt.Sprintf("%v/%v", pod.Namespace, pod.Name)
0000000000000000000000000000000000000000;;						shouldBeTerminating := true
0000000000000000000000000000000000000000;;						for _, neverEvictedPod := range neverEvictedPods {
0000000000000000000000000000000000000000;;							if neverEvictedPod == namespacedName {
0000000000000000000000000000000000000000;;								shouldBeTerminating = false
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						if pod.DeletionTimestamp == nil {
0000000000000000000000000000000000000000;;							seenRunning = append(seenRunning, namespacedName)
0000000000000000000000000000000000000000;;							if shouldBeTerminating {
0000000000000000000000000000000000000000;;								framework.Failf("Pod %v should have been deleted but was seen running", namespacedName)
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					for _, neverEvictedPod := range neverEvictedPods {
0000000000000000000000000000000000000000;;						running := false
0000000000000000000000000000000000000000;;						for _, runningPod := range seenRunning {
0000000000000000000000000000000000000000;;							if runningPod == neverEvictedPod {
0000000000000000000000000000000000000000;;								running = true
0000000000000000000000000000000000000000;;								break
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						if !running {
0000000000000000000000000000000000000000;;							framework.Failf("Pod %v was evicted even though it shouldn't", neverEvictedPod)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
