0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
07a8b302ecb4360689911faa7e85c674306cd479;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Logging soak [Performance] [Slow] [Disruptive]", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("logging-soak")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Not a global constant (irrelevant outside this test), also not a parameter (if you want more logs, use --scale=).
0000000000000000000000000000000000000000;;		kbRateInSeconds := 1 * time.Second
0000000000000000000000000000000000000000;;		totalLogTime := 2 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This test is designed to run and confirm that logs are being generated at a large scale, and that they can be grabbed by the kubelet.
0000000000000000000000000000000000000000;;		// By running it repeatedly in the background, you can simulate large collections of chatty containers.
0000000000000000000000000000000000000000;;		// This can expose problems in your docker configuration (logging), log searching infrastructure, to tune deployments to match high load
0000000000000000000000000000000000000000;;		// scenarios.  TODO jayunit100 add this to the kube CI in a follow on infra patch.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Returns scale (how many waves of pods).
0000000000000000000000000000000000000000;;		// Returns wave interval (how many seconds to wait before dumping the next wave of pods).
0000000000000000000000000000000000000000;;		readConfig := func() (int, time.Duration) {
0000000000000000000000000000000000000000;;			// Read in configuration settings, reasonable defaults.
0000000000000000000000000000000000000000;;			scale := framework.TestContext.LoggingSoak.Scale
0000000000000000000000000000000000000000;;			if framework.TestContext.LoggingSoak.Scale == 0 {
0000000000000000000000000000000000000000;;				scale = 1
0000000000000000000000000000000000000000;;				framework.Logf("Overriding default scale value of zero to %d", scale)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			milliSecondsBetweenWaves := framework.TestContext.LoggingSoak.MilliSecondsBetweenWaves
0000000000000000000000000000000000000000;;			if milliSecondsBetweenWaves == 0 {
0000000000000000000000000000000000000000;;				milliSecondsBetweenWaves = 5000
0000000000000000000000000000000000000000;;				framework.Logf("Overriding default milliseconds value of zero to %d", milliSecondsBetweenWaves)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			return scale, time.Duration(milliSecondsBetweenWaves) * time.Millisecond
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		scale, millisecondsBetweenWaves := readConfig()
0000000000000000000000000000000000000000;;		It(fmt.Sprintf("should survive logging 1KB every %v seconds, for a duration of %v, scaling up to %v pods per node", kbRateInSeconds, totalLogTime, scale), func() {
0000000000000000000000000000000000000000;;			defer GinkgoRecover()
0000000000000000000000000000000000000000;;			var wg sync.WaitGroup
0000000000000000000000000000000000000000;;			wg.Add(scale)
0000000000000000000000000000000000000000;;			for i := 0; i < scale; i++ {
0000000000000000000000000000000000000000;;				go func() {
0000000000000000000000000000000000000000;;					wave := fmt.Sprintf("wave%v", strconv.Itoa(i))
0000000000000000000000000000000000000000;;					framework.Logf("Starting logging soak, wave = %v", wave)
0000000000000000000000000000000000000000;;					RunLogPodsWithSleepOf(f, kbRateInSeconds, wave, totalLogTime)
0000000000000000000000000000000000000000;;					framework.Logf("Completed logging soak, wave %v", i)
0000000000000000000000000000000000000000;;					wg.Done()
0000000000000000000000000000000000000000;;				}()
0000000000000000000000000000000000000000;;				// Niceness.
0000000000000000000000000000000000000000;;				time.Sleep(millisecondsBetweenWaves)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.Logf("Waiting on all %v logging soak waves to complete", scale)
0000000000000000000000000000000000000000;;			wg.Wait()
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunLogPodsWithSleepOf creates a pod on every node, logs continuously (with "sleep" pauses), and verifies that the log string
0000000000000000000000000000000000000000;;	// was produced in each and every pod at least once.  The final arg is the timeout for the test to verify all the pods got logs.
0000000000000000000000000000000000000000;;	func RunLogPodsWithSleepOf(f *framework.Framework, sleep time.Duration, podname string, timeout time.Duration) {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nodes := framework.GetReadySchedulableNodesOrDie(f.ClientSet)
0000000000000000000000000000000000000000;;		totalPods := len(nodes.Items)
0000000000000000000000000000000000000000;;		Expect(totalPods).NotTo(Equal(0))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kilobyte := strings.Repeat("logs-123", 128) // 8*128=1024 = 1KB of text.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		appName := "logging-soak" + podname
0000000000000000000000000000000000000000;;		podlables := f.CreatePodsPerNodeForSimpleApp(
0000000000000000000000000000000000000000;;			appName,
0000000000000000000000000000000000000000;;			func(n v1.Node) v1.PodSpec {
0000000000000000000000000000000000000000;;				return v1.PodSpec{
0000000000000000000000000000000000000000;;					Containers: []v1.Container{{
0000000000000000000000000000000000000000;;						Name:  "logging-soak",
0000000000000000000000000000000000000000;;						Image: "gcr.io/google_containers/busybox:1.24",
0000000000000000000000000000000000000000;;						Args: []string{
0000000000000000000000000000000000000000;;							"/bin/sh",
0000000000000000000000000000000000000000;;							"-c",
0000000000000000000000000000000000000000;;							fmt.Sprintf("while true ; do echo %v ; sleep %v; done", kilobyte, sleep.Seconds()),
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					NodeName:      n.Name,
0000000000000000000000000000000000000000;;					RestartPolicy: v1.RestartPolicyAlways,
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			totalPods,
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		logSoakVerification := f.NewClusterVerification(
0000000000000000000000000000000000000000;;			f.Namespace,
0000000000000000000000000000000000000000;;			framework.PodStateVerification{
0000000000000000000000000000000000000000;;				Selectors:   podlables,
0000000000000000000000000000000000000000;;				ValidPhases: []v1.PodPhase{v1.PodRunning, v1.PodSucceeded},
0000000000000000000000000000000000000000;;				// we don't validate total log data, since there is no gaurantee all logs will be stored forever.
0000000000000000000000000000000000000000;;				// instead, we just validate that some logs are being created in std out.
0000000000000000000000000000000000000000;;				Verify: func(p v1.Pod) (bool, error) {
0000000000000000000000000000000000000000;;					s, err := framework.LookForStringInLog(f.Namespace.Name, p.Name, "logging-soak", "logs-123", 1*time.Second)
0000000000000000000000000000000000000000;;					return s != "", err
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		largeClusterForgiveness := time.Duration(len(nodes.Items)/5) * time.Second // i.e. a 100 node cluster gets an extra 20 seconds to complete.
0000000000000000000000000000000000000000;;		pods, err := logSoakVerification.WaitFor(totalPods, timeout+largeClusterForgiveness)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			framework.Failf("Error in wait... %v", err)
0000000000000000000000000000000000000000;;		} else if len(pods) < totalPods {
0000000000000000000000000000000000000000;;			framework.Failf("Only got %v out of %v", len(pods), totalPods)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
