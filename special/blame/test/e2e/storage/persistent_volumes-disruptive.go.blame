0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
2806e8a1e56d7118a437de601e26dd52c3345c42;test/e2e/persistent_volumes-disruptive.go[test/e2e/persistent_volumes-disruptive.go][test/e2e/storage/persistent_volumes-disruptive.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package storage
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		apierrs "k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type testBody func(c clientset.Interface, f *framework.Framework, clientPod *v1.Pod, pvc *v1.PersistentVolumeClaim, pv *v1.PersistentVolume)
0000000000000000000000000000000000000000;;	type disruptiveTest struct {
0000000000000000000000000000000000000000;;		testItStmt string
0000000000000000000000000000000000000000;;		runTest    testBody
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	type kubeletOpt string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		MinNodes                    = 2
0000000000000000000000000000000000000000;;		NodeStateTimeout            = 1 * time.Minute
0000000000000000000000000000000000000000;;		kStart           kubeletOpt = "start"
0000000000000000000000000000000000000000;;		kStop            kubeletOpt = "stop"
0000000000000000000000000000000000000000;;		kRestart         kubeletOpt = "restart"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = SIGDescribe("PersistentVolumes[Disruptive][Flaky]", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("disruptive-pv")
0000000000000000000000000000000000000000;;		var (
0000000000000000000000000000000000000000;;			c                         clientset.Interface
0000000000000000000000000000000000000000;;			ns                        string
0000000000000000000000000000000000000000;;			nfsServerPod              *v1.Pod
0000000000000000000000000000000000000000;;			nfsPVconfig               framework.PersistentVolumeConfig
0000000000000000000000000000000000000000;;			pvcConfig                 framework.PersistentVolumeClaimConfig
0000000000000000000000000000000000000000;;			nfsServerIP, clientNodeIP string
0000000000000000000000000000000000000000;;			clientNode                *v1.Node
0000000000000000000000000000000000000000;;			volLabel                  labels.Set
0000000000000000000000000000000000000000;;			selector                  *metav1.LabelSelector
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			// To protect the NFS volume pod from the kubelet restart, we isolate it on its own node.
0000000000000000000000000000000000000000;;			framework.SkipUnlessNodeCountIsAtLeast(MinNodes)
0000000000000000000000000000000000000000;;			framework.SkipIfProviderIs("local")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			c = f.ClientSet
0000000000000000000000000000000000000000;;			ns = f.Namespace.Name
0000000000000000000000000000000000000000;;			volLabel = labels.Set{framework.VolumeSelectorKey: ns}
0000000000000000000000000000000000000000;;			selector = metav1.SetAsLabelSelector(volLabel)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Start the NFS server pod.
0000000000000000000000000000000000000000;;			framework.Logf("[BeforeEach] Creating NFS Server Pod")
0000000000000000000000000000000000000000;;			nfsServerPod = initNFSserverPod(c, ns)
0000000000000000000000000000000000000000;;			framework.Logf("NFS server Pod %q created on Node %q", nfsServerPod.Name, nfsServerPod.Spec.NodeName)
0000000000000000000000000000000000000000;;			framework.Logf("[BeforeEach] Configuring PersistentVolume")
0000000000000000000000000000000000000000;;			nfsServerIP = nfsServerPod.Status.PodIP
0000000000000000000000000000000000000000;;			Expect(nfsServerIP).NotTo(BeEmpty())
0000000000000000000000000000000000000000;;			nfsPVconfig = framework.PersistentVolumeConfig{
0000000000000000000000000000000000000000;;				NamePrefix: "nfs-",
0000000000000000000000000000000000000000;;				Labels:     volLabel,
0000000000000000000000000000000000000000;;				PVSource: v1.PersistentVolumeSource{
0000000000000000000000000000000000000000;;					NFS: &v1.NFSVolumeSource{
0000000000000000000000000000000000000000;;						Server:   nfsServerIP,
0000000000000000000000000000000000000000;;						Path:     "/exports",
0000000000000000000000000000000000000000;;						ReadOnly: false,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pvcConfig = framework.PersistentVolumeClaimConfig{
0000000000000000000000000000000000000000;;				Annotations: map[string]string{
0000000000000000000000000000000000000000;;					v1.BetaStorageClassAnnotation: "",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Selector: selector,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Get the first ready node IP that is not hosting the NFS pod.
0000000000000000000000000000000000000000;;			if clientNodeIP == "" {
0000000000000000000000000000000000000000;;				framework.Logf("Designating test node")
0000000000000000000000000000000000000000;;				nodes := framework.GetReadySchedulableNodesOrDie(c)
0000000000000000000000000000000000000000;;				for _, node := range nodes.Items {
0000000000000000000000000000000000000000;;					if node.Name != nfsServerPod.Spec.NodeName {
0000000000000000000000000000000000000000;;						clientNode = &node
0000000000000000000000000000000000000000;;						clientNodeIP = framework.GetNodeExternalIP(clientNode)
0000000000000000000000000000000000000000;;						break
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				Expect(clientNodeIP).NotTo(BeEmpty())
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			framework.DeletePodWithWait(f, c, nfsServerPod)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		Context("when kubelet restarts", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			var (
0000000000000000000000000000000000000000;;				clientPod *v1.Pod
0000000000000000000000000000000000000000;;				pv        *v1.PersistentVolume
0000000000000000000000000000000000000000;;				pvc       *v1.PersistentVolumeClaim
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;			BeforeEach(func() {
0000000000000000000000000000000000000000;;				framework.Logf("Initializing test spec")
0000000000000000000000000000000000000000;;				clientPod, pv, pvc = initTestCase(f, c, nfsPVconfig, pvcConfig, ns, clientNode.Name)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			AfterEach(func() {
0000000000000000000000000000000000000000;;				framework.Logf("Tearing down test spec")
0000000000000000000000000000000000000000;;				tearDownTestCase(c, f, ns, clientPod, pvc, pv)
0000000000000000000000000000000000000000;;				pv, pvc, clientPod = nil, nil, nil
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			// Test table housing the It() title string and test spec.  runTest is type testBody, defined at
0000000000000000000000000000000000000000;;			// the start of this file.  To add tests, define a function mirroring the testBody signature and assign
0000000000000000000000000000000000000000;;			// to runTest.
0000000000000000000000000000000000000000;;			disruptiveTestTable := []disruptiveTest{
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					testItStmt: "Should test that a file written to the mount before kubelet restart is readable after restart.",
0000000000000000000000000000000000000000;;					runTest:    testKubeletRestartsAndRestoresMount,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					testItStmt: "Should test that a volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns.",
0000000000000000000000000000000000000000;;					runTest:    testVolumeUnmountsFromDeletedPod,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Test loop executes each disruptiveTest iteratively.
0000000000000000000000000000000000000000;;			for _, test := range disruptiveTestTable {
0000000000000000000000000000000000000000;;				func(t disruptiveTest) {
0000000000000000000000000000000000000000;;					It(t.testItStmt, func() {
0000000000000000000000000000000000000000;;						By("Executing Spec")
0000000000000000000000000000000000000000;;						t.runTest(c, f, clientPod, pvc, pv)
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;				}(test)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// testKubeletRestartsAndRestoresMount tests that a volume mounted to a pod remains mounted after a kubelet restarts
0000000000000000000000000000000000000000;;	func testKubeletRestartsAndRestoresMount(c clientset.Interface, f *framework.Framework, clientPod *v1.Pod, pvc *v1.PersistentVolumeClaim, pv *v1.PersistentVolume) {
0000000000000000000000000000000000000000;;		By("Writing to the volume.")
0000000000000000000000000000000000000000;;		file := "/mnt/_SUCCESS"
0000000000000000000000000000000000000000;;		out, err := podExec(clientPod, fmt.Sprintf("touch %s", file))
0000000000000000000000000000000000000000;;		framework.Logf(out)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Restarting kubelet")
0000000000000000000000000000000000000000;;		kubeletCommand(kRestart, c, clientPod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Testing that written file is accessible.")
0000000000000000000000000000000000000000;;		out, err = podExec(clientPod, fmt.Sprintf("cat %s", file))
0000000000000000000000000000000000000000;;		framework.Logf(out)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		framework.Logf("Volume mount detected on pod %s and written file %s is readable post-restart.", clientPod.Name, file)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// testVolumeUnmountsFromDeletedPod tests that a volume unmounts if the client pod was deleted while the kubelet was down.
0000000000000000000000000000000000000000;;	func testVolumeUnmountsFromDeletedPod(c clientset.Interface, f *framework.Framework, clientPod *v1.Pod, pvc *v1.PersistentVolumeClaim, pv *v1.PersistentVolume) {
0000000000000000000000000000000000000000;;		nodeIP, err := framework.GetHostExternalAddress(c, clientPod)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		nodeIP = nodeIP + ":22"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Expecting the volume mount to be found.")
0000000000000000000000000000000000000000;;		result, err := framework.SSH(fmt.Sprintf("mount | grep %s", clientPod.UID), nodeIP, framework.TestContext.Provider)
0000000000000000000000000000000000000000;;		framework.LogSSHResult(result)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred(), "Encountered SSH error.")
0000000000000000000000000000000000000000;;		Expect(result.Code).To(BeZero(), fmt.Sprintf("Expected grep exit code of 0, got %d", result.Code))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Stopping the kubelet.")
0000000000000000000000000000000000000000;;		kubeletCommand(kStop, c, clientPod)
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				kubeletCommand(kStart, c, clientPod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Deleting Pod %q", clientPod.Name))
0000000000000000000000000000000000000000;;		err = c.Core().Pods(clientPod.Namespace).Delete(clientPod.Name, &metav1.DeleteOptions{})
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		By("Starting the kubelet and waiting for pod to delete.")
0000000000000000000000000000000000000000;;		kubeletCommand(kStart, c, clientPod)
0000000000000000000000000000000000000000;;		err = f.WaitForPodTerminated(clientPod.Name, "")
0000000000000000000000000000000000000000;;		if !apierrs.IsNotFound(err) && err != nil {
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred(), "Expected pod to terminate.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Expecting the volume mount not to be found.")
0000000000000000000000000000000000000000;;		result, err = framework.SSH(fmt.Sprintf("mount | grep %s", clientPod.UID), nodeIP, framework.TestContext.Provider)
0000000000000000000000000000000000000000;;		framework.LogSSHResult(result)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred(), "Encountered SSH error.")
0000000000000000000000000000000000000000;;		Expect(result.Stdout).To(BeEmpty(), "Expected grep stdout to be empty (i.e. no mount found).")
0000000000000000000000000000000000000000;;		framework.Logf("Volume unmounted on node %s", clientPod.Spec.NodeName)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// initTestCase initializes spec resources (pv, pvc, and pod) and returns pointers to be consumed
0000000000000000000000000000000000000000;;	// by the test.
0000000000000000000000000000000000000000;;	func initTestCase(f *framework.Framework, c clientset.Interface, pvConfig framework.PersistentVolumeConfig, pvcConfig framework.PersistentVolumeClaimConfig, ns, nodeName string) (*v1.Pod, *v1.PersistentVolume, *v1.PersistentVolumeClaim) {
0000000000000000000000000000000000000000;;		pv, pvc, err := framework.CreatePVPVC(c, pvConfig, pvcConfig, ns, false)
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				framework.DeletePersistentVolumeClaim(c, pvc.Name, ns)
0000000000000000000000000000000000000000;;				framework.DeletePersistentVolume(c, pv.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		pod := framework.MakePod(ns, []*v1.PersistentVolumeClaim{pvc}, true, "")
0000000000000000000000000000000000000000;;		pod.Spec.NodeName = nodeName
0000000000000000000000000000000000000000;;		framework.Logf("Creating NFS client pod.")
0000000000000000000000000000000000000000;;		pod, err = c.CoreV1().Pods(ns).Create(pod)
0000000000000000000000000000000000000000;;		framework.Logf("NFS client Pod %q created on Node %q", pod.Name, nodeName)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				framework.DeletePodWithWait(f, c, pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;		err = framework.WaitForPodRunningInNamespace(c, pod)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred(), fmt.Sprintf("Pod %q timed out waiting for phase: Running", pod.Name))
0000000000000000000000000000000000000000;;		// Return created api objects
0000000000000000000000000000000000000000;;		pod, err = c.CoreV1().Pods(ns).Get(pod.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		pvc, err = c.CoreV1().PersistentVolumeClaims(ns).Get(pvc.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		pv, err = c.CoreV1().PersistentVolumes().Get(pv.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		return pod, pv, pvc
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// tearDownTestCase destroy resources created by initTestCase.
0000000000000000000000000000000000000000;;	func tearDownTestCase(c clientset.Interface, f *framework.Framework, ns string, client *v1.Pod, pvc *v1.PersistentVolumeClaim, pv *v1.PersistentVolume) {
0000000000000000000000000000000000000000;;		// Ignore deletion errors.  Failing on them will interrupt test cleanup.
0000000000000000000000000000000000000000;;		framework.DeletePodWithWait(f, c, client)
0000000000000000000000000000000000000000;;		framework.DeletePersistentVolumeClaim(c, pvc.Name, ns)
0000000000000000000000000000000000000000;;		framework.DeletePersistentVolume(c, pv.Name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// kubeletCommand performs `start`, `restart`, or `stop` on the kubelet running on the node of the target pod and waits
0000000000000000000000000000000000000000;;	// for the desired statues..
0000000000000000000000000000000000000000;;	// - First issues the command via `systemctl`
0000000000000000000000000000000000000000;;	// - If `systemctl` returns stderr "command not found, issues the command via `service`
0000000000000000000000000000000000000000;;	// - If `service` also returns stderr "command not found", the test is aborted.
0000000000000000000000000000000000000000;;	// Allowed kubeletOps are `kStart`, `kStop`, and `kRestart`
0000000000000000000000000000000000000000;;	func kubeletCommand(kOp kubeletOpt, c clientset.Interface, pod *v1.Pod) {
0000000000000000000000000000000000000000;;		nodeIP, err := framework.GetHostExternalAddress(c, pod)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		nodeIP = nodeIP + ":22"
0000000000000000000000000000000000000000;;		systemctlCmd := fmt.Sprintf("sudo systemctl %s kubelet", string(kOp))
0000000000000000000000000000000000000000;;		framework.Logf("Attempting `%s`", systemctlCmd)
0000000000000000000000000000000000000000;;		sshResult, err := framework.SSH(systemctlCmd, nodeIP, framework.TestContext.Provider)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred(), fmt.Sprintf("SSH to Node %q errored.", pod.Spec.NodeName))
0000000000000000000000000000000000000000;;		framework.LogSSHResult(sshResult)
0000000000000000000000000000000000000000;;		if strings.Contains(sshResult.Stderr, "command not found") {
0000000000000000000000000000000000000000;;			serviceCmd := fmt.Sprintf("sudo service kubelet %s", string(kOp))
0000000000000000000000000000000000000000;;			framework.Logf("Attempting `%s`", serviceCmd)
0000000000000000000000000000000000000000;;			sshResult, err = framework.SSH(serviceCmd, nodeIP, framework.TestContext.Provider)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred(), fmt.Sprintf("SSH to Node %q errored.", pod.Spec.NodeName))
0000000000000000000000000000000000000000;;			framework.LogSSHResult(sshResult)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Expect(sshResult.Code).To(BeZero(), "Failed to [%s] kubelet:\n%#v", string(kOp), sshResult)
0000000000000000000000000000000000000000;;		// On restart, waiting for node NotReady prevents a race condition where the node takes a few moments to leave the
0000000000000000000000000000000000000000;;		// Ready state which in turn short circuits WaitForNodeToBeReady()
0000000000000000000000000000000000000000;;		if kOp == kStop || kOp == kRestart {
0000000000000000000000000000000000000000;;			if ok := framework.WaitForNodeToBeNotReady(c, pod.Spec.NodeName, NodeStateTimeout); !ok {
0000000000000000000000000000000000000000;;				framework.Failf("Node %s failed to enter NotReady state", pod.Spec.NodeName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kOp == kStart || kOp == kRestart {
0000000000000000000000000000000000000000;;			if ok := framework.WaitForNodeToBeReady(c, pod.Spec.NodeName, NodeStateTimeout); !ok {
0000000000000000000000000000000000000000;;				framework.Failf("Node %s failed to enter Ready state", pod.Spec.NodeName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// podExec wraps RunKubectl to execute a bash cmd in target pod
0000000000000000000000000000000000000000;;	func podExec(pod *v1.Pod, bashExec string) (string, error) {
0000000000000000000000000000000000000000;;		return framework.RunKubectl("exec", fmt.Sprintf("--namespace=%s", pod.Namespace), pod.Name, "--", "/bin/sh", "-c", bashExec)
0000000000000000000000000000000000000000;;	}
