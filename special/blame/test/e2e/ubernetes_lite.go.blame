0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
439b2b22ba0540f7e9a63679909779baf089a581;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"math"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/intstr"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		kubeletapis "k8s.io/kubernetes/pkg/kubelet/apis"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Multi-AZ Clusters", func() {
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("multi-az")
0000000000000000000000000000000000000000;;		var zoneCount int
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		image := framework.ServeHostnameImage
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs("gce", "gke", "aws")
0000000000000000000000000000000000000000;;			if zoneCount <= 0 {
0000000000000000000000000000000000000000;;				zoneCount, err = getZoneCount(f.ClientSet)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Checking for multi-zone cluster.  Zone count = %d", zoneCount))
0000000000000000000000000000000000000000;;			msg := fmt.Sprintf("Zone count is %d, only run for multi-zone clusters, skipping test", zoneCount)
0000000000000000000000000000000000000000;;			framework.SkipUnlessAtLeast(zoneCount, 2, msg)
0000000000000000000000000000000000000000;;			// TODO: SkipUnlessDefaultScheduler() // Non-default schedulers might not spread
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		It("should spread the pods of a service across zones", func() {
0000000000000000000000000000000000000000;;			SpreadServiceOrFail(f, (2*zoneCount)+1, image)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should spread the pods of a replication controller across zones", func() {
0000000000000000000000000000000000000000;;			SpreadRCOrFail(f, int32((2*zoneCount)+1), image)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should schedule pods in the same zones as statically provisioned PVs", func() {
0000000000000000000000000000000000000000;;			PodsUseStaticPVsOrFail(f, (2*zoneCount)+1, image)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Check that the pods comprising a service get spread evenly across available zones
0000000000000000000000000000000000000000;;	func SpreadServiceOrFail(f *framework.Framework, replicaCount int, image string) {
0000000000000000000000000000000000000000;;		// First create the service
0000000000000000000000000000000000000000;;		serviceName := "test-service"
0000000000000000000000000000000000000000;;		serviceSpec := &v1.Service{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:      serviceName,
0000000000000000000000000000000000000000;;				Namespace: f.Namespace.Name,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.ServiceSpec{
0000000000000000000000000000000000000000;;				Selector: map[string]string{
0000000000000000000000000000000000000000;;					"service": serviceName,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Ports: []v1.ServicePort{{
0000000000000000000000000000000000000000;;					Port:       80,
0000000000000000000000000000000000000000;;					TargetPort: intstr.FromInt(80),
0000000000000000000000000000000000000000;;				}},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, err := f.ClientSet.Core().Services(f.Namespace.Name).Create(serviceSpec)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Now create some pods behind the service
0000000000000000000000000000000000000000;;		podSpec := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:   serviceName,
0000000000000000000000000000000000000000;;				Labels: map[string]string{"service": serviceName},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  "test",
0000000000000000000000000000000000000000;;						Image: framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Caution: StartPods requires at least one pod to replicate.
0000000000000000000000000000000000000000;;		// Based on the callers, replicas is always positive number: zoneCount >= 0 implies (2*zoneCount)+1 > 0.
0000000000000000000000000000000000000000;;		// Thus, no need to test for it. Once the precondition changes to zero number of replicas,
0000000000000000000000000000000000000000;;		// test for replicaCount > 0. Otherwise, StartPods panics.
0000000000000000000000000000000000000000;;		framework.ExpectNoError(testutils.StartPods(f.ClientSet, replicaCount, f.Namespace.Name, serviceName, *podSpec, false, framework.Logf))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Wait for all of them to be scheduled
0000000000000000000000000000000000000000;;		selector := labels.SelectorFromSet(labels.Set(map[string]string{"service": serviceName}))
0000000000000000000000000000000000000000;;		pods, err := framework.WaitForPodsWithLabelScheduled(f.ClientSet, f.Namespace.Name, selector)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Now make sure they're spread across zones
0000000000000000000000000000000000000000;;		zoneNames, err := getZoneNames(f.ClientSet)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		Expect(checkZoneSpreading(f.ClientSet, pods, zoneNames)).To(Equal(true))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Find the name of the zone in which a Node is running
0000000000000000000000000000000000000000;;	func getZoneNameForNode(node v1.Node) (string, error) {
0000000000000000000000000000000000000000;;		for key, value := range node.Labels {
0000000000000000000000000000000000000000;;			if key == kubeletapis.LabelZoneFailureDomain {
0000000000000000000000000000000000000000;;				return value, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return "", fmt.Errorf("Zone name for node %s not found. No label with key %s",
0000000000000000000000000000000000000000;;			node.Name, kubeletapis.LabelZoneFailureDomain)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Find the names of all zones in which we have nodes in this cluster.
0000000000000000000000000000000000000000;;	func getZoneNames(c clientset.Interface) ([]string, error) {
0000000000000000000000000000000000000000;;		zoneNames := sets.NewString()
0000000000000000000000000000000000000000;;		nodes, err := c.Core().Nodes().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, node := range nodes.Items {
0000000000000000000000000000000000000000;;			zoneName, err := getZoneNameForNode(node)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			zoneNames.Insert(zoneName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return zoneNames.List(), nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Return the number of zones in which we have nodes in this cluster.
0000000000000000000000000000000000000000;;	func getZoneCount(c clientset.Interface) (int, error) {
0000000000000000000000000000000000000000;;		zoneNames, err := getZoneNames(c)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return -1, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return len(zoneNames), nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Find the name of the zone in which the pod is scheduled
0000000000000000000000000000000000000000;;	func getZoneNameForPod(c clientset.Interface, pod v1.Pod) (string, error) {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Getting zone name for pod %s, on node %s", pod.Name, pod.Spec.NodeName))
0000000000000000000000000000000000000000;;		node, err := c.Core().Nodes().Get(pod.Spec.NodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		return getZoneNameForNode(*node)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Determine whether a set of pods are approximately evenly spread
0000000000000000000000000000000000000000;;	// across a given set of zones
0000000000000000000000000000000000000000;;	func checkZoneSpreading(c clientset.Interface, pods *v1.PodList, zoneNames []string) (bool, error) {
0000000000000000000000000000000000000000;;		podsPerZone := make(map[string]int)
0000000000000000000000000000000000000000;;		for _, zoneName := range zoneNames {
0000000000000000000000000000000000000000;;			podsPerZone[zoneName] = 0
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;			if pod.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			zoneName, err := getZoneNameForPod(c, pod)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			podsPerZone[zoneName] = podsPerZone[zoneName] + 1
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		minPodsPerZone := math.MaxInt32
0000000000000000000000000000000000000000;;		maxPodsPerZone := 0
0000000000000000000000000000000000000000;;		for _, podCount := range podsPerZone {
0000000000000000000000000000000000000000;;			if podCount < minPodsPerZone {
0000000000000000000000000000000000000000;;				minPodsPerZone = podCount
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if podCount > maxPodsPerZone {
0000000000000000000000000000000000000000;;				maxPodsPerZone = podCount
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		Expect(minPodsPerZone).To(BeNumerically("~", maxPodsPerZone, 1),
0000000000000000000000000000000000000000;;			"Pods were not evenly spread across zones.  %d in one zone and %d in another zone",
0000000000000000000000000000000000000000;;			minPodsPerZone, maxPodsPerZone)
0000000000000000000000000000000000000000;;		return true, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Check that the pods comprising a replication controller get spread evenly across available zones
0000000000000000000000000000000000000000;;	func SpreadRCOrFail(f *framework.Framework, replicaCount int32, image string) {
0000000000000000000000000000000000000000;;		name := "ubelite-spread-rc-" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Creating replication controller %s", name))
0000000000000000000000000000000000000000;;		controller, err := f.ClientSet.Core().ReplicationControllers(f.Namespace.Name).Create(&v1.ReplicationController{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Namespace: f.Namespace.Name,
0000000000000000000000000000000000000000;;				Name:      name,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.ReplicationControllerSpec{
0000000000000000000000000000000000000000;;				Replicas: &replicaCount,
0000000000000000000000000000000000000000;;				Selector: map[string]string{
0000000000000000000000000000000000000000;;					"name": name,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Template: &v1.PodTemplateSpec{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;						Labels: map[string]string{"name": name},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;						Containers: []v1.Container{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Name:  name,
0000000000000000000000000000000000000000;;								Image: image,
0000000000000000000000000000000000000000;;								Ports: []v1.ContainerPort{{ContainerPort: 9376}},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		// Cleanup the replication controller when we are done.
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			// Resize the replication controller to zero to get rid of pods.
0000000000000000000000000000000000000000;;			if err := framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, controller.Name); err != nil {
0000000000000000000000000000000000000000;;				framework.Logf("Failed to cleanup replication controller %v: %v.", controller.Name, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;		// List the pods, making sure we observe all the replicas.
0000000000000000000000000000000000000000;;		selector := labels.SelectorFromSet(labels.Set(map[string]string{"name": name}))
0000000000000000000000000000000000000000;;		pods, err := framework.PodsCreated(f.ClientSet, f.Namespace.Name, name, replicaCount)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Wait for all of them to be scheduled
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Waiting for %d replicas of %s to be scheduled.  Selector: %v", replicaCount, name, selector))
0000000000000000000000000000000000000000;;		pods, err = framework.WaitForPodsWithLabelScheduled(f.ClientSet, f.Namespace.Name, selector)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Now make sure they're spread across zones
0000000000000000000000000000000000000000;;		zoneNames, err := getZoneNames(f.ClientSet)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		Expect(checkZoneSpreading(f.ClientSet, pods, zoneNames)).To(Equal(true))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type StaticPVTestConfig struct {
0000000000000000000000000000000000000000;;		pvSource *v1.PersistentVolumeSource
0000000000000000000000000000000000000000;;		pv       *v1.PersistentVolume
0000000000000000000000000000000000000000;;		pvc      *v1.PersistentVolumeClaim
0000000000000000000000000000000000000000;;		pod      *v1.Pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Check that the pods using statically created PVs get scheduled to the same zone that the PV is in.
0000000000000000000000000000000000000000;;	func PodsUseStaticPVsOrFail(f *framework.Framework, podCount int, image string) {
0000000000000000000000000000000000000000;;		// TODO: add GKE after enabling admission plugin in GKE
0000000000000000000000000000000000000000;;		// TODO: add AWS
0000000000000000000000000000000000000000;;		framework.SkipUnlessProviderIs("gce")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		c := f.ClientSet
0000000000000000000000000000000000000000;;		ns := f.Namespace.Name
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		zones, err := getZoneNames(c)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Creating static PVs across zones")
0000000000000000000000000000000000000000;;		configs := make([]*StaticPVTestConfig, podCount)
0000000000000000000000000000000000000000;;		for i := range configs {
0000000000000000000000000000000000000000;;			configs[i] = &StaticPVTestConfig{}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			By("Cleaning up pods and PVs")
0000000000000000000000000000000000000000;;			for _, config := range configs {
0000000000000000000000000000000000000000;;				framework.DeletePodOrFail(c, ns, config.pod.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, config := range configs {
0000000000000000000000000000000000000000;;				framework.WaitForPodNoLongerRunningInNamespace(c, config.pod.Name, ns)
0000000000000000000000000000000000000000;;				framework.PVPVCCleanup(c, ns, config.pv, config.pvc)
0000000000000000000000000000000000000000;;				err = framework.DeletePVSource(config.pvSource)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i, config := range configs {
0000000000000000000000000000000000000000;;			zone := zones[i%len(zones)]
0000000000000000000000000000000000000000;;			config.pvSource, err = framework.CreatePVSource(zone)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			pvConfig := framework.PersistentVolumeConfig{
0000000000000000000000000000000000000000;;				NamePrefix: "multizone-pv",
0000000000000000000000000000000000000000;;				PVSource:   *config.pvSource,
0000000000000000000000000000000000000000;;				Prebind:    nil,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			className := ""
0000000000000000000000000000000000000000;;			pvcConfig := framework.PersistentVolumeClaimConfig{StorageClassName: &className}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			config.pv, config.pvc, err = framework.CreatePVPVC(c, pvConfig, pvcConfig, ns, true)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Waiting for all PVCs to be bound")
0000000000000000000000000000000000000000;;		for _, config := range configs {
0000000000000000000000000000000000000000;;			framework.WaitOnPVandPVC(c, ns, config.pv, config.pvc)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Creating pods for each static PV")
0000000000000000000000000000000000000000;;		for _, config := range configs {
0000000000000000000000000000000000000000;;			podConfig := framework.MakePod(ns, []*v1.PersistentVolumeClaim{config.pvc}, false, "")
0000000000000000000000000000000000000000;;			config.pod, err = c.Core().Pods(ns).Create(podConfig)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Waiting for all pods to be running")
0000000000000000000000000000000000000000;;		for _, config := range configs {
0000000000000000000000000000000000000000;;			err = framework.WaitForPodRunningInNamespace(c, config.pod)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
