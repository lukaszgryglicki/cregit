0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
bc362e4665099a8d4afb225655e5012f8dfb6927;test/e2e/autoscaling.go[test/e2e/autoscaling.go][test/e2e/autoscaling/cluster_size_autoscaling.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package autoscaling
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"bytes"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"io/ioutil"
0000000000000000000000000000000000000000;;		"math"
0000000000000000000000000000000000000000;;		"net/http"
0000000000000000000000000000000000000000;;		"os/exec"
0000000000000000000000000000000000000000;;		"regexp"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		policy "k8s.io/api/policy/v1beta1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/fields"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		utilerrors "k8s.io/apimachinery/pkg/util/errors"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/intstr"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/scheduling"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		defaultTimeout         = 3 * time.Minute
0000000000000000000000000000000000000000;;		resizeTimeout          = 5 * time.Minute
0000000000000000000000000000000000000000;;		scaleUpTimeout         = 5 * time.Minute
0000000000000000000000000000000000000000;;		scaleUpTriggerTimeout  = 2 * time.Minute
0000000000000000000000000000000000000000;;		scaleDownTimeout       = 20 * time.Minute
0000000000000000000000000000000000000000;;		podTimeout             = 2 * time.Minute
0000000000000000000000000000000000000000;;		nodesRecoverTimeout    = 5 * time.Minute
0000000000000000000000000000000000000000;;		rcCreationRetryTimeout = 4 * time.Minute
0000000000000000000000000000000000000000;;		rcCreationRetryDelay   = 20 * time.Second
0000000000000000000000000000000000000000;;		makeSchedulableTimeout = 10 * time.Minute
0000000000000000000000000000000000000000;;		makeSchedulableDelay   = 20 * time.Second
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		gkeEndpoint      = "https://test-container.sandbox.googleapis.com"
0000000000000000000000000000000000000000;;		gkeUpdateTimeout = 15 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		disabledTaint             = "DisabledForAutoscalingTest"
0000000000000000000000000000000000000000;;		criticalAddonsOnlyTaint   = "CriticalAddonsOnly"
0000000000000000000000000000000000000000;;		newNodesForScaledownTests = 2
0000000000000000000000000000000000000000;;		unhealthyClusterThreshold = 4
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		caNoScaleUpStatus      = "NoActivity"
0000000000000000000000000000000000000000;;		caOngoingScaleUpStatus = "InProgress"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Cluster size autoscaling [Slow]", func() {
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("autoscaling")
0000000000000000000000000000000000000000;;		var c clientset.Interface
0000000000000000000000000000000000000000;;		var nodeCount int
0000000000000000000000000000000000000000;;		var coresPerNode int
0000000000000000000000000000000000000000;;		var memCapacityMb int
0000000000000000000000000000000000000000;;		var originalSizes map[string]int
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			c = f.ClientSet
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs("gce", "gke")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			originalSizes = make(map[string]int)
0000000000000000000000000000000000000000;;			sum := 0
0000000000000000000000000000000000000000;;			for _, mig := range strings.Split(framework.TestContext.CloudConfig.NodeInstanceGroup, ",") {
0000000000000000000000000000000000000000;;				size, err := framework.GroupSize(mig)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("Initial size of %s: %d", mig, size))
0000000000000000000000000000000000000000;;				originalSizes[mig] = size
0000000000000000000000000000000000000000;;				sum += size
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Give instances time to spin up
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, sum, scaleUpTimeout))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodes := framework.GetReadySchedulableNodesOrDie(f.ClientSet)
0000000000000000000000000000000000000000;;			nodeCount = len(nodes.Items)
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Initial number of schedulable nodes: %v", nodeCount))
0000000000000000000000000000000000000000;;			Expect(nodeCount).NotTo(BeZero())
0000000000000000000000000000000000000000;;			cpu := nodes.Items[0].Status.Capacity[v1.ResourceCPU]
0000000000000000000000000000000000000000;;			mem := nodes.Items[0].Status.Capacity[v1.ResourceMemory]
0000000000000000000000000000000000000000;;			coresPerNode = int((&cpu).MilliValue() / 1000)
0000000000000000000000000000000000000000;;			memCapacityMb = int((&mem).Value() / 1024 / 1024)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			Expect(nodeCount).Should(Equal(sum))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if framework.ProviderIs("gke") {
0000000000000000000000000000000000000000;;				val, err := isAutoscalerEnabled(3)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;				if !val {
0000000000000000000000000000000000000000;;					err = enableAutoscaler("default-pool", 3, 5)
0000000000000000000000000000000000000000;;					framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Restoring initial size of the cluster"))
0000000000000000000000000000000000000000;;			setMigSizes(originalSizes)
0000000000000000000000000000000000000000;;			expectedNodes := 0
0000000000000000000000000000000000000000;;			for _, size := range originalSizes {
0000000000000000000000000000000000000000;;				expectedNodes += size
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, expectedNodes, scaleDownTimeout))
0000000000000000000000000000000000000000;;			nodes, err := c.Core().Nodes().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			s := time.Now()
0000000000000000000000000000000000000000;;		makeSchedulableLoop:
0000000000000000000000000000000000000000;;			for start := time.Now(); time.Since(start) < makeSchedulableTimeout; time.Sleep(makeSchedulableDelay) {
0000000000000000000000000000000000000000;;				for _, n := range nodes.Items {
0000000000000000000000000000000000000000;;					err = makeNodeSchedulable(c, &n, true)
0000000000000000000000000000000000000000;;					switch err.(type) {
0000000000000000000000000000000000000000;;					case CriticalAddonsOnlyError:
0000000000000000000000000000000000000000;;						continue makeSchedulableLoop
0000000000000000000000000000000000000000;;					default:
0000000000000000000000000000000000000000;;						framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("Made nodes schedulable again in %v", time.Now().Sub(s).String())
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("shouldn't increase cluster size if pending pod is too large [Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			By("Creating unschedulable pod")
0000000000000000000000000000000000000000;;			ReserveMemory(f, "memory-reservation", 1, int(1.1*float64(memCapacityMb)), false, defaultTimeout)
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "memory-reservation")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Waiting for scale up hoping it won't happen")
0000000000000000000000000000000000000000;;			// Verfiy, that the appropreate event was generated.
0000000000000000000000000000000000000000;;			eventFound := false
0000000000000000000000000000000000000000;;		EventsLoop:
0000000000000000000000000000000000000000;;			for start := time.Now(); time.Since(start) < scaleUpTimeout; time.Sleep(20 * time.Second) {
0000000000000000000000000000000000000000;;				By("Waiting for NotTriggerScaleUp event")
0000000000000000000000000000000000000000;;				events, err := f.ClientSet.Core().Events(f.Namespace.Name).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				for _, e := range events.Items {
0000000000000000000000000000000000000000;;					if e.InvolvedObject.Kind == "Pod" && e.Reason == "NotTriggerScaleUp" && strings.Contains(e.Message, "it wouldn't fit if a new node is added") {
0000000000000000000000000000000000000000;;						By("NotTriggerScaleUp event found")
0000000000000000000000000000000000000000;;						eventFound = true
0000000000000000000000000000000000000000;;						break EventsLoop
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Expect(eventFound).Should(Equal(true))
0000000000000000000000000000000000000000;;			// Verify that cluster size is not changed
0000000000000000000000000000000000000000;;			framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet,
0000000000000000000000000000000000000000;;				func(size int) bool { return size <= nodeCount }, time.Second))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		simpleScaleUpTest := func(unready int) {
0000000000000000000000000000000000000000;;			ReserveMemory(f, "memory-reservation", 100, nodeCount*memCapacityMb, false, 1*time.Second)
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "memory-reservation")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Verify that cluster size is increased
0000000000000000000000000000000000000000;;			framework.ExpectNoError(WaitForClusterSizeFuncWithUnready(f.ClientSet,
0000000000000000000000000000000000000000;;				func(size int) bool { return size >= nodeCount+1 }, scaleUpTimeout, unready))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should increase cluster size if pending pods are small [Feature:ClusterSizeAutoscalingScaleUp]",
0000000000000000000000000000000000000000;;			func() { simpleScaleUpTest(0) })
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should increase cluster size if pending pods are small and one node is broken [Feature:ClusterSizeAutoscalingScaleUp]",
0000000000000000000000000000000000000000;;			func() {
0000000000000000000000000000000000000000;;				framework.TestUnderTemporaryNetworkFailure(c, "default", getAnyNode(c), func() { simpleScaleUpTest(1) })
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("shouldn't trigger additional scale-ups during processing scale-up [Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			status, err := getScaleUpStatus(c)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			unmanagedNodes := nodeCount - status.ready
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Schedule more pods than can fit and wait for claster to scale-up")
0000000000000000000000000000000000000000;;			ReserveMemory(f, "memory-reservation", 100, nodeCount*memCapacityMb, false, 1*time.Second)
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "memory-reservation")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			status, err = waitForScaleUpStatus(c, caOngoingScaleUpStatus, scaleUpTriggerTimeout)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			target := status.target
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Expect no more scale-up to be happening after all pods are scheduled")
0000000000000000000000000000000000000000;;			status, err = getScaleUpStatus(c)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			if status.target != target {
0000000000000000000000000000000000000000;;				glog.Warningf("Final number of nodes (%v) does not match initial scale-up target (%v).", status.target, target)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Expect(status.status).Should(Equal(caNoScaleUpStatus))
0000000000000000000000000000000000000000;;			Expect(status.ready).Should(Equal(status.target))
0000000000000000000000000000000000000000;;			Expect(len(framework.GetReadySchedulableNodesOrDie(f.ClientSet).Items)).Should(Equal(status.target + unmanagedNodes))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should increase cluster size if pending pods are small and there is another node pool that is not autoscaled [Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs("gke")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Creating new node-pool with one n1-standard-4 machine")
0000000000000000000000000000000000000000;;			const extraPoolName = "extra-pool"
0000000000000000000000000000000000000000;;			addNodePool(extraPoolName, "n1-standard-4", 1)
0000000000000000000000000000000000000000;;			defer deleteNodePool(extraPoolName)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, nodeCount+1, resizeTimeout))
0000000000000000000000000000000000000000;;			glog.Infof("Not enabling cluster autoscaler for the node pool (on purpose).")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			ReserveMemory(f, "memory-reservation", 100, nodeCount*memCapacityMb, false, defaultTimeout)
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "memory-reservation")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Verify, that cluster size is increased
0000000000000000000000000000000000000000;;			framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet,
0000000000000000000000000000000000000000;;				func(size int) bool { return size >= nodeCount+1 }, scaleUpTimeout))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should disable node pool autoscaling [Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs("gke")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Creating new node-pool with one n1-standard-4 machine")
0000000000000000000000000000000000000000;;			const extraPoolName = "extra-pool"
0000000000000000000000000000000000000000;;			addNodePool(extraPoolName, "n1-standard-4", 1)
0000000000000000000000000000000000000000;;			defer deleteNodePool(extraPoolName)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, nodeCount+1, resizeTimeout))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(enableAutoscaler(extraPoolName, 1, 2))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(disableAutoscaler(extraPoolName, 1, 2))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should increase cluster size if pods are pending due to host port conflict [Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			scheduling.CreateHostPortPods(f, "host-port", nodeCount+2, false)
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "host-port")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet,
0000000000000000000000000000000000000000;;				func(size int) bool { return size >= nodeCount+2 }, scaleUpTimeout))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should increase cluster size if pods are pending due to pod anti-affinity [Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			pods := nodeCount
0000000000000000000000000000000000000000;;			newPods := 2
0000000000000000000000000000000000000000;;			labels := map[string]string{
0000000000000000000000000000000000000000;;				"anti-affinity": "yes",
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			By("starting a pod with anti-affinity on each node")
0000000000000000000000000000000000000000;;			framework.ExpectNoError(runAntiAffinityPods(f, f.Namespace.Name, pods, "some-pod", labels, labels))
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "some-pod")
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("scheduling extra pods with anti-affinity to existing ones")
0000000000000000000000000000000000000000;;			framework.ExpectNoError(runAntiAffinityPods(f, f.Namespace.Name, newPods, "extra-pod", labels, labels))
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "extra-pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, nodeCount+newPods, scaleUpTimeout))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should increase cluster size if pod requesting EmptyDir volume is pending [Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			By("creating pods")
0000000000000000000000000000000000000000;;			pods := nodeCount
0000000000000000000000000000000000000000;;			newPods := 1
0000000000000000000000000000000000000000;;			labels := map[string]string{
0000000000000000000000000000000000000000;;				"anti-affinity": "yes",
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.ExpectNoError(runAntiAffinityPods(f, f.Namespace.Name, pods, "some-pod", labels, labels))
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "some-pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("waiting for all pods before triggering scale up")
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("creating a pod requesting EmptyDir")
0000000000000000000000000000000000000000;;			framework.ExpectNoError(runVolumeAntiAffinityPods(f, f.Namespace.Name, newPods, "extra-pod", labels, labels, emptyDirVolumes))
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "extra-pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, nodeCount+newPods, scaleUpTimeout))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should increase cluster size if pod requesting volume is pending [Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs("gce", "gke")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			volumeLabels := labels.Set{
0000000000000000000000000000000000000000;;				framework.VolumeSelectorKey: f.Namespace.Name,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			selector := metav1.SetAsLabelSelector(volumeLabels)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("creating volume & pvc")
0000000000000000000000000000000000000000;;			diskName, err := framework.CreatePDWithRetry()
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			pvConfig := framework.PersistentVolumeConfig{
0000000000000000000000000000000000000000;;				NamePrefix: "gce-",
0000000000000000000000000000000000000000;;				Labels:     volumeLabels,
0000000000000000000000000000000000000000;;				PVSource: v1.PersistentVolumeSource{
0000000000000000000000000000000000000000;;					GCEPersistentDisk: &v1.GCEPersistentDiskVolumeSource{
0000000000000000000000000000000000000000;;						PDName:   diskName,
0000000000000000000000000000000000000000;;						FSType:   "ext3",
0000000000000000000000000000000000000000;;						ReadOnly: false,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Prebind: nil,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pvcConfig := framework.PersistentVolumeClaimConfig{
0000000000000000000000000000000000000000;;				Annotations: map[string]string{
0000000000000000000000000000000000000000;;					v1.BetaStorageClassAnnotation: "",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Selector: selector,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			pv, pvc, err := framework.CreatePVPVC(c, pvConfig, pvcConfig, f.Namespace.Name, false)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitOnPVandPVC(c, f.Namespace.Name, pv, pvc))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			defer func() {
0000000000000000000000000000000000000000;;				errs := framework.PVPVCCleanup(c, f.Namespace.Name, pv, pvc)
0000000000000000000000000000000000000000;;				if len(errs) > 0 {
0000000000000000000000000000000000000000;;					framework.Failf("failed to delete PVC and/or PV. Errors: %v", utilerrors.NewAggregate(errs))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				pv, pvc = nil, nil
0000000000000000000000000000000000000000;;				if diskName != "" {
0000000000000000000000000000000000000000;;					framework.ExpectNoError(framework.DeletePDWithRetry(diskName))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("creating pods")
0000000000000000000000000000000000000000;;			pods := nodeCount
0000000000000000000000000000000000000000;;			labels := map[string]string{
0000000000000000000000000000000000000000;;				"anti-affinity": "yes",
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.ExpectNoError(runAntiAffinityPods(f, f.Namespace.Name, pods, "some-pod", labels, labels))
0000000000000000000000000000000000000000;;			defer func() {
0000000000000000000000000000000000000000;;				framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "some-pod")
0000000000000000000000000000000000000000;;				glog.Infof("RC and pods not using volume deleted")
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("waiting for all pods before triggering scale up")
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("creating a pod requesting PVC")
0000000000000000000000000000000000000000;;			pvcPodName := "pvc-pod"
0000000000000000000000000000000000000000;;			newPods := 1
0000000000000000000000000000000000000000;;			volumes := buildVolumes(pv, pvc)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(runVolumeAntiAffinityPods(f, f.Namespace.Name, newPods, pvcPodName, labels, labels, volumes))
0000000000000000000000000000000000000000;;			defer func() {
0000000000000000000000000000000000000000;;				framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, pvcPodName)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, nodeCount+newPods, scaleUpTimeout))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should add node to the particular mig [Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			labelKey := "cluster-autoscaling-test.special-node"
0000000000000000000000000000000000000000;;			labelValue := "true"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Finding the smallest MIG")
0000000000000000000000000000000000000000;;			minMig := ""
0000000000000000000000000000000000000000;;			minSize := nodeCount
0000000000000000000000000000000000000000;;			for mig, size := range originalSizes {
0000000000000000000000000000000000000000;;				if size <= minSize {
0000000000000000000000000000000000000000;;					minMig = mig
0000000000000000000000000000000000000000;;					minSize = size
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			removeLabels := func(nodesToClean sets.String) {
0000000000000000000000000000000000000000;;				By("Removing labels from nodes")
0000000000000000000000000000000000000000;;				for node := range nodesToClean {
0000000000000000000000000000000000000000;;					framework.RemoveLabelOffNode(c, node, labelKey)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodes, err := framework.GetGroupNodes(minMig)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			nodesSet := sets.NewString(nodes...)
0000000000000000000000000000000000000000;;			defer removeLabels(nodesSet)
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Annotating nodes of the smallest MIG(%s): %v", minMig, nodes))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for node := range nodesSet {
0000000000000000000000000000000000000000;;				framework.AddOrUpdateLabelOnNode(c, node, labelKey, labelValue)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			CreateNodeSelectorPods(f, "node-selector", minSize+1, map[string]string{labelKey: labelValue}, false)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Waiting for new node to appear and annotating it")
0000000000000000000000000000000000000000;;			framework.WaitForGroupSize(minMig, int32(minSize+1))
0000000000000000000000000000000000000000;;			// Verify, that cluster size is increased
0000000000000000000000000000000000000000;;			framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet,
0000000000000000000000000000000000000000;;				func(size int) bool { return size >= nodeCount+1 }, scaleUpTimeout))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			newNodes, err := framework.GetGroupNodes(minMig)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			newNodesSet := sets.NewString(newNodes...)
0000000000000000000000000000000000000000;;			newNodesSet.Delete(nodes...)
0000000000000000000000000000000000000000;;			if len(newNodesSet) > 1 {
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("Spotted following new nodes in %s: %v", minMig, newNodesSet))
0000000000000000000000000000000000000000;;				glog.Infof("Usually only 1 new node is expected, investigating")
0000000000000000000000000000000000000000;;				glog.Infof("Kubectl:%s\n", framework.RunKubectlOrDie("get", "nodes", "-o", "json"))
0000000000000000000000000000000000000000;;				if output, err := exec.Command("gcloud", "compute", "instances", "list",
0000000000000000000000000000000000000000;;					"--project="+framework.TestContext.CloudConfig.ProjectID,
0000000000000000000000000000000000000000;;					"--zone="+framework.TestContext.CloudConfig.Zone).Output(); err == nil {
0000000000000000000000000000000000000000;;					glog.Infof("Gcloud compute instances list: %s", output)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					glog.Errorf("Failed to get instances list: %v", err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				for newNode := range newNodesSet {
0000000000000000000000000000000000000000;;					if output, err := execCmd("gcloud", "compute", "instances", "describe",
0000000000000000000000000000000000000000;;						newNode,
0000000000000000000000000000000000000000;;						"--project="+framework.TestContext.CloudConfig.ProjectID,
0000000000000000000000000000000000000000;;						"--zone="+framework.TestContext.CloudConfig.Zone).Output(); err == nil {
0000000000000000000000000000000000000000;;						glog.Infof("Gcloud compute instances describe: %s", output)
0000000000000000000000000000000000000000;;					} else {
0000000000000000000000000000000000000000;;						glog.Errorf("Failed to get instances describe: %v", err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// TODO: possibly remove broken node from newNodesSet to prevent removeLabel from crashing.
0000000000000000000000000000000000000000;;				// However at this moment we DO WANT it to crash so that we don't check all test runs for the
0000000000000000000000000000000000000000;;				// rare behavior, but only the broken ones.
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("New nodes: %v\n", newNodesSet))
0000000000000000000000000000000000000000;;			registeredNodes := sets.NewString()
0000000000000000000000000000000000000000;;			for nodeName := range newNodesSet {
0000000000000000000000000000000000000000;;				node, err := f.ClientSet.Core().Nodes().Get(nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				if err == nil && node != nil {
0000000000000000000000000000000000000000;;					registeredNodes.Insert(nodeName)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					glog.Errorf("Failed to get node %v: %v", nodeName, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Setting labels for registered new nodes: %v", registeredNodes.List()))
0000000000000000000000000000000000000000;;			for node := range registeredNodes {
0000000000000000000000000000000000000000;;				framework.AddOrUpdateLabelOnNode(c, node, labelKey, labelValue)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			defer removeLabels(registeredNodes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "node-selector"))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should scale up correct target pool [Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs("gke")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Creating new node-pool with one n1-standard-4 machine")
0000000000000000000000000000000000000000;;			const extraPoolName = "extra-pool"
0000000000000000000000000000000000000000;;			addNodePool(extraPoolName, "n1-standard-4", 1)
0000000000000000000000000000000000000000;;			defer deleteNodePool(extraPoolName)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, nodeCount+1, resizeTimeout))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(enableAutoscaler(extraPoolName, 1, 2))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Creating rc with 2 pods too big to fit default-pool but fitting extra-pool")
0000000000000000000000000000000000000000;;			ReserveMemory(f, "memory-reservation", 2, int(2.1*float64(memCapacityMb)), false, defaultTimeout)
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "memory-reservation")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Apparently GKE master is restarted couple minutes after the node pool is added
0000000000000000000000000000000000000000;;			// reseting all the timers in scale down code. Adding 5 extra minutes to workaround
0000000000000000000000000000000000000000;;			// this issue.
0000000000000000000000000000000000000000;;			// TODO: Remove the extra time when GKE restart is fixed.
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, nodeCount+2, scaleUpTimeout+5*time.Minute))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		simpleScaleDownTest := func(unready int) {
0000000000000000000000000000000000000000;;			cleanup, err := addKubeSystemPdbs(f)
0000000000000000000000000000000000000000;;			defer cleanup()
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Manually increase cluster size")
0000000000000000000000000000000000000000;;			increasedSize := 0
0000000000000000000000000000000000000000;;			newSizes := make(map[string]int)
0000000000000000000000000000000000000000;;			for key, val := range originalSizes {
0000000000000000000000000000000000000000;;				newSizes[key] = val + 2 + unready
0000000000000000000000000000000000000000;;				increasedSize += val + 2 + unready
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			setMigSizes(newSizes)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(WaitForClusterSizeFuncWithUnready(f.ClientSet,
0000000000000000000000000000000000000000;;				func(size int) bool { return size >= increasedSize }, scaleUpTimeout, unready))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Some node should be removed")
0000000000000000000000000000000000000000;;			framework.ExpectNoError(WaitForClusterSizeFuncWithUnready(f.ClientSet,
0000000000000000000000000000000000000000;;				func(size int) bool { return size < increasedSize }, scaleDownTimeout, unready))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should correctly scale down after a node is not needed [Feature:ClusterSizeAutoscalingScaleDown]",
0000000000000000000000000000000000000000;;			func() { simpleScaleDownTest(0) })
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should correctly scale down after a node is not needed and one node is broken [Feature:ClusterSizeAutoscalingScaleDown]",
0000000000000000000000000000000000000000;;			func() {
0000000000000000000000000000000000000000;;				framework.TestUnderTemporaryNetworkFailure(c, "default", getAnyNode(c), func() { simpleScaleDownTest(1) })
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should correctly scale down after a node is not needed when there is non autoscaled pool[Feature:ClusterSizeAutoscalingScaleDown]", func() {
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs("gke")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			increasedSize := manuallyIncreaseClusterSize(f, originalSizes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			const extraPoolName = "extra-pool"
0000000000000000000000000000000000000000;;			addNodePool(extraPoolName, "n1-standard-1", 3)
0000000000000000000000000000000000000000;;			defer deleteNodePool(extraPoolName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet,
0000000000000000000000000000000000000000;;				func(size int) bool { return size >= increasedSize+3 }, scaleUpTimeout))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Some node should be removed")
0000000000000000000000000000000000000000;;			// Apparently GKE master is restarted couple minutes after the node pool is added
0000000000000000000000000000000000000000;;			// reseting all the timers in scale down code. Adding 10 extra minutes to workaround
0000000000000000000000000000000000000000;;			// this issue.
0000000000000000000000000000000000000000;;			// TODO: Remove the extra time when GKE restart is fixed.
0000000000000000000000000000000000000000;;			framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet,
0000000000000000000000000000000000000000;;				func(size int) bool { return size < increasedSize+3 }, scaleDownTimeout+10*time.Minute))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should be able to scale down when rescheduling a pod is required and pdb allows for it[Feature:ClusterSizeAutoscalingScaleDown]", func() {
0000000000000000000000000000000000000000;;			runDrainTest(f, originalSizes, f.Namespace.Name, 1, 1, func(increasedSize int) {
0000000000000000000000000000000000000000;;				By("Some node should be removed")
0000000000000000000000000000000000000000;;				framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet,
0000000000000000000000000000000000000000;;					func(size int) bool { return size < increasedSize }, scaleDownTimeout))
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("shouldn't be able to scale down when rescheduling a pod is required, but pdb doesn't allow drain[Feature:ClusterSizeAutoscalingScaleDown]", func() {
0000000000000000000000000000000000000000;;			runDrainTest(f, originalSizes, f.Namespace.Name, 1, 0, func(increasedSize int) {
0000000000000000000000000000000000000000;;				By("No nodes should be removed")
0000000000000000000000000000000000000000;;				time.Sleep(scaleDownTimeout)
0000000000000000000000000000000000000000;;				nodes := framework.GetReadySchedulableNodesOrDie(f.ClientSet)
0000000000000000000000000000000000000000;;				Expect(len(nodes.Items)).Should(Equal(increasedSize))
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should be able to scale down by draining multiple pods one by one as dictated by pdb[Feature:ClusterSizeAutoscalingScaleDown]", func() {
0000000000000000000000000000000000000000;;			runDrainTest(f, originalSizes, f.Namespace.Name, 2, 1, func(increasedSize int) {
0000000000000000000000000000000000000000;;				By("Some node should be removed")
0000000000000000000000000000000000000000;;				framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet,
0000000000000000000000000000000000000000;;					func(size int) bool { return size < increasedSize }, scaleDownTimeout))
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should be able to scale down by draining system pods with pdb[Feature:ClusterSizeAutoscalingScaleDown]", func() {
0000000000000000000000000000000000000000;;			runDrainTest(f, originalSizes, "kube-system", 2, 1, func(increasedSize int) {
0000000000000000000000000000000000000000;;				By("Some node should be removed")
0000000000000000000000000000000000000000;;				framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet,
0000000000000000000000000000000000000000;;					func(size int) bool { return size < increasedSize }, scaleDownTimeout))
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("Should be able to scale a node group up from 0[Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			framework.SkipUnlessAtLeast(len(originalSizes), 2, "At least 2 node groups are needed for scale-to-0 tests")
0000000000000000000000000000000000000000;;			By("Manually scale smallest node group to 0")
0000000000000000000000000000000000000000;;			minMig := ""
0000000000000000000000000000000000000000;;			minSize := nodeCount
0000000000000000000000000000000000000000;;			for mig, size := range originalSizes {
0000000000000000000000000000000000000000;;				if size <= minSize {
0000000000000000000000000000000000000000;;					minMig = mig
0000000000000000000000000000000000000000;;					minSize = size
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			err := framework.ResizeGroup(minMig, int32(0))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, nodeCount-minSize, resizeTimeout))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Make remaining nodes unschedulable")
0000000000000000000000000000000000000000;;			nodes, err := f.ClientSet.Core().Nodes().List(metav1.ListOptions{FieldSelector: fields.Set{
0000000000000000000000000000000000000000;;				"spec.unschedulable": "false",
0000000000000000000000000000000000000000;;			}.AsSelector().String()})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, node := range nodes.Items {
0000000000000000000000000000000000000000;;				err = makeNodeUnschedulable(f.ClientSet, &node)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				defer func(n v1.Node) {
0000000000000000000000000000000000000000;;					makeNodeSchedulable(f.ClientSet, &n, false)
0000000000000000000000000000000000000000;;				}(node)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Run a scale-up test")
0000000000000000000000000000000000000000;;			ReserveMemory(f, "memory-reservation", 1, 100, false, 1*time.Second)
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "memory-reservation")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Verify that cluster size is increased
0000000000000000000000000000000000000000;;			framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet,
0000000000000000000000000000000000000000;;				func(size int) bool { return size >= len(nodes.Items)+1 }, scaleUpTimeout))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(waitForAllCaPodsReadyInNamespace(f, c))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("Shouldn't perform scale up operation and should list unhealthy status if most of the cluster is broken[Feature:ClusterSizeAutoscalingScaleUp]", func() {
0000000000000000000000000000000000000000;;			clusterSize := nodeCount
0000000000000000000000000000000000000000;;			for clusterSize < unhealthyClusterThreshold+1 {
0000000000000000000000000000000000000000;;				clusterSize = manuallyIncreaseClusterSize(f, originalSizes)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Block network connectivity to some nodes to simulate unhealthy cluster")
0000000000000000000000000000000000000000;;			nodesToBreakCount := int(math.Floor(math.Max(float64(unhealthyClusterThreshold), 0.5*float64(clusterSize))))
0000000000000000000000000000000000000000;;			nodes, err := f.ClientSet.Core().Nodes().List(metav1.ListOptions{FieldSelector: fields.Set{
0000000000000000000000000000000000000000;;				"spec.unschedulable": "false",
0000000000000000000000000000000000000000;;			}.AsSelector().String()})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			Expect(nodesToBreakCount <= len(nodes.Items)).To(BeTrue())
0000000000000000000000000000000000000000;;			nodesToBreak := nodes.Items[:nodesToBreakCount]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// TestUnderTemporaryNetworkFailure only removes connectivity to a single node,
0000000000000000000000000000000000000000;;			// and accepts func() callback. This is expanding the loop to recursive call
0000000000000000000000000000000000000000;;			// to avoid duplicating TestUnderTemporaryNetworkFailure
0000000000000000000000000000000000000000;;			var testFunction func()
0000000000000000000000000000000000000000;;			testFunction = func() {
0000000000000000000000000000000000000000;;				if len(nodesToBreak) > 0 {
0000000000000000000000000000000000000000;;					ntb := &nodesToBreak[0]
0000000000000000000000000000000000000000;;					nodesToBreak = nodesToBreak[1:]
0000000000000000000000000000000000000000;;					framework.TestUnderTemporaryNetworkFailure(c, "default", ntb, testFunction)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					ReserveMemory(f, "memory-reservation", 100, nodeCount*memCapacityMb, false, defaultTimeout)
0000000000000000000000000000000000000000;;					defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, "memory-reservation")
0000000000000000000000000000000000000000;;					time.Sleep(scaleUpTimeout)
0000000000000000000000000000000000000000;;					currentNodes := framework.GetReadySchedulableNodesOrDie(f.ClientSet)
0000000000000000000000000000000000000000;;					framework.Logf("Currently available nodes: %v, nodes available at the start of test: %v, disabled nodes: %v", len(currentNodes.Items), len(nodes.Items), nodesToBreakCount)
0000000000000000000000000000000000000000;;					Expect(len(currentNodes.Items)).Should(Equal(len(nodes.Items) - nodesToBreakCount))
0000000000000000000000000000000000000000;;					status, err := getClusterwideStatus(c)
0000000000000000000000000000000000000000;;					framework.Logf("Clusterwide status: %v", status)
0000000000000000000000000000000000000000;;					framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;					Expect(status).Should(Equal("Unhealthy"))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			testFunction()
0000000000000000000000000000000000000000;;			// Give nodes time to recover from network failure
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForClusterSize(c, len(nodes.Items), nodesRecoverTimeout))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func execCmd(args ...string) *exec.Cmd {
0000000000000000000000000000000000000000;;		glog.Infof("Executing: %s", strings.Join(args, " "))
0000000000000000000000000000000000000000;;		return exec.Command(args[0], args[1:]...)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func runDrainTest(f *framework.Framework, migSizes map[string]int, namespace string, podsPerNode, pdbSize int, verifyFunction func(int)) {
0000000000000000000000000000000000000000;;		increasedSize := manuallyIncreaseClusterSize(f, migSizes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nodes, err := f.ClientSet.Core().Nodes().List(metav1.ListOptions{FieldSelector: fields.Set{
0000000000000000000000000000000000000000;;			"spec.unschedulable": "false",
0000000000000000000000000000000000000000;;		}.AsSelector().String()})
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		numPods := len(nodes.Items) * podsPerNode
0000000000000000000000000000000000000000;;		testId := string(uuid.NewUUID()) // So that we can label and find pods
0000000000000000000000000000000000000000;;		labelMap := map[string]string{"test_id": testId}
0000000000000000000000000000000000000000;;		framework.ExpectNoError(runReplicatedPodOnEachNode(f, nodes.Items, namespace, podsPerNode, "reschedulable-pods", labelMap))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, namespace, "reschedulable-pods")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Create a PodDisruptionBudget")
0000000000000000000000000000000000000000;;		minAvailable := intstr.FromInt(numPods - pdbSize)
0000000000000000000000000000000000000000;;		pdb := &policy.PodDisruptionBudget{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:      "test_pdb",
0000000000000000000000000000000000000000;;				Namespace: namespace,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: policy.PodDisruptionBudgetSpec{
0000000000000000000000000000000000000000;;				Selector:     &metav1.LabelSelector{MatchLabels: labelMap},
0000000000000000000000000000000000000000;;				MinAvailable: &minAvailable,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, err = f.StagingClient.Policy().PodDisruptionBudgets(namespace).Create(pdb)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			f.StagingClient.Policy().PodDisruptionBudgets(namespace).Delete(pdb.Name, &metav1.DeleteOptions{})
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		verifyFunction(increasedSize)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getGKEClusterUrl() string {
0000000000000000000000000000000000000000;;		out, err := execCmd("gcloud", "auth", "print-access-token").Output()
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		token := strings.Replace(string(out), "\n", "", -1)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return fmt.Sprintf("%s/v1/projects/%s/zones/%s/clusters/%s?access_token=%s",
0000000000000000000000000000000000000000;;			gkeEndpoint,
0000000000000000000000000000000000000000;;			framework.TestContext.CloudConfig.ProjectID,
0000000000000000000000000000000000000000;;			framework.TestContext.CloudConfig.Zone,
0000000000000000000000000000000000000000;;			framework.TestContext.CloudConfig.Cluster,
0000000000000000000000000000000000000000;;			token)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func isAutoscalerEnabled(expectedMinNodeCountInTargetPool int) (bool, error) {
0000000000000000000000000000000000000000;;		resp, err := http.Get(getGKEClusterUrl())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer resp.Body.Close()
0000000000000000000000000000000000000000;;		body, err := ioutil.ReadAll(resp.Body)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		strBody := string(body)
0000000000000000000000000000000000000000;;		if strings.Contains(strBody, "\"minNodeCount\": "+strconv.Itoa(expectedMinNodeCountInTargetPool)) {
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func enableAutoscaler(nodePool string, minCount, maxCount int) error {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if nodePool == "default-pool" {
0000000000000000000000000000000000000000;;			glog.Infof("Using gcloud to enable autoscaling for pool %s", nodePool)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			output, err := execCmd("gcloud", "alpha", "container", "clusters", "update", framework.TestContext.CloudConfig.Cluster,
0000000000000000000000000000000000000000;;				"--enable-autoscaling",
0000000000000000000000000000000000000000;;				"--min-nodes="+strconv.Itoa(minCount),
0000000000000000000000000000000000000000;;				"--max-nodes="+strconv.Itoa(maxCount),
0000000000000000000000000000000000000000;;				"--node-pool="+nodePool,
0000000000000000000000000000000000000000;;				"--project="+framework.TestContext.CloudConfig.ProjectID,
0000000000000000000000000000000000000000;;				"--zone="+framework.TestContext.CloudConfig.Zone).Output()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("Failed to enable autoscaling: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("Config update result: %s", output)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.Infof("Using direct api access to enable autoscaling for pool %s", nodePool)
0000000000000000000000000000000000000000;;			updateRequest := "{" +
0000000000000000000000000000000000000000;;				" \"update\": {" +
0000000000000000000000000000000000000000;;				"  \"desiredNodePoolId\": \"" + nodePool + "\"," +
0000000000000000000000000000000000000000;;				"  \"desiredNodePoolAutoscaling\": {" +
0000000000000000000000000000000000000000;;				"   \"enabled\": \"true\"," +
0000000000000000000000000000000000000000;;				"   \"minNodeCount\": \"" + strconv.Itoa(minCount) + "\"," +
0000000000000000000000000000000000000000;;				"   \"maxNodeCount\": \"" + strconv.Itoa(maxCount) + "\"" +
0000000000000000000000000000000000000000;;				"  }" +
0000000000000000000000000000000000000000;;				" }" +
0000000000000000000000000000000000000000;;				"}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			url := getGKEClusterUrl()
0000000000000000000000000000000000000000;;			glog.Infof("Using gke api url %s", url)
0000000000000000000000000000000000000000;;			putResult, err := doPut(url, updateRequest)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("Failed to put %s: %v", url, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("Config update result: %s", putResult)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for startTime := time.Now(); startTime.Add(gkeUpdateTimeout).After(time.Now()); time.Sleep(30 * time.Second) {
0000000000000000000000000000000000000000;;			if val, err := isAutoscalerEnabled(minCount); err == nil && val {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("autoscaler not enabled")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func disableAutoscaler(nodePool string, minCount, maxCount int) error {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if nodePool == "default-pool" {
0000000000000000000000000000000000000000;;			glog.Infof("Using gcloud to disable autoscaling for pool %s", nodePool)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			output, err := execCmd("gcloud", "alpha", "container", "clusters", "update", framework.TestContext.CloudConfig.Cluster,
0000000000000000000000000000000000000000;;				"--no-enable-autoscaling",
0000000000000000000000000000000000000000;;				"--node-pool="+nodePool,
0000000000000000000000000000000000000000;;				"--project="+framework.TestContext.CloudConfig.ProjectID,
0000000000000000000000000000000000000000;;				"--zone="+framework.TestContext.CloudConfig.Zone).Output()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("Failed to enable autoscaling: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("Config update result: %s", output)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.Infof("Using direct api access to disable autoscaling for pool %s", nodePool)
0000000000000000000000000000000000000000;;			updateRequest := "{" +
0000000000000000000000000000000000000000;;				" \"update\": {" +
0000000000000000000000000000000000000000;;				"  \"desiredNodePoolId\": \"" + nodePool + "\"," +
0000000000000000000000000000000000000000;;				"  \"desiredNodePoolAutoscaling\": {" +
0000000000000000000000000000000000000000;;				"   \"enabled\": \"false\"," +
0000000000000000000000000000000000000000;;				"  }" +
0000000000000000000000000000000000000000;;				" }" +
0000000000000000000000000000000000000000;;				"}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			url := getGKEClusterUrl()
0000000000000000000000000000000000000000;;			glog.Infof("Using gke api url %s", url)
0000000000000000000000000000000000000000;;			putResult, err := doPut(url, updateRequest)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("Failed to put %s: %v", url, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("Config update result: %s", putResult)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for startTime := time.Now(); startTime.Add(gkeUpdateTimeout).After(time.Now()); time.Sleep(30 * time.Second) {
0000000000000000000000000000000000000000;;			if val, err := isAutoscalerEnabled(minCount); err == nil && !val {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("autoscaler still enabled")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func addNodePool(name string, machineType string, numNodes int) {
0000000000000000000000000000000000000000;;		output, err := execCmd("gcloud", "alpha", "container", "node-pools", "create", name, "--quiet",
0000000000000000000000000000000000000000;;			"--machine-type="+machineType,
0000000000000000000000000000000000000000;;			"--num-nodes="+strconv.Itoa(numNodes),
0000000000000000000000000000000000000000;;			"--project="+framework.TestContext.CloudConfig.ProjectID,
0000000000000000000000000000000000000000;;			"--zone="+framework.TestContext.CloudConfig.Zone,
0000000000000000000000000000000000000000;;			"--cluster="+framework.TestContext.CloudConfig.Cluster).CombinedOutput()
0000000000000000000000000000000000000000;;		glog.Infof("Creating node-pool %s: %s", name, output)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func deleteNodePool(name string) {
0000000000000000000000000000000000000000;;		glog.Infof("Deleting node pool %s", name)
0000000000000000000000000000000000000000;;		output, err := execCmd("gcloud", "alpha", "container", "node-pools", "delete", name, "--quiet",
0000000000000000000000000000000000000000;;			"--project="+framework.TestContext.CloudConfig.ProjectID,
0000000000000000000000000000000000000000;;			"--zone="+framework.TestContext.CloudConfig.Zone,
0000000000000000000000000000000000000000;;			"--cluster="+framework.TestContext.CloudConfig.Cluster).CombinedOutput()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Infof("Error: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.Infof("Node-pool deletion output: %s", output)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func doPut(url, content string) (string, error) {
0000000000000000000000000000000000000000;;		req, err := http.NewRequest("PUT", url, bytes.NewBuffer([]byte(content)))
0000000000000000000000000000000000000000;;		req.Header.Set("Content-Type", "application/json")
0000000000000000000000000000000000000000;;		client := &http.Client{}
0000000000000000000000000000000000000000;;		resp, err := client.Do(req)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer resp.Body.Close()
0000000000000000000000000000000000000000;;		body, err := ioutil.ReadAll(resp.Body)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		strBody := string(body)
0000000000000000000000000000000000000000;;		return strBody, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func CreateNodeSelectorPods(f *framework.Framework, id string, replicas int, nodeSelector map[string]string, expectRunning bool) {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Running RC which reserves host port and defines node selector"))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		config := &testutils.RCConfig{
0000000000000000000000000000000000000000;;			Client:         f.ClientSet,
0000000000000000000000000000000000000000;;			InternalClient: f.InternalClientset,
0000000000000000000000000000000000000000;;			Name:           id,
0000000000000000000000000000000000000000;;			Namespace:      f.Namespace.Name,
0000000000000000000000000000000000000000;;			Timeout:        defaultTimeout,
0000000000000000000000000000000000000000;;			Image:          framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;			Replicas:       replicas,
0000000000000000000000000000000000000000;;			HostPorts:      map[string]int{"port1": 4321},
0000000000000000000000000000000000000000;;			NodeSelector:   nodeSelector,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err := framework.RunRC(*config)
0000000000000000000000000000000000000000;;		if expectRunning {
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ReserveMemory(f *framework.Framework, id string, replicas, megabytes int, expectRunning bool, timeout time.Duration) {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Running RC which reserves %v MB of memory", megabytes))
0000000000000000000000000000000000000000;;		request := int64(1024 * 1024 * megabytes / replicas)
0000000000000000000000000000000000000000;;		config := &testutils.RCConfig{
0000000000000000000000000000000000000000;;			Client:         f.ClientSet,
0000000000000000000000000000000000000000;;			InternalClient: f.InternalClientset,
0000000000000000000000000000000000000000;;			Name:           id,
0000000000000000000000000000000000000000;;			Namespace:      f.Namespace.Name,
0000000000000000000000000000000000000000;;			Timeout:        timeout,
0000000000000000000000000000000000000000;;			Image:          framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;			Replicas:       replicas,
0000000000000000000000000000000000000000;;			MemRequest:     request,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < rcCreationRetryTimeout; time.Sleep(rcCreationRetryDelay) {
0000000000000000000000000000000000000000;;			err := framework.RunRC(*config)
0000000000000000000000000000000000000000;;			if err != nil && strings.Contains(err.Error(), "Error creating replication controller") {
0000000000000000000000000000000000000000;;				glog.Warningf("Failed to create memory reservation: %v", err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if expectRunning {
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		framework.Failf("Failed to reserve memory within timeout")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForClusterSize waits until the cluster size matches the given function.
0000000000000000000000000000000000000000;;	func WaitForClusterSizeFunc(c clientset.Interface, sizeFunc func(int) bool, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		return WaitForClusterSizeFuncWithUnready(c, sizeFunc, timeout, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForClusterSizeWithUnready waits until the cluster size matches the given function and assumes some unready nodes.
0000000000000000000000000000000000000000;;	func WaitForClusterSizeFuncWithUnready(c clientset.Interface, sizeFunc func(int) bool, timeout time.Duration, expectedUnready int) error {
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(20 * time.Second) {
0000000000000000000000000000000000000000;;			nodes, err := c.Core().Nodes().List(metav1.ListOptions{FieldSelector: fields.Set{
0000000000000000000000000000000000000000;;				"spec.unschedulable": "false",
0000000000000000000000000000000000000000;;			}.AsSelector().String()})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Warningf("Failed to list nodes: %v", err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			numNodes := len(nodes.Items)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Filter out not-ready nodes.
0000000000000000000000000000000000000000;;			framework.FilterNodes(nodes, func(node v1.Node) bool {
0000000000000000000000000000000000000000;;				return framework.IsNodeConditionSetAsExpected(&node, v1.NodeReady, true)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			numReady := len(nodes.Items)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if numNodes == numReady+expectedUnready && sizeFunc(numNodes) {
0000000000000000000000000000000000000000;;				glog.Infof("Cluster has reached the desired size")
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("Waiting for cluster, current size %d, not ready nodes %d", numNodes, numNodes-numReady)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("timeout waiting %v for appropriate cluster size", timeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func waitForAllCaPodsReadyInNamespace(f *framework.Framework, c clientset.Interface) error {
0000000000000000000000000000000000000000;;		var notready []string
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Now().Before(start.Add(scaleUpTimeout)); time.Sleep(20 * time.Second) {
0000000000000000000000000000000000000000;;			pods, err := c.Core().Pods(f.Namespace.Name).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("failed to get pods: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			notready = make([]string, 0)
0000000000000000000000000000000000000000;;			for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;				ready := false
0000000000000000000000000000000000000000;;				for _, c := range pod.Status.Conditions {
0000000000000000000000000000000000000000;;					if c.Type == v1.PodReady && c.Status == v1.ConditionTrue {
0000000000000000000000000000000000000000;;						ready = true
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// Failed pods in this context generally mean that they have been
0000000000000000000000000000000000000000;;				// double scheduled onto a node, but then failed a constraint check.
0000000000000000000000000000000000000000;;				if pod.Status.Phase == v1.PodFailed {
0000000000000000000000000000000000000000;;					glog.Warningf("Pod has failed: %v", pod)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if !ready && pod.Status.Phase != v1.PodFailed {
0000000000000000000000000000000000000000;;					notready = append(notready, pod.Name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(notready) == 0 {
0000000000000000000000000000000000000000;;				glog.Infof("All pods ready")
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("Some pods are not ready yet: %v", notready)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.Info("Timeout on waiting for pods being ready")
0000000000000000000000000000000000000000;;		glog.Info(framework.RunKubectlOrDie("get", "pods", "-o", "json", "--all-namespaces"))
0000000000000000000000000000000000000000;;		glog.Info(framework.RunKubectlOrDie("get", "nodes", "-o", "json"))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Some pods are still not running.
0000000000000000000000000000000000000000;;		return fmt.Errorf("Some pods are still not running: %v", notready)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getAnyNode(c clientset.Interface) *v1.Node {
0000000000000000000000000000000000000000;;		nodes, err := c.Core().Nodes().List(metav1.ListOptions{FieldSelector: fields.Set{
0000000000000000000000000000000000000000;;			"spec.unschedulable": "false",
0000000000000000000000000000000000000000;;		}.AsSelector().String()})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Failed to get node list: %v", err)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(nodes.Items) == 0 {
0000000000000000000000000000000000000000;;			glog.Errorf("No nodes")
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return &nodes.Items[0]
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func setMigSizes(sizes map[string]int) bool {
0000000000000000000000000000000000000000;;		madeChanges := false
0000000000000000000000000000000000000000;;		for mig, desiredSize := range sizes {
0000000000000000000000000000000000000000;;			currentSize, err := framework.GroupSize(mig)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			if desiredSize != currentSize {
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("Setting size of %s to %d", mig, desiredSize))
0000000000000000000000000000000000000000;;				err = framework.ResizeGroup(mig, int32(desiredSize))
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;				madeChanges = true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return madeChanges
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func makeNodeUnschedulable(c clientset.Interface, node *v1.Node) error {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Taint node %s", node.Name))
0000000000000000000000000000000000000000;;		for j := 0; j < 3; j++ {
0000000000000000000000000000000000000000;;			freshNode, err := c.Core().Nodes().Get(node.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, taint := range freshNode.Spec.Taints {
0000000000000000000000000000000000000000;;				if taint.Key == disabledTaint {
0000000000000000000000000000000000000000;;					return nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			freshNode.Spec.Taints = append(freshNode.Spec.Taints, v1.Taint{
0000000000000000000000000000000000000000;;				Key:    disabledTaint,
0000000000000000000000000000000000000000;;				Value:  "DisabledForTest",
0000000000000000000000000000000000000000;;				Effect: v1.TaintEffectNoSchedule,
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			_, err = c.Core().Nodes().Update(freshNode)
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if !errors.IsConflict(err) {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Warningf("Got 409 conflict when trying to taint node, retries left: %v", 3-j)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("Failed to taint node in allowed number of retries")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type CriticalAddonsOnlyError struct{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (_ CriticalAddonsOnlyError) Error() string {
0000000000000000000000000000000000000000;;		return fmt.Sprintf("CriticalAddonsOnly taint found on node")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func makeNodeSchedulable(c clientset.Interface, node *v1.Node, failOnCriticalAddonsOnly bool) error {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Remove taint from node %s", node.Name))
0000000000000000000000000000000000000000;;		for j := 0; j < 3; j++ {
0000000000000000000000000000000000000000;;			freshNode, err := c.Core().Nodes().Get(node.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			newTaints := make([]v1.Taint, 0)
0000000000000000000000000000000000000000;;			for _, taint := range freshNode.Spec.Taints {
0000000000000000000000000000000000000000;;				if failOnCriticalAddonsOnly && taint.Key == criticalAddonsOnlyTaint {
0000000000000000000000000000000000000000;;					return CriticalAddonsOnlyError{}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if taint.Key != disabledTaint {
0000000000000000000000000000000000000000;;					newTaints = append(newTaints, taint)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if len(newTaints) == len(freshNode.Spec.Taints) {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			freshNode.Spec.Taints = newTaints
0000000000000000000000000000000000000000;;			_, err = c.Core().Nodes().Update(freshNode)
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if !errors.IsConflict(err) {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Warningf("Got 409 conflict when trying to taint node, retries left: %v", 3-j)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("Failed to remove taint from node in allowed number of retries")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Create an RC running a given number of pods with anti-affinity
0000000000000000000000000000000000000000;;	func runAntiAffinityPods(f *framework.Framework, namespace string, pods int, id string, podLabels, antiAffinityLabels map[string]string) error {
0000000000000000000000000000000000000000;;		config := &testutils.RCConfig{
0000000000000000000000000000000000000000;;			Affinity:       buildAntiAffinity(antiAffinityLabels),
0000000000000000000000000000000000000000;;			Client:         f.ClientSet,
0000000000000000000000000000000000000000;;			InternalClient: f.InternalClientset,
0000000000000000000000000000000000000000;;			Name:           id,
0000000000000000000000000000000000000000;;			Namespace:      namespace,
0000000000000000000000000000000000000000;;			Timeout:        scaleUpTimeout,
0000000000000000000000000000000000000000;;			Image:          framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;			Replicas:       pods,
0000000000000000000000000000000000000000;;			Labels:         podLabels,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err := framework.RunRC(*config)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, err = f.ClientSet.Core().ReplicationControllers(namespace).Get(id, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func runVolumeAntiAffinityPods(f *framework.Framework, namespace string, pods int, id string, podLabels, antiAffinityLabels map[string]string, volumes []v1.Volume) error {
0000000000000000000000000000000000000000;;		config := &testutils.RCConfig{
0000000000000000000000000000000000000000;;			Affinity:       buildAntiAffinity(antiAffinityLabels),
0000000000000000000000000000000000000000;;			Volumes:        volumes,
0000000000000000000000000000000000000000;;			Client:         f.ClientSet,
0000000000000000000000000000000000000000;;			InternalClient: f.InternalClientset,
0000000000000000000000000000000000000000;;			Name:           id,
0000000000000000000000000000000000000000;;			Namespace:      namespace,
0000000000000000000000000000000000000000;;			Timeout:        scaleUpTimeout,
0000000000000000000000000000000000000000;;			Image:          framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;			Replicas:       pods,
0000000000000000000000000000000000000000;;			Labels:         podLabels,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err := framework.RunRC(*config)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, err = f.ClientSet.Core().ReplicationControllers(namespace).Get(id, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var emptyDirVolumes = []v1.Volume{
0000000000000000000000000000000000000000;;		{
0000000000000000000000000000000000000000;;			Name: "empty-volume",
0000000000000000000000000000000000000000;;			VolumeSource: v1.VolumeSource{
0000000000000000000000000000000000000000;;				EmptyDir: &v1.EmptyDirVolumeSource{},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		},
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func buildVolumes(pv *v1.PersistentVolume, pvc *v1.PersistentVolumeClaim) []v1.Volume {
0000000000000000000000000000000000000000;;		return []v1.Volume{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				Name: pv.Name,
0000000000000000000000000000000000000000;;				VolumeSource: v1.VolumeSource{
0000000000000000000000000000000000000000;;					PersistentVolumeClaim: &v1.PersistentVolumeClaimVolumeSource{
0000000000000000000000000000000000000000;;						ClaimName: pvc.Name,
0000000000000000000000000000000000000000;;						ReadOnly:  false,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func buildAntiAffinity(labels map[string]string) *v1.Affinity {
0000000000000000000000000000000000000000;;		return &v1.Affinity{
0000000000000000000000000000000000000000;;			PodAntiAffinity: &v1.PodAntiAffinity{
0000000000000000000000000000000000000000;;				RequiredDuringSchedulingIgnoredDuringExecution: []v1.PodAffinityTerm{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						LabelSelector: &metav1.LabelSelector{
0000000000000000000000000000000000000000;;							MatchLabels: labels,
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						TopologyKey: "kubernetes.io/hostname",
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Create an RC running a given number of pods on each node without adding any constraint forcing
0000000000000000000000000000000000000000;;	// such pod distribution. This is meant to create a bunch of underutilized (but not unused) nodes
0000000000000000000000000000000000000000;;	// with pods that can be rescheduled on different nodes.
0000000000000000000000000000000000000000;;	// This is achieved using the following method:
0000000000000000000000000000000000000000;;	// 1. disable scheduling on each node
0000000000000000000000000000000000000000;;	// 2. create an empty RC
0000000000000000000000000000000000000000;;	// 3. for each node:
0000000000000000000000000000000000000000;;	// 3a. enable scheduling on that node
0000000000000000000000000000000000000000;;	// 3b. increase number of replicas in RC by podsPerNode
0000000000000000000000000000000000000000;;	func runReplicatedPodOnEachNode(f *framework.Framework, nodes []v1.Node, namespace string, podsPerNode int, id string, labels map[string]string) error {
0000000000000000000000000000000000000000;;		By("Run a pod on each node")
0000000000000000000000000000000000000000;;		for _, node := range nodes {
0000000000000000000000000000000000000000;;			err := makeNodeUnschedulable(f.ClientSet, &node)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			defer func(n v1.Node) {
0000000000000000000000000000000000000000;;				makeNodeSchedulable(f.ClientSet, &n, false)
0000000000000000000000000000000000000000;;			}(node)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		config := &testutils.RCConfig{
0000000000000000000000000000000000000000;;			Client:         f.ClientSet,
0000000000000000000000000000000000000000;;			InternalClient: f.InternalClientset,
0000000000000000000000000000000000000000;;			Name:           id,
0000000000000000000000000000000000000000;;			Namespace:      namespace,
0000000000000000000000000000000000000000;;			Timeout:        defaultTimeout,
0000000000000000000000000000000000000000;;			Image:          framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;			Replicas:       0,
0000000000000000000000000000000000000000;;			Labels:         labels,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err := framework.RunRC(*config)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rc, err := f.ClientSet.Core().ReplicationControllers(namespace).Get(id, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i, node := range nodes {
0000000000000000000000000000000000000000;;			err = makeNodeSchedulable(f.ClientSet, &node, false)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Update replicas count, to create new pods that will be allocated on node
0000000000000000000000000000000000000000;;			// (we retry 409 errors in case rc reference got out of sync)
0000000000000000000000000000000000000000;;			for j := 0; j < 3; j++ {
0000000000000000000000000000000000000000;;				*rc.Spec.Replicas = int32((i + 1) * podsPerNode)
0000000000000000000000000000000000000000;;				rc, err = f.ClientSet.Core().ReplicationControllers(namespace).Update(rc)
0000000000000000000000000000000000000000;;				if err == nil {
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if !errors.IsConflict(err) {
0000000000000000000000000000000000000000;;					return err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				glog.Warningf("Got 409 conflict when trying to scale RC, retries left: %v", 3-j)
0000000000000000000000000000000000000000;;				rc, err = f.ClientSet.Core().ReplicationControllers(namespace).Get(id, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			err = wait.PollImmediate(5*time.Second, podTimeout, func() (bool, error) {
0000000000000000000000000000000000000000;;				rc, err = f.ClientSet.Core().ReplicationControllers(namespace).Get(id, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				if err != nil || rc.Status.ReadyReplicas < int32((i+1)*podsPerNode) {
0000000000000000000000000000000000000000;;					return false, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("failed to coerce RC into spawning a pod on node %s within timeout", node.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			err = makeNodeUnschedulable(f.ClientSet, &node)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Increase cluster size by newNodesForScaledownTests to create some unused nodes
0000000000000000000000000000000000000000;;	// that can be later removed by cluster autoscaler.
0000000000000000000000000000000000000000;;	func manuallyIncreaseClusterSize(f *framework.Framework, originalSizes map[string]int) int {
0000000000000000000000000000000000000000;;		By("Manually increase cluster size")
0000000000000000000000000000000000000000;;		increasedSize := 0
0000000000000000000000000000000000000000;;		newSizes := make(map[string]int)
0000000000000000000000000000000000000000;;		for key, val := range originalSizes {
0000000000000000000000000000000000000000;;			newSizes[key] = val + newNodesForScaledownTests
0000000000000000000000000000000000000000;;			increasedSize += val + newNodesForScaledownTests
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		setMigSizes(newSizes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		checkClusterSize := func(size int) bool {
0000000000000000000000000000000000000000;;			if size >= increasedSize {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			resized := setMigSizes(newSizes)
0000000000000000000000000000000000000000;;			if resized {
0000000000000000000000000000000000000000;;				glog.Warning("Unexpected node group size while waiting for cluster resize. Setting size to target again.")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.ExpectNoError(WaitForClusterSizeFunc(f.ClientSet, checkClusterSize, scaleUpTimeout))
0000000000000000000000000000000000000000;;		return increasedSize
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Try to get clusterwide health from CA status configmap.
0000000000000000000000000000000000000000;;	// Status configmap is not parsing-friendly, so evil regexpery follows.
0000000000000000000000000000000000000000;;	func getClusterwideStatus(c clientset.Interface) (string, error) {
0000000000000000000000000000000000000000;;		configMap, err := c.CoreV1().ConfigMaps("kube-system").Get("cluster-autoscaler-status", metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		status, ok := configMap.Data["status"]
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			return "", fmt.Errorf("Status information not found in configmap")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		matcher, err := regexp.Compile("Cluster-wide:\\s*\n\\s*Health:\\s*([A-Za-z]+)")
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		result := matcher.FindStringSubmatch(status)
0000000000000000000000000000000000000000;;		if len(result) < 2 {
0000000000000000000000000000000000000000;;			return "", fmt.Errorf("Failed to parse CA status configmap, raw status: %v", status)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result[1], nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type scaleUpStatus struct {
0000000000000000000000000000000000000000;;		status string
0000000000000000000000000000000000000000;;		ready  int
0000000000000000000000000000000000000000;;		target int
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Try to get scaleup statuses of all node groups.
0000000000000000000000000000000000000000;;	// Status configmap is not parsing-friendly, so evil regexpery follows.
0000000000000000000000000000000000000000;;	func getScaleUpStatus(c clientset.Interface) (*scaleUpStatus, error) {
0000000000000000000000000000000000000000;;		configMap, err := c.CoreV1().ConfigMaps("kube-system").Get("cluster-autoscaler-status", metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		status, ok := configMap.Data["status"]
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("Status information not found in configmap")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		matcher, err := regexp.Compile("s*ScaleUp:\\s*([A-Za-z]+)\\s*\\(ready=([0-9]+)\\s*cloudProviderTarget=([0-9]+)\\s*\\)")
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		matches := matcher.FindAllStringSubmatch(status, -1)
0000000000000000000000000000000000000000;;		if len(matches) < 1 {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("Failed to parse CA status configmap, raw status: %v", status)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		result := scaleUpStatus{
0000000000000000000000000000000000000000;;			status: caNoScaleUpStatus,
0000000000000000000000000000000000000000;;			ready:  0,
0000000000000000000000000000000000000000;;			target: 0,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, match := range matches {
0000000000000000000000000000000000000000;;			if match[1] == caOngoingScaleUpStatus {
0000000000000000000000000000000000000000;;				result.status = caOngoingScaleUpStatus
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			newReady, err := strconv.Atoi(match[2])
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			result.ready += newReady
0000000000000000000000000000000000000000;;			newTarget, err := strconv.Atoi(match[3])
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			result.target += newTarget
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.Infof("Cluster-Autoscaler scale-up status: %v (%v, %v)", result.status, result.ready, result.target)
0000000000000000000000000000000000000000;;		return &result, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func waitForScaleUpStatus(c clientset.Interface, expected string, timeout time.Duration) (*scaleUpStatus, error) {
0000000000000000000000000000000000000000;;		for start := time.Now(); time.Since(start) < timeout; time.Sleep(5 * time.Second) {
0000000000000000000000000000000000000000;;			status, err := getScaleUpStatus(c)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if status.status == expected {
0000000000000000000000000000000000000000;;				return status, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil, fmt.Errorf("ScaleUp status did not reach expected value: %v", expected)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// This is a temporary fix to allow CA to migrate some kube-system pods
0000000000000000000000000000000000000000;;	// TODO: Remove this when the PDB is added for those components
0000000000000000000000000000000000000000;;	func addKubeSystemPdbs(f *framework.Framework) (func(), error) {
0000000000000000000000000000000000000000;;		By("Create PodDisruptionBudgets for kube-system components, so they can be migrated if required")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newPdbs := make([]string, 0)
0000000000000000000000000000000000000000;;		cleanup := func() {
0000000000000000000000000000000000000000;;			for _, newPdbName := range newPdbs {
0000000000000000000000000000000000000000;;				f.StagingClient.Policy().PodDisruptionBudgets("kube-system").Delete(newPdbName, &metav1.DeleteOptions{})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		type pdbInfo struct {
0000000000000000000000000000000000000000;;			label         string
0000000000000000000000000000000000000000;;			min_available int
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pdbsToAdd := []pdbInfo{
0000000000000000000000000000000000000000;;			{label: "kube-dns-autoscaler", min_available: 1},
0000000000000000000000000000000000000000;;			{label: "kube-dns", min_available: 1},
0000000000000000000000000000000000000000;;			{label: "event-exporter", min_available: 0},
0000000000000000000000000000000000000000;;			{label: "kubernetes-dashboard", min_available: 0},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pdbData := range pdbsToAdd {
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Create PodDisruptionBudget for %v", pdbData.label))
0000000000000000000000000000000000000000;;			labelMap := map[string]string{"k8s-app": pdbData.label}
0000000000000000000000000000000000000000;;			pdbName := fmt.Sprintf("test-pdb-for-%v", pdbData.label)
0000000000000000000000000000000000000000;;			minAvailable := intstr.FromInt(pdbData.min_available)
0000000000000000000000000000000000000000;;			pdb := &policy.PodDisruptionBudget{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					Name:      pdbName,
0000000000000000000000000000000000000000;;					Namespace: "kube-system",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Spec: policy.PodDisruptionBudgetSpec{
0000000000000000000000000000000000000000;;					Selector:     &metav1.LabelSelector{MatchLabels: labelMap},
0000000000000000000000000000000000000000;;					MinAvailable: &minAvailable,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			_, err := f.StagingClient.Policy().PodDisruptionBudgets("kube-system").Create(pdb)
0000000000000000000000000000000000000000;;			newPdbs = append(newPdbs, pdbName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return cleanup, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return cleanup, nil
0000000000000000000000000000000000000000;;	}
