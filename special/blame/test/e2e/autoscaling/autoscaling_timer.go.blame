0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2017 The Kubernetes Authors.
1a1e22f16bbedde7ec1d38282f864c117953d92d;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package autoscaling
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/common"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("[Feature:ClusterSizeAutoscalingScaleUp] [Slow] Autoscaling", func() {
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("autoscaling")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.KubeDescribe("Autoscaling a service", func() {
0000000000000000000000000000000000000000;;			BeforeEach(func() {
0000000000000000000000000000000000000000;;				// Check if Cloud Autoscaler is enabled by trying to get its ConfigMap.
0000000000000000000000000000000000000000;;				_, err := f.ClientSet.CoreV1().ConfigMaps("kube-system").Get("cluster-autoscaler-status", metav1.GetOptions{})
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					framework.Skipf("test expects Cluster Autoscaler to be enabled")
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			Context("from 1 pod and 3 nodes to 8 pods and >=4 nodes", func() {
0000000000000000000000000000000000000000;;				const nodesNum = 3       // Expect there to be 3 nodes before and after the test.
0000000000000000000000000000000000000000;;				var nodeGroupName string // Set by BeforeEach, used by AfterEach to scale this node group down after the test.
0000000000000000000000000000000000000000;;				var nodes *v1.NodeList   // Set by BeforeEach, used by Measure to calculate CPU request based on node's sizes.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				BeforeEach(func() {
0000000000000000000000000000000000000000;;					// Make sure there is only 1 node group, otherwise this test becomes useless.
0000000000000000000000000000000000000000;;					nodeGroups := strings.Split(framework.TestContext.CloudConfig.NodeInstanceGroup, ",")
0000000000000000000000000000000000000000;;					if len(nodeGroups) != 1 {
0000000000000000000000000000000000000000;;						framework.Skipf("test expects 1 node group, found %d", len(nodeGroups))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					nodeGroupName = nodeGroups[0]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Make sure the node group has exactly 'nodesNum' nodes, otherwise this test becomes useless.
0000000000000000000000000000000000000000;;					nodeGroupSize, err := framework.GroupSize(nodeGroupName)
0000000000000000000000000000000000000000;;					framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;					if nodeGroupSize != nodesNum {
0000000000000000000000000000000000000000;;						framework.Skipf("test expects %d nodes, found %d", nodesNum, nodeGroupSize)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Make sure all nodes are schedulable, otherwise we are in some kind of a problem state.
0000000000000000000000000000000000000000;;					nodes = framework.GetReadySchedulableNodesOrDie(f.ClientSet)
0000000000000000000000000000000000000000;;					schedulableCount := len(nodes.Items)
0000000000000000000000000000000000000000;;					Expect(schedulableCount).To(Equal(nodeGroupSize), "not all nodes are schedulable")
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				AfterEach(func() {
0000000000000000000000000000000000000000;;					// Scale down back to only 'nodesNum' nodes, as expected at the start of the test.
0000000000000000000000000000000000000000;;					framework.ExpectNoError(framework.ResizeGroup(nodeGroupName, nodesNum))
0000000000000000000000000000000000000000;;					framework.ExpectNoError(framework.WaitForClusterSize(f.ClientSet, nodesNum, 15*time.Minute))
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				Measure("takes less than 15 minutes", func(b Benchmarker) {
0000000000000000000000000000000000000000;;					// Measured over multiple samples, scaling takes 10 +/- 2 minutes, so 15 minutes should be fully sufficient.
0000000000000000000000000000000000000000;;					const timeToWait = 15 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Calculate the CPU request of the service.
0000000000000000000000000000000000000000;;					// This test expects that 8 pods will not fit in 'nodesNum' nodes, but will fit in >='nodesNum'+1 nodes.
0000000000000000000000000000000000000000;;					// Make it so that 'nodesNum' pods fit perfectly per node (in practice other things take space, so less than that will fit).
0000000000000000000000000000000000000000;;					nodeCpus := nodes.Items[0].Status.Capacity[v1.ResourceCPU]
0000000000000000000000000000000000000000;;					nodeCpuMillis := (&nodeCpus).MilliValue()
0000000000000000000000000000000000000000;;					cpuRequestMillis := int64(nodeCpuMillis / nodesNum)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Start the service we want to scale and wait for it to be up and running.
0000000000000000000000000000000000000000;;					nodeMemoryBytes := nodes.Items[0].Status.Capacity[v1.ResourceMemory]
0000000000000000000000000000000000000000;;					nodeMemoryMB := (&nodeMemoryBytes).Value() / 1024 / 1024
0000000000000000000000000000000000000000;;					memRequestMB := nodeMemoryMB / 10 // Ensure each pod takes not more than 10% of node's total memory.
0000000000000000000000000000000000000000;;					replicas := 1
0000000000000000000000000000000000000000;;					resourceConsumer := common.NewDynamicResourceConsumer("resource-consumer", common.KindDeployment, replicas, 0, 0, 0, cpuRequestMillis, memRequestMB, f)
0000000000000000000000000000000000000000;;					defer resourceConsumer.CleanUp()
0000000000000000000000000000000000000000;;					resourceConsumer.WaitForReplicas(replicas, 1*time.Minute) // Should finish ~immediately, so 1 minute is more than enough.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Enable Horizontal Pod Autoscaler with 50% target utilization and
0000000000000000000000000000000000000000;;					// scale up the CPU usage to trigger autoscaling to 8 pods for target to be satisfied.
0000000000000000000000000000000000000000;;					targetCpuUtilizationPercent := int32(50)
0000000000000000000000000000000000000000;;					hpa := common.CreateCPUHorizontalPodAutoscaler(resourceConsumer, targetCpuUtilizationPercent, 1, 10)
0000000000000000000000000000000000000000;;					defer common.DeleteHorizontalPodAutoscaler(resourceConsumer, hpa.Name)
0000000000000000000000000000000000000000;;					cpuLoad := 8 * cpuRequestMillis * int64(targetCpuUtilizationPercent) / 100 // 8 pods utilized to the target level
0000000000000000000000000000000000000000;;					resourceConsumer.ConsumeCPU(int(cpuLoad))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Measure the time it takes for the service to scale to 8 pods with 50% CPU utilization each.
0000000000000000000000000000000000000000;;					b.Time("total scale-up time", func() {
0000000000000000000000000000000000000000;;						resourceConsumer.WaitForReplicas(8, timeToWait)
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;				}, 1) // Increase to run the test more than once.
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
