0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
13fe4fc0fde6ad12930d5d20f46c295649f6de5a;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/fields"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		kubepod "k8s.io/kubernetes/pkg/kubelet/pod"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func isNotRestartAlwaysMirrorPod(p *v1.Pod) bool {
0000000000000000000000000000000000000000;;		if !kubepod.IsMirrorPod(p) {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return p.Spec.RestartPolicy != v1.RestartPolicyAlways
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func filterIrrelevantPods(pods []*v1.Pod) []*v1.Pod {
0000000000000000000000000000000000000000;;		var results []*v1.Pod
0000000000000000000000000000000000000000;;		for _, p := range pods {
0000000000000000000000000000000000000000;;			if isNotRestartAlwaysMirrorPod(p) {
0000000000000000000000000000000000000000;;				// Mirror pods with restart policy == Never will not get
0000000000000000000000000000000000000000;;				// recreated if they are deleted after the pods have
0000000000000000000000000000000000000000;;				// terminated. For now, we discount such pods.
0000000000000000000000000000000000000000;;				// https://github.com/kubernetes/kubernetes/issues/34003
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			results = append(results, p)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return results
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Restart [Disruptive]", func() {
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("restart")
0000000000000000000000000000000000000000;;		var ps *testutils.PodStore
0000000000000000000000000000000000000000;;		var originalNodeNames []string
0000000000000000000000000000000000000000;;		var originalPodNames []string
0000000000000000000000000000000000000000;;		var numNodes int
0000000000000000000000000000000000000000;;		var systemNamespace string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			// This test requires the ability to restart all nodes, so the provider
0000000000000000000000000000000000000000;;			// check must be identical to that call.
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs("gce", "gke")
0000000000000000000000000000000000000000;;			ps = testutils.NewPodStore(f.ClientSet, metav1.NamespaceSystem, labels.Everything(), fields.Everything())
0000000000000000000000000000000000000000;;			numNodes = framework.TestContext.CloudConfig.NumNodes
0000000000000000000000000000000000000000;;			systemNamespace = metav1.NamespaceSystem
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("ensuring all nodes are ready")
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			originalNodeNames, err = framework.CheckNodesReady(f.ClientSet, framework.NodeReadyInitialTimeout, numNodes)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			framework.Logf("Got the following nodes before restart: %v", originalNodeNames)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("ensuring all pods are running and ready")
0000000000000000000000000000000000000000;;			allPods := ps.List()
0000000000000000000000000000000000000000;;			pods := filterIrrelevantPods(allPods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			originalPodNames = make([]string, len(pods))
0000000000000000000000000000000000000000;;			for i, p := range pods {
0000000000000000000000000000000000000000;;				originalPodNames[i] = p.ObjectMeta.Name
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if !framework.CheckPodsRunningReadyOrSucceeded(f.ClientSet, systemNamespace, originalPodNames, framework.PodReadyBeforeTimeout) {
0000000000000000000000000000000000000000;;				printStatusAndLogsForNotReadyPods(f.ClientSet, systemNamespace, originalPodNames, pods)
0000000000000000000000000000000000000000;;				framework.Failf("At least one pod wasn't running and ready or succeeded at test start.")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			if ps != nil {
0000000000000000000000000000000000000000;;				ps.Stop()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("should restart all nodes and ensure all nodes and pods recover", func() {
0000000000000000000000000000000000000000;;			By("restarting all of the nodes")
0000000000000000000000000000000000000000;;			err := restartNodes(f, originalNodeNames)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("ensuring all nodes are ready after the restart")
0000000000000000000000000000000000000000;;			nodeNamesAfter, err := framework.CheckNodesReady(f.ClientSet, framework.RestartNodeReadyAgainTimeout, numNodes)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			framework.Logf("Got the following nodes after restart: %v", nodeNamesAfter)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Make sure that we have the same number of nodes. We're not checking
0000000000000000000000000000000000000000;;			// that the names match because that's implementation specific.
0000000000000000000000000000000000000000;;			By("ensuring the same number of nodes exist after the restart")
0000000000000000000000000000000000000000;;			if len(originalNodeNames) != len(nodeNamesAfter) {
0000000000000000000000000000000000000000;;				framework.Failf("Had %d nodes before nodes were restarted, but now only have %d",
0000000000000000000000000000000000000000;;					len(originalNodeNames), len(nodeNamesAfter))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Make sure that we have the same number of pods. We're not checking
0000000000000000000000000000000000000000;;			// that the names match because they are recreated with different names
0000000000000000000000000000000000000000;;			// across node restarts.
0000000000000000000000000000000000000000;;			By("ensuring the same number of pods are running and ready after restart")
0000000000000000000000000000000000000000;;			podCheckStart := time.Now()
0000000000000000000000000000000000000000;;			podNamesAfter, err := waitForNPods(ps, len(originalPodNames), framework.RestartPodReadyAgainTimeout)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			remaining := framework.RestartPodReadyAgainTimeout - time.Since(podCheckStart)
0000000000000000000000000000000000000000;;			if !framework.CheckPodsRunningReadyOrSucceeded(f.ClientSet, systemNamespace, podNamesAfter, remaining) {
0000000000000000000000000000000000000000;;				pods := ps.List()
0000000000000000000000000000000000000000;;				printStatusAndLogsForNotReadyPods(f.ClientSet, systemNamespace, podNamesAfter, pods)
0000000000000000000000000000000000000000;;				framework.Failf("At least one pod wasn't running and ready after the restart.")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// waitForNPods tries to list pods using c until it finds expect of them,
0000000000000000000000000000000000000000;;	// returning their names if it can do so before timeout.
0000000000000000000000000000000000000000;;	func waitForNPods(ps *testutils.PodStore, expect int, timeout time.Duration) ([]string, error) {
0000000000000000000000000000000000000000;;		// Loop until we find expect pods or timeout is passed.
0000000000000000000000000000000000000000;;		var pods []*v1.Pod
0000000000000000000000000000000000000000;;		var errLast error
0000000000000000000000000000000000000000;;		found := wait.Poll(framework.Poll, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			allPods := ps.List()
0000000000000000000000000000000000000000;;			pods = filterIrrelevantPods(allPods)
0000000000000000000000000000000000000000;;			if len(pods) != expect {
0000000000000000000000000000000000000000;;				errLast = fmt.Errorf("expected to find %d pods but found only %d", expect, len(pods))
0000000000000000000000000000000000000000;;				framework.Logf("Error getting pods: %v", errLast)
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		}) == nil
0000000000000000000000000000000000000000;;		// Extract the names of all found pods.
0000000000000000000000000000000000000000;;		podNames := make([]string, len(pods))
0000000000000000000000000000000000000000;;		for i, p := range pods {
0000000000000000000000000000000000000000;;			podNames[i] = p.ObjectMeta.Name
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !found {
0000000000000000000000000000000000000000;;			return podNames, fmt.Errorf("couldn't find %d pods within %v; last error: %v",
0000000000000000000000000000000000000000;;				expect, timeout, errLast)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return podNames, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func restartNodes(f *framework.Framework, nodeNames []string) error {
0000000000000000000000000000000000000000;;		// List old boot IDs.
0000000000000000000000000000000000000000;;		oldBootIDs := make(map[string]string)
0000000000000000000000000000000000000000;;		for _, name := range nodeNames {
0000000000000000000000000000000000000000;;			node, err := f.ClientSet.Core().Nodes().Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("error getting node info before reboot: %s", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			oldBootIDs[name] = node.Status.NodeInfo.BootID
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Reboot the nodes.
0000000000000000000000000000000000000000;;		args := []string{
0000000000000000000000000000000000000000;;			"compute",
0000000000000000000000000000000000000000;;			fmt.Sprintf("--project=%s", framework.TestContext.CloudConfig.ProjectID),
0000000000000000000000000000000000000000;;			"instances",
0000000000000000000000000000000000000000;;			"reset",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		args = append(args, nodeNames...)
0000000000000000000000000000000000000000;;		args = append(args, fmt.Sprintf("--zone=%s", framework.TestContext.CloudConfig.Zone))
0000000000000000000000000000000000000000;;		stdout, stderr, err := framework.RunCmd("gcloud", args...)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error restarting nodes: %s\nstdout: %s\nstderr: %s", err, stdout, stderr)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Wait for their boot IDs to change.
0000000000000000000000000000000000000000;;		for _, name := range nodeNames {
0000000000000000000000000000000000000000;;			if err := wait.Poll(30*time.Second, 5*time.Minute, func() (bool, error) {
0000000000000000000000000000000000000000;;				node, err := f.ClientSet.Core().Nodes().Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return false, fmt.Errorf("error getting node info after reboot: %s", err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return node.Status.NodeInfo.BootID != oldBootIDs[name], nil
0000000000000000000000000000000000000000;;			}); err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("error waiting for node %s boot ID to change: %s", name, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
