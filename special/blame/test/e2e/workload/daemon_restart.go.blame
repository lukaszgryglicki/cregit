0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
d0d55a57c8ea276fe337b18ecf6b3e38710ff3fa;test/e2e/daemon_restart.go[test/e2e/daemon_restart.go][test/e2e/workload/daemon_restart.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package workload
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/watch"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/cache"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/master/ports"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// This test primarily checks 2 things:
0000000000000000000000000000000000000000;;	// 1. Daemons restart automatically within some sane time (10m).
0000000000000000000000000000000000000000;;	// 2. They don't take abnormal actions when restarted in the steady state.
0000000000000000000000000000000000000000;;	//	- Controller manager shouldn't overshoot replicas
0000000000000000000000000000000000000000;;	//	- Kubelet shouldn't restart containers
0000000000000000000000000000000000000000;;	//	- Scheduler should continue assigning hosts to new pods
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		restartPollInterval = 5 * time.Second
0000000000000000000000000000000000000000;;		restartTimeout      = 10 * time.Minute
0000000000000000000000000000000000000000;;		numPods             = 10
0000000000000000000000000000000000000000;;		sshPort             = 22
0000000000000000000000000000000000000000;;		ADD                 = "ADD"
0000000000000000000000000000000000000000;;		DEL                 = "DEL"
0000000000000000000000000000000000000000;;		UPDATE              = "UPDATE"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// restartDaemonConfig is a config to restart a running daemon on a node, and wait till
0000000000000000000000000000000000000000;;	// it comes back up. It uses ssh to send a SIGTERM to the daemon.
0000000000000000000000000000000000000000;;	type restartDaemonConfig struct {
0000000000000000000000000000000000000000;;		nodeName     string
0000000000000000000000000000000000000000;;		daemonName   string
0000000000000000000000000000000000000000;;		healthzPort  int
0000000000000000000000000000000000000000;;		pollInterval time.Duration
0000000000000000000000000000000000000000;;		pollTimeout  time.Duration
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewRestartConfig creates a restartDaemonConfig for the given node and daemon.
0000000000000000000000000000000000000000;;	func NewRestartConfig(nodeName, daemonName string, healthzPort int, pollInterval, pollTimeout time.Duration) *restartDaemonConfig {
0000000000000000000000000000000000000000;;		if !framework.ProviderIs("gce") {
0000000000000000000000000000000000000000;;			framework.Logf("WARNING: SSH through the restart config might not work on %s", framework.TestContext.Provider)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return &restartDaemonConfig{
0000000000000000000000000000000000000000;;			nodeName:     nodeName,
0000000000000000000000000000000000000000;;			daemonName:   daemonName,
0000000000000000000000000000000000000000;;			healthzPort:  healthzPort,
0000000000000000000000000000000000000000;;			pollInterval: pollInterval,
0000000000000000000000000000000000000000;;			pollTimeout:  pollTimeout,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (r *restartDaemonConfig) String() string {
0000000000000000000000000000000000000000;;		return fmt.Sprintf("Daemon %v on node %v", r.daemonName, r.nodeName)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// waitUp polls healthz of the daemon till it returns "ok" or the polling hits the pollTimeout
0000000000000000000000000000000000000000;;	func (r *restartDaemonConfig) waitUp() {
0000000000000000000000000000000000000000;;		framework.Logf("Checking if %v is up by polling for a 200 on its /healthz endpoint", r)
0000000000000000000000000000000000000000;;		healthzCheck := fmt.Sprintf(
0000000000000000000000000000000000000000;;			"curl -s -o /dev/null -I -w \"%%{http_code}\" http://localhost:%v/healthz", r.healthzPort)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := wait.Poll(r.pollInterval, r.pollTimeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			result, err := framework.NodeExec(r.nodeName, healthzCheck)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			if result.Code == 0 {
0000000000000000000000000000000000000000;;				httpCode, err := strconv.Atoi(result.Stdout)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					framework.Logf("Unable to parse healthz http return code: %v", err)
0000000000000000000000000000000000000000;;				} else if httpCode == 200 {
0000000000000000000000000000000000000000;;					return true, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.Logf("node %v exec command, '%v' failed with exitcode %v: \n\tstdout: %v\n\tstderr: %v",
0000000000000000000000000000000000000000;;				r.nodeName, healthzCheck, result.Code, result.Stdout, result.Stderr)
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err, "%v did not respond with a 200 via %v within %v", r, healthzCheck, r.pollTimeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// kill sends a SIGTERM to the daemon
0000000000000000000000000000000000000000;;	func (r *restartDaemonConfig) kill() {
0000000000000000000000000000000000000000;;		framework.Logf("Killing %v", r)
0000000000000000000000000000000000000000;;		_, err := framework.NodeExec(r.nodeName, fmt.Sprintf("pgrep %v | xargs -I {} sudo kill {}", r.daemonName))
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Restart checks if the daemon is up, kills it, and waits till it comes back up
0000000000000000000000000000000000000000;;	func (r *restartDaemonConfig) restart() {
0000000000000000000000000000000000000000;;		r.waitUp()
0000000000000000000000000000000000000000;;		r.kill()
0000000000000000000000000000000000000000;;		r.waitUp()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// podTracker records a serial history of events that might've affects pods.
0000000000000000000000000000000000000000;;	type podTracker struct {
0000000000000000000000000000000000000000;;		cache.ThreadSafeStore
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *podTracker) remember(pod *v1.Pod, eventType string) {
0000000000000000000000000000000000000000;;		if eventType == UPDATE && pod.Status.Phase == v1.PodRunning {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		p.Add(fmt.Sprintf("[%v] %v: %v", time.Now(), eventType, pod.Name), pod)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *podTracker) String() (msg string) {
0000000000000000000000000000000000000000;;		for _, k := range p.ListKeys() {
0000000000000000000000000000000000000000;;			obj, exists := p.Get(k)
0000000000000000000000000000000000000000;;			if !exists {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pod := obj.(*v1.Pod)
0000000000000000000000000000000000000000;;			msg += fmt.Sprintf("%v Phase %v Host %v\n", k, pod.Status.Phase, pod.Spec.NodeName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newPodTracker() *podTracker {
0000000000000000000000000000000000000000;;		return &podTracker{cache.NewThreadSafeStore(
0000000000000000000000000000000000000000;;			cache.Indexers{}, cache.Indices{})}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// replacePods replaces content of the store with the given pods.
0000000000000000000000000000000000000000;;	func replacePods(pods []*v1.Pod, store cache.Store) {
0000000000000000000000000000000000000000;;		found := make([]interface{}, 0, len(pods))
0000000000000000000000000000000000000000;;		for i := range pods {
0000000000000000000000000000000000000000;;			found = append(found, pods[i])
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		framework.ExpectNoError(store.Replace(found, "0"))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getContainerRestarts returns the count of container restarts across all pods matching the given labelSelector,
0000000000000000000000000000000000000000;;	// and a list of nodenames across which these containers restarted.
0000000000000000000000000000000000000000;;	func getContainerRestarts(c clientset.Interface, ns string, labelSelector labels.Selector) (int, []string) {
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{LabelSelector: labelSelector.String()}
0000000000000000000000000000000000000000;;		pods, err := c.Core().Pods(ns).List(options)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		failedContainers := 0
0000000000000000000000000000000000000000;;		containerRestartNodes := sets.NewString()
0000000000000000000000000000000000000000;;		for _, p := range pods.Items {
0000000000000000000000000000000000000000;;			for _, v := range testutils.FailedContainers(&p) {
0000000000000000000000000000000000000000;;				failedContainers = failedContainers + v.Restarts
0000000000000000000000000000000000000000;;				containerRestartNodes.Insert(p.Spec.NodeName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return failedContainers, containerRestartNodes.List()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = SIGDescribe("DaemonRestart [Disruptive]", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("daemonrestart")
0000000000000000000000000000000000000000;;		rcName := "daemonrestart" + strconv.Itoa(numPods) + "-" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;		labelSelector := labels.Set(map[string]string{"name": rcName}).AsSelector()
0000000000000000000000000000000000000000;;		existingPods := cache.NewStore(cache.MetaNamespaceKeyFunc)
0000000000000000000000000000000000000000;;		var ns string
0000000000000000000000000000000000000000;;		var config testutils.RCConfig
0000000000000000000000000000000000000000;;		var controller cache.Controller
0000000000000000000000000000000000000000;;		var newPods cache.Store
0000000000000000000000000000000000000000;;		var stopCh chan struct{}
0000000000000000000000000000000000000000;;		var tracker *podTracker
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			// These tests require SSH
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs(framework.ProvidersWithSSH...)
0000000000000000000000000000000000000000;;			ns = f.Namespace.Name
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// All the restart tests need an rc and a watch on pods of the rc.
0000000000000000000000000000000000000000;;			// Additionally some of them might scale the rc during the test.
0000000000000000000000000000000000000000;;			config = testutils.RCConfig{
0000000000000000000000000000000000000000;;				Client:         f.ClientSet,
0000000000000000000000000000000000000000;;				InternalClient: f.InternalClientset,
0000000000000000000000000000000000000000;;				Name:           rcName,
0000000000000000000000000000000000000000;;				Namespace:      ns,
0000000000000000000000000000000000000000;;				Image:          framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;				Replicas:       numPods,
0000000000000000000000000000000000000000;;				CreatedPods:    &[]*v1.Pod{},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Expect(framework.RunRC(config)).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			replacePods(*config.CreatedPods, existingPods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			stopCh = make(chan struct{})
0000000000000000000000000000000000000000;;			tracker = newPodTracker()
0000000000000000000000000000000000000000;;			newPods, controller = cache.NewInformer(
0000000000000000000000000000000000000000;;				&cache.ListWatch{
0000000000000000000000000000000000000000;;					ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
0000000000000000000000000000000000000000;;						options.LabelSelector = labelSelector.String()
0000000000000000000000000000000000000000;;						obj, err := f.ClientSet.Core().Pods(ns).List(options)
0000000000000000000000000000000000000000;;						return runtime.Object(obj), err
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
0000000000000000000000000000000000000000;;						options.LabelSelector = labelSelector.String()
0000000000000000000000000000000000000000;;						return f.ClientSet.Core().Pods(ns).Watch(options)
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				&v1.Pod{},
0000000000000000000000000000000000000000;;				0,
0000000000000000000000000000000000000000;;				cache.ResourceEventHandlerFuncs{
0000000000000000000000000000000000000000;;					AddFunc: func(obj interface{}) {
0000000000000000000000000000000000000000;;						tracker.remember(obj.(*v1.Pod), ADD)
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					UpdateFunc: func(oldObj, newObj interface{}) {
0000000000000000000000000000000000000000;;						tracker.remember(newObj.(*v1.Pod), UPDATE)
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					DeleteFunc: func(obj interface{}) {
0000000000000000000000000000000000000000;;						tracker.remember(obj.(*v1.Pod), DEL)
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;			go controller.Run(stopCh)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			close(stopCh)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("Controller Manager should not create/delete replicas across restart", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Requires master ssh access.
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs("gce", "aws")
0000000000000000000000000000000000000000;;			restarter := NewRestartConfig(
0000000000000000000000000000000000000000;;				framework.GetMasterHost(), "kube-controller", ports.ControllerManagerPort, restartPollInterval, restartTimeout)
0000000000000000000000000000000000000000;;			restarter.restart()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// The intent is to ensure the replication controller manager has observed and reported status of
0000000000000000000000000000000000000000;;			// the replication controller at least once since the manager restarted, so that we can determine
0000000000000000000000000000000000000000;;			// that it had the opportunity to create/delete pods, if it were going to do so. Scaling the RC
0000000000000000000000000000000000000000;;			// to the same size achieves this, because the scale operation advances the RC's sequence number
0000000000000000000000000000000000000000;;			// and awaits it to be observed and reported back in the RC's status.
0000000000000000000000000000000000000000;;			framework.ScaleRC(f.ClientSet, f.InternalClientset, ns, rcName, numPods, true)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Only check the keys, the pods can be different if the kubelet updated it.
0000000000000000000000000000000000000000;;			// TODO: Can it really?
0000000000000000000000000000000000000000;;			existingKeys := sets.NewString()
0000000000000000000000000000000000000000;;			newKeys := sets.NewString()
0000000000000000000000000000000000000000;;			for _, k := range existingPods.ListKeys() {
0000000000000000000000000000000000000000;;				existingKeys.Insert(k)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, k := range newPods.ListKeys() {
0000000000000000000000000000000000000000;;				newKeys.Insert(k)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(newKeys.List()) != len(existingKeys.List()) ||
0000000000000000000000000000000000000000;;				!newKeys.IsSuperset(existingKeys) {
0000000000000000000000000000000000000000;;				framework.Failf("RcManager created/deleted pods after restart \n\n %+v", tracker)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("Scheduler should continue assigning pods to nodes across restart", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Requires master ssh access.
0000000000000000000000000000000000000000;;			framework.SkipUnlessProviderIs("gce", "aws")
0000000000000000000000000000000000000000;;			restarter := NewRestartConfig(
0000000000000000000000000000000000000000;;				framework.GetMasterHost(), "kube-scheduler", ports.SchedulerPort, restartPollInterval, restartTimeout)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Create pods while the scheduler is down and make sure the scheduler picks them up by
0000000000000000000000000000000000000000;;			// scaling the rc to the same size.
0000000000000000000000000000000000000000;;			restarter.waitUp()
0000000000000000000000000000000000000000;;			restarter.kill()
0000000000000000000000000000000000000000;;			// This is best effort to try and create pods while the scheduler is down,
0000000000000000000000000000000000000000;;			// since we don't know exactly when it is restarted after the kill signal.
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.ScaleRC(f.ClientSet, f.InternalClientset, ns, rcName, numPods+5, false))
0000000000000000000000000000000000000000;;			restarter.waitUp()
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.ScaleRC(f.ClientSet, f.InternalClientset, ns, rcName, numPods+5, true))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("Kubelet should not restart containers across restart", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodeIPs, err := framework.GetNodePublicIps(f.ClientSet)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			preRestarts, badNodes := getContainerRestarts(f.ClientSet, ns, labelSelector)
0000000000000000000000000000000000000000;;			if preRestarts != 0 {
0000000000000000000000000000000000000000;;				framework.Logf("WARNING: Non-zero container restart count: %d across nodes %v", preRestarts, badNodes)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, ip := range nodeIPs {
0000000000000000000000000000000000000000;;				restarter := NewRestartConfig(
0000000000000000000000000000000000000000;;					ip, "kubelet", ports.KubeletReadOnlyPort, restartPollInterval, restartTimeout)
0000000000000000000000000000000000000000;;				restarter.restart()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			postRestarts, badNodes := getContainerRestarts(f.ClientSet, ns, labelSelector)
0000000000000000000000000000000000000000;;			if postRestarts != preRestarts {
0000000000000000000000000000000000000000;;				framework.DumpNodeDebugInfo(f.ClientSet, badNodes, framework.Logf)
0000000000000000000000000000000000000000;;				framework.Failf("Net container restart count went from %v -> %v after kubelet restart on nodes %v \n\n %+v", preRestarts, postRestarts, badNodes, tracker)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
