0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
1c172a4d0c77bcdbe84ac5df9c7857d71186852c;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"path/filepath"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// Interval to framework.Poll /runningpods on a node
0000000000000000000000000000000000000000;;		pollInterval = 1 * time.Second
0000000000000000000000000000000000000000;;		// Interval to framework.Poll /stats/container on a node
0000000000000000000000000000000000000000;;		containerStatsPollingInterval = 5 * time.Second
0000000000000000000000000000000000000000;;		// Maximum number of nodes that we constraint to
0000000000000000000000000000000000000000;;		maxNodesToCheck = 10
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getPodMatches returns a set of pod names on the given node that matches the
0000000000000000000000000000000000000000;;	// podNamePrefix and namespace.
0000000000000000000000000000000000000000;;	func getPodMatches(c clientset.Interface, nodeName string, podNamePrefix string, namespace string) sets.String {
0000000000000000000000000000000000000000;;		matches := sets.NewString()
0000000000000000000000000000000000000000;;		framework.Logf("Checking pods on node %v via /runningpods endpoint", nodeName)
0000000000000000000000000000000000000000;;		runningPods, err := framework.GetKubeletPods(c, nodeName)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			framework.Logf("Error checking running pods on %v: %v", nodeName, err)
0000000000000000000000000000000000000000;;			return matches
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pod := range runningPods.Items {
0000000000000000000000000000000000000000;;			if pod.Namespace == namespace && strings.HasPrefix(pod.Name, podNamePrefix) {
0000000000000000000000000000000000000000;;				matches.Insert(pod.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return matches
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// waitTillNPodsRunningOnNodes polls the /runningpods endpoint on kubelet until
0000000000000000000000000000000000000000;;	// it finds targetNumPods pods that match the given criteria (namespace and
0000000000000000000000000000000000000000;;	// podNamePrefix). Note that we usually use label selector to filter pods that
0000000000000000000000000000000000000000;;	// belong to the same RC. However, we use podNamePrefix with namespace here
0000000000000000000000000000000000000000;;	// because pods returned from /runningpods do not contain the original label
0000000000000000000000000000000000000000;;	// information; they are reconstructed by examining the container runtime. In
0000000000000000000000000000000000000000;;	// the scope of this test, we do not expect pod naming conflicts so
0000000000000000000000000000000000000000;;	// podNamePrefix should be sufficient to identify the pods.
0000000000000000000000000000000000000000;;	func waitTillNPodsRunningOnNodes(c clientset.Interface, nodeNames sets.String, podNamePrefix string, namespace string, targetNumPods int, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		return wait.Poll(pollInterval, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			matchCh := make(chan sets.String, len(nodeNames))
0000000000000000000000000000000000000000;;			for _, item := range nodeNames.List() {
0000000000000000000000000000000000000000;;				// Launch a goroutine per node to check the pods running on the nodes.
0000000000000000000000000000000000000000;;				nodeName := item
0000000000000000000000000000000000000000;;				go func() {
0000000000000000000000000000000000000000;;					matchCh <- getPodMatches(c, nodeName, podNamePrefix, namespace)
0000000000000000000000000000000000000000;;				}()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			seen := sets.NewString()
0000000000000000000000000000000000000000;;			for i := 0; i < len(nodeNames.List()); i++ {
0000000000000000000000000000000000000000;;				seen = seen.Union(<-matchCh)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if seen.Len() == targetNumPods {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.Logf("Waiting for %d pods to be running on the node; %d are currently running;", targetNumPods, seen.Len())
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// updates labels of nodes given by nodeNames.
0000000000000000000000000000000000000000;;	// In case a given label already exists, it overwrites it. If label to remove doesn't exist
0000000000000000000000000000000000000000;;	// it silently ignores it.
0000000000000000000000000000000000000000;;	// TODO: migrate to use framework.AddOrUpdateLabelOnNode/framework.RemoveLabelOffNode
0000000000000000000000000000000000000000;;	func updateNodeLabels(c clientset.Interface, nodeNames sets.String, toAdd, toRemove map[string]string) {
0000000000000000000000000000000000000000;;		const maxRetries = 5
0000000000000000000000000000000000000000;;		for nodeName := range nodeNames {
0000000000000000000000000000000000000000;;			var node *v1.Node
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			for i := 0; i < maxRetries; i++ {
0000000000000000000000000000000000000000;;				node, err = c.CoreV1().Nodes().Get(nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					framework.Logf("Error getting node %s: %v", nodeName, err)
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if toAdd != nil {
0000000000000000000000000000000000000000;;					for k, v := range toAdd {
0000000000000000000000000000000000000000;;						node.ObjectMeta.Labels[k] = v
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if toRemove != nil {
0000000000000000000000000000000000000000;;					for k := range toRemove {
0000000000000000000000000000000000000000;;						delete(node.ObjectMeta.Labels, k)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				_, err = c.CoreV1().Nodes().Update(node)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					framework.Logf("Error updating node %s: %v", nodeName, err)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Calls startVolumeServer to create and run a nfs-server pod. Returns server pod and its
0000000000000000000000000000000000000000;;	// ip address.
0000000000000000000000000000000000000000;;	// Note: startVolumeServer() waits for the nfs-server pod to be Running and sleeps some
0000000000000000000000000000000000000000;;	//   so that the nfs server can start up.
0000000000000000000000000000000000000000;;	func createNfsServerPod(c clientset.Interface, config framework.VolumeTestConfig) (*v1.Pod, string) {
0000000000000000000000000000000000000000;;		pod := framework.StartVolumeServer(c, config)
0000000000000000000000000000000000000000;;		Expect(pod).NotTo(BeNil())
0000000000000000000000000000000000000000;;		ip := pod.Status.PodIP
0000000000000000000000000000000000000000;;		Expect(len(ip)).NotTo(BeZero())
0000000000000000000000000000000000000000;;		framework.Logf("NFS server IP address: %v", ip)
0000000000000000000000000000000000000000;;		return pod, ip
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Restart the passed-in nfs-server by issuing a `/usr/sbin/rpc.nfsd 1` command in the
0000000000000000000000000000000000000000;;	// pod's (only) container. This command changes the number of nfs server threads from
0000000000000000000000000000000000000000;;	// (presumably) zero back to 1, and therefore allows nfs to open connections again.
0000000000000000000000000000000000000000;;	func restartNfsServer(serverPod *v1.Pod) {
0000000000000000000000000000000000000000;;		const startcmd = "/usr/sbin/rpc.nfsd 1"
0000000000000000000000000000000000000000;;		ns := fmt.Sprintf("--namespace=%v", serverPod.Namespace)
0000000000000000000000000000000000000000;;		framework.RunKubectlOrDie("exec", ns, serverPod.Name, "--", "/bin/sh", "-c", startcmd)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Stop the passed-in nfs-server by issuing a `/usr/sbin/rpc.nfsd 0` command in the
0000000000000000000000000000000000000000;;	// pod's (only) container. This command changes the number of nfs server threads to 0,
0000000000000000000000000000000000000000;;	// thus closing all open nfs connections.
0000000000000000000000000000000000000000;;	func stopNfsServer(serverPod *v1.Pod) {
0000000000000000000000000000000000000000;;		const stopcmd = "/usr/sbin/rpc.nfsd 0"
0000000000000000000000000000000000000000;;		ns := fmt.Sprintf("--namespace=%v", serverPod.Namespace)
0000000000000000000000000000000000000000;;		framework.RunKubectlOrDie("exec", ns, serverPod.Name, "--", "/bin/sh", "-c", stopcmd)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Creates a pod that mounts an nfs volume that is served by the nfs-server pod. The container
0000000000000000000000000000000000000000;;	// will execute the passed in shell cmd. Waits for the pod to start.
0000000000000000000000000000000000000000;;	// Note: the nfs plugin is defined inline, no PV or PVC.
0000000000000000000000000000000000000000;;	func createPodUsingNfs(f *framework.Framework, c clientset.Interface, ns, nfsIP, cmd string) *v1.Pod {
0000000000000000000000000000000000000000;;		By("create pod using nfs volume")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		isPrivileged := true
0000000000000000000000000000000000000000;;		cmdLine := []string{"-c", cmd}
0000000000000000000000000000000000000000;;		pod := &v1.Pod{
0000000000000000000000000000000000000000;;			TypeMeta: metav1.TypeMeta{
0000000000000000000000000000000000000000;;				Kind:       "Pod",
0000000000000000000000000000000000000000;;				APIVersion: api.Registry.GroupOrDie(v1.GroupName).GroupVersion.String(),
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				GenerateName: "pod-nfs-vol-",
0000000000000000000000000000000000000000;;				Namespace:    ns,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:    "pod-nfs-vol",
0000000000000000000000000000000000000000;;						Image:   "gcr.io/google_containers/busybox:1.24",
0000000000000000000000000000000000000000;;						Command: []string{"/bin/sh"},
0000000000000000000000000000000000000000;;						Args:    cmdLine,
0000000000000000000000000000000000000000;;						VolumeMounts: []v1.VolumeMount{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Name:      "nfs-vol",
0000000000000000000000000000000000000000;;								MountPath: "/mnt",
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						SecurityContext: &v1.SecurityContext{
0000000000000000000000000000000000000000;;							Privileged: &isPrivileged,
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				RestartPolicy: v1.RestartPolicyNever, //don't restart pod
0000000000000000000000000000000000000000;;				Volumes: []v1.Volume{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name: "nfs-vol",
0000000000000000000000000000000000000000;;						VolumeSource: v1.VolumeSource{
0000000000000000000000000000000000000000;;							NFS: &v1.NFSVolumeSource{
0000000000000000000000000000000000000000;;								Server:   nfsIP,
0000000000000000000000000000000000000000;;								Path:     "/exports",
0000000000000000000000000000000000000000;;								ReadOnly: false,
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rtnPod, err := c.CoreV1().Pods(ns).Create(pod)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err = f.WaitForPodReady(rtnPod.Name) // running & ready
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rtnPod, err = c.CoreV1().Pods(ns).Get(rtnPod.Name, metav1.GetOptions{}) // return fresh pod
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		return rtnPod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Checks for a lingering nfs mount and/or uid directory on the pod's host. The host IP is used
0000000000000000000000000000000000000000;;	// so that this test runs in GCE, where it appears that SSH cannot resolve the hostname.
0000000000000000000000000000000000000000;;	// If expectClean is true then we expect the node to be cleaned up and thus commands like
0000000000000000000000000000000000000000;;	// `ls <uid-dir>` should fail (since that dir was removed). If expectClean is false then we expect
0000000000000000000000000000000000000000;;	// the node is not cleaned up, and thus cmds like `ls <uid-dir>` should succeed. We wait for the
0000000000000000000000000000000000000000;;	// kubelet to be cleaned up, afterwhich an error is reported.
0000000000000000000000000000000000000000;;	func checkPodCleanup(c clientset.Interface, pod *v1.Pod, expectClean bool) {
0000000000000000000000000000000000000000;;		timeout := 5 * time.Minute
0000000000000000000000000000000000000000;;		poll := 20 * time.Second
0000000000000000000000000000000000000000;;		podDir := filepath.Join("/var/lib/kubelet/pods", string(pod.UID))
0000000000000000000000000000000000000000;;		mountDir := filepath.Join(podDir, "volumes", "kubernetes.io~nfs")
0000000000000000000000000000000000000000;;		// use ip rather than hostname in GCE
0000000000000000000000000000000000000000;;		nodeIP, err := framework.GetHostExternalAddress(c, pod)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		condMsg := "deleted"
0000000000000000000000000000000000000000;;		if !expectClean {
0000000000000000000000000000000000000000;;			condMsg = "present"
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// table of host tests to perform (order may matter so not using a map)
0000000000000000000000000000000000000000;;		type testT struct {
0000000000000000000000000000000000000000;;			feature string // feature to test
0000000000000000000000000000000000000000;;			cmd     string // remote command to execute on node
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		tests := []testT{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				feature: "pod UID directory",
0000000000000000000000000000000000000000;;				cmd:     fmt.Sprintf("sudo ls %v", podDir),
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				feature: "pod nfs mount",
0000000000000000000000000000000000000000;;				cmd:     fmt.Sprintf("sudo mount | grep %v", mountDir),
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, test := range tests {
0000000000000000000000000000000000000000;;			framework.Logf("Wait up to %v for host's (%v) %q to be %v", timeout, nodeIP, test.feature, condMsg)
0000000000000000000000000000000000000000;;			err = wait.Poll(poll, timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;				result, err := framework.NodeExec(nodeIP, test.cmd)
0000000000000000000000000000000000000000;;				Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;				framework.LogSSHResult(result)
0000000000000000000000000000000000000000;;				ok := (result.Code == 0 && len(result.Stdout) > 0 && len(result.Stderr) == 0)
0000000000000000000000000000000000000000;;				if expectClean && ok { // keep trying
0000000000000000000000000000000000000000;;					return false, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if !expectClean && !ok { // stop wait loop
0000000000000000000000000000000000000000;;					return true, fmt.Errorf("%v is gone but expected to exist", test.feature)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return true, nil // done, host is as expected
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred(), fmt.Sprintf("Host (%v) cleanup error: %v. Expected %q to be %v", nodeIP, err, test.feature, condMsg))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if expectClean {
0000000000000000000000000000000000000000;;			framework.Logf("Pod's host has been cleaned up")
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			framework.Logf("Pod's host has not been cleaned up (per expectation)")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("kubelet", func() {
0000000000000000000000000000000000000000;;		var (
0000000000000000000000000000000000000000;;			c  clientset.Interface
0000000000000000000000000000000000000000;;			ns string
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("kubelet")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			c = f.ClientSet
0000000000000000000000000000000000000000;;			ns = f.Namespace.Name
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.KubeDescribe("Clean up pods on node", func() {
0000000000000000000000000000000000000000;;			var (
0000000000000000000000000000000000000000;;				numNodes        int
0000000000000000000000000000000000000000;;				nodeNames       sets.String
0000000000000000000000000000000000000000;;				nodeLabels      map[string]string
0000000000000000000000000000000000000000;;				resourceMonitor *framework.ResourceMonitor
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;			type DeleteTest struct {
0000000000000000000000000000000000000000;;				podsPerNode int
0000000000000000000000000000000000000000;;				timeout     time.Duration
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			deleteTests := []DeleteTest{
0000000000000000000000000000000000000000;;				{podsPerNode: 10, timeout: 1 * time.Minute},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			BeforeEach(func() {
0000000000000000000000000000000000000000;;				// Use node labels to restrict the pods to be assigned only to the
0000000000000000000000000000000000000000;;				// nodes we observe initially.
0000000000000000000000000000000000000000;;				nodeLabels = make(map[string]string)
0000000000000000000000000000000000000000;;				nodeLabels["kubelet_cleanup"] = "true"
0000000000000000000000000000000000000000;;				nodes := framework.GetReadySchedulableNodesOrDie(c)
0000000000000000000000000000000000000000;;				numNodes = len(nodes.Items)
0000000000000000000000000000000000000000;;				Expect(numNodes).NotTo(BeZero())
0000000000000000000000000000000000000000;;				nodeNames = sets.NewString()
0000000000000000000000000000000000000000;;				// If there are a lot of nodes, we don't want to use all of them
0000000000000000000000000000000000000000;;				// (if there are 1000 nodes in the cluster, starting 10 pods/node
0000000000000000000000000000000000000000;;				// will take ~10 minutes today). And there is also deletion phase.
0000000000000000000000000000000000000000;;				// Instead, we choose at most 10 nodes.
0000000000000000000000000000000000000000;;				if numNodes > maxNodesToCheck {
0000000000000000000000000000000000000000;;					numNodes = maxNodesToCheck
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for i := 0; i < numNodes; i++ {
0000000000000000000000000000000000000000;;					nodeNames.Insert(nodes.Items[i].Name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				updateNodeLabels(c, nodeNames, nodeLabels, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Start resourceMonitor only in small clusters.
0000000000000000000000000000000000000000;;				if len(nodes.Items) <= maxNodesToCheck {
0000000000000000000000000000000000000000;;					resourceMonitor = framework.NewResourceMonitor(f.ClientSet, framework.TargetContainers(), containerStatsPollingInterval)
0000000000000000000000000000000000000000;;					resourceMonitor.Start()
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			AfterEach(func() {
0000000000000000000000000000000000000000;;				if resourceMonitor != nil {
0000000000000000000000000000000000000000;;					resourceMonitor.Stop()
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// If we added labels to nodes in this test, remove them now.
0000000000000000000000000000000000000000;;				updateNodeLabels(c, nodeNames, nil, nodeLabels)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, itArg := range deleteTests {
0000000000000000000000000000000000000000;;				name := fmt.Sprintf(
0000000000000000000000000000000000000000;;					"kubelet should be able to delete %d pods per node in %v.", itArg.podsPerNode, itArg.timeout)
0000000000000000000000000000000000000000;;				It(name, func() {
0000000000000000000000000000000000000000;;					totalPods := itArg.podsPerNode * numNodes
0000000000000000000000000000000000000000;;					By(fmt.Sprintf("Creating a RC of %d pods and wait until all pods of this RC are running", totalPods))
0000000000000000000000000000000000000000;;					rcName := fmt.Sprintf("cleanup%d-%s", totalPods, string(uuid.NewUUID()))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					Expect(framework.RunRC(testutils.RCConfig{
0000000000000000000000000000000000000000;;						Client:         f.ClientSet,
0000000000000000000000000000000000000000;;						InternalClient: f.InternalClientset,
0000000000000000000000000000000000000000;;						Name:           rcName,
0000000000000000000000000000000000000000;;						Namespace:      f.Namespace.Name,
0000000000000000000000000000000000000000;;						Image:          framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;						Replicas:       totalPods,
0000000000000000000000000000000000000000;;						NodeSelector:   nodeLabels,
0000000000000000000000000000000000000000;;					})).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;					// Perform a sanity check so that we know all desired pods are
0000000000000000000000000000000000000000;;					// running on the nodes according to kubelet. The timeout is set to
0000000000000000000000000000000000000000;;					// only 30 seconds here because framework.RunRC already waited for all pods to
0000000000000000000000000000000000000000;;					// transition to the running status.
0000000000000000000000000000000000000000;;					Expect(waitTillNPodsRunningOnNodes(f.ClientSet, nodeNames, rcName, ns, totalPods,
0000000000000000000000000000000000000000;;						time.Second*30)).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;					if resourceMonitor != nil {
0000000000000000000000000000000000000000;;						resourceMonitor.LogLatest()
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By("Deleting the RC")
0000000000000000000000000000000000000000;;					framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, rcName)
0000000000000000000000000000000000000000;;					// Check that the pods really are gone by querying /runningpods on the
0000000000000000000000000000000000000000;;					// node. The /runningpods handler checks the container runtime (or its
0000000000000000000000000000000000000000;;					// cache) and  returns a list of running pods. Some possible causes of
0000000000000000000000000000000000000000;;					// failures are:
0000000000000000000000000000000000000000;;					//   - kubelet deadlock
0000000000000000000000000000000000000000;;					//   - a bug in graceful termination (if it is enabled)
0000000000000000000000000000000000000000;;					//   - docker slow to delete pods (or resource problems causing slowness)
0000000000000000000000000000000000000000;;					start := time.Now()
0000000000000000000000000000000000000000;;					Expect(waitTillNPodsRunningOnNodes(f.ClientSet, nodeNames, rcName, ns, 0,
0000000000000000000000000000000000000000;;						itArg.timeout)).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;					framework.Logf("Deleting %d pods on %d nodes completed in %v after the RC was deleted", totalPods, len(nodeNames),
0000000000000000000000000000000000000000;;						time.Since(start))
0000000000000000000000000000000000000000;;					if resourceMonitor != nil {
0000000000000000000000000000000000000000;;						resourceMonitor.LogCPUSummary()
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Test host cleanup when disrupting the volume environment.
0000000000000000000000000000000000000000;;		framework.KubeDescribe("host cleanup with volume mounts [sig-storage][HostCleanup][Flaky]", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			type hostCleanupTest struct {
0000000000000000000000000000000000000000;;				itDescr string
0000000000000000000000000000000000000000;;				podCmd  string
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Disrupt the nfs-server pod after a client pod accesses the nfs volume.
0000000000000000000000000000000000000000;;			// Note: the nfs-server is stopped NOT deleted. This is done to preserve its ip addr.
0000000000000000000000000000000000000000;;			//       If the nfs-server pod is deleted the client pod's mount can not be unmounted.
0000000000000000000000000000000000000000;;			//       If the nfs-server pod is deleted and re-created, due to having a different ip
0000000000000000000000000000000000000000;;			//       addr, the client pod's mount still cannot be unmounted.
0000000000000000000000000000000000000000;;			Context("Host cleanup after disrupting NFS volume [NFS]", func() {
0000000000000000000000000000000000000000;;				// issue #31272
0000000000000000000000000000000000000000;;				var (
0000000000000000000000000000000000000000;;					nfsServerPod *v1.Pod
0000000000000000000000000000000000000000;;					nfsIP        string
0000000000000000000000000000000000000000;;					NFSconfig    framework.VolumeTestConfig
0000000000000000000000000000000000000000;;					pod          *v1.Pod // client pod
0000000000000000000000000000000000000000;;				)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// fill in test slice for this context
0000000000000000000000000000000000000000;;				testTbl := []hostCleanupTest{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						itDescr: "after stopping the nfs-server and deleting the (sleeping) client pod, the NFS mount and the pod's UID directory should be removed.",
0000000000000000000000000000000000000000;;						podCmd:  "sleep 6000", // keep pod running
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						itDescr: "after stopping the nfs-server and deleting the (active) client pod, the NFS mount and the pod's UID directory should be removed.",
0000000000000000000000000000000000000000;;						podCmd:  "while true; do echo FeFieFoFum >>/mnt/SUCCESS; sleep 1; cat /mnt/SUCCESS; done",
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				BeforeEach(func() {
0000000000000000000000000000000000000000;;					framework.SkipUnlessProviderIs(framework.ProvidersWithSSH...)
0000000000000000000000000000000000000000;;					NFSconfig = framework.VolumeTestConfig{
0000000000000000000000000000000000000000;;						Namespace:   ns,
0000000000000000000000000000000000000000;;						Prefix:      "nfs",
0000000000000000000000000000000000000000;;						ServerImage: framework.NfsServerImage,
0000000000000000000000000000000000000000;;						ServerPorts: []int{2049},
0000000000000000000000000000000000000000;;						ServerArgs:  []string{"-G", "777", "/exports"},
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					nfsServerPod, nfsIP = createNfsServerPod(c, NFSconfig)
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				AfterEach(func() {
0000000000000000000000000000000000000000;;					framework.ExpectNoError(framework.DeletePodWithWait(f, c, pod), "AfterEach: Failed to delete pod ", pod.Name)
0000000000000000000000000000000000000000;;					framework.ExpectNoError(framework.DeletePodWithWait(f, c, nfsServerPod), "AfterEach: Failed to delete pod ", nfsServerPod.Name)
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// execute It blocks from above table of tests
0000000000000000000000000000000000000000;;				for _, t := range testTbl {
0000000000000000000000000000000000000000;;					It(t.itDescr, func() {
0000000000000000000000000000000000000000;;						pod = createPodUsingNfs(f, c, ns, nfsIP, t.podCmd)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						By("Stop the NFS server")
0000000000000000000000000000000000000000;;						stopNfsServer(nfsServerPod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						By("Delete the pod mounted to the NFS volume")
0000000000000000000000000000000000000000;;						framework.ExpectNoError(framework.DeletePodWithWait(f, c, pod), "Failed to delete pod ", pod.Name)
0000000000000000000000000000000000000000;;						// pod object is now stale, but is intentionally not nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						By("Check if pod's host has been cleaned up -- expect not")
0000000000000000000000000000000000000000;;						checkPodCleanup(c, pod, false)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						By("Restart the nfs server")
0000000000000000000000000000000000000000;;						restartNfsServer(nfsServerPod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						By("Verify host running the deleted pod is now cleaned up")
0000000000000000000000000000000000000000;;						checkPodCleanup(c, pod, true)
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
