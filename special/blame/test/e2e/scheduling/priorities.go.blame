0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2017 The Kubernetes Authors.
55645a5c9097db9ba17c098fb9401aaadfe82c83;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package scheduling
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"encoding/json"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"math"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;		_ "github.com/stretchr/testify/assert"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		priorityutil "k8s.io/kubernetes/plugin/pkg/scheduler/algorithm/priorities/util"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/common"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type Resource struct {
0000000000000000000000000000000000000000;;		MilliCPU int64
0000000000000000000000000000000000000000;;		Memory   int64
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var balancePodLabel map[string]string = map[string]string{"name": "priority-balanced-memory"}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var podRequestedResource *v1.ResourceRequirements = &v1.ResourceRequirements{
0000000000000000000000000000000000000000;;		Limits: v1.ResourceList{
0000000000000000000000000000000000000000;;			v1.ResourceMemory: resource.MustParse("100Mi"),
0000000000000000000000000000000000000000;;			v1.ResourceCPU:    resource.MustParse("100m"),
0000000000000000000000000000000000000000;;		},
0000000000000000000000000000000000000000;;		Requests: v1.ResourceList{
0000000000000000000000000000000000000000;;			v1.ResourceMemory: resource.MustParse("100Mi"),
0000000000000000000000000000000000000000;;			v1.ResourceCPU:    resource.MustParse("100m"),
0000000000000000000000000000000000000000;;		},
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// This test suite is used to verifies scheduler priority functions based on the default provider
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("SchedulerPriorities [Serial]", func() {
0000000000000000000000000000000000000000;;		var cs clientset.Interface
0000000000000000000000000000000000000000;;		var nodeList *v1.NodeList
0000000000000000000000000000000000000000;;		var systemPodsNo int
0000000000000000000000000000000000000000;;		var ns string
0000000000000000000000000000000000000000;;		var masterNodes sets.String
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("sched-priority")
0000000000000000000000000000000000000000;;		ignoreLabels := framework.ImagePullerLabels
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			cs = f.ClientSet
0000000000000000000000000000000000000000;;			ns = f.Namespace.Name
0000000000000000000000000000000000000000;;			nodeList = &v1.NodeList{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.WaitForAllNodesHealthy(cs, time.Minute)
0000000000000000000000000000000000000000;;			masterNodes, nodeList = framework.GetMasterAndWorkerNodesOrDie(cs)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			err := framework.CheckTestingNSDeletedExcept(cs, ns)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			err = framework.WaitForPodsRunningReady(cs, metav1.NamespaceSystem, int32(systemPodsNo), 0, framework.PodReadyBeforeTimeout, ignoreLabels)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("Pod should be prefer scheduled to node that satisify the NodeAffinity", func() {
0000000000000000000000000000000000000000;;			nodeName := GetNodeThatCanRunPod(f)
0000000000000000000000000000000000000000;;			By("Trying to apply a label on the found node.")
0000000000000000000000000000000000000000;;			k := fmt.Sprintf("kubernetes.io/e2e-%s", "node-topologyKey")
0000000000000000000000000000000000000000;;			v := "topologyvalue"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, k)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// make the nodes have balanced cpu,mem usage ratio
0000000000000000000000000000000000000000;;			err := createBalancedPodForNodes(f, cs, ns, nodeList.Items, podRequestedResource, 0.6)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			By("Trying to relaunch the pod, now with labels.")
0000000000000000000000000000000000000000;;			labelPodName := "pod-with-node-affinity"
0000000000000000000000000000000000000000;;			pod := createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name: labelPodName,
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					NodeAffinity: &v1.NodeAffinity{
0000000000000000000000000000000000000000;;						PreferredDuringSchedulingIgnoredDuringExecution: []v1.PreferredSchedulingTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Preference: v1.NodeSelectorTerm{
0000000000000000000000000000000000000000;;									MatchExpressions: []v1.NodeSelectorRequirement{
0000000000000000000000000000000000000000;;										{
0000000000000000000000000000000000000000;;											Key:      k,
0000000000000000000000000000000000000000;;											Operator: v1.NodeSelectorOpIn,
0000000000000000000000000000000000000000;;											Values:   []string{v},
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;								Weight: 20,
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			By("Wait the pod becomes running.")
0000000000000000000000000000000000000000;;			framework.ExpectNoError(f.WaitForPodRunning(pod.Name))
0000000000000000000000000000000000000000;;			labelPod, err := cs.CoreV1().Pods(ns).Get(labelPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			By("Verify the pod was scheduled to the expected node.")
0000000000000000000000000000000000000000;;			Expect(labelPod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("Pod should be schedule to node that satisify the PodAffinity", func() {
0000000000000000000000000000000000000000;;			By("Trying to launch a pod with a label to get a node which can launch it.")
0000000000000000000000000000000000000000;;			pod := runPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name:      "with-label-security-s1",
0000000000000000000000000000000000000000;;				Labels:    map[string]string{"service": "S1"},
0000000000000000000000000000000000000000;;				Resources: podRequestedResource,
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			nodeName := pod.Spec.NodeName
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply label on the found node.")
0000000000000000000000000000000000000000;;			k := fmt.Sprintf("kubernetes.io/e2e-%s", "node-topologyKey")
0000000000000000000000000000000000000000;;			v := "topologyvalue"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, k)
0000000000000000000000000000000000000000;;			// make the nodes have balanced cpu,mem usage
0000000000000000000000000000000000000000;;			err := createBalancedPodForNodes(f, cs, ns, nodeList.Items, podRequestedResource, 0.6)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			By("Trying to launch the pod, now with podAffinity.")
0000000000000000000000000000000000000000;;			labelPodName := "pod-with-podaffinity"
0000000000000000000000000000000000000000;;			pod = createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Resources: podRequestedResource,
0000000000000000000000000000000000000000;;				Name:      labelPodName,
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					PodAffinity: &v1.PodAffinity{
0000000000000000000000000000000000000000;;						PreferredDuringSchedulingIgnoredDuringExecution: []v1.WeightedPodAffinityTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								PodAffinityTerm: v1.PodAffinityTerm{
0000000000000000000000000000000000000000;;									LabelSelector: &metav1.LabelSelector{
0000000000000000000000000000000000000000;;										MatchExpressions: []metav1.LabelSelectorRequirement{
0000000000000000000000000000000000000000;;											{
0000000000000000000000000000000000000000;;												Key:      "service",
0000000000000000000000000000000000000000;;												Operator: metav1.LabelSelectorOpIn,
0000000000000000000000000000000000000000;;												Values:   []string{"S1", "value2"},
0000000000000000000000000000000000000000;;											},
0000000000000000000000000000000000000000;;											{
0000000000000000000000000000000000000000;;												Key:      "service",
0000000000000000000000000000000000000000;;												Operator: metav1.LabelSelectorOpNotIn,
0000000000000000000000000000000000000000;;												Values:   []string{"S2"},
0000000000000000000000000000000000000000;;											}, {
0000000000000000000000000000000000000000;;												Key:      "service",
0000000000000000000000000000000000000000;;												Operator: metav1.LabelSelectorOpExists,
0000000000000000000000000000000000000000;;											},
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;									TopologyKey: k,
0000000000000000000000000000000000000000;;									Namespaces:  []string{ns},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;								Weight: 20,
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			By("Wait the pod becomes running.")
0000000000000000000000000000000000000000;;			framework.ExpectNoError(f.WaitForPodRunning(pod.Name))
0000000000000000000000000000000000000000;;			labelPod, err := cs.CoreV1().Pods(ns).Get(labelPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			By("Verify the pod was scheduled to the expected node.")
0000000000000000000000000000000000000000;;			Expect(labelPod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("Pod should be schedule to node that don't match the PodAntiAffinity terms", func() {
0000000000000000000000000000000000000000;;			By("Trying to launch a pod with a label to get a node which can launch it.")
0000000000000000000000000000000000000000;;			pod := runPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name:   "pod-with-label-security-s1",
0000000000000000000000000000000000000000;;				Labels: map[string]string{"security": "S1"},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			nodeName := pod.Spec.NodeName
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a label on the found node.")
0000000000000000000000000000000000000000;;			k := fmt.Sprintf("kubernetes.io/e2e-%s", "node-topologyKey")
0000000000000000000000000000000000000000;;			v := "topologyvalue"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, k)
0000000000000000000000000000000000000000;;			// make the nodes have balanced cpu,mem usage
0000000000000000000000000000000000000000;;			err := createBalancedPodForNodes(f, cs, ns, nodeList.Items, podRequestedResource, 0.6)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			By("Trying to launch the pod with podAntiAffinity.")
0000000000000000000000000000000000000000;;			labelPodName := "pod-with-pod-antiaffinity"
0000000000000000000000000000000000000000;;			pod = createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Resources: podRequestedResource,
0000000000000000000000000000000000000000;;				Name:      labelPodName,
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					PodAntiAffinity: &v1.PodAntiAffinity{
0000000000000000000000000000000000000000;;						PreferredDuringSchedulingIgnoredDuringExecution: []v1.WeightedPodAffinityTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								PodAffinityTerm: v1.PodAffinityTerm{
0000000000000000000000000000000000000000;;									LabelSelector: &metav1.LabelSelector{
0000000000000000000000000000000000000000;;										MatchExpressions: []metav1.LabelSelectorRequirement{
0000000000000000000000000000000000000000;;											{
0000000000000000000000000000000000000000;;												Key:      "security",
0000000000000000000000000000000000000000;;												Operator: metav1.LabelSelectorOpIn,
0000000000000000000000000000000000000000;;												Values:   []string{"S1", "value2"},
0000000000000000000000000000000000000000;;											},
0000000000000000000000000000000000000000;;											{
0000000000000000000000000000000000000000;;												Key:      "security",
0000000000000000000000000000000000000000;;												Operator: metav1.LabelSelectorOpNotIn,
0000000000000000000000000000000000000000;;												Values:   []string{"S2"},
0000000000000000000000000000000000000000;;											}, {
0000000000000000000000000000000000000000;;												Key:      "security",
0000000000000000000000000000000000000000;;												Operator: metav1.LabelSelectorOpExists,
0000000000000000000000000000000000000000;;											},
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;									TopologyKey: k,
0000000000000000000000000000000000000000;;									Namespaces:  []string{ns},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;								Weight: 10,
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			By("Wait the pod becomes running")
0000000000000000000000000000000000000000;;			framework.ExpectNoError(f.WaitForPodRunning(pod.Name))
0000000000000000000000000000000000000000;;			labelPod, err := cs.CoreV1().Pods(ns).Get(labelPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			By("Verify the pod was scheduled to the expected node.")
0000000000000000000000000000000000000000;;			Expect(labelPod.Spec.NodeName).NotTo(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("Pod should avoid to schedule to node that have avoidPod annotation", func() {
0000000000000000000000000000000000000000;;			nodeName := nodeList.Items[0].Name
0000000000000000000000000000000000000000;;			// make the nodes have balanced cpu,mem usage
0000000000000000000000000000000000000000;;			err := createBalancedPodForNodes(f, cs, ns, nodeList.Items, podRequestedResource, 0.5)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			By("Create a RC, with 0 replicas")
0000000000000000000000000000000000000000;;			rc := createRC(ns, "scheduler-priority-avoid-pod", int32(0), map[string]string{"name": "scheduler-priority-avoid-pod"}, f, podRequestedResource)
0000000000000000000000000000000000000000;;			// Cleanup the replication controller when we are done.
0000000000000000000000000000000000000000;;			defer func() {
0000000000000000000000000000000000000000;;				// Resize the replication controller to zero to get rid of pods.
0000000000000000000000000000000000000000;;				if err := framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, f.Namespace.Name, rc.Name); err != nil {
0000000000000000000000000000000000000000;;					framework.Logf("Failed to cleanup replication controller %v: %v.", rc.Name, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply avoidPod annotations on the first node.")
0000000000000000000000000000000000000000;;			avoidPod := v1.AvoidPods{
0000000000000000000000000000000000000000;;				PreferAvoidPods: []v1.PreferAvoidPodsEntry{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						PodSignature: v1.PodSignature{
0000000000000000000000000000000000000000;;							PodController: &metav1.OwnerReference{
0000000000000000000000000000000000000000;;								APIVersion: "v1",
0000000000000000000000000000000000000000;;								Kind:       "ReplicationController",
0000000000000000000000000000000000000000;;								Name:       rc.Name,
0000000000000000000000000000000000000000;;								UID:        rc.UID,
0000000000000000000000000000000000000000;;								Controller: func() *bool { b := true; return &b }(),
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						Reason:  "some reson",
0000000000000000000000000000000000000000;;						Message: "some message",
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			action := func() error {
0000000000000000000000000000000000000000;;				framework.AddOrUpdateAvoidPodOnNode(cs, nodeName, avoidPod)
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			predicate := func(node *v1.Node) bool {
0000000000000000000000000000000000000000;;				val, err := json.Marshal(avoidPod)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return node.Annotations[v1.PreferAvoidPodsAnnotationKey] == string(val)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			success, err := common.ObserveNodeUpdateAfterAction(f, nodeName, predicate, action)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			Expect(success).To(Equal(true))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			defer framework.RemoveAvoidPodsOffNode(cs, nodeName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Scale the RC: %s to len(nodeList.Item)-1 : %v.", rc.Name, len(nodeList.Items)-1))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.ScaleRC(f.ClientSet, f.InternalClientset, ns, rc.Name, uint(len(nodeList.Items)-1), true)
0000000000000000000000000000000000000000;;			testPods, err := cs.CoreV1().Pods(ns).List(metav1.ListOptions{
0000000000000000000000000000000000000000;;				LabelSelector: "name=scheduler-priority-avoid-pod",
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Verify the pods should not scheduled to the node: %s", nodeName))
0000000000000000000000000000000000000000;;			for _, pod := range testPods.Items {
0000000000000000000000000000000000000000;;				Expect(pod.Spec.NodeName).NotTo(Equal(nodeName))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("Pod should perfer to scheduled to nodes pod can tolerate", func() {
0000000000000000000000000000000000000000;;			// make the nodes have balanced cpu,mem usage ratio
0000000000000000000000000000000000000000;;			err := createBalancedPodForNodes(f, cs, ns, nodeList.Items, podRequestedResource, 0.5)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			//we need apply more taints on a node, because one match toleration only count 1
0000000000000000000000000000000000000000;;			By("Trying to apply 10 taint on the nodes except first one.")
0000000000000000000000000000000000000000;;			nodeName := nodeList.Items[0].Name
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for index, node := range nodeList.Items {
0000000000000000000000000000000000000000;;				if index == 0 {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for i := 0; i < 10; i++ {
0000000000000000000000000000000000000000;;					testTaint := addRandomTaitToNode(cs, node.Name)
0000000000000000000000000000000000000000;;					defer framework.RemoveTaintOffNode(cs, node.Name, *testTaint)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			By("Create a pod without any tolerations")
0000000000000000000000000000000000000000;;			tolerationPodName := "without-tolerations"
0000000000000000000000000000000000000000;;			pod := createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name: tolerationPodName,
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(f.WaitForPodRunning(pod.Name))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Pod should prefer scheduled to the node don't have the taint.")
0000000000000000000000000000000000000000;;			tolePod, err := cs.CoreV1().Pods(ns).Get(tolerationPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			Expect(tolePod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply 10 taint on the first node.")
0000000000000000000000000000000000000000;;			var tolerations []v1.Toleration
0000000000000000000000000000000000000000;;			for i := 0; i < 10; i++ {
0000000000000000000000000000000000000000;;				testTaint := addRandomTaitToNode(cs, nodeName)
0000000000000000000000000000000000000000;;				tolerations = append(tolerations, v1.Toleration{Key: testTaint.Key, Value: testTaint.Value, Effect: testTaint.Effect})
0000000000000000000000000000000000000000;;				defer framework.RemoveTaintOffNode(cs, nodeName, *testTaint)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			tolerationPodName = "with-tolerations"
0000000000000000000000000000000000000000;;			By("Create a pod that tolerates all the taints of the first node.")
0000000000000000000000000000000000000000;;			pod = createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name:        tolerationPodName,
0000000000000000000000000000000000000000;;				Tolerations: tolerations,
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(f.WaitForPodRunning(pod.Name))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Pod should prefer scheduled to the node that pod can tolerate.")
0000000000000000000000000000000000000000;;			tolePod, err = cs.CoreV1().Pods(ns).Get(tolerationPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			Expect(tolePod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// createBalancedPodForNodes creates a pod per node that asks for enough resources to make all nodes have the same mem/cpu usage ratio.
0000000000000000000000000000000000000000;;	func createBalancedPodForNodes(f *framework.Framework, cs clientset.Interface, ns string, nodes []v1.Node, requestedResource *v1.ResourceRequirements, ratio float64) error {
0000000000000000000000000000000000000000;;		// find the max, if the node has the max,use the one, if not,use the ratio parameter
0000000000000000000000000000000000000000;;		var maxCPUFraction, maxMemFraction float64 = ratio, ratio
0000000000000000000000000000000000000000;;		var cpuFractionMap = make(map[string]float64)
0000000000000000000000000000000000000000;;		var memFractionMap = make(map[string]float64)
0000000000000000000000000000000000000000;;		for _, node := range nodes {
0000000000000000000000000000000000000000;;			cpuFraction, memFraction := computeCpuMemFraction(cs, node, requestedResource)
0000000000000000000000000000000000000000;;			cpuFractionMap[node.Name] = cpuFraction
0000000000000000000000000000000000000000;;			memFractionMap[node.Name] = memFraction
0000000000000000000000000000000000000000;;			if cpuFraction > maxCPUFraction {
0000000000000000000000000000000000000000;;				maxCPUFraction = cpuFraction
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if memFraction > maxMemFraction {
0000000000000000000000000000000000000000;;				maxMemFraction = memFraction
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// we need the max one to keep the same cpu/mem use rate
0000000000000000000000000000000000000000;;		ratio = math.Max(maxCPUFraction, maxMemFraction)
0000000000000000000000000000000000000000;;		for _, node := range nodes {
0000000000000000000000000000000000000000;;			memAllocatable, found := node.Status.Allocatable["memory"]
0000000000000000000000000000000000000000;;			Expect(found).To(Equal(true))
0000000000000000000000000000000000000000;;			memAllocatableVal := memAllocatable.Value()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			cpuAllocatable, found := node.Status.Allocatable["cpu"]
0000000000000000000000000000000000000000;;			Expect(found).To(Equal(true))
0000000000000000000000000000000000000000;;			cpuAllocatableMil := cpuAllocatable.MilliValue()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			needCreateResource := v1.ResourceList{}
0000000000000000000000000000000000000000;;			cpuFraction := cpuFractionMap[node.Name]
0000000000000000000000000000000000000000;;			memFraction := memFractionMap[node.Name]
0000000000000000000000000000000000000000;;			needCreateResource["cpu"] = *resource.NewMilliQuantity(int64((ratio-cpuFraction)*float64(cpuAllocatableMil)), resource.DecimalSI)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			needCreateResource["memory"] = *resource.NewQuantity(int64((ratio-memFraction)*float64(memAllocatableVal)), resource.BinarySI)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			err := testutils.StartPods(cs, 1, ns, string(uuid.NewUUID()),
0000000000000000000000000000000000000000;;				*initPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;					Name:   "",
0000000000000000000000000000000000000000;;					Labels: balancePodLabel,
0000000000000000000000000000000000000000;;					Resources: &v1.ResourceRequirements{
0000000000000000000000000000000000000000;;						Limits:   needCreateResource,
0000000000000000000000000000000000000000;;						Requests: needCreateResource,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					NodeName: node.Name,
0000000000000000000000000000000000000000;;				}), true, framework.Logf)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, node := range nodes {
0000000000000000000000000000000000000000;;			By("Compute Cpu, Mem Fraction after create balanced pods.")
0000000000000000000000000000000000000000;;			computeCpuMemFraction(cs, node, requestedResource)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func computeCpuMemFraction(cs clientset.Interface, node v1.Node, resource *v1.ResourceRequirements) (float64, float64) {
0000000000000000000000000000000000000000;;		framework.Logf("ComputeCpuMemFraction for node: %v", node.Name)
0000000000000000000000000000000000000000;;		totalRequestedCpuResource := resource.Requests.Cpu().MilliValue()
0000000000000000000000000000000000000000;;		totalRequestedMemResource := resource.Requests.Memory().Value()
0000000000000000000000000000000000000000;;		allpods, err := cs.CoreV1().Pods(metav1.NamespaceAll).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			framework.Failf("Expect error of invalid, got : %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pod := range allpods.Items {
0000000000000000000000000000000000000000;;			if pod.Spec.NodeName == node.Name {
0000000000000000000000000000000000000000;;				framework.Logf("Pod for on the node: %v, Cpu: %v, Mem: %v", pod.Name, getNonZeroRequests(&pod).MilliCPU, getNonZeroRequests(&pod).Memory)
0000000000000000000000000000000000000000;;				totalRequestedCpuResource += getNonZeroRequests(&pod).MilliCPU
0000000000000000000000000000000000000000;;				totalRequestedMemResource += getNonZeroRequests(&pod).Memory
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		cpuAllocatable, found := node.Status.Allocatable["cpu"]
0000000000000000000000000000000000000000;;		Expect(found).To(Equal(true))
0000000000000000000000000000000000000000;;		cpuAllocatableMil := cpuAllocatable.MilliValue()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cpuFraction := float64(totalRequestedCpuResource) / float64(cpuAllocatableMil)
0000000000000000000000000000000000000000;;		memAllocatable, found := node.Status.Allocatable["memory"]
0000000000000000000000000000000000000000;;		Expect(found).To(Equal(true))
0000000000000000000000000000000000000000;;		memAllocatableVal := memAllocatable.Value()
0000000000000000000000000000000000000000;;		memFraction := float64(totalRequestedMemResource) / float64(memAllocatableVal)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		framework.Logf("Node: %v, totalRequestedCpuResource: %v, cpuAllocatableMil: %v, cpuFraction: %v", node.Name, totalRequestedCpuResource, cpuAllocatableMil, cpuFraction)
0000000000000000000000000000000000000000;;		framework.Logf("Node: %v, totalRequestedMemResource: %v, memAllocatableVal: %v, memFraction: %v", node.Name, totalRequestedMemResource, memAllocatableVal, memFraction)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return cpuFraction, memFraction
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getNonZeroRequests(pod *v1.Pod) Resource {
0000000000000000000000000000000000000000;;		result := Resource{}
0000000000000000000000000000000000000000;;		for i := range pod.Spec.Containers {
0000000000000000000000000000000000000000;;			container := &pod.Spec.Containers[i]
0000000000000000000000000000000000000000;;			cpu, memory := priorityutil.GetNonzeroRequests(&container.Resources.Requests)
0000000000000000000000000000000000000000;;			result.MilliCPU += cpu
0000000000000000000000000000000000000000;;			result.Memory += memory
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func createRC(ns, rsName string, replicas int32, rcPodLabels map[string]string, f *framework.Framework, resource *v1.ResourceRequirements) *v1.ReplicationController {
0000000000000000000000000000000000000000;;		rc := &v1.ReplicationController{
0000000000000000000000000000000000000000;;			TypeMeta: metav1.TypeMeta{
0000000000000000000000000000000000000000;;				Kind:       "ReplicationController",
0000000000000000000000000000000000000000;;				APIVersion: "v1",
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name: rsName,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.ReplicationControllerSpec{
0000000000000000000000000000000000000000;;				Replicas: &replicas,
0000000000000000000000000000000000000000;;				Template: &v1.PodTemplateSpec{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;						Labels: rcPodLabels,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;						Containers: []v1.Container{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Name:      rsName,
0000000000000000000000000000000000000000;;								Image:     framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;								Resources: *resource,
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rc, err := f.ClientSet.CoreV1().ReplicationControllers(ns).Create(rc)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		return rc
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func addRandomTaitToNode(cs clientset.Interface, nodeName string) *v1.Taint {
0000000000000000000000000000000000000000;;		testTaint := v1.Taint{
0000000000000000000000000000000000000000;;			Key:    fmt.Sprintf("kubernetes.io/e2e-taint-key-%s", string(uuid.NewUUID())),
0000000000000000000000000000000000000000;;			Value:  fmt.Sprintf("testing-taint-value-%s", string(uuid.NewUUID())),
0000000000000000000000000000000000000000;;			Effect: v1.TaintEffectPreferNoSchedule,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		framework.AddOrUpdateTaintOnNode(cs, nodeName, testTaint)
0000000000000000000000000000000000000000;;		framework.ExpectNodeHasTaint(cs, nodeName, &testTaint)
0000000000000000000000000000000000000000;;		return &testTaint
0000000000000000000000000000000000000000;;	}
