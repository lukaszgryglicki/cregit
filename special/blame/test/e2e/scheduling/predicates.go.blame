0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
d34e09b63300f581a42d415f6f0439773f124cca;test/e2e/scheduler_predicates.go[test/e2e/scheduler_predicates.go][test/e2e/scheduling/predicates.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package scheduling
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		podutil "k8s.io/kubernetes/pkg/api/v1/pod"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/common"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;		_ "github.com/stretchr/testify/assert"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const maxNumberOfPods int64 = 10
0000000000000000000000000000000000000000;;	const minPodCPURequest int64 = 500
0000000000000000000000000000000000000000;;	const imagePrePullingTimeout = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// variable set in BeforeEach, never modified afterwards
0000000000000000000000000000000000000000;;	var masterNodes sets.String
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type pausePodConfig struct {
0000000000000000000000000000000000000000;;		Name                              string
0000000000000000000000000000000000000000;;		Affinity                          *v1.Affinity
0000000000000000000000000000000000000000;;		Annotations, Labels, NodeSelector map[string]string
0000000000000000000000000000000000000000;;		Resources                         *v1.ResourceRequirements
0000000000000000000000000000000000000000;;		Tolerations                       []v1.Toleration
0000000000000000000000000000000000000000;;		NodeName                          string
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("SchedulerPredicates [Serial]", func() {
0000000000000000000000000000000000000000;;		var cs clientset.Interface
0000000000000000000000000000000000000000;;		var nodeList *v1.NodeList
0000000000000000000000000000000000000000;;		var systemPodsNo int
0000000000000000000000000000000000000000;;		var totalPodCapacity int64
0000000000000000000000000000000000000000;;		var RCName string
0000000000000000000000000000000000000000;;		var ns string
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("sched-pred")
0000000000000000000000000000000000000000;;		ignoreLabels := framework.ImagePullerLabels
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			rc, err := cs.Core().ReplicationControllers(ns).Get(RCName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err == nil && *(rc.Spec.Replicas) != 0 {
0000000000000000000000000000000000000000;;				By("Cleaning up the replication controller")
0000000000000000000000000000000000000000;;				err := framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, ns, RCName)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			cs = f.ClientSet
0000000000000000000000000000000000000000;;			ns = f.Namespace.Name
0000000000000000000000000000000000000000;;			nodeList = &v1.NodeList{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.WaitForAllNodesHealthy(cs, time.Minute)
0000000000000000000000000000000000000000;;			masterNodes, nodeList = framework.GetMasterAndWorkerNodesOrDie(cs)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			err := framework.CheckTestingNSDeletedExcept(cs, ns)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Every test case in this suite assumes that cluster add-on pods stay stable and
0000000000000000000000000000000000000000;;			// cannot be run in parallel with any other test that touches Nodes or Pods.
0000000000000000000000000000000000000000;;			// It is so because we need to have precise control on what's running in the cluster.
0000000000000000000000000000000000000000;;			systemPods, err := framework.GetPodsInNamespace(cs, ns, ignoreLabels)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			systemPodsNo = 0
0000000000000000000000000000000000000000;;			for _, pod := range systemPods {
0000000000000000000000000000000000000000;;				if !masterNodes.Has(pod.Spec.NodeName) && pod.DeletionTimestamp == nil {
0000000000000000000000000000000000000000;;					systemPodsNo++
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			err = framework.WaitForPodsRunningReady(cs, metav1.NamespaceSystem, int32(systemPodsNo), 0, framework.PodReadyBeforeTimeout, ignoreLabels)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			err = framework.WaitForPodsSuccess(cs, metav1.NamespaceSystem, framework.ImagePullerLabels, imagePrePullingTimeout)
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, node := range nodeList.Items {
0000000000000000000000000000000000000000;;				framework.Logf("\nLogging pods the kubelet thinks is on node %v before test", node.Name)
0000000000000000000000000000000000000000;;				framework.PrintAllKubeletPods(cs, node.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This test verifies that max-pods flag works as advertised. It assumes that cluster add-on pods stay stable
0000000000000000000000000000000000000000;;		// and cannot be run in parallel with any other test that touches Nodes or Pods. It is so because to check
0000000000000000000000000000000000000000;;		// if max-pods is working we need to fully saturate the cluster and keep it in this state for few seconds.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// Slow PR #13315 (8 min)
0000000000000000000000000000000000000000;;		It("validates MaxPods limit number of pods that are allowed to run [Slow]", func() {
0000000000000000000000000000000000000000;;			totalPodCapacity = 0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, node := range nodeList.Items {
0000000000000000000000000000000000000000;;				framework.Logf("Node: %v", node)
0000000000000000000000000000000000000000;;				podCapacity, found := node.Status.Capacity["pods"]
0000000000000000000000000000000000000000;;				Expect(found).To(Equal(true))
0000000000000000000000000000000000000000;;				totalPodCapacity += podCapacity.Value()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			currentlyScheduledPods := framework.WaitForStableCluster(cs, masterNodes)
0000000000000000000000000000000000000000;;			podsNeededForSaturation := int(totalPodCapacity) - currentlyScheduledPods
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Starting additional %v Pods to fully saturate the cluster max pods and trying to start another one", podsNeededForSaturation))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// As the pods are distributed randomly among nodes,
0000000000000000000000000000000000000000;;			// it can easily happen that all nodes are satured
0000000000000000000000000000000000000000;;			// and there is no need to create additional pods.
0000000000000000000000000000000000000000;;			// StartPods requires at least one pod to replicate.
0000000000000000000000000000000000000000;;			if podsNeededForSaturation > 0 {
0000000000000000000000000000000000000000;;				framework.ExpectNoError(testutils.StartPods(cs, podsNeededForSaturation, ns, "maxp",
0000000000000000000000000000000000000000;;					*initPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;						Name:   "",
0000000000000000000000000000000000000000;;						Labels: map[string]string{"name": ""},
0000000000000000000000000000000000000000;;					}), true, framework.Logf))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			podName := "additional-pod"
0000000000000000000000000000000000000000;;			WaitForSchedulerAfterAction(f, createPausePodAction(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name:   podName,
0000000000000000000000000000000000000000;;				Labels: map[string]string{"name": "additional"},
0000000000000000000000000000000000000000;;			}), podName, false)
0000000000000000000000000000000000000000;;			verifyResult(cs, podsNeededForSaturation, 1, ns)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This test verifies we don't allow scheduling of pods in a way that sum of limits of pods is greater than machines capacity.
0000000000000000000000000000000000000000;;		// It assumes that cluster add-on pods stay stable and cannot be run in parallel with any other test that touches Nodes or Pods.
0000000000000000000000000000000000000000;;		// It is so because we need to have precise control on what's running in the cluster.
0000000000000000000000000000000000000000;;		It("validates resource limits of pods that are allowed to run [Conformance]", func() {
0000000000000000000000000000000000000000;;			nodeMaxAllocatable := int64(0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodeToAllocatableMap := make(map[string]int64)
0000000000000000000000000000000000000000;;			for _, node := range nodeList.Items {
0000000000000000000000000000000000000000;;				allocatable, found := node.Status.Allocatable["cpu"]
0000000000000000000000000000000000000000;;				Expect(found).To(Equal(true))
0000000000000000000000000000000000000000;;				nodeToAllocatableMap[node.Name] = allocatable.MilliValue()
0000000000000000000000000000000000000000;;				if nodeMaxAllocatable < allocatable.MilliValue() {
0000000000000000000000000000000000000000;;					nodeMaxAllocatable = allocatable.MilliValue()
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.WaitForStableCluster(cs, masterNodes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			pods, err := cs.Core().Pods(metav1.NamespaceAll).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;				_, found := nodeToAllocatableMap[pod.Spec.NodeName]
0000000000000000000000000000000000000000;;				if found && pod.Status.Phase != v1.PodSucceeded && pod.Status.Phase != v1.PodFailed {
0000000000000000000000000000000000000000;;					framework.Logf("Pod %v requesting resource cpu=%vm on Node %v", pod.Name, getRequestedCPU(pod), pod.Spec.NodeName)
0000000000000000000000000000000000000000;;					nodeToAllocatableMap[pod.Spec.NodeName] -= getRequestedCPU(pod)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			var podsNeededForSaturation int
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			milliCpuPerPod := nodeMaxAllocatable / maxNumberOfPods
0000000000000000000000000000000000000000;;			if milliCpuPerPod < minPodCPURequest {
0000000000000000000000000000000000000000;;				milliCpuPerPod = minPodCPURequest
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.Logf("Using pod capacity: %vm", milliCpuPerPod)
0000000000000000000000000000000000000000;;			for name, leftAllocatable := range nodeToAllocatableMap {
0000000000000000000000000000000000000000;;				framework.Logf("Node: %v has cpu allocatable: %vm", name, leftAllocatable)
0000000000000000000000000000000000000000;;				podsNeededForSaturation += (int)(leftAllocatable / milliCpuPerPod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By(fmt.Sprintf("Starting additional %v Pods to fully saturate the cluster CPU and trying to start another one", podsNeededForSaturation))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// As the pods are distributed randomly among nodes,
0000000000000000000000000000000000000000;;			// it can easily happen that all nodes are saturated
0000000000000000000000000000000000000000;;			// and there is no need to create additional pods.
0000000000000000000000000000000000000000;;			// StartPods requires at least one pod to replicate.
0000000000000000000000000000000000000000;;			if podsNeededForSaturation > 0 {
0000000000000000000000000000000000000000;;				framework.ExpectNoError(testutils.StartPods(cs, podsNeededForSaturation, ns, "overcommit",
0000000000000000000000000000000000000000;;					*initPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;						Name:   "",
0000000000000000000000000000000000000000;;						Labels: map[string]string{"name": ""},
0000000000000000000000000000000000000000;;						Resources: &v1.ResourceRequirements{
0000000000000000000000000000000000000000;;							Limits: v1.ResourceList{
0000000000000000000000000000000000000000;;								"cpu": *resource.NewMilliQuantity(milliCpuPerPod, "DecimalSI"),
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;							Requests: v1.ResourceList{
0000000000000000000000000000000000000000;;								"cpu": *resource.NewMilliQuantity(milliCpuPerPod, "DecimalSI"),
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					}), true, framework.Logf))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			podName := "additional-pod"
0000000000000000000000000000000000000000;;			conf := pausePodConfig{
0000000000000000000000000000000000000000;;				Name:   podName,
0000000000000000000000000000000000000000;;				Labels: map[string]string{"name": "additional"},
0000000000000000000000000000000000000000;;				Resources: &v1.ResourceRequirements{
0000000000000000000000000000000000000000;;					Limits: v1.ResourceList{
0000000000000000000000000000000000000000;;						"cpu": *resource.NewMilliQuantity(milliCpuPerPod, "DecimalSI"),
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			WaitForSchedulerAfterAction(f, createPausePodAction(f, conf), podName, false)
0000000000000000000000000000000000000000;;			verifyResult(cs, podsNeededForSaturation, 1, ns)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Test Nodes does not have any label, hence it should be impossible to schedule Pod with
0000000000000000000000000000000000000000;;		// nonempty Selector set.
0000000000000000000000000000000000000000;;		It("validates that NodeSelector is respected if not matching [Conformance]", func() {
0000000000000000000000000000000000000000;;			By("Trying to schedule Pod with nonempty NodeSelector.")
0000000000000000000000000000000000000000;;			podName := "restricted-pod"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.WaitForStableCluster(cs, masterNodes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			conf := pausePodConfig{
0000000000000000000000000000000000000000;;				Name:   podName,
0000000000000000000000000000000000000000;;				Labels: map[string]string{"name": "restricted"},
0000000000000000000000000000000000000000;;				NodeSelector: map[string]string{
0000000000000000000000000000000000000000;;					"label": "nonempty",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			WaitForSchedulerAfterAction(f, createPausePodAction(f, conf), podName, false)
0000000000000000000000000000000000000000;;			verifyResult(cs, 0, 1, ns)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("validates that a pod with an invalid NodeAffinity is rejected", func() {
0000000000000000000000000000000000000000;;			By("Trying to launch a pod with an invalid Affinity data.")
0000000000000000000000000000000000000000;;			podName := "without-label"
0000000000000000000000000000000000000000;;			_, err := cs.CoreV1().Pods(ns).Create(initPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name: podName,
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					NodeAffinity: &v1.NodeAffinity{
0000000000000000000000000000000000000000;;						RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{
0000000000000000000000000000000000000000;;							NodeSelectorTerms: []v1.NodeSelectorTerm{
0000000000000000000000000000000000000000;;								{
0000000000000000000000000000000000000000;;									MatchExpressions: []v1.NodeSelectorRequirement{},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if err == nil || !errors.IsInvalid(err) {
0000000000000000000000000000000000000000;;				framework.Failf("Expect error of invalid, got : %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It("validates that NodeSelector is respected if matching [Conformance]", func() {
0000000000000000000000000000000000000000;;			nodeName := GetNodeThatCanRunPod(f)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a random label on the found node.")
0000000000000000000000000000000000000000;;			k := fmt.Sprintf("kubernetes.io/e2e-%s", string(uuid.NewUUID()))
0000000000000000000000000000000000000000;;			v := "42"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, k)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to relaunch the pod, now with labels.")
0000000000000000000000000000000000000000;;			labelPodName := "with-labels"
0000000000000000000000000000000000000000;;			createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name: labelPodName,
0000000000000000000000000000000000000000;;				NodeSelector: map[string]string{
0000000000000000000000000000000000000000;;					"kubernetes.io/hostname": nodeName,
0000000000000000000000000000000000000000;;					k: v,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// check that pod got scheduled. We intentionally DO NOT check that the
0000000000000000000000000000000000000000;;			// pod is running because this will create a race condition with the
0000000000000000000000000000000000000000;;			// kubelet and the scheduler: the scheduler might have scheduled a pod
0000000000000000000000000000000000000000;;			// already when the kubelet does not know about its new label yet. The
0000000000000000000000000000000000000000;;			// kubelet will then refuse to launch the pod.
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForPodNotPending(cs, ns, labelPodName))
0000000000000000000000000000000000000000;;			labelPod, err := cs.Core().Pods(ns).Get(labelPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			Expect(labelPod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Test Nodes does not have any label, hence it should be impossible to schedule Pod with
0000000000000000000000000000000000000000;;		// non-nil NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution.
0000000000000000000000000000000000000000;;		It("validates that NodeAffinity is respected if not matching", func() {
0000000000000000000000000000000000000000;;			By("Trying to schedule Pod with nonempty NodeSelector.")
0000000000000000000000000000000000000000;;			podName := "restricted-pod"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.WaitForStableCluster(cs, masterNodes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			conf := pausePodConfig{
0000000000000000000000000000000000000000;;				Name: podName,
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					NodeAffinity: &v1.NodeAffinity{
0000000000000000000000000000000000000000;;						RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{
0000000000000000000000000000000000000000;;							NodeSelectorTerms: []v1.NodeSelectorTerm{
0000000000000000000000000000000000000000;;								{
0000000000000000000000000000000000000000;;									MatchExpressions: []v1.NodeSelectorRequirement{
0000000000000000000000000000000000000000;;										{
0000000000000000000000000000000000000000;;											Key:      "foo",
0000000000000000000000000000000000000000;;											Operator: v1.NodeSelectorOpIn,
0000000000000000000000000000000000000000;;											Values:   []string{"bar", "value2"},
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								}, {
0000000000000000000000000000000000000000;;									MatchExpressions: []v1.NodeSelectorRequirement{
0000000000000000000000000000000000000000;;										{
0000000000000000000000000000000000000000;;											Key:      "diffkey",
0000000000000000000000000000000000000000;;											Operator: v1.NodeSelectorOpIn,
0000000000000000000000000000000000000000;;											Values:   []string{"wrong", "value2"},
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Labels: map[string]string{"name": "restricted"},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			WaitForSchedulerAfterAction(f, createPausePodAction(f, conf), podName, false)
0000000000000000000000000000000000000000;;			verifyResult(cs, 0, 1, ns)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Keep the same steps with the test on NodeSelector,
0000000000000000000000000000000000000000;;		// but specify Affinity in Pod.Annotations, instead of NodeSelector.
0000000000000000000000000000000000000000;;		It("validates that required NodeAffinity setting is respected if matching", func() {
0000000000000000000000000000000000000000;;			nodeName := GetNodeThatCanRunPod(f)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a random label on the found node.")
0000000000000000000000000000000000000000;;			k := fmt.Sprintf("kubernetes.io/e2e-%s", string(uuid.NewUUID()))
0000000000000000000000000000000000000000;;			v := "42"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, k)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to relaunch the pod, now with labels.")
0000000000000000000000000000000000000000;;			labelPodName := "with-labels"
0000000000000000000000000000000000000000;;			createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name: labelPodName,
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					NodeAffinity: &v1.NodeAffinity{
0000000000000000000000000000000000000000;;						RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{
0000000000000000000000000000000000000000;;							NodeSelectorTerms: []v1.NodeSelectorTerm{
0000000000000000000000000000000000000000;;								{
0000000000000000000000000000000000000000;;									MatchExpressions: []v1.NodeSelectorRequirement{
0000000000000000000000000000000000000000;;										{
0000000000000000000000000000000000000000;;											Key:      "kubernetes.io/hostname",
0000000000000000000000000000000000000000;;											Operator: v1.NodeSelectorOpIn,
0000000000000000000000000000000000000000;;											Values:   []string{nodeName},
0000000000000000000000000000000000000000;;										}, {
0000000000000000000000000000000000000000;;											Key:      k,
0000000000000000000000000000000000000000;;											Operator: v1.NodeSelectorOpIn,
0000000000000000000000000000000000000000;;											Values:   []string{v},
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// check that pod got scheduled. We intentionally DO NOT check that the
0000000000000000000000000000000000000000;;			// pod is running because this will create a race condition with the
0000000000000000000000000000000000000000;;			// kubelet and the scheduler: the scheduler might have scheduled a pod
0000000000000000000000000000000000000000;;			// already when the kubelet does not know about its new label yet. The
0000000000000000000000000000000000000000;;			// kubelet will then refuse to launch the pod.
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForPodNotPending(cs, ns, labelPodName))
0000000000000000000000000000000000000000;;			labelPod, err := cs.CoreV1().Pods(ns).Get(labelPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			Expect(labelPod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// labelSelector Operator is DoesNotExist but values are there in requiredDuringSchedulingIgnoredDuringExecution
0000000000000000000000000000000000000000;;		// part of podAffinity,so validation fails.
0000000000000000000000000000000000000000;;		It("validates that a pod with an invalid podAffinity is rejected because of the LabelSelectorRequirement is invalid", func() {
0000000000000000000000000000000000000000;;			By("Trying to launch a pod with an invalid pod Affinity data.")
0000000000000000000000000000000000000000;;			podName := "without-label-" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;			_, err := cs.CoreV1().Pods(ns).Create(initPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name:   podName,
0000000000000000000000000000000000000000;;				Labels: map[string]string{"name": "without-label"},
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					PodAffinity: &v1.PodAffinity{
0000000000000000000000000000000000000000;;						RequiredDuringSchedulingIgnoredDuringExecution: []v1.PodAffinityTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								LabelSelector: &metav1.LabelSelector{
0000000000000000000000000000000000000000;;									MatchExpressions: []metav1.LabelSelectorRequirement{
0000000000000000000000000000000000000000;;										{
0000000000000000000000000000000000000000;;											Key:      "security",
0000000000000000000000000000000000000000;;											Operator: metav1.LabelSelectorOpDoesNotExist,
0000000000000000000000000000000000000000;;											Values:   []string{"securityscan"},
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;								TopologyKey: "kubernetes.io/hostname",
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if err == nil || !errors.IsInvalid(err) {
0000000000000000000000000000000000000000;;				framework.Failf("Expect error of invalid, got : %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Test Nodes does not have any pod, hence it should be impossible to schedule a Pod with pod affinity.
0000000000000000000000000000000000000000;;		It("validates that Inter-pod-Affinity is respected if not matching", func() {
0000000000000000000000000000000000000000;;			By("Trying to schedule Pod with nonempty Pod Affinity.")
0000000000000000000000000000000000000000;;			framework.WaitForStableCluster(cs, masterNodes)
0000000000000000000000000000000000000000;;			podName := "without-label-" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;			conf := pausePodConfig{
0000000000000000000000000000000000000000;;				Name: podName,
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					PodAffinity: &v1.PodAffinity{
0000000000000000000000000000000000000000;;						RequiredDuringSchedulingIgnoredDuringExecution: []v1.PodAffinityTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								LabelSelector: &metav1.LabelSelector{
0000000000000000000000000000000000000000;;									MatchExpressions: []metav1.LabelSelectorRequirement{
0000000000000000000000000000000000000000;;										{
0000000000000000000000000000000000000000;;											Key:      "service",
0000000000000000000000000000000000000000;;											Operator: metav1.LabelSelectorOpIn,
0000000000000000000000000000000000000000;;											Values:   []string{"securityscan", "value2"},
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;								TopologyKey: "kubernetes.io/hostname",
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			WaitForSchedulerAfterAction(f, createPausePodAction(f, conf), podName, false)
0000000000000000000000000000000000000000;;			verifyResult(cs, 0, 1, ns)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// test the pod affinity successful matching scenario.
0000000000000000000000000000000000000000;;		It("validates that InterPodAffinity is respected if matching", func() {
0000000000000000000000000000000000000000;;			nodeName, _ := runAndKeepPodWithLabelAndGetNodeName(f)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a random label on the found node.")
0000000000000000000000000000000000000000;;			k := "e2e.inter-pod-affinity.kubernetes.io/zone"
0000000000000000000000000000000000000000;;			v := "china-e2etest"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, k)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to launch the pod, now with podAffinity.")
0000000000000000000000000000000000000000;;			labelPodName := "with-podaffinity-" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;			createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name: labelPodName,
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					PodAffinity: &v1.PodAffinity{
0000000000000000000000000000000000000000;;						RequiredDuringSchedulingIgnoredDuringExecution: []v1.PodAffinityTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								LabelSelector: &metav1.LabelSelector{
0000000000000000000000000000000000000000;;									MatchExpressions: []metav1.LabelSelectorRequirement{
0000000000000000000000000000000000000000;;										{
0000000000000000000000000000000000000000;;											Key:      "security",
0000000000000000000000000000000000000000;;											Operator: metav1.LabelSelectorOpIn,
0000000000000000000000000000000000000000;;											Values:   []string{"S1", "value2"},
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;								TopologyKey: k,
0000000000000000000000000000000000000000;;								Namespaces:  []string{ns},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// check that pod got scheduled. We intentionally DO NOT check that the
0000000000000000000000000000000000000000;;			// pod is running because this will create a race condition with the
0000000000000000000000000000000000000000;;			// kubelet and the scheduler: the scheduler might have scheduled a pod
0000000000000000000000000000000000000000;;			// already when the kubelet does not know about its new label yet. The
0000000000000000000000000000000000000000;;			// kubelet will then refuse to launch the pod.
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForPodNotPending(cs, ns, labelPodName))
0000000000000000000000000000000000000000;;			labelPod, err := cs.CoreV1().Pods(ns).Get(labelPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			Expect(labelPod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// test when the pod anti affinity rule is not satisfied, the pod would stay pending.
0000000000000000000000000000000000000000;;		It("validates that InterPodAntiAffinity is respected if matching 2", func() {
0000000000000000000000000000000000000000;;			// launch pods to find nodes which can launch a pod. We intentionally do
0000000000000000000000000000000000000000;;			// not just take the node list and choose the first and the second of them.
0000000000000000000000000000000000000000;;			// Depending on the cluster and the scheduler it might be that a "normal" pod
0000000000000000000000000000000000000000;;			// cannot be scheduled onto it.
0000000000000000000000000000000000000000;;			By("Launching two pods on two distinct nodes to get two node names")
0000000000000000000000000000000000000000;;			CreateHostPortPods(f, "host-port", 2, true)
0000000000000000000000000000000000000000;;			defer framework.DeleteRCAndPods(f.ClientSet, f.InternalClientset, ns, "host-port")
0000000000000000000000000000000000000000;;			podList, err := cs.CoreV1().Pods(ns).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			Expect(len(podList.Items)).To(Equal(2))
0000000000000000000000000000000000000000;;			nodeNames := []string{podList.Items[0].Spec.NodeName, podList.Items[1].Spec.NodeName}
0000000000000000000000000000000000000000;;			Expect(nodeNames[0]).ToNot(Equal(nodeNames[1]))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Applying a random label to both nodes.")
0000000000000000000000000000000000000000;;			k := "e2e.inter-pod-affinity.kubernetes.io/zone"
0000000000000000000000000000000000000000;;			v := "china-e2etest"
0000000000000000000000000000000000000000;;			for _, nodeName := range nodeNames {
0000000000000000000000000000000000000000;;				framework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;				framework.ExpectNodeHasLabel(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;				defer framework.RemoveLabelOffNode(cs, nodeName, k)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to launch another pod on the first node with the service label.")
0000000000000000000000000000000000000000;;			podName := "with-label-" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			runPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name:         podName,
0000000000000000000000000000000000000000;;				Labels:       map[string]string{"service": "S1"},
0000000000000000000000000000000000000000;;				NodeSelector: map[string]string{k: v}, // only launch on our two nodes
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to launch another pod, now with podAntiAffinity with same Labels.")
0000000000000000000000000000000000000000;;			labelPodName := "with-podantiaffinity-" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;			conf := pausePodConfig{
0000000000000000000000000000000000000000;;				Name:         labelPodName,
0000000000000000000000000000000000000000;;				Labels:       map[string]string{"service": "Diff"},
0000000000000000000000000000000000000000;;				NodeSelector: map[string]string{k: v}, // only launch on our two nodes, contradicting the podAntiAffinity
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					PodAntiAffinity: &v1.PodAntiAffinity{
0000000000000000000000000000000000000000;;						RequiredDuringSchedulingIgnoredDuringExecution: []v1.PodAffinityTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								LabelSelector: &metav1.LabelSelector{
0000000000000000000000000000000000000000;;									MatchExpressions: []metav1.LabelSelectorRequirement{
0000000000000000000000000000000000000000;;										{
0000000000000000000000000000000000000000;;											Key:      "service",
0000000000000000000000000000000000000000;;											Operator: metav1.LabelSelectorOpIn,
0000000000000000000000000000000000000000;;											Values:   []string{"S1", "value2"},
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;								TopologyKey: k,
0000000000000000000000000000000000000000;;								Namespaces:  []string{ns},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			WaitForSchedulerAfterAction(f, createPausePodAction(f, conf), labelPodName, false)
0000000000000000000000000000000000000000;;			verifyResult(cs, 3, 1, ns)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// test the pod affinity successful matching scenario with multiple Label Operators.
0000000000000000000000000000000000000000;;		It("validates that InterPodAffinity is respected if matching with multiple Affinities", func() {
0000000000000000000000000000000000000000;;			nodeName, _ := runAndKeepPodWithLabelAndGetNodeName(f)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a random label on the found node.")
0000000000000000000000000000000000000000;;			k := "e2e.inter-pod-affinity.kubernetes.io/zone"
0000000000000000000000000000000000000000;;			v := "kubernetes-e2e"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, k)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to launch the pod, now with multiple pod affinities with diff LabelOperators.")
0000000000000000000000000000000000000000;;			labelPodName := "with-podaffinity-" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;			createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name: labelPodName,
0000000000000000000000000000000000000000;;				Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;					PodAffinity: &v1.PodAffinity{
0000000000000000000000000000000000000000;;						RequiredDuringSchedulingIgnoredDuringExecution: []v1.PodAffinityTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								LabelSelector: &metav1.LabelSelector{
0000000000000000000000000000000000000000;;									MatchExpressions: []metav1.LabelSelectorRequirement{
0000000000000000000000000000000000000000;;										{
0000000000000000000000000000000000000000;;											Key:      "security",
0000000000000000000000000000000000000000;;											Operator: metav1.LabelSelectorOpIn,
0000000000000000000000000000000000000000;;											Values:   []string{"S1", "value2"},
0000000000000000000000000000000000000000;;										}, {
0000000000000000000000000000000000000000;;											Key:      "security",
0000000000000000000000000000000000000000;;											Operator: metav1.LabelSelectorOpNotIn,
0000000000000000000000000000000000000000;;											Values:   []string{"S2"},
0000000000000000000000000000000000000000;;										}, {
0000000000000000000000000000000000000000;;											Key:      "security",
0000000000000000000000000000000000000000;;											Operator: metav1.LabelSelectorOpExists,
0000000000000000000000000000000000000000;;										},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;								TopologyKey: k,
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// check that pod got scheduled. We intentionally DO NOT check that the
0000000000000000000000000000000000000000;;			// pod is running because this will create a race condition with the
0000000000000000000000000000000000000000;;			// kubelet and the scheduler: the scheduler might have scheduled a pod
0000000000000000000000000000000000000000;;			// already when the kubelet does not know about its new label yet. The
0000000000000000000000000000000000000000;;			// kubelet will then refuse to launch the pod.
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForPodNotPending(cs, ns, labelPodName))
0000000000000000000000000000000000000000;;			labelPod, err := cs.CoreV1().Pods(ns).Get(labelPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			Expect(labelPod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// test the pod affinity and anti affinity successful matching scenario.
0000000000000000000000000000000000000000;;		It("validates that InterPod Affinity and AntiAffinity is respected if matching", func() {
0000000000000000000000000000000000000000;;			nodeName, _ := runAndKeepPodWithLabelAndGetNodeName(f)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a random label on the found node.")
0000000000000000000000000000000000000000;;			k := "e2e.inter-pod-affinity.kubernetes.io/zone"
0000000000000000000000000000000000000000;;			v := "e2e-testing"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, k)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to launch the pod, now with Pod affinity and anti affinity.")
0000000000000000000000000000000000000000;;			pod := createPodWithPodAffinity(f, k)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// check that pod got scheduled. We intentionally DO NOT check that the
0000000000000000000000000000000000000000;;			// pod is running because this will create a race condition with the
0000000000000000000000000000000000000000;;			// kubelet and the scheduler: the scheduler might have scheduled a pod
0000000000000000000000000000000000000000;;			// already when the kubelet does not know about its new label yet. The
0000000000000000000000000000000000000000;;			// kubelet will then refuse to launch the pod.
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForPodNotPending(cs, ns, pod.Name))
0000000000000000000000000000000000000000;;			labelPod, err := cs.Core().Pods(ns).Get(pod.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			Expect(labelPod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Verify that an escaped JSON string of pod affinity and pod anti affinity in a YAML PodSpec works.
0000000000000000000000000000000000000000;;		It("validates that embedding the JSON PodAffinity and PodAntiAffinity setting as a string in the annotation value work", func() {
0000000000000000000000000000000000000000;;			nodeName, _ := runAndKeepPodWithLabelAndGetNodeName(f)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a label with fake az info on the found node.")
0000000000000000000000000000000000000000;;			k := "e2e.inter-pod-affinity.kubernetes.io/zone"
0000000000000000000000000000000000000000;;			v := "e2e-az1"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, k, v)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, k)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to launch a pod that with PodAffinity & PodAntiAffinity setting as embedded JSON string in the annotation value.")
0000000000000000000000000000000000000000;;			pod := createPodWithPodAffinity(f, "kubernetes.io/hostname")
0000000000000000000000000000000000000000;;			// check that pod got scheduled. We intentionally DO NOT check that the
0000000000000000000000000000000000000000;;			// pod is running because this will create a race condition with the
0000000000000000000000000000000000000000;;			// kubelet and the scheduler: the scheduler might have scheduled a pod
0000000000000000000000000000000000000000;;			// already when the kubelet does not know about its new label yet. The
0000000000000000000000000000000000000000;;			// kubelet will then refuse to launch the pod.
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForPodNotPending(cs, ns, pod.Name))
0000000000000000000000000000000000000000;;			labelPod, err := cs.Core().Pods(ns).Get(pod.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			Expect(labelPod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 1. Run a pod to get an available node, then delete the pod
0000000000000000000000000000000000000000;;		// 2. Taint the node with a random taint
0000000000000000000000000000000000000000;;		// 3. Try to relaunch the pod with tolerations tolerate the taints on node,
0000000000000000000000000000000000000000;;		// and the pod's nodeName specified to the name of node found in step 1
0000000000000000000000000000000000000000;;		It("validates that taints-tolerations is respected if matching", func() {
0000000000000000000000000000000000000000;;			nodeName := getNodeThatCanRunPodWithoutToleration(f)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a random taint on the found node.")
0000000000000000000000000000000000000000;;			testTaint := v1.Taint{
0000000000000000000000000000000000000000;;				Key:    fmt.Sprintf("kubernetes.io/e2e-taint-key-%s", string(uuid.NewUUID())),
0000000000000000000000000000000000000000;;				Value:  "testing-taint-value",
0000000000000000000000000000000000000000;;				Effect: v1.TaintEffectNoSchedule,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.AddOrUpdateTaintOnNode(cs, nodeName, testTaint)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasTaint(cs, nodeName, &testTaint)
0000000000000000000000000000000000000000;;			defer framework.RemoveTaintOffNode(cs, nodeName, testTaint)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a random label on the found node.")
0000000000000000000000000000000000000000;;			labelKey := fmt.Sprintf("kubernetes.io/e2e-label-key-%s", string(uuid.NewUUID()))
0000000000000000000000000000000000000000;;			labelValue := "testing-label-value"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, labelKey, labelValue)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, labelKey, labelValue)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, labelKey)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to relaunch the pod, now with tolerations.")
0000000000000000000000000000000000000000;;			tolerationPodName := "with-tolerations"
0000000000000000000000000000000000000000;;			createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;				Name:         tolerationPodName,
0000000000000000000000000000000000000000;;				Tolerations:  []v1.Toleration{{Key: testTaint.Key, Value: testTaint.Value, Effect: testTaint.Effect}},
0000000000000000000000000000000000000000;;				NodeSelector: map[string]string{labelKey: labelValue},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// check that pod got scheduled. We intentionally DO NOT check that the
0000000000000000000000000000000000000000;;			// pod is running because this will create a race condition with the
0000000000000000000000000000000000000000;;			// kubelet and the scheduler: the scheduler might have scheduled a pod
0000000000000000000000000000000000000000;;			// already when the kubelet does not know about its new taint yet. The
0000000000000000000000000000000000000000;;			// kubelet will then refuse to launch the pod.
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.WaitForPodNotPending(cs, ns, tolerationPodName))
0000000000000000000000000000000000000000;;			deployedPod, err := cs.Core().Pods(ns).Get(tolerationPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			Expect(deployedPod.Spec.NodeName).To(Equal(nodeName))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 1. Run a pod to get an available node, then delete the pod
0000000000000000000000000000000000000000;;		// 2. Taint the node with a random taint
0000000000000000000000000000000000000000;;		// 3. Try to relaunch the pod still no tolerations,
0000000000000000000000000000000000000000;;		// and the pod's nodeName specified to the name of node found in step 1
0000000000000000000000000000000000000000;;		It("validates that taints-tolerations is respected if not matching", func() {
0000000000000000000000000000000000000000;;			nodeName := getNodeThatCanRunPodWithoutToleration(f)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a random taint on the found node.")
0000000000000000000000000000000000000000;;			testTaint := v1.Taint{
0000000000000000000000000000000000000000;;				Key:    fmt.Sprintf("kubernetes.io/e2e-taint-key-%s", string(uuid.NewUUID())),
0000000000000000000000000000000000000000;;				Value:  "testing-taint-value",
0000000000000000000000000000000000000000;;				Effect: v1.TaintEffectNoSchedule,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.AddOrUpdateTaintOnNode(cs, nodeName, testTaint)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasTaint(cs, nodeName, &testTaint)
0000000000000000000000000000000000000000;;			defer framework.RemoveTaintOffNode(cs, nodeName, testTaint)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to apply a random label on the found node.")
0000000000000000000000000000000000000000;;			labelKey := fmt.Sprintf("kubernetes.io/e2e-label-key-%s", string(uuid.NewUUID()))
0000000000000000000000000000000000000000;;			labelValue := "testing-label-value"
0000000000000000000000000000000000000000;;			framework.AddOrUpdateLabelOnNode(cs, nodeName, labelKey, labelValue)
0000000000000000000000000000000000000000;;			framework.ExpectNodeHasLabel(cs, nodeName, labelKey, labelValue)
0000000000000000000000000000000000000000;;			defer framework.RemoveLabelOffNode(cs, nodeName, labelKey)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Trying to relaunch the pod, still no tolerations.")
0000000000000000000000000000000000000000;;			podNameNoTolerations := "still-no-tolerations"
0000000000000000000000000000000000000000;;			conf := pausePodConfig{
0000000000000000000000000000000000000000;;				Name:         podNameNoTolerations,
0000000000000000000000000000000000000000;;				NodeSelector: map[string]string{labelKey: labelValue},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			WaitForSchedulerAfterAction(f, createPausePodAction(f, conf), podNameNoTolerations, false)
0000000000000000000000000000000000000000;;			verifyResult(cs, 0, 1, ns)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("Removing taint off the node")
0000000000000000000000000000000000000000;;			WaitForSchedulerAfterAction(f, removeTaintFromNodeAction(cs, nodeName, testTaint), podNameNoTolerations, true)
0000000000000000000000000000000000000000;;			verifyResult(cs, 1, 0, ns)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func initPausePod(f *framework.Framework, conf pausePodConfig) *v1.Pod {
0000000000000000000000000000000000000000;;		pod := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:        conf.Name,
0000000000000000000000000000000000000000;;				Labels:      conf.Labels,
0000000000000000000000000000000000000000;;				Annotations: conf.Annotations,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				NodeSelector: conf.NodeSelector,
0000000000000000000000000000000000000000;;				Affinity:     conf.Affinity,
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  conf.Name,
0000000000000000000000000000000000000000;;						Image: framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Tolerations: conf.Tolerations,
0000000000000000000000000000000000000000;;				NodeName:    conf.NodeName,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if conf.Resources != nil {
0000000000000000000000000000000000000000;;			pod.Spec.Containers[0].Resources = *conf.Resources
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func createPausePod(f *framework.Framework, conf pausePodConfig) *v1.Pod {
0000000000000000000000000000000000000000;;		pod, err := f.ClientSet.Core().Pods(f.Namespace.Name).Create(initPausePod(f, conf))
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		return pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func runPausePod(f *framework.Framework, conf pausePodConfig) *v1.Pod {
0000000000000000000000000000000000000000;;		pod := createPausePod(f, conf)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(framework.WaitForPodRunningInNamespace(f.ClientSet, pod))
0000000000000000000000000000000000000000;;		pod, err := f.ClientSet.Core().Pods(f.Namespace.Name).Get(conf.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		return pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func runPodAndGetNodeName(f *framework.Framework, conf pausePodConfig) string {
0000000000000000000000000000000000000000;;		// launch a pod to find a node which can launch a pod. We intentionally do
0000000000000000000000000000000000000000;;		// not just take the node list and choose the first of them. Depending on the
0000000000000000000000000000000000000000;;		// cluster and the scheduler it might be that a "normal" pod cannot be
0000000000000000000000000000000000000000;;		// scheduled onto it.
0000000000000000000000000000000000000000;;		pod := runPausePod(f, conf)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Explicitly delete pod here to free the resource it takes.")
0000000000000000000000000000000000000000;;		err := f.ClientSet.Core().Pods(f.Namespace.Name).Delete(pod.Name, metav1.NewDeleteOptions(0))
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return pod.Spec.NodeName
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func createPodWithNodeAffinity(f *framework.Framework) *v1.Pod {
0000000000000000000000000000000000000000;;		return createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;			Name: "with-nodeaffinity-" + string(uuid.NewUUID()),
0000000000000000000000000000000000000000;;			Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;				NodeAffinity: &v1.NodeAffinity{
0000000000000000000000000000000000000000;;					RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{
0000000000000000000000000000000000000000;;						NodeSelectorTerms: []v1.NodeSelectorTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								MatchExpressions: []v1.NodeSelectorRequirement{
0000000000000000000000000000000000000000;;									{
0000000000000000000000000000000000000000;;										Key:      "kubernetes.io/e2e-az-name",
0000000000000000000000000000000000000000;;										Operator: v1.NodeSelectorOpIn,
0000000000000000000000000000000000000000;;										Values:   []string{"e2e-az1", "e2e-az2"},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func createPodWithPodAffinity(f *framework.Framework, topologyKey string) *v1.Pod {
0000000000000000000000000000000000000000;;		return createPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;			Name: "with-podantiaffinity-" + string(uuid.NewUUID()),
0000000000000000000000000000000000000000;;			Affinity: &v1.Affinity{
0000000000000000000000000000000000000000;;				PodAffinity: &v1.PodAffinity{
0000000000000000000000000000000000000000;;					RequiredDuringSchedulingIgnoredDuringExecution: []v1.PodAffinityTerm{
0000000000000000000000000000000000000000;;						{
0000000000000000000000000000000000000000;;							LabelSelector: &metav1.LabelSelector{
0000000000000000000000000000000000000000;;								MatchExpressions: []metav1.LabelSelectorRequirement{
0000000000000000000000000000000000000000;;									{
0000000000000000000000000000000000000000;;										Key:      "security",
0000000000000000000000000000000000000000;;										Operator: metav1.LabelSelectorOpIn,
0000000000000000000000000000000000000000;;										Values:   []string{"S1"},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;							TopologyKey: topologyKey,
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				PodAntiAffinity: &v1.PodAntiAffinity{
0000000000000000000000000000000000000000;;					RequiredDuringSchedulingIgnoredDuringExecution: []v1.PodAffinityTerm{
0000000000000000000000000000000000000000;;						{
0000000000000000000000000000000000000000;;							LabelSelector: &metav1.LabelSelector{
0000000000000000000000000000000000000000;;								MatchExpressions: []metav1.LabelSelectorRequirement{
0000000000000000000000000000000000000000;;									{
0000000000000000000000000000000000000000;;										Key:      "security",
0000000000000000000000000000000000000000;;										Operator: metav1.LabelSelectorOpIn,
0000000000000000000000000000000000000000;;										Values:   []string{"S2"},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;							TopologyKey: topologyKey,
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Returns a number of currently scheduled and not scheduled Pods.
0000000000000000000000000000000000000000;;	func getPodsScheduled(pods *v1.PodList) (scheduledPods, notScheduledPods []v1.Pod) {
0000000000000000000000000000000000000000;;		for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;			if !masterNodes.Has(pod.Spec.NodeName) {
0000000000000000000000000000000000000000;;				if pod.Spec.NodeName != "" {
0000000000000000000000000000000000000000;;					_, scheduledCondition := podutil.GetPodCondition(&pod.Status, v1.PodScheduled)
0000000000000000000000000000000000000000;;					// We can't assume that the scheduledCondition is always set if Pod is assigned to Node,
0000000000000000000000000000000000000000;;					// as e.g. DaemonController doesn't set it when assigning Pod to a Node. Currently
0000000000000000000000000000000000000000;;					// Kubelet sets this condition when it gets a Pod without it, but if we were expecting
0000000000000000000000000000000000000000;;					// that it would always be not nil, this would cause a rare race condition.
0000000000000000000000000000000000000000;;					if scheduledCondition != nil {
0000000000000000000000000000000000000000;;						Expect(scheduledCondition.Status).To(Equal(v1.ConditionTrue))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					scheduledPods = append(scheduledPods, pod)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					_, scheduledCondition := podutil.GetPodCondition(&pod.Status, v1.PodScheduled)
0000000000000000000000000000000000000000;;					if scheduledCondition != nil {
0000000000000000000000000000000000000000;;						Expect(scheduledCondition.Status).To(Equal(v1.ConditionFalse))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if scheduledCondition.Reason == "Unschedulable" {
0000000000000000000000000000000000000000;;						notScheduledPods = append(notScheduledPods, pod)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getRequestedCPU(pod v1.Pod) int64 {
0000000000000000000000000000000000000000;;		var result int64
0000000000000000000000000000000000000000;;		for _, container := range pod.Spec.Containers {
0000000000000000000000000000000000000000;;			result += container.Resources.Requests.Cpu().MilliValue()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// removeTaintFromNodeAction returns a closure that removes the given taint
0000000000000000000000000000000000000000;;	// from the given node upon invocation.
0000000000000000000000000000000000000000;;	func removeTaintFromNodeAction(cs clientset.Interface, nodeName string, testTaint v1.Taint) common.Action {
0000000000000000000000000000000000000000;;		return func() error {
0000000000000000000000000000000000000000;;			framework.RemoveTaintOffNode(cs, nodeName, testTaint)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// createPausePodAction returns a closure that creates a pause pod upon invocation.
0000000000000000000000000000000000000000;;	func createPausePodAction(f *framework.Framework, conf pausePodConfig) common.Action {
0000000000000000000000000000000000000000;;		return func() error {
0000000000000000000000000000000000000000;;			_, err := f.ClientSet.CoreV1().Pods(f.Namespace.Name).Create(initPausePod(f, conf))
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WaitForSchedulerAfterAction performs the provided action and then waits for
0000000000000000000000000000000000000000;;	// scheduler to act on the given pod.
0000000000000000000000000000000000000000;;	func WaitForSchedulerAfterAction(f *framework.Framework, action common.Action, podName string, expectSuccess bool) {
0000000000000000000000000000000000000000;;		predicate := scheduleFailureEvent(podName)
0000000000000000000000000000000000000000;;		if expectSuccess {
0000000000000000000000000000000000000000;;			predicate = scheduleSuccessEvent(podName, "" /* any node */)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		success, err := common.ObserveEventAfterAction(f, predicate, action)
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		Expect(success).To(Equal(true))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO: upgrade calls in PodAffinity tests when we're able to run them
0000000000000000000000000000000000000000;;	func verifyResult(c clientset.Interface, expectedScheduled int, expectedNotScheduled int, ns string) {
0000000000000000000000000000000000000000;;		allPods, err := c.CoreV1().Pods(ns).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		scheduledPods, notScheduledPods := framework.GetPodsScheduled(masterNodes, allPods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		printed := false
0000000000000000000000000000000000000000;;		printOnce := func(msg string) string {
0000000000000000000000000000000000000000;;			if !printed {
0000000000000000000000000000000000000000;;				printed = true
0000000000000000000000000000000000000000;;				return msg
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				return ""
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Expect(len(notScheduledPods)).To(Equal(expectedNotScheduled), printOnce(fmt.Sprintf("Not scheduled Pods: %#v", notScheduledPods)))
0000000000000000000000000000000000000000;;		Expect(len(scheduledPods)).To(Equal(expectedScheduled), printOnce(fmt.Sprintf("Scheduled Pods: %#v", scheduledPods)))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func runAndKeepPodWithLabelAndGetNodeName(f *framework.Framework) (string, string) {
0000000000000000000000000000000000000000;;		// launch a pod to find a node which can launch a pod. We intentionally do
0000000000000000000000000000000000000000;;		// not just take the node list and choose the first of them. Depending on the
0000000000000000000000000000000000000000;;		// cluster and the scheduler it might be that a "normal" pod cannot be
0000000000000000000000000000000000000000;;		// scheduled onto it.
0000000000000000000000000000000000000000;;		By("Trying to launch a pod with a label to get a node which can launch it.")
0000000000000000000000000000000000000000;;		pod := runPausePod(f, pausePodConfig{
0000000000000000000000000000000000000000;;			Name:   "with-label-" + string(uuid.NewUUID()),
0000000000000000000000000000000000000000;;			Labels: map[string]string{"security": "S1"},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		return pod.Spec.NodeName, pod.Name
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func GetNodeThatCanRunPod(f *framework.Framework) string {
0000000000000000000000000000000000000000;;		By("Trying to launch a pod without a label to get a node which can launch it.")
0000000000000000000000000000000000000000;;		return runPodAndGetNodeName(f, pausePodConfig{Name: "without-label"})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getNodeThatCanRunPodWithoutToleration(f *framework.Framework) string {
0000000000000000000000000000000000000000;;		By("Trying to launch a pod without a toleration to get a node which can launch it.")
0000000000000000000000000000000000000000;;		return runPodAndGetNodeName(f, pausePodConfig{Name: "without-toleration"})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func CreateHostPortPods(f *framework.Framework, id string, replicas int, expectRunning bool) {
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("Running RC which reserves host port"))
0000000000000000000000000000000000000000;;		config := &testutils.RCConfig{
0000000000000000000000000000000000000000;;			Client:         f.ClientSet,
0000000000000000000000000000000000000000;;			InternalClient: f.InternalClientset,
0000000000000000000000000000000000000000;;			Name:           id,
0000000000000000000000000000000000000000;;			Namespace:      f.Namespace.Name,
0000000000000000000000000000000000000000;;			Timeout:        defaultTimeout,
0000000000000000000000000000000000000000;;			Image:          framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;			Replicas:       replicas,
0000000000000000000000000000000000000000;;			HostPorts:      map[string]int{"port1": 4321},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err := framework.RunRC(*config)
0000000000000000000000000000000000000000;;		if expectRunning {
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
