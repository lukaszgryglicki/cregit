0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
6c6f1c04e70c2a34a7f26a697d9882eb89de9590;test/e2e/density.go[test/e2e/density.go][test/e2e/scalability/density.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package scalability
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"math"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/fields"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime/schema"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		utiluuid "k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/watch"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/cache"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/workqueue"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/batch"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/extensions"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		MinSaturationThreshold     = 2 * time.Minute
0000000000000000000000000000000000000000;;		MinPodsPerSecondThroughput = 8
0000000000000000000000000000000000000000;;		DensityPollInterval        = 10 * time.Second
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Maximum container failures this test tolerates before failing.
0000000000000000000000000000000000000000;;	var MaxContainerFailures = 0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type DensityTestConfig struct {
0000000000000000000000000000000000000000;;		Configs            []testutils.RunObjectConfig
0000000000000000000000000000000000000000;;		ClientSets         []clientset.Interface
0000000000000000000000000000000000000000;;		InternalClientsets []internalclientset.Interface
0000000000000000000000000000000000000000;;		PollInterval       time.Duration
0000000000000000000000000000000000000000;;		PodCount           int
0000000000000000000000000000000000000000;;		// What kind of resource we want to create
0000000000000000000000000000000000000000;;		kind             schema.GroupKind
0000000000000000000000000000000000000000;;		SecretConfigs    []*testutils.SecretConfig
0000000000000000000000000000000000000000;;		ConfigMapConfigs []*testutils.ConfigMapConfig
0000000000000000000000000000000000000000;;		DaemonConfigs    []*testutils.DaemonConfig
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func density30AddonResourceVerifier(numNodes int) map[string]framework.ResourceConstraint {
0000000000000000000000000000000000000000;;		var apiserverMem uint64
0000000000000000000000000000000000000000;;		var controllerMem uint64
0000000000000000000000000000000000000000;;		var schedulerMem uint64
0000000000000000000000000000000000000000;;		apiserverCPU := math.MaxFloat32
0000000000000000000000000000000000000000;;		apiserverMem = math.MaxUint64
0000000000000000000000000000000000000000;;		controllerCPU := math.MaxFloat32
0000000000000000000000000000000000000000;;		controllerMem = math.MaxUint64
0000000000000000000000000000000000000000;;		schedulerCPU := math.MaxFloat32
0000000000000000000000000000000000000000;;		schedulerMem = math.MaxUint64
0000000000000000000000000000000000000000;;		framework.Logf("Setting resource constraings for provider: %s", framework.TestContext.Provider)
0000000000000000000000000000000000000000;;		if framework.ProviderIs("kubemark") {
0000000000000000000000000000000000000000;;			if numNodes <= 5 {
0000000000000000000000000000000000000000;;				apiserverCPU = 0.35
0000000000000000000000000000000000000000;;				apiserverMem = 150 * (1024 * 1024)
0000000000000000000000000000000000000000;;				controllerCPU = 0.15
0000000000000000000000000000000000000000;;				controllerMem = 100 * (1024 * 1024)
0000000000000000000000000000000000000000;;				schedulerCPU = 0.05
0000000000000000000000000000000000000000;;				schedulerMem = 50 * (1024 * 1024)
0000000000000000000000000000000000000000;;			} else if numNodes <= 100 {
0000000000000000000000000000000000000000;;				apiserverCPU = 1.5
0000000000000000000000000000000000000000;;				apiserverMem = 1500 * (1024 * 1024)
0000000000000000000000000000000000000000;;				controllerCPU = 0.5
0000000000000000000000000000000000000000;;				controllerMem = 500 * (1024 * 1024)
0000000000000000000000000000000000000000;;				schedulerCPU = 0.4
0000000000000000000000000000000000000000;;				schedulerMem = 180 * (1024 * 1024)
0000000000000000000000000000000000000000;;			} else if numNodes <= 500 {
0000000000000000000000000000000000000000;;				apiserverCPU = 3.5
0000000000000000000000000000000000000000;;				apiserverMem = 3400 * (1024 * 1024)
0000000000000000000000000000000000000000;;				controllerCPU = 1.3
0000000000000000000000000000000000000000;;				controllerMem = 1100 * (1024 * 1024)
0000000000000000000000000000000000000000;;				schedulerCPU = 1.5
0000000000000000000000000000000000000000;;				schedulerMem = 500 * (1024 * 1024)
0000000000000000000000000000000000000000;;			} else if numNodes <= 1000 {
0000000000000000000000000000000000000000;;				apiserverCPU = 5.5
0000000000000000000000000000000000000000;;				apiserverMem = 4000 * (1024 * 1024)
0000000000000000000000000000000000000000;;				controllerCPU = 3
0000000000000000000000000000000000000000;;				controllerMem = 2000 * (1024 * 1024)
0000000000000000000000000000000000000000;;				schedulerCPU = 1.5
0000000000000000000000000000000000000000;;				schedulerMem = 750 * (1024 * 1024)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			if numNodes <= 100 {
0000000000000000000000000000000000000000;;				apiserverCPU = 1.8
0000000000000000000000000000000000000000;;				apiserverMem = 1500 * (1024 * 1024)
0000000000000000000000000000000000000000;;				controllerCPU = 0.5
0000000000000000000000000000000000000000;;				controllerMem = 500 * (1024 * 1024)
0000000000000000000000000000000000000000;;				schedulerCPU = 0.4
0000000000000000000000000000000000000000;;				schedulerMem = 180 * (1024 * 1024)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		constraints := make(map[string]framework.ResourceConstraint)
0000000000000000000000000000000000000000;;		constraints["fluentd-elasticsearch"] = framework.ResourceConstraint{
0000000000000000000000000000000000000000;;			CPUConstraint:    0.2,
0000000000000000000000000000000000000000;;			MemoryConstraint: 250 * (1024 * 1024),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		constraints["elasticsearch-logging"] = framework.ResourceConstraint{
0000000000000000000000000000000000000000;;			CPUConstraint: 2,
0000000000000000000000000000000000000000;;			// TODO: bring it down to 750MB again, when we lower Kubelet verbosity level. I.e. revert #19164
0000000000000000000000000000000000000000;;			MemoryConstraint: 5000 * (1024 * 1024),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		constraints["heapster"] = framework.ResourceConstraint{
0000000000000000000000000000000000000000;;			CPUConstraint:    2,
0000000000000000000000000000000000000000;;			MemoryConstraint: 1800 * (1024 * 1024),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		constraints["kibana-logging"] = framework.ResourceConstraint{
0000000000000000000000000000000000000000;;			CPUConstraint:    0.2,
0000000000000000000000000000000000000000;;			MemoryConstraint: 100 * (1024 * 1024),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		constraints["kube-proxy"] = framework.ResourceConstraint{
0000000000000000000000000000000000000000;;			CPUConstraint:    0.15,
0000000000000000000000000000000000000000;;			MemoryConstraint: 100 * (1024 * 1024),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		constraints["l7-lb-controller"] = framework.ResourceConstraint{
0000000000000000000000000000000000000000;;			CPUConstraint:    0.15,
0000000000000000000000000000000000000000;;			MemoryConstraint: 75 * (1024 * 1024),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		constraints["influxdb"] = framework.ResourceConstraint{
0000000000000000000000000000000000000000;;			CPUConstraint:    2,
0000000000000000000000000000000000000000;;			MemoryConstraint: 500 * (1024 * 1024),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		constraints["kube-apiserver"] = framework.ResourceConstraint{
0000000000000000000000000000000000000000;;			CPUConstraint:    apiserverCPU,
0000000000000000000000000000000000000000;;			MemoryConstraint: apiserverMem,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		constraints["kube-controller-manager"] = framework.ResourceConstraint{
0000000000000000000000000000000000000000;;			CPUConstraint:    controllerCPU,
0000000000000000000000000000000000000000;;			MemoryConstraint: controllerMem,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		constraints["kube-scheduler"] = framework.ResourceConstraint{
0000000000000000000000000000000000000000;;			CPUConstraint:    schedulerCPU,
0000000000000000000000000000000000000000;;			MemoryConstraint: schedulerMem,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return constraints
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func logPodStartupStatus(c clientset.Interface, expectedPods int, observedLabels map[string]string, period time.Duration, stopCh chan struct{}) {
0000000000000000000000000000000000000000;;		label := labels.SelectorFromSet(labels.Set(observedLabels))
0000000000000000000000000000000000000000;;		podStore := testutils.NewPodStore(c, metav1.NamespaceAll, label, fields.Everything())
0000000000000000000000000000000000000000;;		defer podStore.Stop()
0000000000000000000000000000000000000000;;		ticker := time.NewTicker(period)
0000000000000000000000000000000000000000;;		defer ticker.Stop()
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case <-ticker.C:
0000000000000000000000000000000000000000;;				pods := podStore.List()
0000000000000000000000000000000000000000;;				startupStatus := testutils.ComputeRCStartupStatus(pods, expectedPods)
0000000000000000000000000000000000000000;;				framework.Logf(startupStatus.String("Density"))
0000000000000000000000000000000000000000;;			case <-stopCh:
0000000000000000000000000000000000000000;;				pods := podStore.List()
0000000000000000000000000000000000000000;;				startupStatus := testutils.ComputeRCStartupStatus(pods, expectedPods)
0000000000000000000000000000000000000000;;				framework.Logf(startupStatus.String("Density"))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// runDensityTest will perform a density test and return the time it took for
0000000000000000000000000000000000000000;;	// all pods to start
0000000000000000000000000000000000000000;;	func runDensityTest(dtc DensityTestConfig) time.Duration {
0000000000000000000000000000000000000000;;		defer GinkgoRecover()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Create all secrets, configmaps and daemons.
0000000000000000000000000000000000000000;;		for i := range dtc.SecretConfigs {
0000000000000000000000000000000000000000;;			dtc.SecretConfigs[i].Run()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := range dtc.ConfigMapConfigs {
0000000000000000000000000000000000000000;;			dtc.ConfigMapConfigs[i].Run()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := range dtc.DaemonConfigs {
0000000000000000000000000000000000000000;;			dtc.DaemonConfigs[i].Run()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start all replication controllers.
0000000000000000000000000000000000000000;;		startTime := time.Now()
0000000000000000000000000000000000000000;;		wg := sync.WaitGroup{}
0000000000000000000000000000000000000000;;		wg.Add(len(dtc.Configs))
0000000000000000000000000000000000000000;;		for i := range dtc.Configs {
0000000000000000000000000000000000000000;;			config := dtc.Configs[i]
0000000000000000000000000000000000000000;;			go func() {
0000000000000000000000000000000000000000;;				defer GinkgoRecover()
0000000000000000000000000000000000000000;;				// Call wg.Done() in defer to avoid blocking whole test
0000000000000000000000000000000000000000;;				// in case of error from RunRC.
0000000000000000000000000000000000000000;;				defer wg.Done()
0000000000000000000000000000000000000000;;				framework.ExpectNoError(config.Run())
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		logStopCh := make(chan struct{})
0000000000000000000000000000000000000000;;		go logPodStartupStatus(dtc.ClientSets[0], dtc.PodCount, map[string]string{"type": "densityPod"}, dtc.PollInterval, logStopCh)
0000000000000000000000000000000000000000;;		wg.Wait()
0000000000000000000000000000000000000000;;		startupTime := time.Now().Sub(startTime)
0000000000000000000000000000000000000000;;		close(logStopCh)
0000000000000000000000000000000000000000;;		framework.Logf("E2E startup time for %d pods: %v", dtc.PodCount, startupTime)
0000000000000000000000000000000000000000;;		framework.Logf("Throughput (pods/s) during cluster saturation phase: %v", float32(dtc.PodCount)/float32(startupTime/time.Second))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Print some data about Pod to Node allocation
0000000000000000000000000000000000000000;;		By("Printing Pod to Node allocation data")
0000000000000000000000000000000000000000;;		podList, err := dtc.ClientSets[0].Core().Pods(metav1.NamespaceAll).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		pausePodAllocation := make(map[string]int)
0000000000000000000000000000000000000000;;		systemPodAllocation := make(map[string][]string)
0000000000000000000000000000000000000000;;		for _, pod := range podList.Items {
0000000000000000000000000000000000000000;;			if pod.Namespace == metav1.NamespaceSystem {
0000000000000000000000000000000000000000;;				systemPodAllocation[pod.Spec.NodeName] = append(systemPodAllocation[pod.Spec.NodeName], pod.Name)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				pausePodAllocation[pod.Spec.NodeName]++
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		nodeNames := make([]string, 0)
0000000000000000000000000000000000000000;;		for k := range pausePodAllocation {
0000000000000000000000000000000000000000;;			nodeNames = append(nodeNames, k)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		sort.Strings(nodeNames)
0000000000000000000000000000000000000000;;		for _, node := range nodeNames {
0000000000000000000000000000000000000000;;			framework.Logf("%v: %v pause pods, system pods: %v", node, pausePodAllocation[node], systemPodAllocation[node])
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return startupTime
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func cleanupDensityTest(dtc DensityTestConfig) {
0000000000000000000000000000000000000000;;		defer GinkgoRecover()
0000000000000000000000000000000000000000;;		By("Deleting created Collections")
0000000000000000000000000000000000000000;;		numberOfClients := len(dtc.ClientSets)
0000000000000000000000000000000000000000;;		// We explicitly delete all pods to have API calls necessary for deletion accounted in metrics.
0000000000000000000000000000000000000000;;		for i := range dtc.Configs {
0000000000000000000000000000000000000000;;			name := dtc.Configs[i].GetName()
0000000000000000000000000000000000000000;;			namespace := dtc.Configs[i].GetNamespace()
0000000000000000000000000000000000000000;;			kind := dtc.Configs[i].GetKind()
0000000000000000000000000000000000000000;;			if framework.TestContext.GarbageCollectorEnabled && kindSupportsGarbageCollector(kind) {
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("Cleaning up only the %v, garbage collector will clean up the pods", kind))
0000000000000000000000000000000000000000;;				err := framework.DeleteResourceAndWaitForGC(dtc.ClientSets[i%numberOfClients], kind, namespace, name)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("Cleaning up the %v and pods", kind))
0000000000000000000000000000000000000000;;				err := framework.DeleteResourceAndPods(dtc.ClientSets[i%numberOfClients], dtc.InternalClientsets[i%numberOfClients], kind, namespace, name)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Delete all secrets, configmaps and daemons.
0000000000000000000000000000000000000000;;		for i := range dtc.SecretConfigs {
0000000000000000000000000000000000000000;;			dtc.SecretConfigs[i].Stop()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := range dtc.ConfigMapConfigs {
0000000000000000000000000000000000000000;;			dtc.ConfigMapConfigs[i].Stop()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := range dtc.DaemonConfigs {
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.DeleteResourceAndPods(
0000000000000000000000000000000000000000;;				dtc.ClientSets[i%numberOfClients],
0000000000000000000000000000000000000000;;				dtc.InternalClientsets[i%numberOfClients],
0000000000000000000000000000000000000000;;				extensions.Kind("DaemonSet"),
0000000000000000000000000000000000000000;;				dtc.DaemonConfigs[i].Namespace,
0000000000000000000000000000000000000000;;				dtc.DaemonConfigs[i].Name,
0000000000000000000000000000000000000000;;			))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// This test suite can take a long time to run, and can affect or be affected by other tests.
0000000000000000000000000000000000000000;;	// So by default it is added to the ginkgo.skip list (see driver.go).
0000000000000000000000000000000000000000;;	// To run this suite you must explicitly ask for it by setting the
0000000000000000000000000000000000000000;;	// -t/--test flag or ginkgo.focus flag.
0000000000000000000000000000000000000000;;	// IMPORTANT: This test is designed to work on large (>= 100 Nodes) clusters. For smaller ones
0000000000000000000000000000000000000000;;	// results will not be representative for control-plane performance as we'll start hitting
0000000000000000000000000000000000000000;;	// limits on Docker's concurrent container startup.
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Density", func() {
0000000000000000000000000000000000000000;;		var c clientset.Interface
0000000000000000000000000000000000000000;;		var nodeCount int
0000000000000000000000000000000000000000;;		var additionalPodsPrefix string
0000000000000000000000000000000000000000;;		var ns string
0000000000000000000000000000000000000000;;		var uuid string
0000000000000000000000000000000000000000;;		var e2eStartupTime time.Duration
0000000000000000000000000000000000000000;;		var totalPods int
0000000000000000000000000000000000000000;;		var nodeCpuCapacity int64
0000000000000000000000000000000000000000;;		var nodeMemCapacity int64
0000000000000000000000000000000000000000;;		var nodes *v1.NodeList
0000000000000000000000000000000000000000;;		var masters sets.String
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		testCaseBaseName := "density"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Gathers data prior to framework namespace teardown
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			saturationThreshold := time.Duration((totalPods / MinPodsPerSecondThroughput)) * time.Second
0000000000000000000000000000000000000000;;			if saturationThreshold < MinSaturationThreshold {
0000000000000000000000000000000000000000;;				saturationThreshold = MinSaturationThreshold
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Expect(e2eStartupTime).NotTo(BeNumerically(">", saturationThreshold))
0000000000000000000000000000000000000000;;			saturationData := framework.SaturationTime{
0000000000000000000000000000000000000000;;				TimeToSaturate: e2eStartupTime,
0000000000000000000000000000000000000000;;				NumberOfNodes:  nodeCount,
0000000000000000000000000000000000000000;;				NumberOfPods:   totalPods,
0000000000000000000000000000000000000000;;				Throughput:     float32(totalPods) / float32(e2eStartupTime/time.Second),
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			framework.Logf("Cluster saturation time: %s", framework.PrettyPrintJSON(saturationData))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			summaries := make([]framework.TestDataSummary, 0, 2)
0000000000000000000000000000000000000000;;			// Verify latency metrics.
0000000000000000000000000000000000000000;;			highLatencyRequests, metrics, err := framework.HighLatencyRequests(c)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				summaries = append(summaries, metrics)
0000000000000000000000000000000000000000;;				Expect(highLatencyRequests).NotTo(BeNumerically(">", 0), "There should be no high-latency requests")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Verify scheduler metrics.
0000000000000000000000000000000000000000;;			// TODO: Reset metrics at the beginning of the test.
0000000000000000000000000000000000000000;;			// We should do something similar to how we do it for APIserver.
0000000000000000000000000000000000000000;;			latency, err := framework.VerifySchedulerLatency(c)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				summaries = append(summaries, latency)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.PrintSummaries(summaries, testCaseBaseName)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		options := framework.FrameworkOptions{
0000000000000000000000000000000000000000;;			ClientQPS:   50.0,
0000000000000000000000000000000000000000;;			ClientBurst: 100,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Explicitly put here, to delete namespace at the end of the test
0000000000000000000000000000000000000000;;		// (after measuring latency metrics, etc.).
0000000000000000000000000000000000000000;;		f := framework.NewFramework(testCaseBaseName, options, nil)
0000000000000000000000000000000000000000;;		f.NamespaceDeletionTimeout = time.Hour
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			c = f.ClientSet
0000000000000000000000000000000000000000;;			ns = f.Namespace.Name
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			masters, nodes = framework.GetMasterAndWorkerNodesOrDie(c)
0000000000000000000000000000000000000000;;			nodeCount = len(nodes.Items)
0000000000000000000000000000000000000000;;			Expect(nodeCount).NotTo(BeZero())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodeCpuCapacity = nodes.Items[0].Status.Allocatable.Cpu().MilliValue()
0000000000000000000000000000000000000000;;			nodeMemCapacity = nodes.Items[0].Status.Allocatable.Memory().Value()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Terminating a namespace (deleting the remaining objects from it - which
0000000000000000000000000000000000000000;;			// generally means events) can affect the current run. Thus we wait for all
0000000000000000000000000000000000000000;;			// terminating namespace to be finally deleted before starting this test.
0000000000000000000000000000000000000000;;			err := framework.CheckTestingNSDeletedExcept(c, ns)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			uuid = string(utiluuid.NewUUID())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.ResetMetrics(c))
0000000000000000000000000000000000000000;;			framework.ExpectNoError(os.Mkdir(fmt.Sprintf(framework.TestContext.OutputDir+"/%s", uuid), 0777))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.Logf("Listing nodes for easy debugging:\n")
0000000000000000000000000000000000000000;;			for _, node := range nodes.Items {
0000000000000000000000000000000000000000;;				var internalIP, externalIP string
0000000000000000000000000000000000000000;;				for _, address := range node.Status.Addresses {
0000000000000000000000000000000000000000;;					if address.Type == v1.NodeInternalIP {
0000000000000000000000000000000000000000;;						internalIP = address.Address
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if address.Type == v1.NodeExternalIP {
0000000000000000000000000000000000000000;;						externalIP = address.Address
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				framework.Logf("Name: %v, clusterIP: %v, externalIP: %v", node.ObjectMeta.Name, internalIP, externalIP)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		type Density struct {
0000000000000000000000000000000000000000;;			// Controls if e2e latency tests should be run (they are slow)
0000000000000000000000000000000000000000;;			runLatencyTest bool
0000000000000000000000000000000000000000;;			podsPerNode    int
0000000000000000000000000000000000000000;;			// Controls how often the apiserver is polled for pods
0000000000000000000000000000000000000000;;			interval time.Duration
0000000000000000000000000000000000000000;;			// What kind of resource we should be creating. Default: ReplicationController
0000000000000000000000000000000000000000;;			kind             schema.GroupKind
0000000000000000000000000000000000000000;;			secretsPerPod    int
0000000000000000000000000000000000000000;;			configMapsPerPod int
0000000000000000000000000000000000000000;;			daemonsPerNode   int
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		densityTests := []Density{
0000000000000000000000000000000000000000;;			// TODO: Expose runLatencyTest as ginkgo flag.
0000000000000000000000000000000000000000;;			{podsPerNode: 3, runLatencyTest: false, kind: api.Kind("ReplicationController")},
0000000000000000000000000000000000000000;;			{podsPerNode: 30, runLatencyTest: true, kind: api.Kind("ReplicationController")},
0000000000000000000000000000000000000000;;			{podsPerNode: 50, runLatencyTest: false, kind: api.Kind("ReplicationController")},
0000000000000000000000000000000000000000;;			{podsPerNode: 95, runLatencyTest: true, kind: api.Kind("ReplicationController")},
0000000000000000000000000000000000000000;;			{podsPerNode: 100, runLatencyTest: false, kind: api.Kind("ReplicationController")},
0000000000000000000000000000000000000000;;			// Tests for other resource types:
0000000000000000000000000000000000000000;;			{podsPerNode: 30, runLatencyTest: true, kind: extensions.Kind("Deployment")},
0000000000000000000000000000000000000000;;			{podsPerNode: 30, runLatencyTest: true, kind: batch.Kind("Job")},
0000000000000000000000000000000000000000;;			// Test scheduling when daemons are preset
0000000000000000000000000000000000000000;;			{podsPerNode: 30, runLatencyTest: true, kind: api.Kind("ReplicationController"), daemonsPerNode: 2},
0000000000000000000000000000000000000000;;			// Test with secrets
0000000000000000000000000000000000000000;;			{podsPerNode: 30, runLatencyTest: true, kind: extensions.Kind("Deployment"), secretsPerPod: 2},
0000000000000000000000000000000000000000;;			// Test with configmaps
0000000000000000000000000000000000000000;;			{podsPerNode: 30, runLatencyTest: true, kind: extensions.Kind("Deployment"), configMapsPerPod: 2},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, testArg := range densityTests {
0000000000000000000000000000000000000000;;			feature := "ManualPerformance"
0000000000000000000000000000000000000000;;			switch testArg.podsPerNode {
0000000000000000000000000000000000000000;;			case 30:
0000000000000000000000000000000000000000;;				if testArg.kind == api.Kind("ReplicationController") && testArg.daemonsPerNode == 0 && testArg.secretsPerPod == 0 && testArg.configMapsPerPod == 0 {
0000000000000000000000000000000000000000;;					feature = "Performance"
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case 95:
0000000000000000000000000000000000000000;;				feature = "HighDensityPerformance"
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			name := fmt.Sprintf("[Feature:%s] should allow starting %d pods per node using %v with %v secrets, %v configmaps and %v daemons",
0000000000000000000000000000000000000000;;				feature,
0000000000000000000000000000000000000000;;				testArg.podsPerNode,
0000000000000000000000000000000000000000;;				testArg.kind,
0000000000000000000000000000000000000000;;				testArg.secretsPerPod,
0000000000000000000000000000000000000000;;				testArg.configMapsPerPod,
0000000000000000000000000000000000000000;;				testArg.daemonsPerNode,
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;			itArg := testArg
0000000000000000000000000000000000000000;;			It(name, func() {
0000000000000000000000000000000000000000;;				nodePreparer := framework.NewE2ETestNodePreparer(
0000000000000000000000000000000000000000;;					f.ClientSet,
0000000000000000000000000000000000000000;;					[]testutils.CountToStrategy{{Count: nodeCount, Strategy: &testutils.TrivialNodePrepareStrategy{}}},
0000000000000000000000000000000000000000;;				)
0000000000000000000000000000000000000000;;				framework.ExpectNoError(nodePreparer.PrepareNodes())
0000000000000000000000000000000000000000;;				defer nodePreparer.CleanupNodes()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				podsPerNode := itArg.podsPerNode
0000000000000000000000000000000000000000;;				if podsPerNode == 30 {
0000000000000000000000000000000000000000;;					f.AddonResourceConstraints = func() map[string]framework.ResourceConstraint { return density30AddonResourceVerifier(nodeCount) }()
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				totalPods = (podsPerNode - itArg.daemonsPerNode) * nodeCount
0000000000000000000000000000000000000000;;				fileHndl, err := os.Create(fmt.Sprintf(framework.TestContext.OutputDir+"/%s/pod_states.csv", uuid))
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;				defer fileHndl.Close()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// nodeCountPerNamespace and CreateNamespaces are defined in load.go
0000000000000000000000000000000000000000;;				numberOfCollections := (nodeCount + nodeCountPerNamespace - 1) / nodeCountPerNamespace
0000000000000000000000000000000000000000;;				namespaces, err := CreateNamespaces(f, numberOfCollections, fmt.Sprintf("density-%v", testArg.podsPerNode))
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				configs := make([]testutils.RunObjectConfig, numberOfCollections)
0000000000000000000000000000000000000000;;				secretConfigs := make([]*testutils.SecretConfig, 0, numberOfCollections*itArg.secretsPerPod)
0000000000000000000000000000000000000000;;				configMapConfigs := make([]*testutils.ConfigMapConfig, 0, numberOfCollections*itArg.configMapsPerPod)
0000000000000000000000000000000000000000;;				// Since all RCs are created at the same time, timeout for each config
0000000000000000000000000000000000000000;;				// has to assume that it will be run at the very end.
0000000000000000000000000000000000000000;;				podThroughput := 20
0000000000000000000000000000000000000000;;				timeout := time.Duration(totalPods/podThroughput)*time.Second + 3*time.Minute
0000000000000000000000000000000000000000;;				// createClients is defined in load.go
0000000000000000000000000000000000000000;;				clients, internalClients, err := createClients(numberOfCollections)
0000000000000000000000000000000000000000;;				for i := 0; i < numberOfCollections; i++ {
0000000000000000000000000000000000000000;;					nsName := namespaces[i].Name
0000000000000000000000000000000000000000;;					secretNames := []string{}
0000000000000000000000000000000000000000;;					for j := 0; j < itArg.secretsPerPod; j++ {
0000000000000000000000000000000000000000;;						secretName := fmt.Sprintf("density-secret-%v-%v", i, j)
0000000000000000000000000000000000000000;;						secretConfigs = append(secretConfigs, &testutils.SecretConfig{
0000000000000000000000000000000000000000;;							Content:   map[string]string{"foo": "bar"},
0000000000000000000000000000000000000000;;							Client:    clients[i],
0000000000000000000000000000000000000000;;							Name:      secretName,
0000000000000000000000000000000000000000;;							Namespace: nsName,
0000000000000000000000000000000000000000;;							LogFunc:   framework.Logf,
0000000000000000000000000000000000000000;;						})
0000000000000000000000000000000000000000;;						secretNames = append(secretNames, secretName)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					configMapNames := []string{}
0000000000000000000000000000000000000000;;					for j := 0; j < itArg.configMapsPerPod; j++ {
0000000000000000000000000000000000000000;;						configMapName := fmt.Sprintf("density-configmap-%v-%v", i, j)
0000000000000000000000000000000000000000;;						configMapConfigs = append(configMapConfigs, &testutils.ConfigMapConfig{
0000000000000000000000000000000000000000;;							Content:   map[string]string{"foo": "bar"},
0000000000000000000000000000000000000000;;							Client:    clients[i],
0000000000000000000000000000000000000000;;							Name:      configMapName,
0000000000000000000000000000000000000000;;							Namespace: nsName,
0000000000000000000000000000000000000000;;							LogFunc:   framework.Logf,
0000000000000000000000000000000000000000;;						})
0000000000000000000000000000000000000000;;						configMapNames = append(configMapNames, configMapName)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					name := fmt.Sprintf("density%v-%v-%v", totalPods, i, uuid)
0000000000000000000000000000000000000000;;					baseConfig := &testutils.RCConfig{
0000000000000000000000000000000000000000;;						Client:               clients[i],
0000000000000000000000000000000000000000;;						InternalClient:       internalClients[i],
0000000000000000000000000000000000000000;;						Image:                framework.GetPauseImageName(f.ClientSet),
0000000000000000000000000000000000000000;;						Name:                 name,
0000000000000000000000000000000000000000;;						Namespace:            nsName,
0000000000000000000000000000000000000000;;						Labels:               map[string]string{"type": "densityPod"},
0000000000000000000000000000000000000000;;						PollInterval:         DensityPollInterval,
0000000000000000000000000000000000000000;;						Timeout:              timeout,
0000000000000000000000000000000000000000;;						PodStatusFile:        fileHndl,
0000000000000000000000000000000000000000;;						Replicas:             (totalPods + numberOfCollections - 1) / numberOfCollections,
0000000000000000000000000000000000000000;;						CpuRequest:           nodeCpuCapacity / 100,
0000000000000000000000000000000000000000;;						MemRequest:           nodeMemCapacity / 100,
0000000000000000000000000000000000000000;;						MaxContainerFailures: &MaxContainerFailures,
0000000000000000000000000000000000000000;;						Silent:               true,
0000000000000000000000000000000000000000;;						LogFunc:              framework.Logf,
0000000000000000000000000000000000000000;;						SecretNames:          secretNames,
0000000000000000000000000000000000000000;;						ConfigMapNames:       configMapNames,
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					switch itArg.kind {
0000000000000000000000000000000000000000;;					case api.Kind("ReplicationController"):
0000000000000000000000000000000000000000;;						configs[i] = baseConfig
0000000000000000000000000000000000000000;;					case extensions.Kind("ReplicaSet"):
0000000000000000000000000000000000000000;;						configs[i] = &testutils.ReplicaSetConfig{RCConfig: *baseConfig}
0000000000000000000000000000000000000000;;					case extensions.Kind("Deployment"):
0000000000000000000000000000000000000000;;						configs[i] = &testutils.DeploymentConfig{RCConfig: *baseConfig}
0000000000000000000000000000000000000000;;					case batch.Kind("Job"):
0000000000000000000000000000000000000000;;						configs[i] = &testutils.JobConfig{RCConfig: *baseConfig}
0000000000000000000000000000000000000000;;					default:
0000000000000000000000000000000000000000;;						framework.Failf("Unsupported kind: %v", itArg.kind)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Single client is running out of http2 connections in delete phase, hence we need more.
0000000000000000000000000000000000000000;;				clients, internalClients, err = createClients(2)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				dConfig := DensityTestConfig{
0000000000000000000000000000000000000000;;					ClientSets:         clients,
0000000000000000000000000000000000000000;;					InternalClientsets: internalClients,
0000000000000000000000000000000000000000;;					Configs:            configs,
0000000000000000000000000000000000000000;;					PodCount:           totalPods,
0000000000000000000000000000000000000000;;					PollInterval:       DensityPollInterval,
0000000000000000000000000000000000000000;;					kind:               itArg.kind,
0000000000000000000000000000000000000000;;					SecretConfigs:      secretConfigs,
0000000000000000000000000000000000000000;;					ConfigMapConfigs:   configMapConfigs,
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				for i := 0; i < itArg.daemonsPerNode; i++ {
0000000000000000000000000000000000000000;;					dConfig.DaemonConfigs = append(dConfig.DaemonConfigs,
0000000000000000000000000000000000000000;;						&testutils.DaemonConfig{
0000000000000000000000000000000000000000;;							Client:    f.ClientSet,
0000000000000000000000000000000000000000;;							Name:      fmt.Sprintf("density-daemon-%v", i),
0000000000000000000000000000000000000000;;							Namespace: f.Namespace.Name,
0000000000000000000000000000000000000000;;							LogFunc:   framework.Logf,
0000000000000000000000000000000000000000;;						})
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				e2eStartupTime = runDensityTest(dConfig)
0000000000000000000000000000000000000000;;				if itArg.runLatencyTest {
0000000000000000000000000000000000000000;;					By("Scheduling additional Pods to measure startup latencies")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					createTimes := make(map[string]metav1.Time, 0)
0000000000000000000000000000000000000000;;					nodeNames := make(map[string]string, 0)
0000000000000000000000000000000000000000;;					scheduleTimes := make(map[string]metav1.Time, 0)
0000000000000000000000000000000000000000;;					runTimes := make(map[string]metav1.Time, 0)
0000000000000000000000000000000000000000;;					watchTimes := make(map[string]metav1.Time, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					var mutex sync.Mutex
0000000000000000000000000000000000000000;;					checkPod := func(p *v1.Pod) {
0000000000000000000000000000000000000000;;						mutex.Lock()
0000000000000000000000000000000000000000;;						defer mutex.Unlock()
0000000000000000000000000000000000000000;;						defer GinkgoRecover()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						if p.Status.Phase == v1.PodRunning {
0000000000000000000000000000000000000000;;							if _, found := watchTimes[p.Name]; !found {
0000000000000000000000000000000000000000;;								watchTimes[p.Name] = metav1.Now()
0000000000000000000000000000000000000000;;								createTimes[p.Name] = p.CreationTimestamp
0000000000000000000000000000000000000000;;								nodeNames[p.Name] = p.Spec.NodeName
0000000000000000000000000000000000000000;;								var startTime metav1.Time
0000000000000000000000000000000000000000;;								for _, cs := range p.Status.ContainerStatuses {
0000000000000000000000000000000000000000;;									if cs.State.Running != nil {
0000000000000000000000000000000000000000;;										if startTime.Before(cs.State.Running.StartedAt) {
0000000000000000000000000000000000000000;;											startTime = cs.State.Running.StartedAt
0000000000000000000000000000000000000000;;										}
0000000000000000000000000000000000000000;;									}
0000000000000000000000000000000000000000;;								}
0000000000000000000000000000000000000000;;								if startTime != metav1.NewTime(time.Time{}) {
0000000000000000000000000000000000000000;;									runTimes[p.Name] = startTime
0000000000000000000000000000000000000000;;								} else {
0000000000000000000000000000000000000000;;									framework.Failf("Pod %v is reported to be running, but none of its containers is", p.Name)
0000000000000000000000000000000000000000;;								}
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					additionalPodsPrefix = "density-latency-pod"
0000000000000000000000000000000000000000;;					stopCh := make(chan struct{})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					latencyPodStores := make([]cache.Store, len(namespaces))
0000000000000000000000000000000000000000;;					for i := 0; i < len(namespaces); i++ {
0000000000000000000000000000000000000000;;						nsName := namespaces[i].Name
0000000000000000000000000000000000000000;;						latencyPodsStore, controller := cache.NewInformer(
0000000000000000000000000000000000000000;;							&cache.ListWatch{
0000000000000000000000000000000000000000;;								ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
0000000000000000000000000000000000000000;;									options.LabelSelector = labels.SelectorFromSet(labels.Set{"type": additionalPodsPrefix}).String()
0000000000000000000000000000000000000000;;									obj, err := c.Core().Pods(nsName).List(options)
0000000000000000000000000000000000000000;;									return runtime.Object(obj), err
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;								WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
0000000000000000000000000000000000000000;;									options.LabelSelector = labels.SelectorFromSet(labels.Set{"type": additionalPodsPrefix}).String()
0000000000000000000000000000000000000000;;									return c.Core().Pods(nsName).Watch(options)
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;							&v1.Pod{},
0000000000000000000000000000000000000000;;							0,
0000000000000000000000000000000000000000;;							cache.ResourceEventHandlerFuncs{
0000000000000000000000000000000000000000;;								AddFunc: func(obj interface{}) {
0000000000000000000000000000000000000000;;									p, ok := obj.(*v1.Pod)
0000000000000000000000000000000000000000;;									if !ok {
0000000000000000000000000000000000000000;;										framework.Logf("Failed to cast observed object to *v1.Pod.")
0000000000000000000000000000000000000000;;									}
0000000000000000000000000000000000000000;;									Expect(ok).To(Equal(true))
0000000000000000000000000000000000000000;;									go checkPod(p)
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;								UpdateFunc: func(oldObj, newObj interface{}) {
0000000000000000000000000000000000000000;;									p, ok := newObj.(*v1.Pod)
0000000000000000000000000000000000000000;;									if !ok {
0000000000000000000000000000000000000000;;										framework.Logf("Failed to cast observed object to *v1.Pod.")
0000000000000000000000000000000000000000;;									}
0000000000000000000000000000000000000000;;									Expect(ok).To(Equal(true))
0000000000000000000000000000000000000000;;									go checkPod(p)
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						)
0000000000000000000000000000000000000000;;						latencyPodStores[i] = latencyPodsStore
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						go controller.Run(stopCh)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Create some additional pods with throughput ~5 pods/sec.
0000000000000000000000000000000000000000;;					var wg sync.WaitGroup
0000000000000000000000000000000000000000;;					wg.Add(nodeCount)
0000000000000000000000000000000000000000;;					// Explicitly set requests here.
0000000000000000000000000000000000000000;;					// Thanks to it we trigger increasing priority function by scheduling
0000000000000000000000000000000000000000;;					// a pod to a node, which in turn will result in spreading latency pods
0000000000000000000000000000000000000000;;					// more evenly between nodes.
0000000000000000000000000000000000000000;;					cpuRequest := *resource.NewMilliQuantity(nodeCpuCapacity/5, resource.DecimalSI)
0000000000000000000000000000000000000000;;					memRequest := *resource.NewQuantity(nodeMemCapacity/5, resource.DecimalSI)
0000000000000000000000000000000000000000;;					if podsPerNode > 30 {
0000000000000000000000000000000000000000;;						// This is to make them schedulable on high-density tests
0000000000000000000000000000000000000000;;						// (e.g. 100 pods/node kubemark).
0000000000000000000000000000000000000000;;						cpuRequest = *resource.NewMilliQuantity(0, resource.DecimalSI)
0000000000000000000000000000000000000000;;						memRequest = *resource.NewQuantity(0, resource.DecimalSI)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					rcNameToNsMap := map[string]string{}
0000000000000000000000000000000000000000;;					for i := 1; i <= nodeCount; i++ {
0000000000000000000000000000000000000000;;						name := additionalPodsPrefix + "-" + strconv.Itoa(i)
0000000000000000000000000000000000000000;;						nsName := namespaces[i%len(namespaces)].Name
0000000000000000000000000000000000000000;;						rcNameToNsMap[name] = nsName
0000000000000000000000000000000000000000;;						go createRunningPodFromRC(&wg, c, name, nsName, framework.GetPauseImageName(f.ClientSet), additionalPodsPrefix, cpuRequest, memRequest)
0000000000000000000000000000000000000000;;						time.Sleep(200 * time.Millisecond)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					wg.Wait()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By("Waiting for all Pods begin observed by the watch...")
0000000000000000000000000000000000000000;;					waitTimeout := 10 * time.Minute
0000000000000000000000000000000000000000;;					for start := time.Now(); len(watchTimes) < nodeCount; time.Sleep(10 * time.Second) {
0000000000000000000000000000000000000000;;						if time.Since(start) < waitTimeout {
0000000000000000000000000000000000000000;;							framework.Failf("Timeout reached waiting for all Pods being observed by the watch.")
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					close(stopCh)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					nodeToLatencyPods := make(map[string]int)
0000000000000000000000000000000000000000;;					for i := range latencyPodStores {
0000000000000000000000000000000000000000;;						for _, item := range latencyPodStores[i].List() {
0000000000000000000000000000000000000000;;							pod := item.(*v1.Pod)
0000000000000000000000000000000000000000;;							nodeToLatencyPods[pod.Spec.NodeName]++
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						for node, count := range nodeToLatencyPods {
0000000000000000000000000000000000000000;;							if count > 1 {
0000000000000000000000000000000000000000;;								framework.Logf("%d latency pods scheduled on %s", count, node)
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					for i := 0; i < len(namespaces); i++ {
0000000000000000000000000000000000000000;;						nsName := namespaces[i].Name
0000000000000000000000000000000000000000;;						selector := fields.Set{
0000000000000000000000000000000000000000;;							"involvedObject.kind":      "Pod",
0000000000000000000000000000000000000000;;							"involvedObject.namespace": nsName,
0000000000000000000000000000000000000000;;							"source":                   v1.DefaultSchedulerName,
0000000000000000000000000000000000000000;;						}.AsSelector().String()
0000000000000000000000000000000000000000;;						options := metav1.ListOptions{FieldSelector: selector}
0000000000000000000000000000000000000000;;						schedEvents, err := c.Core().Events(nsName).List(options)
0000000000000000000000000000000000000000;;						framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;						for k := range createTimes {
0000000000000000000000000000000000000000;;							for _, event := range schedEvents.Items {
0000000000000000000000000000000000000000;;								if event.InvolvedObject.Name == k {
0000000000000000000000000000000000000000;;									scheduleTimes[k] = event.FirstTimestamp
0000000000000000000000000000000000000000;;									break
0000000000000000000000000000000000000000;;								}
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					scheduleLag := make([]framework.PodLatencyData, 0)
0000000000000000000000000000000000000000;;					startupLag := make([]framework.PodLatencyData, 0)
0000000000000000000000000000000000000000;;					watchLag := make([]framework.PodLatencyData, 0)
0000000000000000000000000000000000000000;;					schedToWatchLag := make([]framework.PodLatencyData, 0)
0000000000000000000000000000000000000000;;					e2eLag := make([]framework.PodLatencyData, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					for name, create := range createTimes {
0000000000000000000000000000000000000000;;						sched, ok := scheduleTimes[name]
0000000000000000000000000000000000000000;;						if !ok {
0000000000000000000000000000000000000000;;							framework.Logf("Failed to find schedule time for %v", name)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						Expect(ok).To(Equal(true))
0000000000000000000000000000000000000000;;						run, ok := runTimes[name]
0000000000000000000000000000000000000000;;						if !ok {
0000000000000000000000000000000000000000;;							framework.Logf("Failed to find run time for %v", name)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						Expect(ok).To(Equal(true))
0000000000000000000000000000000000000000;;						watch, ok := watchTimes[name]
0000000000000000000000000000000000000000;;						if !ok {
0000000000000000000000000000000000000000;;							framework.Logf("Failed to find watch time for %v", name)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						Expect(ok).To(Equal(true))
0000000000000000000000000000000000000000;;						node, ok := nodeNames[name]
0000000000000000000000000000000000000000;;						if !ok {
0000000000000000000000000000000000000000;;							framework.Logf("Failed to find node for %v", name)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						Expect(ok).To(Equal(true))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						scheduleLag = append(scheduleLag, framework.PodLatencyData{Name: name, Node: node, Latency: sched.Time.Sub(create.Time)})
0000000000000000000000000000000000000000;;						startupLag = append(startupLag, framework.PodLatencyData{Name: name, Node: node, Latency: run.Time.Sub(sched.Time)})
0000000000000000000000000000000000000000;;						watchLag = append(watchLag, framework.PodLatencyData{Name: name, Node: node, Latency: watch.Time.Sub(run.Time)})
0000000000000000000000000000000000000000;;						schedToWatchLag = append(schedToWatchLag, framework.PodLatencyData{Name: name, Node: node, Latency: watch.Time.Sub(sched.Time)})
0000000000000000000000000000000000000000;;						e2eLag = append(e2eLag, framework.PodLatencyData{Name: name, Node: node, Latency: watch.Time.Sub(create.Time)})
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					sort.Sort(framework.LatencySlice(scheduleLag))
0000000000000000000000000000000000000000;;					sort.Sort(framework.LatencySlice(startupLag))
0000000000000000000000000000000000000000;;					sort.Sort(framework.LatencySlice(watchLag))
0000000000000000000000000000000000000000;;					sort.Sort(framework.LatencySlice(schedToWatchLag))
0000000000000000000000000000000000000000;;					sort.Sort(framework.LatencySlice(e2eLag))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					framework.PrintLatencies(scheduleLag, "worst schedule latencies")
0000000000000000000000000000000000000000;;					framework.PrintLatencies(startupLag, "worst run-after-schedule latencies")
0000000000000000000000000000000000000000;;					framework.PrintLatencies(watchLag, "worst watch latencies")
0000000000000000000000000000000000000000;;					framework.PrintLatencies(schedToWatchLag, "worst scheduled-to-end total latencies")
0000000000000000000000000000000000000000;;					framework.PrintLatencies(e2eLag, "worst e2e total latencies")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Test whether e2e pod startup time is acceptable.
0000000000000000000000000000000000000000;;					podStartupLatency := &framework.PodStartupLatency{Latency: framework.ExtractLatencyMetrics(e2eLag)}
0000000000000000000000000000000000000000;;					f.TestSummaries = append(f.TestSummaries, podStartupLatency)
0000000000000000000000000000000000000000;;					framework.ExpectNoError(framework.VerifyPodStartupLatency(podStartupLatency))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					framework.LogSuspiciousLatency(startupLag, e2eLag, nodeCount, c)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					By("Removing additional replication controllers")
0000000000000000000000000000000000000000;;					deleteRC := func(i int) {
0000000000000000000000000000000000000000;;						defer GinkgoRecover()
0000000000000000000000000000000000000000;;						name := additionalPodsPrefix + "-" + strconv.Itoa(i+1)
0000000000000000000000000000000000000000;;						framework.ExpectNoError(framework.DeleteRCAndWaitForGC(c, rcNameToNsMap[name], name))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					workqueue.Parallelize(25, nodeCount, deleteRC)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				cleanupDensityTest(dConfig)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func createRunningPodFromRC(wg *sync.WaitGroup, c clientset.Interface, name, ns, image, podType string, cpuRequest, memRequest resource.Quantity) {
0000000000000000000000000000000000000000;;		defer GinkgoRecover()
0000000000000000000000000000000000000000;;		defer wg.Done()
0000000000000000000000000000000000000000;;		labels := map[string]string{
0000000000000000000000000000000000000000;;			"type": podType,
0000000000000000000000000000000000000000;;			"name": name,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rc := &v1.ReplicationController{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:   name,
0000000000000000000000000000000000000000;;				Labels: labels,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.ReplicationControllerSpec{
0000000000000000000000000000000000000000;;				Replicas: func(i int) *int32 { x := int32(i); return &x }(1),
0000000000000000000000000000000000000000;;				Selector: labels,
0000000000000000000000000000000000000000;;				Template: &v1.PodTemplateSpec{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;						Labels: labels,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;						Containers: []v1.Container{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Name:  name,
0000000000000000000000000000000000000000;;								Image: image,
0000000000000000000000000000000000000000;;								Resources: v1.ResourceRequirements{
0000000000000000000000000000000000000000;;									Requests: v1.ResourceList{
0000000000000000000000000000000000000000;;										v1.ResourceCPU:    cpuRequest,
0000000000000000000000000000000000000000;;										v1.ResourceMemory: memRequest,
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						DNSPolicy: v1.DNSDefault,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, err := c.Core().ReplicationControllers(ns).Create(rc)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(framework.WaitForControlledPodsRunning(c, ns, name, api.Kind("ReplicationController")))
0000000000000000000000000000000000000000;;		framework.Logf("Found pod '%s' running", name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func kindSupportsGarbageCollector(kind schema.GroupKind) bool {
0000000000000000000000000000000000000000;;		return kind != extensions.Kind("Deployment") && kind != batch.Kind("Job")
0000000000000000000000000000000000000000;;	}
