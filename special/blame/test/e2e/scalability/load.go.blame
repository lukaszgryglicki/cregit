0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
7ada03d665919c90140a9615cb289da281e57036;test/e2e/load.go[test/e2e/load.go][test/e2e/scalability/load.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package scalability
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"math"
0000000000000000000000000000000000000000;;		"math/rand"
0000000000000000000000000000000000000000;;		"net"
0000000000000000000000000000000000000000;;		"net/http"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime/schema"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/intstr"
0000000000000000000000000000000000000000;;		utilnet "k8s.io/apimachinery/pkg/util/net"
0000000000000000000000000000000000000000;;		restclient "k8s.io/client-go/rest"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/transport"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/workqueue"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/batch"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/extensions"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		smallGroupSize  = 5
0000000000000000000000000000000000000000;;		mediumGroupSize = 30
0000000000000000000000000000000000000000;;		bigGroupSize    = 250
0000000000000000000000000000000000000000;;		smallGroupName  = "load-small"
0000000000000000000000000000000000000000;;		mediumGroupName = "load-medium"
0000000000000000000000000000000000000000;;		bigGroupName    = "load-big"
0000000000000000000000000000000000000000;;		// We start RCs/Services/pods/... in different namespace in this test.
0000000000000000000000000000000000000000;;		// nodeCountPerNamespace determines how many namespaces we will be using
0000000000000000000000000000000000000000;;		// depending on the number of nodes in the underlying cluster.
0000000000000000000000000000000000000000;;		nodeCountPerNamespace = 100
0000000000000000000000000000000000000000;;		// How many threads will be used to create/delete services during this test.
0000000000000000000000000000000000000000;;		serviceOperationsParallelism = 1
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var randomKind = schema.GroupKind{Kind: "Random"}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var knownKinds = []schema.GroupKind{
0000000000000000000000000000000000000000;;		api.Kind("ReplicationController"),
0000000000000000000000000000000000000000;;		extensions.Kind("Deployment"),
0000000000000000000000000000000000000000;;		// TODO: uncomment when Jobs are fixed: #38497
0000000000000000000000000000000000000000;;		//batch.Kind("Job"),
0000000000000000000000000000000000000000;;		extensions.Kind("ReplicaSet"),
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// This test suite can take a long time to run, so by default it is added to
0000000000000000000000000000000000000000;;	// the ginkgo.skip list (see driver.go).
0000000000000000000000000000000000000000;;	// To run this suite you must explicitly ask for it by setting the
0000000000000000000000000000000000000000;;	// -t/--test flag or ginkgo.focus flag.
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Load capacity", func() {
0000000000000000000000000000000000000000;;		var clientset clientset.Interface
0000000000000000000000000000000000000000;;		var nodeCount int
0000000000000000000000000000000000000000;;		var ns string
0000000000000000000000000000000000000000;;		var configs []testutils.RunObjectConfig
0000000000000000000000000000000000000000;;		var secretConfigs []*testutils.SecretConfig
0000000000000000000000000000000000000000;;		var configMapConfigs []*testutils.ConfigMapConfig
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		testCaseBaseName := "load"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Gathers metrics before teardown
0000000000000000000000000000000000000000;;		// TODO add flag that allows to skip cleanup on failure
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			// Verify latency metrics
0000000000000000000000000000000000000000;;			highLatencyRequests, metrics, err := framework.HighLatencyRequests(clientset)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				summaries := make([]framework.TestDataSummary, 0, 1)
0000000000000000000000000000000000000000;;				summaries = append(summaries, metrics)
0000000000000000000000000000000000000000;;				framework.PrintSummaries(summaries, testCaseBaseName)
0000000000000000000000000000000000000000;;				Expect(highLatencyRequests).NotTo(BeNumerically(">", 0), "There should be no high-latency requests")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// We assume a default throughput of 10 pods/second throughput.
0000000000000000000000000000000000000000;;		// We may want to revisit it in the future.
0000000000000000000000000000000000000000;;		// However, this can be overriden by LOAD_TEST_THROUGHPUT env var.
0000000000000000000000000000000000000000;;		throughput := 10
0000000000000000000000000000000000000000;;		if throughputEnv := os.Getenv("LOAD_TEST_THROUGHPUT"); throughputEnv != "" {
0000000000000000000000000000000000000000;;			if newThroughput, err := strconv.Atoi(throughputEnv); err == nil {
0000000000000000000000000000000000000000;;				throughput = newThroughput
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Explicitly put here, to delete namespace at the end of the test
0000000000000000000000000000000000000000;;		// (after measuring latency metrics, etc.).
0000000000000000000000000000000000000000;;		options := framework.FrameworkOptions{
0000000000000000000000000000000000000000;;			ClientQPS:   float32(math.Max(50.0, float64(2*throughput))),
0000000000000000000000000000000000000000;;			ClientBurst: int(math.Max(100.0, float64(4*throughput))),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		f := framework.NewFramework(testCaseBaseName, options, nil)
0000000000000000000000000000000000000000;;		f.NamespaceDeletionTimeout = time.Hour
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			clientset = f.ClientSet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			ns = f.Namespace.Name
0000000000000000000000000000000000000000;;			nodes := framework.GetReadySchedulableNodesOrDie(clientset)
0000000000000000000000000000000000000000;;			nodeCount = len(nodes.Items)
0000000000000000000000000000000000000000;;			Expect(nodeCount).NotTo(BeZero())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Terminating a namespace (deleting the remaining objects from it - which
0000000000000000000000000000000000000000;;			// generally means events) can affect the current run. Thus we wait for all
0000000000000000000000000000000000000000;;			// terminating namespace to be finally deleted before starting this test.
0000000000000000000000000000000000000000;;			err := framework.CheckTestingNSDeletedExcept(clientset, ns)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.ResetMetrics(clientset))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		type Load struct {
0000000000000000000000000000000000000000;;			podsPerNode int
0000000000000000000000000000000000000000;;			image       string
0000000000000000000000000000000000000000;;			command     []string
0000000000000000000000000000000000000000;;			// What kind of resource we want to create
0000000000000000000000000000000000000000;;			kind             schema.GroupKind
0000000000000000000000000000000000000000;;			services         bool
0000000000000000000000000000000000000000;;			secretsPerPod    int
0000000000000000000000000000000000000000;;			configMapsPerPod int
0000000000000000000000000000000000000000;;			daemonsPerNode   int
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		loadTests := []Load{
0000000000000000000000000000000000000000;;			// The container will consume 1 cpu and 512mb of memory.
0000000000000000000000000000000000000000;;			{podsPerNode: 3, image: "jess/stress", command: []string{"stress", "-c", "1", "-m", "2"}, kind: api.Kind("ReplicationController")},
0000000000000000000000000000000000000000;;			{podsPerNode: 30, image: framework.ServeHostnameImage, kind: api.Kind("ReplicationController")},
0000000000000000000000000000000000000000;;			// Tests for other resource types
0000000000000000000000000000000000000000;;			{podsPerNode: 30, image: framework.ServeHostnameImage, kind: extensions.Kind("Deployment")},
0000000000000000000000000000000000000000;;			{podsPerNode: 30, image: framework.ServeHostnameImage, kind: batch.Kind("Job")},
0000000000000000000000000000000000000000;;			// Test scheduling when daemons are preset
0000000000000000000000000000000000000000;;			{podsPerNode: 30, image: framework.ServeHostnameImage, kind: api.Kind("ReplicationController"), daemonsPerNode: 2},
0000000000000000000000000000000000000000;;			// Test with secrets
0000000000000000000000000000000000000000;;			{podsPerNode: 30, image: framework.ServeHostnameImage, kind: extensions.Kind("Deployment"), secretsPerPod: 2},
0000000000000000000000000000000000000000;;			// Test with configmaps
0000000000000000000000000000000000000000;;			{podsPerNode: 30, image: framework.ServeHostnameImage, kind: extensions.Kind("Deployment"), configMapsPerPod: 2},
0000000000000000000000000000000000000000;;			// Special test case which randomizes created resources
0000000000000000000000000000000000000000;;			{podsPerNode: 30, image: framework.ServeHostnameImage, kind: randomKind},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, testArg := range loadTests {
0000000000000000000000000000000000000000;;			feature := "ManualPerformance"
0000000000000000000000000000000000000000;;			if testArg.podsPerNode == 30 && testArg.kind == api.Kind("ReplicationController") && testArg.daemonsPerNode == 0 && testArg.secretsPerPod == 0 && testArg.configMapsPerPod == 0 {
0000000000000000000000000000000000000000;;				feature = "Performance"
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			name := fmt.Sprintf("[Feature:%s] should be able to handle %v pods per node %v with %v secrets, %v configmaps and %v daemons",
0000000000000000000000000000000000000000;;				feature,
0000000000000000000000000000000000000000;;				testArg.podsPerNode,
0000000000000000000000000000000000000000;;				testArg.kind,
0000000000000000000000000000000000000000;;				testArg.secretsPerPod,
0000000000000000000000000000000000000000;;				testArg.configMapsPerPod,
0000000000000000000000000000000000000000;;				testArg.daemonsPerNode,
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;			itArg := testArg
0000000000000000000000000000000000000000;;			itArg.services = os.Getenv("CREATE_SERVICES") != "false"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			It(name, func() {
0000000000000000000000000000000000000000;;				// Create a number of namespaces.
0000000000000000000000000000000000000000;;				namespaceCount := (nodeCount + nodeCountPerNamespace - 1) / nodeCountPerNamespace
0000000000000000000000000000000000000000;;				namespaces, err := CreateNamespaces(f, namespaceCount, fmt.Sprintf("load-%v-nodepods", itArg.podsPerNode))
0000000000000000000000000000000000000000;;				framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				totalPods := (itArg.podsPerNode - itArg.daemonsPerNode) * nodeCount
0000000000000000000000000000000000000000;;				configs, secretConfigs, configMapConfigs = generateConfigs(totalPods, itArg.image, itArg.command, namespaces, itArg.kind, itArg.secretsPerPod, itArg.configMapsPerPod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if itArg.services {
0000000000000000000000000000000000000000;;					framework.Logf("Creating services")
0000000000000000000000000000000000000000;;					services := generateServicesForConfigs(configs)
0000000000000000000000000000000000000000;;					createService := func(i int) {
0000000000000000000000000000000000000000;;						defer GinkgoRecover()
0000000000000000000000000000000000000000;;						_, err := clientset.Core().Services(services[i].Namespace).Create(services[i])
0000000000000000000000000000000000000000;;						framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					workqueue.Parallelize(serviceOperationsParallelism, len(services), createService)
0000000000000000000000000000000000000000;;					framework.Logf("%v Services created.", len(services))
0000000000000000000000000000000000000000;;					defer func(services []*v1.Service) {
0000000000000000000000000000000000000000;;						framework.Logf("Starting to delete services...")
0000000000000000000000000000000000000000;;						deleteService := func(i int) {
0000000000000000000000000000000000000000;;							defer GinkgoRecover()
0000000000000000000000000000000000000000;;							err := clientset.Core().Services(services[i].Namespace).Delete(services[i].Name, nil)
0000000000000000000000000000000000000000;;							framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						workqueue.Parallelize(serviceOperationsParallelism, len(services), deleteService)
0000000000000000000000000000000000000000;;						framework.Logf("Services deleted")
0000000000000000000000000000000000000000;;					}(services)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					framework.Logf("Skipping service creation")
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// Create all secrets.
0000000000000000000000000000000000000000;;				for i := range secretConfigs {
0000000000000000000000000000000000000000;;					secretConfigs[i].Run()
0000000000000000000000000000000000000000;;					defer secretConfigs[i].Stop()
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// Create all configmaps.
0000000000000000000000000000000000000000;;				for i := range configMapConfigs {
0000000000000000000000000000000000000000;;					configMapConfigs[i].Run()
0000000000000000000000000000000000000000;;					defer configMapConfigs[i].Stop()
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// StartDeamon if needed
0000000000000000000000000000000000000000;;				for i := 0; i < itArg.daemonsPerNode; i++ {
0000000000000000000000000000000000000000;;					daemonName := fmt.Sprintf("load-daemon-%v", i)
0000000000000000000000000000000000000000;;					daemonConfig := &testutils.DaemonConfig{
0000000000000000000000000000000000000000;;						Client:    f.ClientSet,
0000000000000000000000000000000000000000;;						Name:      daemonName,
0000000000000000000000000000000000000000;;						Namespace: f.Namespace.Name,
0000000000000000000000000000000000000000;;						LogFunc:   framework.Logf,
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					daemonConfig.Run()
0000000000000000000000000000000000000000;;					defer func(config *testutils.DaemonConfig) {
0000000000000000000000000000000000000000;;						framework.ExpectNoError(framework.DeleteResourceAndPods(
0000000000000000000000000000000000000000;;							f.ClientSet,
0000000000000000000000000000000000000000;;							f.InternalClientset,
0000000000000000000000000000000000000000;;							extensions.Kind("DaemonSet"),
0000000000000000000000000000000000000000;;							config.Namespace,
0000000000000000000000000000000000000000;;							config.Name,
0000000000000000000000000000000000000000;;						))
0000000000000000000000000000000000000000;;					}(daemonConfig)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Simulate lifetime of RC:
0000000000000000000000000000000000000000;;				//  * create with initial size
0000000000000000000000000000000000000000;;				//  * scale RC to a random size and list all pods
0000000000000000000000000000000000000000;;				//  * scale RC to a random size and list all pods
0000000000000000000000000000000000000000;;				//  * delete it
0000000000000000000000000000000000000000;;				//
0000000000000000000000000000000000000000;;				// This will generate ~5 creations/deletions per second assuming:
0000000000000000000000000000000000000000;;				//  - X small RCs each 5 pods   [ 5 * X = totalPods / 2 ]
0000000000000000000000000000000000000000;;				//  - Y medium RCs each 30 pods [ 30 * Y = totalPods / 4 ]
0000000000000000000000000000000000000000;;				//  - Z big RCs each 250 pods   [ 250 * Z = totalPods / 4]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// We would like to spread creating replication controllers over time
0000000000000000000000000000000000000000;;				// to make it possible to create/schedule them in the meantime.
0000000000000000000000000000000000000000;;				// Currently we assume <throughput> pods/second average throughput.
0000000000000000000000000000000000000000;;				// We may want to revisit it in the future.
0000000000000000000000000000000000000000;;				framework.Logf("Starting to create ReplicationControllers...")
0000000000000000000000000000000000000000;;				creatingTime := time.Duration(totalPods/throughput) * time.Second
0000000000000000000000000000000000000000;;				createAllResources(configs, creatingTime)
0000000000000000000000000000000000000000;;				By("============================================================================")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// We would like to spread scaling replication controllers over time
0000000000000000000000000000000000000000;;				// to make it possible to create/schedule & delete them in the meantime.
0000000000000000000000000000000000000000;;				// Currently we assume that <throughput> pods/second average throughput.
0000000000000000000000000000000000000000;;				// The expected number of created/deleted pods is less than totalPods/3.
0000000000000000000000000000000000000000;;				scalingTime := time.Duration(totalPods/(3*throughput)) * time.Second
0000000000000000000000000000000000000000;;				framework.Logf("Starting to scale ReplicationControllers first time...")
0000000000000000000000000000000000000000;;				scaleAllResources(configs, scalingTime)
0000000000000000000000000000000000000000;;				By("============================================================================")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				framework.Logf("Starting to scale ReplicationControllers second time...")
0000000000000000000000000000000000000000;;				scaleAllResources(configs, scalingTime)
0000000000000000000000000000000000000000;;				By("============================================================================")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Cleanup all created replication controllers.
0000000000000000000000000000000000000000;;				// Currently we assume <throughput> pods/second average deletion throughput.
0000000000000000000000000000000000000000;;				// We may want to revisit it in the future.
0000000000000000000000000000000000000000;;				deletingTime := time.Duration(totalPods/throughput) * time.Second
0000000000000000000000000000000000000000;;				framework.Logf("Starting to delete ReplicationControllers...")
0000000000000000000000000000000000000000;;				deleteAllResources(configs, deletingTime)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func createClients(numberOfClients int) ([]clientset.Interface, []internalclientset.Interface, error) {
0000000000000000000000000000000000000000;;		clients := make([]clientset.Interface, numberOfClients)
0000000000000000000000000000000000000000;;		internalClients := make([]internalclientset.Interface, numberOfClients)
0000000000000000000000000000000000000000;;		for i := 0; i < numberOfClients; i++ {
0000000000000000000000000000000000000000;;			config, err := framework.LoadConfig()
0000000000000000000000000000000000000000;;			Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;			config.QPS = 100
0000000000000000000000000000000000000000;;			config.Burst = 200
0000000000000000000000000000000000000000;;			if framework.TestContext.KubeAPIContentType != "" {
0000000000000000000000000000000000000000;;				config.ContentType = framework.TestContext.KubeAPIContentType
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// For the purpose of this test, we want to force that clients
0000000000000000000000000000000000000000;;			// do not share underlying transport (which is a default behavior
0000000000000000000000000000000000000000;;			// in Kubernetes). Thus, we are explicitly creating transport for
0000000000000000000000000000000000000000;;			// each client here.
0000000000000000000000000000000000000000;;			transportConfig, err := config.TransportConfig()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			tlsConfig, err := transport.TLSConfigFor(transportConfig)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			config.Transport = utilnet.SetTransportDefaults(&http.Transport{
0000000000000000000000000000000000000000;;				Proxy:               http.ProxyFromEnvironment,
0000000000000000000000000000000000000000;;				TLSHandshakeTimeout: 10 * time.Second,
0000000000000000000000000000000000000000;;				TLSClientConfig:     tlsConfig,
0000000000000000000000000000000000000000;;				MaxIdleConnsPerHost: 100,
0000000000000000000000000000000000000000;;				Dial: (&net.Dialer{
0000000000000000000000000000000000000000;;					Timeout:   30 * time.Second,
0000000000000000000000000000000000000000;;					KeepAlive: 30 * time.Second,
0000000000000000000000000000000000000000;;				}).Dial,
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			// Overwrite TLS-related fields from config to avoid collision with
0000000000000000000000000000000000000000;;			// Transport field.
0000000000000000000000000000000000000000;;			config.TLSClientConfig = restclient.TLSClientConfig{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			c, err := clientset.NewForConfig(config)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			clients[i] = c
0000000000000000000000000000000000000000;;			internalClient, err := internalclientset.NewForConfig(config)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			internalClients[i] = internalClient
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return clients, internalClients, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func computePodCounts(total int) (int, int, int) {
0000000000000000000000000000000000000000;;		// Small RCs owns ~0.5 of total number of pods, medium and big RCs ~0.25 each.
0000000000000000000000000000000000000000;;		// For example for 3000 pods (100 nodes, 30 pods per node) there are:
0000000000000000000000000000000000000000;;		//  - 300 small RCs each 5 pods
0000000000000000000000000000000000000000;;		//  - 25 medium RCs each 30 pods
0000000000000000000000000000000000000000;;		//  - 3 big RCs each 250 pods
0000000000000000000000000000000000000000;;		bigGroupCount := total / 4 / bigGroupSize
0000000000000000000000000000000000000000;;		total -= bigGroupCount * bigGroupSize
0000000000000000000000000000000000000000;;		mediumGroupCount := total / 3 / mediumGroupSize
0000000000000000000000000000000000000000;;		total -= mediumGroupCount * mediumGroupSize
0000000000000000000000000000000000000000;;		smallGroupCount := total / smallGroupSize
0000000000000000000000000000000000000000;;		return smallGroupCount, mediumGroupCount, bigGroupCount
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func generateConfigs(
0000000000000000000000000000000000000000;;		totalPods int,
0000000000000000000000000000000000000000;;		image string,
0000000000000000000000000000000000000000;;		command []string,
0000000000000000000000000000000000000000;;		nss []*v1.Namespace,
0000000000000000000000000000000000000000;;		kind schema.GroupKind,
0000000000000000000000000000000000000000;;		secretsPerPod int,
0000000000000000000000000000000000000000;;		configMapsPerPod int,
0000000000000000000000000000000000000000;;	) ([]testutils.RunObjectConfig, []*testutils.SecretConfig, []*testutils.ConfigMapConfig) {
0000000000000000000000000000000000000000;;		configs := make([]testutils.RunObjectConfig, 0)
0000000000000000000000000000000000000000;;		secretConfigs := make([]*testutils.SecretConfig, 0)
0000000000000000000000000000000000000000;;		configMapConfigs := make([]*testutils.ConfigMapConfig, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		smallGroupCount, mediumGroupCount, bigGroupCount := computePodCounts(totalPods)
0000000000000000000000000000000000000000;;		newConfigs, newSecretConfigs, newConfigMapConfigs := GenerateConfigsForGroup(nss, smallGroupName, smallGroupSize, smallGroupCount, image, command, kind, secretsPerPod, configMapsPerPod)
0000000000000000000000000000000000000000;;		configs = append(configs, newConfigs...)
0000000000000000000000000000000000000000;;		secretConfigs = append(secretConfigs, newSecretConfigs...)
0000000000000000000000000000000000000000;;		configMapConfigs = append(configMapConfigs, newConfigMapConfigs...)
0000000000000000000000000000000000000000;;		newConfigs, newSecretConfigs, newConfigMapConfigs = GenerateConfigsForGroup(nss, mediumGroupName, mediumGroupSize, mediumGroupCount, image, command, kind, secretsPerPod, configMapsPerPod)
0000000000000000000000000000000000000000;;		configs = append(configs, newConfigs...)
0000000000000000000000000000000000000000;;		secretConfigs = append(secretConfigs, newSecretConfigs...)
0000000000000000000000000000000000000000;;		configMapConfigs = append(configMapConfigs, newConfigMapConfigs...)
0000000000000000000000000000000000000000;;		newConfigs, newSecretConfigs, newConfigMapConfigs = GenerateConfigsForGroup(nss, bigGroupName, bigGroupSize, bigGroupCount, image, command, kind, secretsPerPod, configMapsPerPod)
0000000000000000000000000000000000000000;;		configs = append(configs, newConfigs...)
0000000000000000000000000000000000000000;;		secretConfigs = append(secretConfigs, newSecretConfigs...)
0000000000000000000000000000000000000000;;		configMapConfigs = append(configMapConfigs, newConfigMapConfigs...)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Create a number of clients to better simulate real usecase
0000000000000000000000000000000000000000;;		// where not everyone is using exactly the same client.
0000000000000000000000000000000000000000;;		rcsPerClient := 20
0000000000000000000000000000000000000000;;		clients, internalClients, err := createClients((len(configs) + rcsPerClient - 1) / rcsPerClient)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i := 0; i < len(configs); i++ {
0000000000000000000000000000000000000000;;			configs[i].SetClient(clients[i%len(clients)])
0000000000000000000000000000000000000000;;			configs[i].SetInternalClient(internalClients[i%len(internalClients)])
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := 0; i < len(secretConfigs); i++ {
0000000000000000000000000000000000000000;;			secretConfigs[i].Client = clients[i%len(clients)]
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := 0; i < len(configMapConfigs); i++ {
0000000000000000000000000000000000000000;;			configMapConfigs[i].Client = clients[i%len(clients)]
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return configs, secretConfigs, configMapConfigs
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func GenerateConfigsForGroup(
0000000000000000000000000000000000000000;;		nss []*v1.Namespace,
0000000000000000000000000000000000000000;;		groupName string,
0000000000000000000000000000000000000000;;		size, count int,
0000000000000000000000000000000000000000;;		image string,
0000000000000000000000000000000000000000;;		command []string,
0000000000000000000000000000000000000000;;		kind schema.GroupKind,
0000000000000000000000000000000000000000;;		secretsPerPod int,
0000000000000000000000000000000000000000;;		configMapsPerPod int,
0000000000000000000000000000000000000000;;	) ([]testutils.RunObjectConfig, []*testutils.SecretConfig, []*testutils.ConfigMapConfig) {
0000000000000000000000000000000000000000;;		configs := make([]testutils.RunObjectConfig, 0, count)
0000000000000000000000000000000000000000;;		secretConfigs := make([]*testutils.SecretConfig, 0, count*secretsPerPod)
0000000000000000000000000000000000000000;;		configMapConfigs := make([]*testutils.ConfigMapConfig, 0, count*configMapsPerPod)
0000000000000000000000000000000000000000;;		savedKind := kind
0000000000000000000000000000000000000000;;		for i := 1; i <= count; i++ {
0000000000000000000000000000000000000000;;			kind = savedKind
0000000000000000000000000000000000000000;;			namespace := nss[i%len(nss)].Name
0000000000000000000000000000000000000000;;			secretNames := make([]string, 0, secretsPerPod)
0000000000000000000000000000000000000000;;			configMapNames := make([]string, 0, configMapsPerPod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for j := 0; j < secretsPerPod; j++ {
0000000000000000000000000000000000000000;;				secretName := fmt.Sprintf("%v-%v-secret-%v", groupName, i, j)
0000000000000000000000000000000000000000;;				secretConfigs = append(secretConfigs, &testutils.SecretConfig{
0000000000000000000000000000000000000000;;					Content:   map[string]string{"foo": "bar"},
0000000000000000000000000000000000000000;;					Client:    nil, // this will be overwritten later
0000000000000000000000000000000000000000;;					Name:      secretName,
0000000000000000000000000000000000000000;;					Namespace: namespace,
0000000000000000000000000000000000000000;;					LogFunc:   framework.Logf,
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;				secretNames = append(secretNames, secretName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for j := 0; j < configMapsPerPod; j++ {
0000000000000000000000000000000000000000;;				configMapName := fmt.Sprintf("%v-%v-configmap-%v", groupName, i, j)
0000000000000000000000000000000000000000;;				configMapConfigs = append(configMapConfigs, &testutils.ConfigMapConfig{
0000000000000000000000000000000000000000;;					Content:   map[string]string{"foo": "bar"},
0000000000000000000000000000000000000000;;					Client:    nil, // this will be overwritten later
0000000000000000000000000000000000000000;;					Name:      configMapName,
0000000000000000000000000000000000000000;;					Namespace: namespace,
0000000000000000000000000000000000000000;;					LogFunc:   framework.Logf,
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;				configMapNames = append(configMapNames, configMapName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			baseConfig := &testutils.RCConfig{
0000000000000000000000000000000000000000;;				Client:         nil, // this will be overwritten later
0000000000000000000000000000000000000000;;				InternalClient: nil, // this will be overwritten later
0000000000000000000000000000000000000000;;				Name:           groupName + "-" + strconv.Itoa(i),
0000000000000000000000000000000000000000;;				Namespace:      namespace,
0000000000000000000000000000000000000000;;				Timeout:        10 * time.Minute,
0000000000000000000000000000000000000000;;				Image:          image,
0000000000000000000000000000000000000000;;				Command:        command,
0000000000000000000000000000000000000000;;				Replicas:       size,
0000000000000000000000000000000000000000;;				CpuRequest:     10,       // 0.01 core
0000000000000000000000000000000000000000;;				MemRequest:     26214400, // 25MB
0000000000000000000000000000000000000000;;				SecretNames:    secretNames,
0000000000000000000000000000000000000000;;				ConfigMapNames: configMapNames,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if kind == randomKind {
0000000000000000000000000000000000000000;;				kind = knownKinds[rand.Int()%len(knownKinds)]
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			var config testutils.RunObjectConfig
0000000000000000000000000000000000000000;;			switch kind {
0000000000000000000000000000000000000000;;			case api.Kind("ReplicationController"):
0000000000000000000000000000000000000000;;				config = baseConfig
0000000000000000000000000000000000000000;;			case extensions.Kind("ReplicaSet"):
0000000000000000000000000000000000000000;;				config = &testutils.ReplicaSetConfig{RCConfig: *baseConfig}
0000000000000000000000000000000000000000;;			case extensions.Kind("Deployment"):
0000000000000000000000000000000000000000;;				config = &testutils.DeploymentConfig{RCConfig: *baseConfig}
0000000000000000000000000000000000000000;;			case batch.Kind("Job"):
0000000000000000000000000000000000000000;;				config = &testutils.JobConfig{RCConfig: *baseConfig}
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				framework.Failf("Unsupported kind for config creation: %v", kind)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			configs = append(configs, config)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return configs, secretConfigs, configMapConfigs
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func generateServicesForConfigs(configs []testutils.RunObjectConfig) []*v1.Service {
0000000000000000000000000000000000000000;;		services := make([]*v1.Service, 0, len(configs))
0000000000000000000000000000000000000000;;		for _, config := range configs {
0000000000000000000000000000000000000000;;			serviceName := config.GetName() + "-svc"
0000000000000000000000000000000000000000;;			labels := map[string]string{"name": config.GetName()}
0000000000000000000000000000000000000000;;			service := &v1.Service{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					Name:      serviceName,
0000000000000000000000000000000000000000;;					Namespace: config.GetNamespace(),
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Spec: v1.ServiceSpec{
0000000000000000000000000000000000000000;;					Selector: labels,
0000000000000000000000000000000000000000;;					Ports: []v1.ServicePort{{
0000000000000000000000000000000000000000;;						Port:       80,
0000000000000000000000000000000000000000;;						TargetPort: intstr.FromInt(80),
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			services = append(services, service)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return services
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func sleepUpTo(d time.Duration) {
0000000000000000000000000000000000000000;;		time.Sleep(time.Duration(rand.Int63n(d.Nanoseconds())))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func createAllResources(configs []testutils.RunObjectConfig, creatingTime time.Duration) {
0000000000000000000000000000000000000000;;		var wg sync.WaitGroup
0000000000000000000000000000000000000000;;		wg.Add(len(configs))
0000000000000000000000000000000000000000;;		for _, config := range configs {
0000000000000000000000000000000000000000;;			go createResource(&wg, config, creatingTime)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		wg.Wait()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func createResource(wg *sync.WaitGroup, config testutils.RunObjectConfig, creatingTime time.Duration) {
0000000000000000000000000000000000000000;;		defer GinkgoRecover()
0000000000000000000000000000000000000000;;		defer wg.Done()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sleepUpTo(creatingTime)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(config.Run(), fmt.Sprintf("creating %v %s", config.GetKind(), config.GetName()))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func scaleAllResources(configs []testutils.RunObjectConfig, scalingTime time.Duration) {
0000000000000000000000000000000000000000;;		var wg sync.WaitGroup
0000000000000000000000000000000000000000;;		wg.Add(len(configs))
0000000000000000000000000000000000000000;;		for _, config := range configs {
0000000000000000000000000000000000000000;;			go scaleResource(&wg, config, scalingTime)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		wg.Wait()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Scales RC to a random size within [0.5*size, 1.5*size] and lists all the pods afterwards.
0000000000000000000000000000000000000000;;	// Scaling happens always based on original size, not the current size.
0000000000000000000000000000000000000000;;	func scaleResource(wg *sync.WaitGroup, config testutils.RunObjectConfig, scalingTime time.Duration) {
0000000000000000000000000000000000000000;;		defer GinkgoRecover()
0000000000000000000000000000000000000000;;		defer wg.Done()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sleepUpTo(scalingTime)
0000000000000000000000000000000000000000;;		newSize := uint(rand.Intn(config.GetReplicas()) + config.GetReplicas()/2)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(framework.ScaleResource(
0000000000000000000000000000000000000000;;			config.GetClient(), config.GetInternalClient(), config.GetNamespace(), config.GetName(), newSize, true, config.GetKind()),
0000000000000000000000000000000000000000;;			fmt.Sprintf("scaling rc %s for the first time", config.GetName()))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		selector := labels.SelectorFromSet(labels.Set(map[string]string{"name": config.GetName()}))
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{
0000000000000000000000000000000000000000;;			LabelSelector:   selector.String(),
0000000000000000000000000000000000000000;;			ResourceVersion: "0",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, err := config.GetClient().Core().Pods(config.GetNamespace()).List(options)
0000000000000000000000000000000000000000;;		framework.ExpectNoError(err, fmt.Sprintf("listing pods from rc %v", config.GetName()))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func deleteAllResources(configs []testutils.RunObjectConfig, deletingTime time.Duration) {
0000000000000000000000000000000000000000;;		var wg sync.WaitGroup
0000000000000000000000000000000000000000;;		wg.Add(len(configs))
0000000000000000000000000000000000000000;;		for _, config := range configs {
0000000000000000000000000000000000000000;;			go deleteResource(&wg, config, deletingTime)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		wg.Wait()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func deleteResource(wg *sync.WaitGroup, config testutils.RunObjectConfig, deletingTime time.Duration) {
0000000000000000000000000000000000000000;;		defer GinkgoRecover()
0000000000000000000000000000000000000000;;		defer wg.Done()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sleepUpTo(deletingTime)
0000000000000000000000000000000000000000;;		if framework.TestContext.GarbageCollectorEnabled && config.GetKind() != extensions.Kind("Deployment") {
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.DeleteResourceAndWaitForGC(
0000000000000000000000000000000000000000;;				config.GetClient(), config.GetKind(), config.GetNamespace(), config.GetName()),
0000000000000000000000000000000000000000;;				fmt.Sprintf("deleting %v %s", config.GetKind(), config.GetName()))
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			framework.ExpectNoError(framework.DeleteResourceAndPods(
0000000000000000000000000000000000000000;;				config.GetClient(), config.GetInternalClient(), config.GetKind(), config.GetNamespace(), config.GetName()),
0000000000000000000000000000000000000000;;				fmt.Sprintf("deleting %v %s", config.GetKind(), config.GetName()))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func CreateNamespaces(f *framework.Framework, namespaceCount int, namePrefix string) ([]*v1.Namespace, error) {
0000000000000000000000000000000000000000;;		namespaces := []*v1.Namespace{}
0000000000000000000000000000000000000000;;		for i := 1; i <= namespaceCount; i++ {
0000000000000000000000000000000000000000;;			namespace, err := f.CreateNamespace(fmt.Sprintf("%v-%d", namePrefix, i), nil)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return []*v1.Namespace{}, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			namespaces = append(namespaces, namespace)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return namespaces, nil
0000000000000000000000000000000000000000;;	}
