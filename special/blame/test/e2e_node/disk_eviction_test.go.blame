0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
f3710a425f009628a35acf246e377c0731e8f54e;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e_node
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// podCheckInterval is the interval seconds between pod status checks.
0000000000000000000000000000000000000000;;		podCheckInterval = time.Second * 2
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// containerGCPeriod is the period of container garbage collect loop. It should be the same
0000000000000000000000000000000000000000;;		// with ContainerGCPeriod in kubelet.go. However we don't want to include kubelet package
0000000000000000000000000000000000000000;;		// directly which will introduce a lot more dependencies.
0000000000000000000000000000000000000000;;		containerGCPeriod = time.Minute * 1
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dummyFile = "dummy."
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO: Leverage dynamic Kubelet settings when it's implemented to only modify the kubelet eviction option in this test.
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Kubelet Eviction Manager [Serial] [Disruptive]", func() {
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("kubelet-eviction-manager")
0000000000000000000000000000000000000000;;		var podClient *framework.PodClient
0000000000000000000000000000000000000000;;		var c clientset.Interface
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			podClient = f.PodClient()
0000000000000000000000000000000000000000;;			c = f.ClientSet
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Describe("hard eviction test", func() {
0000000000000000000000000000000000000000;;			Context("pod using the most disk space gets evicted when the node disk usage is above the eviction hard threshold", func() {
0000000000000000000000000000000000000000;;				var busyPodName, idlePodName, verifyPodName string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				BeforeEach(func() {
0000000000000000000000000000000000000000;;					if !isImageSupported() {
0000000000000000000000000000000000000000;;						framework.Skipf("test skipped because the image is not supported by the test")
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if !evictionOptionIsSet() {
0000000000000000000000000000000000000000;;						framework.Skipf("test skipped because eviction option is not set")
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					busyPodName = "to-evict" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;					idlePodName = "idle" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;					verifyPodName = "verify" + string(uuid.NewUUID())
0000000000000000000000000000000000000000;;					createIdlePod(idlePodName, podClient)
0000000000000000000000000000000000000000;;					podClient.Create(&v1.Pod{
0000000000000000000000000000000000000000;;						ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;							Name: busyPodName,
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;							RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;							Containers: []v1.Container{
0000000000000000000000000000000000000000;;								{
0000000000000000000000000000000000000000;;									Image: "gcr.io/google_containers/busybox:1.24",
0000000000000000000000000000000000000000;;									Name:  busyPodName,
0000000000000000000000000000000000000000;;									// Filling the disk
0000000000000000000000000000000000000000;;									Command: []string{"sh", "-c",
0000000000000000000000000000000000000000;;										fmt.Sprintf("for NUM in `seq 1 1 100000`; do dd if=/dev/urandom of=%s.$NUM bs=50000000 count=10; sleep 0.5; done",
0000000000000000000000000000000000000000;;											dummyFile)},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				AfterEach(func() {
0000000000000000000000000000000000000000;;					if !isImageSupported() || !evictionOptionIsSet() { // Skip the after each
0000000000000000000000000000000000000000;;						return
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					podClient.DeleteSync(busyPodName, &metav1.DeleteOptions{}, framework.DefaultPodDeletionTimeout)
0000000000000000000000000000000000000000;;					podClient.DeleteSync(idlePodName, &metav1.DeleteOptions{}, framework.DefaultPodDeletionTimeout)
0000000000000000000000000000000000000000;;					podClient.DeleteSync(verifyPodName, &metav1.DeleteOptions{}, framework.DefaultPodDeletionTimeout)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Wait for 2 container gc loop to ensure that the containers are deleted. The containers
0000000000000000000000000000000000000000;;					// created in this test consume a lot of disk, we don't want them to trigger disk eviction
0000000000000000000000000000000000000000;;					// again after the test.
0000000000000000000000000000000000000000;;					time.Sleep(containerGCPeriod * 2)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					if framework.TestContext.PrepullImages {
0000000000000000000000000000000000000000;;						// The disk eviction test may cause the prepulled images to be evicted,
0000000000000000000000000000000000000000;;						// prepull those images again to ensure this test not affect following tests.
0000000000000000000000000000000000000000;;						PrePullAllImages()
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				It("should evict the pod using the most disk space [Slow]", func() {
0000000000000000000000000000000000000000;;					evictionOccurred := false
0000000000000000000000000000000000000000;;					nodeDiskPressureCondition := false
0000000000000000000000000000000000000000;;					podRescheduleable := false
0000000000000000000000000000000000000000;;					Eventually(func() error {
0000000000000000000000000000000000000000;;						// Avoid the test using up all the disk space
0000000000000000000000000000000000000000;;						err := checkDiskUsage(0.05)
0000000000000000000000000000000000000000;;						if err != nil {
0000000000000000000000000000000000000000;;							return err
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// The pod should be evicted.
0000000000000000000000000000000000000000;;						if !evictionOccurred {
0000000000000000000000000000000000000000;;							podData, err := podClient.Get(busyPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;							if err != nil {
0000000000000000000000000000000000000000;;								return err
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							err = verifyPodEviction(podData)
0000000000000000000000000000000000000000;;							if err != nil {
0000000000000000000000000000000000000000;;								return err
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							podData, err = podClient.Get(idlePodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;							if err != nil {
0000000000000000000000000000000000000000;;								return err
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							if podData.Status.Phase != v1.PodRunning {
0000000000000000000000000000000000000000;;								err = verifyPodEviction(podData)
0000000000000000000000000000000000000000;;								if err != nil {
0000000000000000000000000000000000000000;;									return err
0000000000000000000000000000000000000000;;								}
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							evictionOccurred = true
0000000000000000000000000000000000000000;;							return fmt.Errorf("waiting for node disk pressure condition to be set")
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// The node should have disk pressure condition after the pods are evicted.
0000000000000000000000000000000000000000;;						if !nodeDiskPressureCondition {
0000000000000000000000000000000000000000;;							if !nodeHasDiskPressure(f.ClientSet) {
0000000000000000000000000000000000000000;;								return fmt.Errorf("expected disk pressure condition is not set")
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							nodeDiskPressureCondition = true
0000000000000000000000000000000000000000;;							return fmt.Errorf("waiting for node disk pressure condition to be cleared")
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// After eviction happens the pod is evicted so eventually the node disk pressure should be relieved.
0000000000000000000000000000000000000000;;						if !podRescheduleable {
0000000000000000000000000000000000000000;;							if nodeHasDiskPressure(f.ClientSet) {
0000000000000000000000000000000000000000;;								return fmt.Errorf("expected disk pressure condition relief has not happened")
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							createIdlePod(verifyPodName, podClient)
0000000000000000000000000000000000000000;;							podRescheduleable = true
0000000000000000000000000000000000000000;;							return fmt.Errorf("waiting for the node to accept a new pod")
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// The new pod should be able to be scheduled and run after the disk pressure is relieved.
0000000000000000000000000000000000000000;;						podData, err := podClient.Get(verifyPodName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;						if err != nil {
0000000000000000000000000000000000000000;;							return err
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						if podData.Status.Phase != v1.PodRunning {
0000000000000000000000000000000000000000;;							return fmt.Errorf("waiting for the new pod to be running")
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						return nil
0000000000000000000000000000000000000000;;					}, time.Minute*15 /* based on n1-standard-1 machine type */, podCheckInterval).Should(BeNil())
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func createIdlePod(podName string, podClient *framework.PodClient) {
0000000000000000000000000000000000000000;;		podClient.Create(&v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name: podName,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Image: framework.GetPauseImageNameForHostArch(),
0000000000000000000000000000000000000000;;						Name:  podName,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func verifyPodEviction(podData *v1.Pod) error {
0000000000000000000000000000000000000000;;		if podData.Status.Phase != v1.PodFailed {
0000000000000000000000000000000000000000;;			return fmt.Errorf("expected phase to be failed. got %+v", podData.Status.Phase)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if podData.Status.Reason != "Evicted" {
0000000000000000000000000000000000000000;;			return fmt.Errorf("expected failed reason to be evicted. got %+v", podData.Status.Reason)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func nodeHasDiskPressure(cs clientset.Interface) bool {
0000000000000000000000000000000000000000;;		nodeList := framework.GetReadySchedulableNodesOrDie(cs)
0000000000000000000000000000000000000000;;		for _, condition := range nodeList.Items[0].Status.Conditions {
0000000000000000000000000000000000000000;;			if condition.Type == v1.NodeDiskPressure {
0000000000000000000000000000000000000000;;				return condition.Status == v1.ConditionTrue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func evictionOptionIsSet() bool {
0000000000000000000000000000000000000000;;		return len(framework.TestContext.KubeletConfig.EvictionHard) > 0
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO(random-liu): Use OSImage in node status to do the check.
0000000000000000000000000000000000000000;;	func isImageSupported() bool {
0000000000000000000000000000000000000000;;		// TODO: Only images with image fs is selected for testing for now. When the kubelet settings can be dynamically updated,
0000000000000000000000000000000000000000;;		// instead of skipping images the eviction thresholds should be adjusted based on the images.
0000000000000000000000000000000000000000;;		return strings.Contains(framework.TestContext.NodeName, "-gci-dev-")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// checkDiskUsage verifies that the available bytes on disk are above the limit.
0000000000000000000000000000000000000000;;	func checkDiskUsage(limit float64) error {
0000000000000000000000000000000000000000;;		summary, err := getNodeSummary()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if nodeFs := summary.Node.Fs; nodeFs != nil {
0000000000000000000000000000000000000000;;			if nodeFs.AvailableBytes != nil && nodeFs.CapacityBytes != nil {
0000000000000000000000000000000000000000;;				if float64(*nodeFs.CapacityBytes)*limit > float64(*nodeFs.AvailableBytes) {
0000000000000000000000000000000000000000;;					return fmt.Errorf("available nodefs byte is less than %v%%", limit*float64(100))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if summary.Node.Runtime != nil {
0000000000000000000000000000000000000000;;			if imageFs := summary.Node.Runtime.ImageFs; imageFs != nil {
0000000000000000000000000000000000000000;;				if float64(*imageFs.CapacityBytes)*limit > float64(*imageFs.AvailableBytes) {
0000000000000000000000000000000000000000;;					return fmt.Errorf("available imagefs byte is less than %v%%", limit*float64(100))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
