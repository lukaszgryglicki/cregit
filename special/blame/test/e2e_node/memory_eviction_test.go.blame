0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
18bbb33af0187ec9f6dc5139227077fa0364c285;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e_node
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		nodeutil "k8s.io/kubernetes/pkg/api/v1/node"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/componentconfig"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Eviction Policy is described here:
0000000000000000000000000000000000000000;;	// https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/kubelet-eviction.md
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("MemoryEviction [Slow] [Serial] [Disruptive]", func() {
0000000000000000000000000000000000000000;;		const (
0000000000000000000000000000000000000000;;			evictionHard = "memory.available<40%"
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("eviction-test")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This is a dummy context to wrap the outer AfterEach, which will run after the inner AfterEach.
0000000000000000000000000000000000000000;;		// We want to list all of the node and pod events, including any that occur while waiting for
0000000000000000000000000000000000000000;;		// memory pressure reduction, even if we time out while waiting.
0000000000000000000000000000000000000000;;		Context("", func() {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			AfterEach(func() {
0000000000000000000000000000000000000000;;				// Print events
0000000000000000000000000000000000000000;;				logNodeEvents(f)
0000000000000000000000000000000000000000;;				logPodEvents(f)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			Context("", func() {
0000000000000000000000000000000000000000;;				tempSetCurrentKubeletConfig(f, func(c *componentconfig.KubeletConfiguration) {
0000000000000000000000000000000000000000;;					c.EvictionHard = evictionHard
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				Context("when there is memory pressure", func() {
0000000000000000000000000000000000000000;;					AfterEach(func() {
0000000000000000000000000000000000000000;;						// Wait for the memory pressure condition to disappear from the node status before continuing.
0000000000000000000000000000000000000000;;						By("waiting for the memory pressure condition on the node to disappear before ending the test.")
0000000000000000000000000000000000000000;;						Eventually(func() error {
0000000000000000000000000000000000000000;;							nodeList, err := f.ClientSet.Core().Nodes().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;							if err != nil {
0000000000000000000000000000000000000000;;								return fmt.Errorf("tried to get node list but got error: %v", err)
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							// Assuming that there is only one node, because this is a node e2e test.
0000000000000000000000000000000000000000;;							if len(nodeList.Items) != 1 {
0000000000000000000000000000000000000000;;								return fmt.Errorf("expected 1 node, but see %d. List: %v", len(nodeList.Items), nodeList.Items)
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							node := nodeList.Items[0]
0000000000000000000000000000000000000000;;							_, pressure := nodeutil.GetNodeCondition(&node.Status, v1.NodeMemoryPressure)
0000000000000000000000000000000000000000;;							if pressure != nil && pressure.Status == v1.ConditionTrue {
0000000000000000000000000000000000000000;;								return fmt.Errorf("node is still reporting memory pressure condition: %s", pressure)
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							return nil
0000000000000000000000000000000000000000;;						}, 5*time.Minute, 15*time.Second).Should(BeNil())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// Check available memory after condition disappears, just in case:
0000000000000000000000000000000000000000;;						// Wait for available memory to decrease to a reasonable level before ending the test.
0000000000000000000000000000000000000000;;						// This helps prevent interference with tests that start immediately after this one.
0000000000000000000000000000000000000000;;						By("waiting for available memory to decrease to a reasonable level before ending the test.")
0000000000000000000000000000000000000000;;						Eventually(func() error {
0000000000000000000000000000000000000000;;							summary, err := getNodeSummary()
0000000000000000000000000000000000000000;;							if err != nil {
0000000000000000000000000000000000000000;;								return err
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							if summary.Node.Memory.AvailableBytes == nil {
0000000000000000000000000000000000000000;;								return fmt.Errorf("summary.Node.Memory.AvailableBytes was nil, cannot get memory stats.")
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							if summary.Node.Memory.WorkingSetBytes == nil {
0000000000000000000000000000000000000000;;								return fmt.Errorf("summary.Node.Memory.WorkingSetBytes was nil, cannot get memory stats.")
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							avail := *summary.Node.Memory.AvailableBytes
0000000000000000000000000000000000000000;;							wset := *summary.Node.Memory.WorkingSetBytes
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							// memory limit = avail + wset
0000000000000000000000000000000000000000;;							limit := avail + wset
0000000000000000000000000000000000000000;;							halflimit := limit / 2
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							// Wait for at least half of memory limit to be available
0000000000000000000000000000000000000000;;							if avail >= halflimit {
0000000000000000000000000000000000000000;;								return nil
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							return fmt.Errorf("current available memory is: %d bytes. Expected at least %d bytes available.", avail, halflimit)
0000000000000000000000000000000000000000;;						}, 5*time.Minute, 15*time.Second).Should(BeNil())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// TODO(mtaufen): 5 minute wait to stop flaky test bleeding while we figure out what is actually going on.
0000000000000000000000000000000000000000;;						//                If related to pressure transition period in eviction manager, probably only need to wait
0000000000000000000000000000000000000000;;						//                just over 30s becasue that is the transition period set for node e2e tests. But since we
0000000000000000000000000000000000000000;;						//                know 5 min works and we don't know if transition period is the problem, wait 5 min for now.
0000000000000000000000000000000000000000;;						time.Sleep(5 * time.Minute)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// Finally, try starting a new pod and wait for it to be scheduled and running.
0000000000000000000000000000000000000000;;						// This is the final check to try to prevent interference with subsequent tests.
0000000000000000000000000000000000000000;;						podName := "admit-best-effort-pod"
0000000000000000000000000000000000000000;;						f.PodClient().CreateSync(&v1.Pod{
0000000000000000000000000000000000000000;;							ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;								Name: podName,
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;							Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;								RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;								Containers: []v1.Container{
0000000000000000000000000000000000000000;;									{
0000000000000000000000000000000000000000;;										Image: framework.GetPauseImageNameForHostArch(),
0000000000000000000000000000000000000000;;										Name:  podName,
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						})
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					It("should evict pods in the correct order (besteffort first, then burstable, then guaranteed)", func() {
0000000000000000000000000000000000000000;;						By("creating a guaranteed pod, a burstable pod, and a besteffort pod.")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// A pod is guaranteed only when requests and limits are specified for all the containers and they are equal.
0000000000000000000000000000000000000000;;						guaranteed := getMemhogPod("guaranteed-pod", "guaranteed", v1.ResourceRequirements{
0000000000000000000000000000000000000000;;							Requests: v1.ResourceList{
0000000000000000000000000000000000000000;;								"cpu":    resource.MustParse("100m"),
0000000000000000000000000000000000000000;;								"memory": resource.MustParse("100Mi"),
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;							Limits: v1.ResourceList{
0000000000000000000000000000000000000000;;								"cpu":    resource.MustParse("100m"),
0000000000000000000000000000000000000000;;								"memory": resource.MustParse("100Mi"),
0000000000000000000000000000000000000000;;							}})
0000000000000000000000000000000000000000;;						guaranteed = f.PodClient().CreateSync(guaranteed)
0000000000000000000000000000000000000000;;						glog.Infof("pod created with name: %s", guaranteed.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// A pod is burstable if limits and requests do not match across all containers.
0000000000000000000000000000000000000000;;						burstable := getMemhogPod("burstable-pod", "burstable", v1.ResourceRequirements{
0000000000000000000000000000000000000000;;							Requests: v1.ResourceList{
0000000000000000000000000000000000000000;;								"cpu":    resource.MustParse("100m"),
0000000000000000000000000000000000000000;;								"memory": resource.MustParse("100Mi"),
0000000000000000000000000000000000000000;;							}})
0000000000000000000000000000000000000000;;						burstable = f.PodClient().CreateSync(burstable)
0000000000000000000000000000000000000000;;						glog.Infof("pod created with name: %s", burstable.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// A pod is besteffort if none of its containers have specified any requests or limits	.
0000000000000000000000000000000000000000;;						besteffort := getMemhogPod("besteffort-pod", "besteffort", v1.ResourceRequirements{})
0000000000000000000000000000000000000000;;						besteffort = f.PodClient().CreateSync(besteffort)
0000000000000000000000000000000000000000;;						glog.Infof("pod created with name: %s", besteffort.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						// We poll until timeout or all pods are killed.
0000000000000000000000000000000000000000;;						// Inside the func, we check that all pods are in a valid phase with
0000000000000000000000000000000000000000;;						// respect to the eviction order of best effort, then burstable, then guaranteed.
0000000000000000000000000000000000000000;;						By("polling the Status.Phase of each pod and checking for violations of the eviction order.")
0000000000000000000000000000000000000000;;						Eventually(func() error {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							gteed, gtErr := f.ClientSet.Core().Pods(f.Namespace.Name).Get(guaranteed.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;							framework.ExpectNoError(gtErr, fmt.Sprintf("getting pod %s", guaranteed.Name))
0000000000000000000000000000000000000000;;							gteedPh := gteed.Status.Phase
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							burst, buErr := f.ClientSet.Core().Pods(f.Namespace.Name).Get(burstable.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;							framework.ExpectNoError(buErr, fmt.Sprintf("getting pod %s", burstable.Name))
0000000000000000000000000000000000000000;;							burstPh := burst.Status.Phase
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							best, beErr := f.ClientSet.Core().Pods(f.Namespace.Name).Get(besteffort.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;							framework.ExpectNoError(beErr, fmt.Sprintf("getting pod %s", besteffort.Name))
0000000000000000000000000000000000000000;;							bestPh := best.Status.Phase
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							glog.Infof("pod phase: guaranteed: %v, burstable: %v, besteffort: %v", gteedPh, burstPh, bestPh)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							// NOTE/TODO(mtaufen): This should help us debug why burstable appears to fail before besteffort in some
0000000000000000000000000000000000000000;;							//                     scenarios. We have seen some evidence that the eviction manager has in fact done the
0000000000000000000000000000000000000000;;							//                     right thing and evicted the besteffort first, and attempted to change the besteffort phase
0000000000000000000000000000000000000000;;							//                     to "Failed" when it evicts it, but that for some reason the test isn't seeing the updated
0000000000000000000000000000000000000000;;							//                     phase. I'm trying to confirm or deny this.
0000000000000000000000000000000000000000;;							//                     The eviction manager starts trying to evict things when the node comes under memory
0000000000000000000000000000000000000000;;							//                     pressure, and the eviction manager reports this information in the pressure condition. If we
0000000000000000000000000000000000000000;;							//                     see the eviction manager reporting a pressure condition for a while without the besteffort failing,
0000000000000000000000000000000000000000;;							//                     and we see that the manager did in fact evict the besteffort (this should be in the Kubelet log), we
0000000000000000000000000000000000000000;;							//                     will have more reason to believe the phase is out of date.
0000000000000000000000000000000000000000;;							nodeList, err := f.ClientSet.Core().Nodes().List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;							if err != nil {
0000000000000000000000000000000000000000;;								glog.Errorf("tried to get node list but got error: %v", err)
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							if len(nodeList.Items) != 1 {
0000000000000000000000000000000000000000;;								glog.Errorf("expected 1 node, but see %d. List: %v", len(nodeList.Items), nodeList.Items)
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							node := nodeList.Items[0]
0000000000000000000000000000000000000000;;							_, pressure := nodeutil.GetNodeCondition(&node.Status, v1.NodeMemoryPressure)
0000000000000000000000000000000000000000;;							glog.Infof("node pressure condition: %s", pressure)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							// NOTE/TODO(mtaufen): Also log (at least temporarily) the actual memory consumption on the node.
0000000000000000000000000000000000000000;;							//                     I used this to plot memory usage from a successful test run and it looks the
0000000000000000000000000000000000000000;;							//                     way I would expect. I want to see what the plot from a flake looks like.
0000000000000000000000000000000000000000;;							summary, err := getNodeSummary()
0000000000000000000000000000000000000000;;							if err != nil {
0000000000000000000000000000000000000000;;								return err
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							if summary.Node.Memory.WorkingSetBytes != nil {
0000000000000000000000000000000000000000;;								wset := *summary.Node.Memory.WorkingSetBytes
0000000000000000000000000000000000000000;;								glog.Infof("Node's working set is (bytes): %v", wset)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							if bestPh == v1.PodRunning {
0000000000000000000000000000000000000000;;								Expect(burstPh).NotTo(Equal(v1.PodFailed), "burstable pod failed before best effort pod")
0000000000000000000000000000000000000000;;								Expect(gteedPh).NotTo(Equal(v1.PodFailed), "guaranteed pod failed before best effort pod")
0000000000000000000000000000000000000000;;							} else if burstPh == v1.PodRunning {
0000000000000000000000000000000000000000;;								Expect(gteedPh).NotTo(Equal(v1.PodFailed), "guaranteed pod failed before burstable pod")
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;							// When both besteffort and burstable have been evicted, the test has completed.
0000000000000000000000000000000000000000;;							if bestPh == v1.PodFailed && burstPh == v1.PodFailed {
0000000000000000000000000000000000000000;;								return nil
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							return fmt.Errorf("besteffort and burstable have not yet both been evicted.")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;						}, 60*time.Minute, 5*time.Second).Should(BeNil())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getMemhogPod(podName string, ctnName string, res v1.ResourceRequirements) *v1.Pod {
0000000000000000000000000000000000000000;;		env := []v1.EnvVar{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				Name: "MEMORY_LIMIT",
0000000000000000000000000000000000000000;;				ValueFrom: &v1.EnvVarSource{
0000000000000000000000000000000000000000;;					ResourceFieldRef: &v1.ResourceFieldSelector{
0000000000000000000000000000000000000000;;						Resource: "limits.memory",
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If there is a limit specified, pass 80% of it for -mem-total, otherwise use the downward API
0000000000000000000000000000000000000000;;		// to pass limits.memory, which will be the total memory available.
0000000000000000000000000000000000000000;;		// This helps prevent a guaranteed pod from triggering an OOM kill due to it's low memory limit,
0000000000000000000000000000000000000000;;		// which will cause the test to fail inappropriately.
0000000000000000000000000000000000000000;;		var memLimit string
0000000000000000000000000000000000000000;;		if limit, ok := res.Limits["memory"]; ok {
0000000000000000000000000000000000000000;;			memLimit = strconv.Itoa(int(
0000000000000000000000000000000000000000;;				float64(limit.Value()) * 0.8))
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			memLimit = "$(MEMORY_LIMIT)"
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name: podName,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:            ctnName,
0000000000000000000000000000000000000000;;						Image:           "gcr.io/google-containers/stress:v1",
0000000000000000000000000000000000000000;;						ImagePullPolicy: "Always",
0000000000000000000000000000000000000000;;						Env:             env,
0000000000000000000000000000000000000000;;						// 60 min timeout * 60s / tick per 10s = 360 ticks before timeout => ~11.11Mi/tick
0000000000000000000000000000000000000000;;						// to fill ~4Gi of memory, so initial ballpark 12Mi/tick.
0000000000000000000000000000000000000000;;						// We might see flakes due to timeout if the total memory on the nodes increases.
0000000000000000000000000000000000000000;;						Args:      []string{"-mem-alloc-size", "12Mi", "-mem-alloc-sleep", "10s", "-mem-total", memLimit},
0000000000000000000000000000000000000000;;						Resources: res,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
