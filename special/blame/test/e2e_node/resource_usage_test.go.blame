0000000000000000000000000000000000000000;;	// +build linux
9ab5ddcae6f7d20694160d5532a9ed99ddc9c1b6;;	
0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e_node
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		stats "k8s.io/kubernetes/pkg/kubelet/apis/stats/v1alpha1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("Resource-usage [Serial] [Slow]", func() {
0000000000000000000000000000000000000000;;		const (
0000000000000000000000000000000000000000;;			// Interval to poll /stats/container on a node
0000000000000000000000000000000000000000;;			containerStatsPollingPeriod = 10 * time.Second
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var (
0000000000000000000000000000000000000000;;			rc *ResourceCollector
0000000000000000000000000000000000000000;;			om *framework.RuntimeOperationMonitor
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("resource-usage")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			om = framework.NewRuntimeOperationMonitor(f.ClientSet)
0000000000000000000000000000000000000000;;			// The test collects resource usage from a standalone Cadvisor pod.
0000000000000000000000000000000000000000;;			// The Cadvsior of Kubelet has a housekeeping interval of 10s, which is too long to
0000000000000000000000000000000000000000;;			// show the resource usage spikes. But changing its interval increases the overhead
0000000000000000000000000000000000000000;;			// of kubelet. Hence we use a Cadvisor pod.
0000000000000000000000000000000000000000;;			f.PodClient().CreateSync(getCadvisorPod())
0000000000000000000000000000000000000000;;			rc = NewResourceCollector(containerStatsPollingPeriod)
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			result := om.GetLatestRuntimeOperationErrorRate()
0000000000000000000000000000000000000000;;			framework.Logf("runtime operation error metrics:\n%s", framework.FormatRuntimeOperationErrorRate(result))
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This test measures and verifies the steady resource usage of node is within limit
0000000000000000000000000000000000000000;;		// It collects data from a standalone Cadvisor with housekeeping interval 1s.
0000000000000000000000000000000000000000;;		// It verifies CPU percentiles and the lastest memory usage.
0000000000000000000000000000000000000000;;		Context("regular resource usage tracking", func() {
0000000000000000000000000000000000000000;;			rTests := []resourceTest{
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					podsNr: 10,
0000000000000000000000000000000000000000;;					cpuLimits: framework.ContainersCPUSummary{
0000000000000000000000000000000000000000;;						stats.SystemContainerKubelet: {0.50: 0.30, 0.95: 0.35},
0000000000000000000000000000000000000000;;						stats.SystemContainerRuntime: {0.50: 0.30, 0.95: 0.40},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					memLimits: framework.ResourceUsagePerContainer{
0000000000000000000000000000000000000000;;						stats.SystemContainerKubelet: &framework.ContainerResourceUsage{MemoryRSSInBytes: 200 * 1024 * 1024},
0000000000000000000000000000000000000000;;						stats.SystemContainerRuntime: &framework.ContainerResourceUsage{MemoryRSSInBytes: 400 * 1024 * 1024},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, testArg := range rTests {
0000000000000000000000000000000000000000;;				itArg := testArg
0000000000000000000000000000000000000000;;				desc := fmt.Sprintf("resource tracking for %d pods per node", itArg.podsNr)
0000000000000000000000000000000000000000;;				It(desc, func() {
0000000000000000000000000000000000000000;;					testInfo := getTestNodeInfo(f, itArg.getTestName(), desc)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					runResourceUsageTest(f, rc, itArg)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Log and verify resource usage
0000000000000000000000000000000000000000;;					logAndVerifyResource(f, rc, itArg.cpuLimits, itArg.memLimits, testInfo, true)
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Context("regular resource usage tracking", func() {
0000000000000000000000000000000000000000;;			rTests := []resourceTest{
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					podsNr: 0,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					podsNr: 10,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					podsNr: 35,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					podsNr: 105,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, testArg := range rTests {
0000000000000000000000000000000000000000;;				itArg := testArg
0000000000000000000000000000000000000000;;				desc := fmt.Sprintf("resource tracking for %d pods per node [Benchmark]", itArg.podsNr)
0000000000000000000000000000000000000000;;				It(desc, func() {
0000000000000000000000000000000000000000;;					testInfo := getTestNodeInfo(f, itArg.getTestName(), desc)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					runResourceUsageTest(f, rc, itArg)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Log and verify resource usage
0000000000000000000000000000000000000000;;					logAndVerifyResource(f, rc, itArg.cpuLimits, itArg.memLimits, testInfo, false)
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type resourceTest struct {
0000000000000000000000000000000000000000;;		podsNr    int
0000000000000000000000000000000000000000;;		cpuLimits framework.ContainersCPUSummary
0000000000000000000000000000000000000000;;		memLimits framework.ResourceUsagePerContainer
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (rt *resourceTest) getTestName() string {
0000000000000000000000000000000000000000;;		return fmt.Sprintf("resource_%d", rt.podsNr)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// runResourceUsageTest runs the resource usage test
0000000000000000000000000000000000000000;;	func runResourceUsageTest(f *framework.Framework, rc *ResourceCollector, testArg resourceTest) {
0000000000000000000000000000000000000000;;		const (
0000000000000000000000000000000000000000;;			// The monitoring time for one test
0000000000000000000000000000000000000000;;			monitoringTime = 10 * time.Minute
0000000000000000000000000000000000000000;;			// The periodic reporting period
0000000000000000000000000000000000000000;;			reportingPeriod = 5 * time.Minute
0000000000000000000000000000000000000000;;			// sleep for an interval here to measure steady data
0000000000000000000000000000000000000000;;			sleepAfterCreatePods = 10 * time.Second
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		pods := newTestPods(testArg.podsNr, true, framework.GetPauseImageNameForHostArch(), "test_pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rc.Start()
0000000000000000000000000000000000000000;;		// Explicitly delete pods to prevent namespace controller cleanning up timeout
0000000000000000000000000000000000000000;;		defer deletePodsSync(f, append(pods, getCadvisorPod()))
0000000000000000000000000000000000000000;;		defer rc.Stop()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Creating a batch of Pods")
0000000000000000000000000000000000000000;;		f.PodClient().CreateBatch(pods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// wait for a while to let the node be steady
0000000000000000000000000000000000000000;;		time.Sleep(sleepAfterCreatePods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Log once and flush the stats.
0000000000000000000000000000000000000000;;		rc.LogLatest()
0000000000000000000000000000000000000000;;		rc.Reset()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Start monitoring resource usage")
0000000000000000000000000000000000000000;;		// Periodically dump the cpu summary until the deadline is met.
0000000000000000000000000000000000000000;;		// Note that without calling framework.ResourceMonitor.Reset(), the stats
0000000000000000000000000000000000000000;;		// would occupy increasingly more memory. This should be fine
0000000000000000000000000000000000000000;;		// for the current test duration, but we should reclaim the
0000000000000000000000000000000000000000;;		// entries if we plan to monitor longer (e.g., 8 hours).
0000000000000000000000000000000000000000;;		deadline := time.Now().Add(monitoringTime)
0000000000000000000000000000000000000000;;		for time.Now().Before(deadline) {
0000000000000000000000000000000000000000;;			timeLeft := deadline.Sub(time.Now())
0000000000000000000000000000000000000000;;			framework.Logf("Still running...%v left", timeLeft)
0000000000000000000000000000000000000000;;			if timeLeft < reportingPeriod {
0000000000000000000000000000000000000000;;				time.Sleep(timeLeft)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				time.Sleep(reportingPeriod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			logPods(f.ClientSet)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		By("Reporting overall resource usage")
0000000000000000000000000000000000000000;;		logPods(f.ClientSet)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// logAndVerifyResource prints the resource usage as perf data and verifies whether resource usage satisfies the limit.
0000000000000000000000000000000000000000;;	func logAndVerifyResource(f *framework.Framework, rc *ResourceCollector, cpuLimits framework.ContainersCPUSummary,
0000000000000000000000000000000000000000;;		memLimits framework.ResourceUsagePerContainer, testInfo map[string]string, isVerify bool) {
0000000000000000000000000000000000000000;;		nodeName := framework.TestContext.NodeName
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Obtain memory PerfData
0000000000000000000000000000000000000000;;		usagePerContainer, err := rc.GetLatest()
0000000000000000000000000000000000000000;;		Expect(err).NotTo(HaveOccurred())
0000000000000000000000000000000000000000;;		framework.Logf("%s", formatResourceUsageStats(usagePerContainer))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		usagePerNode := make(framework.ResourceUsagePerNode)
0000000000000000000000000000000000000000;;		usagePerNode[nodeName] = usagePerContainer
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Obtain CPU PerfData
0000000000000000000000000000000000000000;;		cpuSummary := rc.GetCPUSummary()
0000000000000000000000000000000000000000;;		framework.Logf("%s", formatCPUSummary(cpuSummary))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cpuSummaryPerNode := make(framework.NodesCPUSummary)
0000000000000000000000000000000000000000;;		cpuSummaryPerNode[nodeName] = cpuSummary
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Print resource usage
0000000000000000000000000000000000000000;;		logPerfData(framework.ResourceUsageToPerfDataWithLabels(usagePerNode, testInfo), "memory")
0000000000000000000000000000000000000000;;		logPerfData(framework.CPUUsageToPerfDataWithLabels(cpuSummaryPerNode, testInfo), "cpu")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Verify resource usage
0000000000000000000000000000000000000000;;		if isVerify {
0000000000000000000000000000000000000000;;			verifyMemoryLimits(f.ClientSet, memLimits, usagePerNode)
0000000000000000000000000000000000000000;;			verifyCPULimits(cpuLimits, cpuSummaryPerNode)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func verifyMemoryLimits(c clientset.Interface, expected framework.ResourceUsagePerContainer, actual framework.ResourceUsagePerNode) {
0000000000000000000000000000000000000000;;		if expected == nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var errList []string
0000000000000000000000000000000000000000;;		for nodeName, nodeSummary := range actual {
0000000000000000000000000000000000000000;;			var nodeErrs []string
0000000000000000000000000000000000000000;;			for cName, expectedResult := range expected {
0000000000000000000000000000000000000000;;				container, ok := nodeSummary[cName]
0000000000000000000000000000000000000000;;				if !ok {
0000000000000000000000000000000000000000;;					nodeErrs = append(nodeErrs, fmt.Sprintf("container %q: missing", cName))
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				expectedValue := expectedResult.MemoryRSSInBytes
0000000000000000000000000000000000000000;;				actualValue := container.MemoryRSSInBytes
0000000000000000000000000000000000000000;;				if expectedValue != 0 && actualValue > expectedValue {
0000000000000000000000000000000000000000;;					nodeErrs = append(nodeErrs, fmt.Sprintf("container %q: expected RSS memory (MB) < %d; got %d",
0000000000000000000000000000000000000000;;						cName, expectedValue, actualValue))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(nodeErrs) > 0 {
0000000000000000000000000000000000000000;;				errList = append(errList, fmt.Sprintf("node %v:\n %s", nodeName, strings.Join(nodeErrs, ", ")))
0000000000000000000000000000000000000000;;				heapStats, err := framework.GetKubeletHeapStats(c, nodeName)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					framework.Logf("Unable to get heap stats from %q", nodeName)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					framework.Logf("Heap stats on %q\n:%v", nodeName, heapStats)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(errList) > 0 {
0000000000000000000000000000000000000000;;			framework.Failf("Memory usage exceeding limits:\n %s", strings.Join(errList, "\n"))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func verifyCPULimits(expected framework.ContainersCPUSummary, actual framework.NodesCPUSummary) {
0000000000000000000000000000000000000000;;		if expected == nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var errList []string
0000000000000000000000000000000000000000;;		for nodeName, perNodeSummary := range actual {
0000000000000000000000000000000000000000;;			var nodeErrs []string
0000000000000000000000000000000000000000;;			for cName, expectedResult := range expected {
0000000000000000000000000000000000000000;;				perContainerSummary, ok := perNodeSummary[cName]
0000000000000000000000000000000000000000;;				if !ok {
0000000000000000000000000000000000000000;;					nodeErrs = append(nodeErrs, fmt.Sprintf("container %q: missing", cName))
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for p, expectedValue := range expectedResult {
0000000000000000000000000000000000000000;;					actualValue, ok := perContainerSummary[p]
0000000000000000000000000000000000000000;;					if !ok {
0000000000000000000000000000000000000000;;						nodeErrs = append(nodeErrs, fmt.Sprintf("container %q: missing percentile %v", cName, p))
0000000000000000000000000000000000000000;;						continue
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if actualValue > expectedValue {
0000000000000000000000000000000000000000;;						nodeErrs = append(nodeErrs, fmt.Sprintf("container %q: expected %.0fth%% usage < %.3f; got %.3f",
0000000000000000000000000000000000000000;;							cName, p*100, expectedValue, actualValue))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(nodeErrs) > 0 {
0000000000000000000000000000000000000000;;				errList = append(errList, fmt.Sprintf("node %v:\n %s", nodeName, strings.Join(nodeErrs, ", ")))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(errList) > 0 {
0000000000000000000000000000000000000000;;			framework.Failf("CPU usage exceeding limits:\n %s", strings.Join(errList, "\n"))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func logPods(c clientset.Interface) {
0000000000000000000000000000000000000000;;		nodeName := framework.TestContext.NodeName
0000000000000000000000000000000000000000;;		podList, err := framework.GetKubeletRunningPods(c, nodeName)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			framework.Logf("Unable to retrieve kubelet pods for node %v", nodeName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		framework.Logf("%d pods are running on node %v", len(podList.Items), nodeName)
0000000000000000000000000000000000000000;;	}
