0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
ef7c000b87e1cca42eea77cdfc9fe5de8c379993;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package e2e_node
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"path/filepath"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		nodeutil "k8s.io/kubernetes/pkg/api/v1/node"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/componentconfig"
0000000000000000000000000000000000000000;;		kubeletmetrics "k8s.io/kubernetes/pkg/kubelet/metrics"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		. "github.com/onsi/ginkgo"
0000000000000000000000000000000000000000;;		. "github.com/onsi/gomega"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Eviction Policy is described here:
0000000000000000000000000000000000000000;;	// https://github.com/kubernetes/kubernetes/blob/master/docs/proposals/kubelet-eviction.md
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		postTestConditionMonitoringPeriod = 2 * time.Minute
0000000000000000000000000000000000000000;;		evictionPollInterval              = 2 * time.Second
0000000000000000000000000000000000000000;;		// pressure conditions often surface after evictions because of delay in propegation of metrics to pressure
0000000000000000000000000000000000000000;;		// we wait this period after evictions to make sure that we wait out this delay
0000000000000000000000000000000000000000;;		pressureDelay = 20 * time.Second
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ = framework.KubeDescribe("InodeEviction [Slow] [Serial] [Disruptive] [Flaky]", func() {
0000000000000000000000000000000000000000;;		f := framework.NewDefaultFramework("inode-eviction-test")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		volumeMountPath := "/test-empty-dir-mnt"
0000000000000000000000000000000000000000;;		podTestSpecs := []podTestSpec{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				evictionPriority: 1, // This pod should be evicted before the normal memory usage pod
0000000000000000000000000000000000000000;;				pod: &v1.Pod{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{Name: "container-inode-hog-pod"},
0000000000000000000000000000000000000000;;					Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;						RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;						Containers: []v1.Container{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Image:   "gcr.io/google_containers/busybox:1.24",
0000000000000000000000000000000000000000;;								Name:    "container-inode-hog-container",
0000000000000000000000000000000000000000;;								Command: getInodeConsumingCommand(""),
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				evictionPriority: 1, // This pod should be evicted before the normal memory usage pod
0000000000000000000000000000000000000000;;				pod: &v1.Pod{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{Name: "volume-inode-hog-pod"},
0000000000000000000000000000000000000000;;					Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;						RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;						Containers: []v1.Container{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Image:   "gcr.io/google_containers/busybox:1.24",
0000000000000000000000000000000000000000;;								Name:    "volume-inode-hog-container",
0000000000000000000000000000000000000000;;								Command: getInodeConsumingCommand(volumeMountPath),
0000000000000000000000000000000000000000;;								VolumeMounts: []v1.VolumeMount{
0000000000000000000000000000000000000000;;									{MountPath: volumeMountPath, Name: "test-empty-dir"},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						Volumes: []v1.Volume{
0000000000000000000000000000000000000000;;							{Name: "test-empty-dir", VolumeSource: v1.VolumeSource{EmptyDir: &v1.EmptyDirVolumeSource{}}},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				evictionPriority: 0, // This pod should never be evicted
0000000000000000000000000000000000000000;;				pod:              getInnocentPod(),
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		evictionTestTimeout := 30 * time.Minute
0000000000000000000000000000000000000000;;		testCondition := "Disk Pressure due to Inodes"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		Context(fmt.Sprintf("when we run containers that should cause %s", testCondition), func() {
0000000000000000000000000000000000000000;;			tempSetCurrentKubeletConfig(f, func(initialConfig *componentconfig.KubeletConfiguration) {
0000000000000000000000000000000000000000;;				initialConfig.EvictionHard = "nodefs.inodesFree<70%"
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			// Place the remainder of the test within a context so that the kubelet config is set before and after the test.
0000000000000000000000000000000000000000;;			Context("With kubeconfig updated", func() {
0000000000000000000000000000000000000000;;				runEvictionTest(f, testCondition, podTestSpecs, evictionTestTimeout, hasInodePressure)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Struct used by runEvictionTest that specifies the pod, and when that pod should be evicted, relative to other pods
0000000000000000000000000000000000000000;;	type podTestSpec struct {
0000000000000000000000000000000000000000;;		// 0 should never be evicted, 1 shouldn't evict before 2, etc.
0000000000000000000000000000000000000000;;		// If two are ranked at 1, either is permitted to fail before the other.
0000000000000000000000000000000000000000;;		// The test ends when all other than the 0 have been evicted
0000000000000000000000000000000000000000;;		evictionPriority int
0000000000000000000000000000000000000000;;		pod              *v1.Pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// runEvictionTest sets up a testing environment given the provided nodes, and checks a few things:
0000000000000000000000000000000000000000;;	//		It ensures that the desired testCondition is actually triggered.
0000000000000000000000000000000000000000;;	//		It ensures that evictionPriority 0 pods are not evicted
0000000000000000000000000000000000000000;;	//		It ensures that lower evictionPriority pods are always evicted before higher evictionPriority pods (2 evicted before 1, etc.)
0000000000000000000000000000000000000000;;	//		It ensures that all lower evictionPriority pods are eventually evicted.
0000000000000000000000000000000000000000;;	// runEvictionTest then cleans up the testing environment by deleting provided nodes, and ensures that testCondition no longer exists
0000000000000000000000000000000000000000;;	func runEvictionTest(f *framework.Framework, testCondition string, podTestSpecs []podTestSpec, evictionTestTimeout time.Duration,
0000000000000000000000000000000000000000;;		hasPressureCondition func(*framework.Framework, string) (bool, error)) {
0000000000000000000000000000000000000000;;		BeforeEach(func() {
0000000000000000000000000000000000000000;;			By("seting up pods to be used by tests")
0000000000000000000000000000000000000000;;			for _, spec := range podTestSpecs {
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("creating pod with container: %s", spec.pod.Name))
0000000000000000000000000000000000000000;;				f.PodClient().CreateSync(spec.pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		It(fmt.Sprintf("should eventually see %s, and then evict all of the correct pods", testCondition), func() {
0000000000000000000000000000000000000000;;			configEnabled, err := isKubeletConfigEnabled(f)
0000000000000000000000000000000000000000;;			framework.ExpectNoError(err)
0000000000000000000000000000000000000000;;			if !configEnabled {
0000000000000000000000000000000000000000;;				framework.Skipf("Dynamic kubelet config must be enabled for this test to run.")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			Eventually(func() error {
0000000000000000000000000000000000000000;;				hasPressure, err := hasPressureCondition(f, testCondition)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if hasPressure {
0000000000000000000000000000000000000000;;					return nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return fmt.Errorf("Condition: %s not encountered", testCondition)
0000000000000000000000000000000000000000;;			}, evictionTestTimeout, evictionPollInterval).Should(BeNil())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			Eventually(func() error {
0000000000000000000000000000000000000000;;				// Gather current information
0000000000000000000000000000000000000000;;				updatedPodList, err := f.ClientSet.Core().Pods(f.Namespace.Name).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;				updatedPods := updatedPodList.Items
0000000000000000000000000000000000000000;;				for _, p := range updatedPods {
0000000000000000000000000000000000000000;;					framework.Logf("fetching pod %s; phase= %v", p.Name, p.Status.Phase)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				logKubeletMetrics(kubeletmetrics.EvictionStatsAgeKey)
0000000000000000000000000000000000000000;;				_, err = hasPressureCondition(f, testCondition)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				By("checking eviction ordering and ensuring important pods dont fail")
0000000000000000000000000000000000000000;;				done := true
0000000000000000000000000000000000000000;;				for _, priorityPodSpec := range podTestSpecs {
0000000000000000000000000000000000000000;;					var priorityPod v1.Pod
0000000000000000000000000000000000000000;;					for _, p := range updatedPods {
0000000000000000000000000000000000000000;;						if p.Name == priorityPodSpec.pod.Name {
0000000000000000000000000000000000000000;;							priorityPod = p
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					Expect(priorityPod).NotTo(BeNil())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Check eviction ordering.
0000000000000000000000000000000000000000;;					// Note: it is alright for a priority 1 and priority 2 pod (for example) to fail in the same round
0000000000000000000000000000000000000000;;					for _, lowPriorityPodSpec := range podTestSpecs {
0000000000000000000000000000000000000000;;						var lowPriorityPod v1.Pod
0000000000000000000000000000000000000000;;						for _, p := range updatedPods {
0000000000000000000000000000000000000000;;							if p.Name == lowPriorityPodSpec.pod.Name {
0000000000000000000000000000000000000000;;								lowPriorityPod = p
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						Expect(lowPriorityPod).NotTo(BeNil())
0000000000000000000000000000000000000000;;						if priorityPodSpec.evictionPriority < lowPriorityPodSpec.evictionPriority && lowPriorityPod.Status.Phase == v1.PodRunning {
0000000000000000000000000000000000000000;;							Expect(priorityPod.Status.Phase).NotTo(Equal(v1.PodFailed),
0000000000000000000000000000000000000000;;								fmt.Sprintf("%s pod failed before %s pod", priorityPodSpec.pod.Name, lowPriorityPodSpec.pod.Name))
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// EvictionPriority 0 pods should not fail
0000000000000000000000000000000000000000;;					if priorityPodSpec.evictionPriority == 0 {
0000000000000000000000000000000000000000;;						Expect(priorityPod.Status.Phase).NotTo(Equal(v1.PodFailed),
0000000000000000000000000000000000000000;;							fmt.Sprintf("%s pod failed (and shouldn't have failed)", priorityPod.Name))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// If a pod that is not evictionPriority 0 has not been evicted, we are not done
0000000000000000000000000000000000000000;;					if priorityPodSpec.evictionPriority != 0 && priorityPod.Status.Phase != v1.PodFailed {
0000000000000000000000000000000000000000;;						done = false
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if done {
0000000000000000000000000000000000000000;;					return nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return fmt.Errorf("pods that caused %s have not been evicted.", testCondition)
0000000000000000000000000000000000000000;;			}, evictionTestTimeout, evictionPollInterval).Should(BeNil())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// We observe pressure from the API server.  The eviction manager observes pressure from the kubelet internal stats.
0000000000000000000000000000000000000000;;			// This means the eviction manager will observe pressure before we will, creating a delay between when the eviction manager
0000000000000000000000000000000000000000;;			// evicts a pod, and when we observe the pressure by querrying the API server.  Add a delay here to account for this delay
0000000000000000000000000000000000000000;;			By("making sure pressure from test has surfaced before continuing")
0000000000000000000000000000000000000000;;			time.Sleep(pressureDelay)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("making sure conditions eventually return to normal")
0000000000000000000000000000000000000000;;			Eventually(func() error {
0000000000000000000000000000000000000000;;				hasPressure, err := hasPressureCondition(f, testCondition)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if hasPressure {
0000000000000000000000000000000000000000;;					return fmt.Errorf("Conditions havent returned to normal, we still have %s", testCondition)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}, evictionTestTimeout, evictionPollInterval).Should(BeNil())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("making sure conditions do not return, and that pods that shouldnt fail dont fail")
0000000000000000000000000000000000000000;;			Consistently(func() error {
0000000000000000000000000000000000000000;;				hasPressure, err := hasPressureCondition(f, testCondition)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					// Race conditions sometimes occur when checking pressure condition due to #38710 (Docker bug)
0000000000000000000000000000000000000000;;					// Do not fail the test when this occurs, since this is expected to happen occasionally.
0000000000000000000000000000000000000000;;					framework.Logf("Failed to check pressure condition. Error: %v", err)
0000000000000000000000000000000000000000;;					return nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if hasPressure {
0000000000000000000000000000000000000000;;					return fmt.Errorf("%s dissappeared and then reappeared", testCondition)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// Gather current information
0000000000000000000000000000000000000000;;				updatedPodList, _ := f.ClientSet.Core().Pods(f.Namespace.Name).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;				for _, priorityPodSpec := range podTestSpecs {
0000000000000000000000000000000000000000;;					// EvictionPriority 0 pods should not fail
0000000000000000000000000000000000000000;;					if priorityPodSpec.evictionPriority == 0 {
0000000000000000000000000000000000000000;;						for _, p := range updatedPodList.Items {
0000000000000000000000000000000000000000;;							if p.Name == priorityPodSpec.pod.Name && p.Status.Phase == v1.PodFailed {
0000000000000000000000000000000000000000;;								logKubeletMetrics(kubeletmetrics.EvictionStatsAgeKey)
0000000000000000000000000000000000000000;;								return fmt.Errorf("%s pod failed (delayed) and shouldn't have failed", p.Name)
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}, postTestConditionMonitoringPeriod, evictionPollInterval).Should(BeNil())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			By("making sure we can start a new pod after the test")
0000000000000000000000000000000000000000;;			podName := "test-admit-pod"
0000000000000000000000000000000000000000;;			f.PodClient().CreateSync(&v1.Pod{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					Name: podName,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;					RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;					Containers: []v1.Container{
0000000000000000000000000000000000000000;;						{
0000000000000000000000000000000000000000;;							Image: framework.GetPauseImageNameForHostArch(),
0000000000000000000000000000000000000000;;							Name:  podName,
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		AfterEach(func() {
0000000000000000000000000000000000000000;;			By("deleting pods")
0000000000000000000000000000000000000000;;			for _, spec := range podTestSpecs {
0000000000000000000000000000000000000000;;				By(fmt.Sprintf("deleting pod: %s", spec.pod.Name))
0000000000000000000000000000000000000000;;				f.PodClient().DeleteSync(spec.pod.Name, &metav1.DeleteOptions{}, framework.DefaultPodDeletionTimeout)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if CurrentGinkgoTestDescription().Failed {
0000000000000000000000000000000000000000;;				if framework.TestContext.DumpLogsOnFailure {
0000000000000000000000000000000000000000;;					logPodEvents(f)
0000000000000000000000000000000000000000;;					logNodeEvents(f)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				By("sleeping to allow for cleanup of test")
0000000000000000000000000000000000000000;;				time.Sleep(postTestConditionMonitoringPeriod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Returns TRUE if the node has disk pressure due to inodes exists on the node, FALSE otherwise
0000000000000000000000000000000000000000;;	func hasInodePressure(f *framework.Framework, testCondition string) (bool, error) {
0000000000000000000000000000000000000000;;		localNodeStatus := getLocalNode(f).Status
0000000000000000000000000000000000000000;;		_, pressure := nodeutil.GetNodeCondition(&localNodeStatus, v1.NodeDiskPressure)
0000000000000000000000000000000000000000;;		Expect(pressure).NotTo(BeNil())
0000000000000000000000000000000000000000;;		hasPressure := pressure.Status == v1.ConditionTrue
0000000000000000000000000000000000000000;;		By(fmt.Sprintf("checking if pod has %s: %v", testCondition, hasPressure))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Additional Logging relating to Inodes
0000000000000000000000000000000000000000;;		summary, err := getNodeSummary()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if summary.Node.Runtime != nil && summary.Node.Runtime.ImageFs != nil && summary.Node.Runtime.ImageFs.Inodes != nil && summary.Node.Runtime.ImageFs.InodesFree != nil {
0000000000000000000000000000000000000000;;			framework.Logf("imageFsInfo.Inodes: %d, imageFsInfo.InodesFree: %d", *summary.Node.Runtime.ImageFs.Inodes, *summary.Node.Runtime.ImageFs.InodesFree)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if summary.Node.Fs != nil && summary.Node.Fs.Inodes != nil && summary.Node.Fs.InodesFree != nil {
0000000000000000000000000000000000000000;;			framework.Logf("rootFsInfo.Inodes: %d, rootFsInfo.InodesFree: %d", *summary.Node.Fs.Inodes, *summary.Node.Fs.InodesFree)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pod := range summary.Pods {
0000000000000000000000000000000000000000;;			framework.Logf("Pod: %s", pod.PodRef.Name)
0000000000000000000000000000000000000000;;			for _, container := range pod.Containers {
0000000000000000000000000000000000000000;;				if container.Rootfs != nil && container.Rootfs.InodesUsed != nil {
0000000000000000000000000000000000000000;;					framework.Logf("--- summary Container: %s inodeUsage: %d", container.Name, *container.Rootfs.InodesUsed)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, volume := range pod.VolumeStats {
0000000000000000000000000000000000000000;;				if volume.FsStats.InodesUsed != nil {
0000000000000000000000000000000000000000;;					framework.Logf("--- summary Volume: %s inodeUsage: %d", volume.Name, *volume.FsStats.InodesUsed)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return hasPressure, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// returns a pod that does not use any resources
0000000000000000000000000000000000000000;;	func getInnocentPod() *v1.Pod {
0000000000000000000000000000000000000000;;		return &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{Name: "innocent-pod"},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				RestartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Image: "gcr.io/google_containers/busybox:1.24",
0000000000000000000000000000000000000000;;						Name:  "innocent-container",
0000000000000000000000000000000000000000;;						Command: []string{
0000000000000000000000000000000000000000;;							"sh",
0000000000000000000000000000000000000000;;							"-c", //make one large file
0000000000000000000000000000000000000000;;							"dd if=/dev/urandom of=largefile bs=5000000000 count=1; while true; do sleep 5; done",
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getInodeConsumingCommand(path string) []string {
0000000000000000000000000000000000000000;;		return []string{
0000000000000000000000000000000000000000;;			"sh",
0000000000000000000000000000000000000000;;			"-c",
0000000000000000000000000000000000000000;;			fmt.Sprintf("i=0; while true; do touch %s${i}.txt; sleep 0.001; i=$((i+=1)); done;", filepath.Join(path, "smallfile")),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
