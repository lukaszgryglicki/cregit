0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
df69b371be631e8b847cb4d76099b51d5bde6c6f;test/integration/scheduler_test.go[test/integration/scheduler_test.go][test/integration/scheduler/scheduler_test.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package scheduler
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// This file tests the scheduler.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"testing"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		clientv1 "k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		clientv1core "k8s.io/client-go/kubernetes/typed/core/v1"
0000000000000000000000000000000000000000;;		restclient "k8s.io/client-go/rest"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/cache"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		informers "k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions"
0000000000000000000000000000000000000000;;		corelisters "k8s.io/kubernetes/pkg/client/listers/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/cmd/kube-scheduler/app"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/cmd/kube-scheduler/app/options"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm"
0000000000000000000000000000000000000000;;		_ "k8s.io/kubernetes/plugin/pkg/scheduler/algorithmprovider"
0000000000000000000000000000000000000000;;		schedulerapi "k8s.io/kubernetes/plugin/pkg/scheduler/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/factory"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/schedulercache"
0000000000000000000000000000000000000000;;		e2e "k8s.io/kubernetes/test/e2e/framework"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/integration/framework"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type nodeMutationFunc func(t *testing.T, n *v1.Node, nodeLister corelisters.NodeLister, c clientset.Interface)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type nodeStateManager struct {
0000000000000000000000000000000000000000;;		makeSchedulable   nodeMutationFunc
0000000000000000000000000000000000000000;;		makeUnSchedulable nodeMutationFunc
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func PredicateOne(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func PredicateTwo(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func PriorityOne(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) {
0000000000000000000000000000000000000000;;		return []schedulerapi.HostPriority{}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func PriorityTwo(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) {
0000000000000000000000000000000000000000;;		return []schedulerapi.HostPriority{}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestSchedulerCreationFromConfigMap verifies that scheduler can be created
0000000000000000000000000000000000000000;;	// from configurations provided by a ConfigMap object and then verifies that the
0000000000000000000000000000000000000000;;	// configuration is applied correctly.
0000000000000000000000000000000000000000;;	func TestSchedulerCreationFromConfigMap(t *testing.T) {
0000000000000000000000000000000000000000;;		_, s, closeFn := framework.RunAMaster(nil)
0000000000000000000000000000000000000000;;		defer closeFn()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ns := framework.CreateTestingNamespace("configmap", s, t)
0000000000000000000000000000000000000000;;		defer framework.DeleteTestingNamespace(ns, s, t)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		clientSet := clientset.NewForConfigOrDie(&restclient.Config{Host: s.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		defer clientSet.Core().Nodes().DeleteCollection(nil, metav1.ListOptions{})
0000000000000000000000000000000000000000;;		informerFactory := informers.NewSharedInformerFactory(clientSet, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Pre-register some predicate and priority functions
0000000000000000000000000000000000000000;;		factory.RegisterFitPredicate("PredicateOne", PredicateOne)
0000000000000000000000000000000000000000;;		factory.RegisterFitPredicate("PredicateTwo", PredicateTwo)
0000000000000000000000000000000000000000;;		factory.RegisterPriorityFunction("PriorityOne", PriorityOne, 1)
0000000000000000000000000000000000000000;;		factory.RegisterPriorityFunction("PriorityTwo", PriorityTwo, 1)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Add a ConfigMap object.
0000000000000000000000000000000000000000;;		configPolicyName := "scheduler-custom-policy-config"
0000000000000000000000000000000000000000;;		policyConfigMap := v1.ConfigMap{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{Namespace: metav1.NamespaceSystem, Name: configPolicyName},
0000000000000000000000000000000000000000;;			Data: map[string]string{
0000000000000000000000000000000000000000;;				options.SchedulerPolicyConfigMapKey: `{
0000000000000000000000000000000000000000;;				"kind" : "Policy",
0000000000000000000000000000000000000000;;				"apiVersion" : "v1",
0000000000000000000000000000000000000000;;				"predicates" : [
0000000000000000000000000000000000000000;;					{"name" : "PredicateOne"},
0000000000000000000000000000000000000000;;					{"name" : "PredicateTwo"}
0000000000000000000000000000000000000000;;				],
0000000000000000000000000000000000000000;;				"priorities" : [
0000000000000000000000000000000000000000;;					{"name" : "PriorityOne", "weight" : 1},
0000000000000000000000000000000000000000;;					{"name" : "PriorityTwo", "weight" : 5}
0000000000000000000000000000000000000000;;				]
0000000000000000000000000000000000000000;;				}`,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		policyConfigMap.APIVersion = api.Registry.GroupOrDie(v1.GroupName).GroupVersion.String()
0000000000000000000000000000000000000000;;		clientSet.Core().ConfigMaps(metav1.NamespaceSystem).Create(&policyConfigMap)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		eventBroadcaster := record.NewBroadcaster()
0000000000000000000000000000000000000000;;		eventBroadcaster.StartRecordingToSink(&clientv1core.EventSinkImpl{Interface: clientv1core.New(clientSet.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;		ss := options.NewSchedulerServer()
0000000000000000000000000000000000000000;;		ss.HardPodAffinitySymmetricWeight = v1.DefaultHardPodAffinitySymmetricWeight
0000000000000000000000000000000000000000;;		ss.PolicyConfigMapName = configPolicyName
0000000000000000000000000000000000000000;;		sched, err := app.CreateScheduler(ss, clientSet,
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Nodes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Pods(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumeClaims(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().ReplicationControllers(),
0000000000000000000000000000000000000000;;			informerFactory.Extensions().V1beta1().ReplicaSets(),
0000000000000000000000000000000000000000;;			informerFactory.Apps().V1beta1().StatefulSets(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Services(),
0000000000000000000000000000000000000000;;			eventBroadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: v1.DefaultSchedulerName}),
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Error creating scheduler: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Verify that the config is applied correctly.
0000000000000000000000000000000000000000;;		schedPredicates := sched.Config().Algorithm.Predicates()
0000000000000000000000000000000000000000;;		schedPrioritizers := sched.Config().Algorithm.Prioritizers()
0000000000000000000000000000000000000000;;		if len(schedPredicates) != 2 || len(schedPrioritizers) != 2 {
0000000000000000000000000000000000000000;;			t.Errorf("Unexpected number of predicates or priority functions. Number of predicates: %v, number of prioritizers: %v", len(schedPredicates), len(schedPrioritizers))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Check a predicate and a priority function.
0000000000000000000000000000000000000000;;		if schedPredicates["PredicateTwo"] == nil {
0000000000000000000000000000000000000000;;			t.Errorf("Expected to have a PodFitsHostPorts predicate.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if schedPrioritizers[1].Function == nil || schedPrioritizers[1].Weight != 5 {
0000000000000000000000000000000000000000;;			t.Errorf("Unexpected prioritizer: func: %v, weight: %v", schedPrioritizers[1].Function, schedPrioritizers[1].Weight)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		defer close(sched.Config().StopEverything)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestSchedulerCreationFromNonExistentConfigMap ensures that creation of the
0000000000000000000000000000000000000000;;	// scheduler from a non-existent ConfigMap fails.
0000000000000000000000000000000000000000;;	func TestSchedulerCreationFromNonExistentConfigMap(t *testing.T) {
0000000000000000000000000000000000000000;;		_, s, closeFn := framework.RunAMaster(nil)
0000000000000000000000000000000000000000;;		defer closeFn()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ns := framework.CreateTestingNamespace("configmap", s, t)
0000000000000000000000000000000000000000;;		defer framework.DeleteTestingNamespace(ns, s, t)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		clientSet := clientset.NewForConfigOrDie(&restclient.Config{Host: s.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		defer clientSet.Core().Nodes().DeleteCollection(nil, metav1.ListOptions{})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		informerFactory := informers.NewSharedInformerFactory(clientSet, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		eventBroadcaster := record.NewBroadcaster()
0000000000000000000000000000000000000000;;		eventBroadcaster.StartRecordingToSink(&clientv1core.EventSinkImpl{Interface: clientv1core.New(clientSet.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ss := options.NewSchedulerServer()
0000000000000000000000000000000000000000;;		ss.PolicyConfigMapName = "non-existent-config"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		_, err := app.CreateScheduler(ss, clientSet,
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Nodes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Pods(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumeClaims(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().ReplicationControllers(),
0000000000000000000000000000000000000000;;			informerFactory.Extensions().V1beta1().ReplicaSets(),
0000000000000000000000000000000000000000;;			informerFactory.Apps().V1beta1().StatefulSets(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Services(),
0000000000000000000000000000000000000000;;			eventBroadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: v1.DefaultSchedulerName}),
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Creation of scheduler didn't fail while the policy ConfigMap didn't exist.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestSchedulerCreationInLegacyMode ensures that creation of the scheduler
0000000000000000000000000000000000000000;;	// works fine when legacy mode is enabled.
0000000000000000000000000000000000000000;;	func TestSchedulerCreationInLegacyMode(t *testing.T) {
0000000000000000000000000000000000000000;;		_, s, closeFn := framework.RunAMaster(nil)
0000000000000000000000000000000000000000;;		defer closeFn()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ns := framework.CreateTestingNamespace("configmap", s, t)
0000000000000000000000000000000000000000;;		defer framework.DeleteTestingNamespace(ns, s, t)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		clientSet := clientset.NewForConfigOrDie(&restclient.Config{Host: s.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		defer clientSet.Core().Nodes().DeleteCollection(nil, metav1.ListOptions{})
0000000000000000000000000000000000000000;;		informerFactory := informers.NewSharedInformerFactory(clientSet, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		eventBroadcaster := record.NewBroadcaster()
0000000000000000000000000000000000000000;;		eventBroadcaster.StartRecordingToSink(&clientv1core.EventSinkImpl{Interface: clientv1core.New(clientSet.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ss := options.NewSchedulerServer()
0000000000000000000000000000000000000000;;		ss.HardPodAffinitySymmetricWeight = v1.DefaultHardPodAffinitySymmetricWeight
0000000000000000000000000000000000000000;;		ss.PolicyConfigMapName = "non-existent-configmap"
0000000000000000000000000000000000000000;;		ss.UseLegacyPolicyConfig = true
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sched, err := app.CreateScheduler(ss, clientSet,
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Nodes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Pods(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumeClaims(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().ReplicationControllers(),
0000000000000000000000000000000000000000;;			informerFactory.Extensions().V1beta1().ReplicaSets(),
0000000000000000000000000000000000000000;;			informerFactory.Apps().V1beta1().StatefulSets(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Services(),
0000000000000000000000000000000000000000;;			eventBroadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: v1.DefaultSchedulerName}),
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Creation of scheduler in legacy mode failed: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		informerFactory.Start(sched.Config().StopEverything)
0000000000000000000000000000000000000000;;		defer close(sched.Config().StopEverything)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sched.Run()
0000000000000000000000000000000000000000;;		DoTestUnschedulableNodes(t, clientSet, ns, informerFactory.Core().V1().Nodes().Lister())
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestUnschedulableNodes(t *testing.T) {
0000000000000000000000000000000000000000;;		_, s, closeFn := framework.RunAMaster(nil)
0000000000000000000000000000000000000000;;		defer closeFn()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ns := framework.CreateTestingNamespace("unschedulable-nodes", s, t)
0000000000000000000000000000000000000000;;		defer framework.DeleteTestingNamespace(ns, s, t)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		clientSet := clientset.NewForConfigOrDie(&restclient.Config{Host: s.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		informerFactory := informers.NewSharedInformerFactory(clientSet, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		schedulerConfigFactory := factory.NewConfigFactory(
0000000000000000000000000000000000000000;;			v1.DefaultSchedulerName,
0000000000000000000000000000000000000000;;			clientSet,
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Nodes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Pods(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumeClaims(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().ReplicationControllers(),
0000000000000000000000000000000000000000;;			informerFactory.Extensions().V1beta1().ReplicaSets(),
0000000000000000000000000000000000000000;;			informerFactory.Apps().V1beta1().StatefulSets(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Services(),
0000000000000000000000000000000000000000;;			v1.DefaultHardPodAffinitySymmetricWeight,
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		schedulerConfig, err := schedulerConfigFactory.Create()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Couldn't create scheduler config: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		eventBroadcaster := record.NewBroadcaster()
0000000000000000000000000000000000000000;;		schedulerConfig.Recorder = eventBroadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: v1.DefaultSchedulerName})
0000000000000000000000000000000000000000;;		eventBroadcaster.StartRecordingToSink(&clientv1core.EventSinkImpl{Interface: clientv1core.New(clientSet.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;		informerFactory.Start(schedulerConfig.StopEverything)
0000000000000000000000000000000000000000;;		sched, _ := scheduler.NewFromConfigurator(&scheduler.FakeConfigurator{Config: schedulerConfig}, nil...)
0000000000000000000000000000000000000000;;		sched.Run()
0000000000000000000000000000000000000000;;		defer close(schedulerConfig.StopEverything)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		DoTestUnschedulableNodes(t, clientSet, ns, schedulerConfigFactory.GetNodeLister())
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podScheduled(c clientset.Interface, podNamespace, podName string) wait.ConditionFunc {
0000000000000000000000000000000000000000;;		return func() (bool, error) {
0000000000000000000000000000000000000000;;			pod, err := c.Core().Pods(podNamespace).Get(podName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				// This could be a connection error so we want to retry.
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if pod.Spec.NodeName == "" {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return true, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Wait till the passFunc confirms that the object it expects to see is in the store.
0000000000000000000000000000000000000000;;	// Used to observe reflected events.
0000000000000000000000000000000000000000;;	func waitForReflection(t *testing.T, nodeLister corelisters.NodeLister, key string, passFunc func(n interface{}) bool) error {
0000000000000000000000000000000000000000;;		nodes := []*v1.Node{}
0000000000000000000000000000000000000000;;		err := wait.Poll(time.Millisecond*100, wait.ForeverTestTimeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			n, err := nodeLister.Get(key)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case err == nil && passFunc(n):
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			case errors.IsNotFound(err):
0000000000000000000000000000000000000000;;				nodes = append(nodes, nil)
0000000000000000000000000000000000000000;;			case err != nil:
0000000000000000000000000000000000000000;;				t.Errorf("Unexpected error: %v", err)
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				nodes = append(nodes, n)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Logf("Logging consecutive node versions received from store:")
0000000000000000000000000000000000000000;;			for i, n := range nodes {
0000000000000000000000000000000000000000;;				t.Logf("%d: %#v", i, n)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func DoTestUnschedulableNodes(t *testing.T, cs clientset.Interface, ns *v1.Namespace, nodeLister corelisters.NodeLister) {
0000000000000000000000000000000000000000;;		// NOTE: This test cannot run in parallel, because it is creating and deleting
0000000000000000000000000000000000000000;;		// non-namespaced objects (Nodes).
0000000000000000000000000000000000000000;;		defer cs.Core().Nodes().DeleteCollection(nil, metav1.ListOptions{})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		goodCondition := v1.NodeCondition{
0000000000000000000000000000000000000000;;			Type:              v1.NodeReady,
0000000000000000000000000000000000000000;;			Status:            v1.ConditionTrue,
0000000000000000000000000000000000000000;;			Reason:            fmt.Sprintf("schedulable condition"),
0000000000000000000000000000000000000000;;			LastHeartbeatTime: metav1.Time{Time: time.Now()},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		badCondition := v1.NodeCondition{
0000000000000000000000000000000000000000;;			Type:              v1.NodeReady,
0000000000000000000000000000000000000000;;			Status:            v1.ConditionUnknown,
0000000000000000000000000000000000000000;;			Reason:            fmt.Sprintf("unschedulable condition"),
0000000000000000000000000000000000000000;;			LastHeartbeatTime: metav1.Time{Time: time.Now()},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Create a new schedulable node, since we're first going to apply
0000000000000000000000000000000000000000;;		// the unschedulable condition and verify that pods aren't scheduled.
0000000000000000000000000000000000000000;;		node := &v1.Node{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{Name: "node-scheduling-test-node"},
0000000000000000000000000000000000000000;;			Spec:       v1.NodeSpec{Unschedulable: false},
0000000000000000000000000000000000000000;;			Status: v1.NodeStatus{
0000000000000000000000000000000000000000;;				Capacity: v1.ResourceList{
0000000000000000000000000000000000000000;;					v1.ResourcePods: *resource.NewQuantity(32, resource.DecimalSI),
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Conditions: []v1.NodeCondition{goodCondition},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		nodeKey, err := cache.MetaNamespaceKeyFunc(node)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Couldn't retrieve key for node %v", node.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The test does the following for each nodeStateManager in this list:
0000000000000000000000000000000000000000;;		//	1. Create a new node
0000000000000000000000000000000000000000;;		//	2. Apply the makeUnSchedulable function
0000000000000000000000000000000000000000;;		//	3. Create a new pod
0000000000000000000000000000000000000000;;		//  4. Check that the pod doesn't get assigned to the node
0000000000000000000000000000000000000000;;		//  5. Apply the schedulable function
0000000000000000000000000000000000000000;;		//  6. Check that the pod *does* get assigned to the node
0000000000000000000000000000000000000000;;		//  7. Delete the pod and node.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nodeModifications := []nodeStateManager{
0000000000000000000000000000000000000000;;			// Test node.Spec.Unschedulable=true/false
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				makeUnSchedulable: func(t *testing.T, n *v1.Node, nodeLister corelisters.NodeLister, c clientset.Interface) {
0000000000000000000000000000000000000000;;					n.Spec.Unschedulable = true
0000000000000000000000000000000000000000;;					if _, err := c.Core().Nodes().Update(n); err != nil {
0000000000000000000000000000000000000000;;						t.Fatalf("Failed to update node with unschedulable=true: %v", err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					err = waitForReflection(t, nodeLister, nodeKey, func(node interface{}) bool {
0000000000000000000000000000000000000000;;						// An unschedulable node should still be present in the store
0000000000000000000000000000000000000000;;						// Nodes that are unschedulable or that are not ready or
0000000000000000000000000000000000000000;;						// have their disk full (Node.Spec.Conditions) are excluded
0000000000000000000000000000000000000000;;						// based on NodeConditionPredicate, a separate check
0000000000000000000000000000000000000000;;						return node != nil && node.(*v1.Node).Spec.Unschedulable == true
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						t.Fatalf("Failed to observe reflected update for setting unschedulable=true: %v", err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				makeSchedulable: func(t *testing.T, n *v1.Node, nodeLister corelisters.NodeLister, c clientset.Interface) {
0000000000000000000000000000000000000000;;					n.Spec.Unschedulable = false
0000000000000000000000000000000000000000;;					if _, err := c.Core().Nodes().Update(n); err != nil {
0000000000000000000000000000000000000000;;						t.Fatalf("Failed to update node with unschedulable=false: %v", err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					err = waitForReflection(t, nodeLister, nodeKey, func(node interface{}) bool {
0000000000000000000000000000000000000000;;						return node != nil && node.(*v1.Node).Spec.Unschedulable == false
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						t.Fatalf("Failed to observe reflected update for setting unschedulable=false: %v", err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			// Test node.Status.Conditions=ConditionTrue/Unknown
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				makeUnSchedulable: func(t *testing.T, n *v1.Node, nodeLister corelisters.NodeLister, c clientset.Interface) {
0000000000000000000000000000000000000000;;					n.Status = v1.NodeStatus{
0000000000000000000000000000000000000000;;						Capacity: v1.ResourceList{
0000000000000000000000000000000000000000;;							v1.ResourcePods: *resource.NewQuantity(32, resource.DecimalSI),
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						Conditions: []v1.NodeCondition{badCondition},
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if _, err = c.Core().Nodes().UpdateStatus(n); err != nil {
0000000000000000000000000000000000000000;;						t.Fatalf("Failed to update node with bad status condition: %v", err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					err = waitForReflection(t, nodeLister, nodeKey, func(node interface{}) bool {
0000000000000000000000000000000000000000;;						return node != nil && node.(*v1.Node).Status.Conditions[0].Status == v1.ConditionUnknown
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						t.Fatalf("Failed to observe reflected update for status condition update: %v", err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				makeSchedulable: func(t *testing.T, n *v1.Node, nodeLister corelisters.NodeLister, c clientset.Interface) {
0000000000000000000000000000000000000000;;					n.Status = v1.NodeStatus{
0000000000000000000000000000000000000000;;						Capacity: v1.ResourceList{
0000000000000000000000000000000000000000;;							v1.ResourcePods: *resource.NewQuantity(32, resource.DecimalSI),
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						Conditions: []v1.NodeCondition{goodCondition},
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if _, err = c.Core().Nodes().UpdateStatus(n); err != nil {
0000000000000000000000000000000000000000;;						t.Fatalf("Failed to update node with healthy status condition: %v", err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					err = waitForReflection(t, nodeLister, nodeKey, func(node interface{}) bool {
0000000000000000000000000000000000000000;;						return node != nil && node.(*v1.Node).Status.Conditions[0].Status == v1.ConditionTrue
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						t.Fatalf("Failed to observe reflected update for status condition update: %v", err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i, mod := range nodeModifications {
0000000000000000000000000000000000000000;;			unSchedNode, err := cs.Core().Nodes().Create(node)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				t.Fatalf("Failed to create node: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Apply the unschedulable modification to the node, and wait for the reflection
0000000000000000000000000000000000000000;;			mod.makeUnSchedulable(t, unSchedNode, nodeLister, cs)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Create the new pod, note that this needs to happen post unschedulable
0000000000000000000000000000000000000000;;			// modification or we have a race in the test.
0000000000000000000000000000000000000000;;			pod := &v1.Pod{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{Name: "node-scheduling-test-pod"},
0000000000000000000000000000000000000000;;				Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;					Containers: []v1.Container{{Name: "container", Image: e2e.GetPauseImageName(cs)}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			myPod, err := cs.Core().Pods(ns.Name).Create(pod)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				t.Fatalf("Failed to create pod: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// There are no schedulable nodes - the pod shouldn't be scheduled.
0000000000000000000000000000000000000000;;			err = wait.Poll(time.Second, wait.ForeverTestTimeout, podScheduled(cs, myPod.Namespace, myPod.Name))
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				t.Errorf("Pod scheduled successfully on unschedulable nodes")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if err != wait.ErrWaitTimeout {
0000000000000000000000000000000000000000;;				t.Errorf("Test %d: failed while trying to confirm the pod does not get scheduled on the node: %v", i, err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				t.Logf("Test %d: Pod did not get scheduled on an unschedulable node", i)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Apply the schedulable modification to the node, and wait for the reflection
0000000000000000000000000000000000000000;;			schedNode, err := cs.Core().Nodes().Get(unSchedNode.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				t.Fatalf("Failed to get node: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			mod.makeSchedulable(t, schedNode, nodeLister, cs)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Wait until the pod is scheduled.
0000000000000000000000000000000000000000;;			err = wait.Poll(time.Second, wait.ForeverTestTimeout, podScheduled(cs, myPod.Namespace, myPod.Name))
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				t.Errorf("Test %d: failed to schedule a pod: %v", i, err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				t.Logf("Test %d: Pod got scheduled on a schedulable node", i)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			err = cs.Core().Pods(ns.Name).Delete(myPod.Name, metav1.NewDeleteOptions(0))
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				t.Errorf("Failed to delete pod: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			err = cs.Core().Nodes().Delete(schedNode.Name, nil)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				t.Errorf("Failed to delete node: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestMultiScheduler(t *testing.T) {
0000000000000000000000000000000000000000;;		_, s, _ := framework.RunAMaster(nil)
0000000000000000000000000000000000000000;;		// TODO: Uncomment when fix #19254
0000000000000000000000000000000000000000;;		// This seems to be a different issue - it still doesn't work.
0000000000000000000000000000000000000000;;		// defer s.Close()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ns := framework.CreateTestingNamespace("multi-scheduler", s, t)
0000000000000000000000000000000000000000;;		defer framework.DeleteTestingNamespace(ns, s, t)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		/*
0000000000000000000000000000000000000000;;			This integration tests the multi-scheduler feature in the following way:
0000000000000000000000000000000000000000;;			1. create a default scheduler
0000000000000000000000000000000000000000;;			2. create a node
0000000000000000000000000000000000000000;;			3. create 3 pods: testPodNoAnnotation, testPodWithAnnotationFitsDefault and testPodWithAnnotationFitsFoo
0000000000000000000000000000000000000000;;				- note: the first two should be picked and scheduled by default scheduler while the last one should be
0000000000000000000000000000000000000000;;				        picked by scheduler of name "foo-scheduler" which does not exist yet.
0000000000000000000000000000000000000000;;			4. **check point-1**:
0000000000000000000000000000000000000000;;				- testPodNoAnnotation, testPodWithAnnotationFitsDefault should be scheduled
0000000000000000000000000000000000000000;;				- testPodWithAnnotationFitsFoo should NOT be scheduled
0000000000000000000000000000000000000000;;			5. create a scheduler with name "foo-scheduler"
0000000000000000000000000000000000000000;;			6. **check point-2**:
0000000000000000000000000000000000000000;;				- testPodWithAnnotationFitsFoo should be scheduled
0000000000000000000000000000000000000000;;			7. stop default scheduler
0000000000000000000000000000000000000000;;			8. create 2 pods: testPodNoAnnotation2 and testPodWithAnnotationFitsDefault2
0000000000000000000000000000000000000000;;				- note: these two pods belong to default scheduler which no longer exists
0000000000000000000000000000000000000000;;			9. **check point-3**:
0000000000000000000000000000000000000000;;				- testPodNoAnnotation2 and testPodWithAnnotationFitsDefault2 should NOT be scheduled
0000000000000000000000000000000000000000;;		*/
0000000000000000000000000000000000000000;;		// 1. create and start default-scheduler
0000000000000000000000000000000000000000;;		clientSet := clientset.NewForConfigOrDie(&restclient.Config{Host: s.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// NOTE: This test cannot run in parallel, because it is creating and deleting
0000000000000000000000000000000000000000;;		// non-namespaced objects (Nodes).
0000000000000000000000000000000000000000;;		defer clientSet.Core().Nodes().DeleteCollection(nil, metav1.ListOptions{})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		informerFactory := informers.NewSharedInformerFactory(clientSet, 0)
0000000000000000000000000000000000000000;;		schedulerConfigFactory := factory.NewConfigFactory(
0000000000000000000000000000000000000000;;			v1.DefaultSchedulerName,
0000000000000000000000000000000000000000;;			clientSet,
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Nodes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Pods(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumeClaims(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().ReplicationControllers(),
0000000000000000000000000000000000000000;;			informerFactory.Extensions().V1beta1().ReplicaSets(),
0000000000000000000000000000000000000000;;			informerFactory.Apps().V1beta1().StatefulSets(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Services(),
0000000000000000000000000000000000000000;;			v1.DefaultHardPodAffinitySymmetricWeight,
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		schedulerConfig, err := schedulerConfigFactory.Create()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Couldn't create scheduler config: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		eventBroadcaster := record.NewBroadcaster()
0000000000000000000000000000000000000000;;		schedulerConfig.Recorder = eventBroadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: v1.DefaultSchedulerName})
0000000000000000000000000000000000000000;;		eventBroadcaster.StartRecordingToSink(&clientv1core.EventSinkImpl{Interface: clientv1core.New(clientSet.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;		informerFactory.Start(schedulerConfig.StopEverything)
0000000000000000000000000000000000000000;;		sched, _ := scheduler.NewFromConfigurator(&scheduler.FakeConfigurator{Config: schedulerConfig}, nil...)
0000000000000000000000000000000000000000;;		sched.Run()
0000000000000000000000000000000000000000;;		// default-scheduler will be stopped later
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 2. create a node
0000000000000000000000000000000000000000;;		node := &v1.Node{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{Name: "node-multi-scheduler-test-node"},
0000000000000000000000000000000000000000;;			Spec:       v1.NodeSpec{Unschedulable: false},
0000000000000000000000000000000000000000;;			Status: v1.NodeStatus{
0000000000000000000000000000000000000000;;				Capacity: v1.ResourceList{
0000000000000000000000000000000000000000;;					v1.ResourcePods: *resource.NewQuantity(32, resource.DecimalSI),
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		clientSet.Core().Nodes().Create(node)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 3. create 3 pods for testing
0000000000000000000000000000000000000000;;		podWithoutSchedulerName := createPod(clientSet, "pod-without-scheduler-name", "")
0000000000000000000000000000000000000000;;		testPod, err := clientSet.Core().Pods(ns.Name).Create(podWithoutSchedulerName)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Failed to create pod: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		schedulerFitsDefault := "default-scheduler"
0000000000000000000000000000000000000000;;		podFitsDefault := createPod(clientSet, "pod-fits-default", schedulerFitsDefault)
0000000000000000000000000000000000000000;;		testPodFitsDefault, err := clientSet.Core().Pods(ns.Name).Create(podFitsDefault)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Failed to create pod: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		schedulerFitsFoo := "foo-scheduler"
0000000000000000000000000000000000000000;;		podFitsFoo := createPod(clientSet, "pod-fits-foo", schedulerFitsFoo)
0000000000000000000000000000000000000000;;		testPodFitsFoo, err := clientSet.Core().Pods(ns.Name).Create(podFitsFoo)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Failed to create pod: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 4. **check point-1**:
0000000000000000000000000000000000000000;;		//		- testPod, testPodFitsDefault should be scheduled
0000000000000000000000000000000000000000;;		//		- testPodFitsFoo should NOT be scheduled
0000000000000000000000000000000000000000;;		err = wait.Poll(time.Second, time.Second*5, podScheduled(clientSet, testPod.Namespace, testPod.Name))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Test MultiScheduler: %s Pod not scheduled: %v", testPod.Name, err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			t.Logf("Test MultiScheduler: %s Pod scheduled", testPod.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err = wait.Poll(time.Second, time.Second*5, podScheduled(clientSet, testPodFitsDefault.Namespace, testPodFitsDefault.Name))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Test MultiScheduler: %s Pod not scheduled: %v", testPodFitsDefault.Name, err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			t.Logf("Test MultiScheduler: %s Pod scheduled", testPodFitsDefault.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err = wait.Poll(time.Second, time.Second*5, podScheduled(clientSet, testPodFitsFoo.Namespace, testPodFitsFoo.Name))
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			t.Errorf("Test MultiScheduler: %s Pod got scheduled, %v", testPodFitsFoo.Name, err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			t.Logf("Test MultiScheduler: %s Pod not scheduled", testPodFitsFoo.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 5. create and start a scheduler with name "foo-scheduler"
0000000000000000000000000000000000000000;;		clientSet2 := clientset.NewForConfigOrDie(&restclient.Config{Host: s.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		informerFactory2 := informers.NewSharedInformerFactory(clientSet, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		schedulerConfigFactory2 := factory.NewConfigFactory(
0000000000000000000000000000000000000000;;			"foo-scheduler",
0000000000000000000000000000000000000000;;			clientSet2,
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Nodes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Pods(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumeClaims(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().ReplicationControllers(),
0000000000000000000000000000000000000000;;			informerFactory.Extensions().V1beta1().ReplicaSets(),
0000000000000000000000000000000000000000;;			informerFactory.Apps().V1beta1().StatefulSets(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Services(),
0000000000000000000000000000000000000000;;			v1.DefaultHardPodAffinitySymmetricWeight,
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		schedulerConfig2, err := schedulerConfigFactory2.Create()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Couldn't create scheduler config: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		eventBroadcaster2 := record.NewBroadcaster()
0000000000000000000000000000000000000000;;		schedulerConfig2.Recorder = eventBroadcaster2.NewRecorder(api.Scheme, clientv1.EventSource{Component: "foo-scheduler"})
0000000000000000000000000000000000000000;;		eventBroadcaster.StartRecordingToSink(&clientv1core.EventSinkImpl{Interface: clientv1core.New(clientSet2.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;		informerFactory2.Start(schedulerConfig2.StopEverything)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sched2, _ := scheduler.NewFromConfigurator(&scheduler.FakeConfigurator{Config: schedulerConfig2}, nil...)
0000000000000000000000000000000000000000;;		sched2.Run()
0000000000000000000000000000000000000000;;		defer close(schedulerConfig2.StopEverything)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		//	6. **check point-2**:
0000000000000000000000000000000000000000;;		//		- testPodWithAnnotationFitsFoo should be scheduled
0000000000000000000000000000000000000000;;		err = wait.Poll(time.Second, time.Second*5, podScheduled(clientSet, testPodFitsFoo.Namespace, testPodFitsFoo.Name))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Test MultiScheduler: %s Pod not scheduled, %v", testPodFitsFoo.Name, err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			t.Logf("Test MultiScheduler: %s Pod scheduled", testPodFitsFoo.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		//	7. delete the pods that were scheduled by the default scheduler, and stop the default scheduler
0000000000000000000000000000000000000000;;		err = clientSet.Core().Pods(ns.Name).Delete(testPod.Name, metav1.NewDeleteOptions(0))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Failed to delete pod: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err = clientSet.Core().Pods(ns.Name).Delete(testPodFitsDefault.Name, metav1.NewDeleteOptions(0))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Failed to delete pod: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The rest of this test assumes that closing StopEverything will cause the
0000000000000000000000000000000000000000;;		// scheduler thread to stop immediately.  It won't, and in fact it will often
0000000000000000000000000000000000000000;;		// schedule 1 more pod before finally exiting.  Comment out until we fix that.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// See https://github.com/kubernetes/kubernetes/issues/23715 for more details.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		/*
0000000000000000000000000000000000000000;;			close(schedulerConfig.StopEverything)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			//	8. create 2 pods: testPodNoAnnotation2 and testPodWithAnnotationFitsDefault2
0000000000000000000000000000000000000000;;			//		- note: these two pods belong to default scheduler which no longer exists
0000000000000000000000000000000000000000;;			podWithNoAnnotation2 := createPod("pod-with-no-annotation2", nil)
0000000000000000000000000000000000000000;;			podWithAnnotationFitsDefault2 := createPod("pod-with-annotation-fits-default2", schedulerAnnotationFitsDefault)
0000000000000000000000000000000000000000;;			testPodNoAnnotation2, err := clientSet.Core().Pods(ns.Name).Create(podWithNoAnnotation2)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				t.Fatalf("Failed to create pod: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			testPodWithAnnotationFitsDefault2, err := clientSet.Core().Pods(ns.Name).Create(podWithAnnotationFitsDefault2)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				t.Fatalf("Failed to create pod: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			//	9. **check point-3**:
0000000000000000000000000000000000000000;;			//		- testPodNoAnnotation2 and testPodWithAnnotationFitsDefault2 should NOT be scheduled
0000000000000000000000000000000000000000;;			err = wait.Poll(time.Second, time.Second*5, podScheduled(clientSet, testPodNoAnnotation2.Namespace, testPodNoAnnotation2.Name))
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				t.Errorf("Test MultiScheduler: %s Pod got scheduled, %v", testPodNoAnnotation2.Name, err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				t.Logf("Test MultiScheduler: %s Pod not scheduled", testPodNoAnnotation2.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			err = wait.Poll(time.Second, time.Second*5, podScheduled(clientSet, testPodWithAnnotationFitsDefault2.Namespace, testPodWithAnnotationFitsDefault2.Name))
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				t.Errorf("Test MultiScheduler: %s Pod got scheduled, %v", testPodWithAnnotationFitsDefault2.Name, err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				t.Logf("Test MultiScheduler: %s Pod scheduled", testPodWithAnnotationFitsDefault2.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		*/
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func createPod(client clientset.Interface, name string, scheduler string) *v1.Pod {
0000000000000000000000000000000000000000;;		return &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{Name: name},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers:    []v1.Container{{Name: "container", Image: e2e.GetPauseImageName(client)}},
0000000000000000000000000000000000000000;;				SchedulerName: scheduler,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// This test will verify scheduler can work well regardless of whether kubelet is allocatable aware or not.
0000000000000000000000000000000000000000;;	func TestAllocatable(t *testing.T) {
0000000000000000000000000000000000000000;;		_, s, closeFn := framework.RunAMaster(nil)
0000000000000000000000000000000000000000;;		defer closeFn()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ns := framework.CreateTestingNamespace("allocatable", s, t)
0000000000000000000000000000000000000000;;		defer framework.DeleteTestingNamespace(ns, s, t)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 1. create and start default-scheduler
0000000000000000000000000000000000000000;;		clientSet := clientset.NewForConfigOrDie(&restclient.Config{Host: s.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		informerFactory := informers.NewSharedInformerFactory(clientSet, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// NOTE: This test cannot run in parallel, because it is creating and deleting
0000000000000000000000000000000000000000;;		// non-namespaced objects (Nodes).
0000000000000000000000000000000000000000;;		defer clientSet.Core().Nodes().DeleteCollection(nil, metav1.ListOptions{})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		schedulerConfigFactory := factory.NewConfigFactory(
0000000000000000000000000000000000000000;;			v1.DefaultSchedulerName,
0000000000000000000000000000000000000000;;			clientSet,
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Nodes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Pods(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumes(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().PersistentVolumeClaims(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().ReplicationControllers(),
0000000000000000000000000000000000000000;;			informerFactory.Extensions().V1beta1().ReplicaSets(),
0000000000000000000000000000000000000000;;			informerFactory.Apps().V1beta1().StatefulSets(),
0000000000000000000000000000000000000000;;			informerFactory.Core().V1().Services(),
0000000000000000000000000000000000000000;;			v1.DefaultHardPodAffinitySymmetricWeight,
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		schedulerConfig, err := schedulerConfigFactory.Create()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Couldn't create scheduler config: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		eventBroadcaster := record.NewBroadcaster()
0000000000000000000000000000000000000000;;		schedulerConfig.Recorder = eventBroadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: v1.DefaultSchedulerName})
0000000000000000000000000000000000000000;;		eventBroadcaster.StartRecordingToSink(&clientv1core.EventSinkImpl{Interface: clientv1core.New(clientSet.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;		informerFactory.Start(schedulerConfig.StopEverything)
0000000000000000000000000000000000000000;;		sched, _ := scheduler.NewFromConfigurator(&scheduler.FakeConfigurator{Config: schedulerConfig}, nil...)
0000000000000000000000000000000000000000;;		sched.Run()
0000000000000000000000000000000000000000;;		// default-scheduler will be stopped later
0000000000000000000000000000000000000000;;		defer close(schedulerConfig.StopEverything)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 2. create a node without allocatable awareness
0000000000000000000000000000000000000000;;		node := &v1.Node{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{Name: "node-allocatable-scheduler-test-node"},
0000000000000000000000000000000000000000;;			Spec:       v1.NodeSpec{Unschedulable: false},
0000000000000000000000000000000000000000;;			Status: v1.NodeStatus{
0000000000000000000000000000000000000000;;				Capacity: v1.ResourceList{
0000000000000000000000000000000000000000;;					v1.ResourcePods:   *resource.NewQuantity(32, resource.DecimalSI),
0000000000000000000000000000000000000000;;					v1.ResourceCPU:    *resource.NewMilliQuantity(30, resource.DecimalSI),
0000000000000000000000000000000000000000;;					v1.ResourceMemory: *resource.NewQuantity(30, resource.BinarySI),
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		allocNode, err := clientSet.Core().Nodes().Create(node)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Failed to create node: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 3. create resource pod which requires less than Capacity
0000000000000000000000000000000000000000;;		podResource := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{Name: "pod-test-allocatable"},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  "container",
0000000000000000000000000000000000000000;;						Image: e2e.GetPauseImageName(clientSet),
0000000000000000000000000000000000000000;;						Resources: v1.ResourceRequirements{
0000000000000000000000000000000000000000;;							Requests: v1.ResourceList{
0000000000000000000000000000000000000000;;								v1.ResourceCPU:    *resource.NewMilliQuantity(20, resource.DecimalSI),
0000000000000000000000000000000000000000;;								v1.ResourceMemory: *resource.NewQuantity(20, resource.BinarySI),
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		testAllocPod, err := clientSet.Core().Pods(ns.Name).Create(podResource)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Test allocatable unawareness failed to create pod: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 4. Test: this test pod should be scheduled since api-server will use Capacity as Allocatable
0000000000000000000000000000000000000000;;		err = wait.Poll(time.Second, time.Second*5, podScheduled(clientSet, testAllocPod.Namespace, testAllocPod.Name))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Test allocatable unawareness: %s Pod not scheduled: %v", testAllocPod.Name, err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			t.Logf("Test allocatable unawareness: %s Pod scheduled", testAllocPod.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 5. Change the node status to allocatable aware, note that Allocatable is less than Pod's requirement
0000000000000000000000000000000000000000;;		allocNode.Status = v1.NodeStatus{
0000000000000000000000000000000000000000;;			Capacity: v1.ResourceList{
0000000000000000000000000000000000000000;;				v1.ResourcePods:   *resource.NewQuantity(32, resource.DecimalSI),
0000000000000000000000000000000000000000;;				v1.ResourceCPU:    *resource.NewMilliQuantity(30, resource.DecimalSI),
0000000000000000000000000000000000000000;;				v1.ResourceMemory: *resource.NewQuantity(30, resource.BinarySI),
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Allocatable: v1.ResourceList{
0000000000000000000000000000000000000000;;				v1.ResourcePods:   *resource.NewQuantity(32, resource.DecimalSI),
0000000000000000000000000000000000000000;;				v1.ResourceCPU:    *resource.NewMilliQuantity(10, resource.DecimalSI),
0000000000000000000000000000000000000000;;				v1.ResourceMemory: *resource.NewQuantity(10, resource.BinarySI),
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if _, err := clientSet.Core().Nodes().UpdateStatus(allocNode); err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Failed to update node with Status.Allocatable: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err := clientSet.Core().Pods(ns.Name).Delete(podResource.Name, &metav1.DeleteOptions{}); err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Failed to remove first resource pod: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 6. Make another pod with different name, same resource request
0000000000000000000000000000000000000000;;		podResource.ObjectMeta.Name = "pod-test-allocatable2"
0000000000000000000000000000000000000000;;		testAllocPod2, err := clientSet.Core().Pods(ns.Name).Create(podResource)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("Test allocatable awareness failed to create pod: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 7. Test: this test pod should not be scheduled since it request more than Allocatable
0000000000000000000000000000000000000000;;		err = wait.Poll(time.Second, time.Second*5, podScheduled(clientSet, testAllocPod2.Namespace, testAllocPod2.Name))
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			t.Errorf("Test allocatable awareness: %s Pod got scheduled unexpectedly, %v", testAllocPod2.Name, err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			t.Logf("Test allocatable awareness: %s Pod not scheduled as expected", testAllocPod2.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
