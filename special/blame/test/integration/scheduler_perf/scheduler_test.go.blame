0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
1643b37e8d8753761e7d0446ae4f768c9ae37417;test/component/scheduler/perf/scheduler_test.go[test/component/scheduler/perf/scheduler_test.go][test/integration/scheduler_perf/scheduler_test.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package benchmark
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/test/integration/framework"
0000000000000000000000000000000000000000;;		testutils "k8s.io/kubernetes/test/utils"
0000000000000000000000000000000000000000;;		"math"
0000000000000000000000000000000000000000;;		"testing"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		warning3K    = 100
0000000000000000000000000000000000000000;;		threshold3K  = 30
0000000000000000000000000000000000000000;;		threshold30K = 30
0000000000000000000000000000000000000000;;		threshold60K = 30
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestSchedule100Node3KPods schedules 3k pods on 100 nodes.
0000000000000000000000000000000000000000;;	func TestSchedule100Node3KPods(t *testing.T) {
0000000000000000000000000000000000000000;;		if testing.Short() {
0000000000000000000000000000000000000000;;			t.Skip("Skipping because we want to run short tests")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		config := getBaseConfig(100, 3000)
0000000000000000000000000000000000000000;;		writePodAndNodeTopologyToConfig(config)
0000000000000000000000000000000000000000;;		min := schedulePods(config)
0000000000000000000000000000000000000000;;		if min < threshold3K {
0000000000000000000000000000000000000000;;			t.Errorf("Failing: Scheduling rate was too low for an interval, we saw rate of %v, which is the allowed minimum of %v ! ", min, threshold3K)
0000000000000000000000000000000000000000;;		} else if min < warning3K {
0000000000000000000000000000000000000000;;			fmt.Printf("Warning: pod scheduling throughput for 3k pods was slow for an interval... Saw a interval with very low (%v) scheduling rate!", min)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			fmt.Printf("Minimal observed throughput for 3k pod test: %v\n", min)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestSchedule100Node3KNodeAffinityPods schedules 3k pods using Node affinity on 100 nodes.
0000000000000000000000000000000000000000;;	func TestSchedule100Node3KNodeAffinityPods(t *testing.T) {
0000000000000000000000000000000000000000;;		if testing.Short() {
0000000000000000000000000000000000000000;;			t.Skip("Skipping because we want to run short tests")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		config := getBaseConfig(100, 3000)
0000000000000000000000000000000000000000;;		// number of Node-Pod sets with Pods NodeAffinity matching given Nodes.
0000000000000000000000000000000000000000;;		numGroups := 10
0000000000000000000000000000000000000000;;		nodeAffinityKey := "kubernetes.io/sched-perf-node-affinity"
0000000000000000000000000000000000000000;;		nodeStrategies := make([]testutils.CountToStrategy, 0, numGroups)
0000000000000000000000000000000000000000;;		for i := 0; i < numGroups; i++ {
0000000000000000000000000000000000000000;;			nodeStrategies = append(nodeStrategies, testutils.CountToStrategy{
0000000000000000000000000000000000000000;;				Count:    config.numNodes / numGroups,
0000000000000000000000000000000000000000;;				Strategy: testutils.NewLabelNodePrepareStrategy(nodeAffinityKey, fmt.Sprintf("%v", i)),
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		config.nodePreparer = framework.NewIntegrationTestNodePreparer(
0000000000000000000000000000000000000000;;			config.schedulerSupportFunctions.GetClient(),
0000000000000000000000000000000000000000;;			nodeStrategies,
0000000000000000000000000000000000000000;;			"scheduler-perf-",
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podCreatorConfig := testutils.NewTestPodCreatorConfig()
0000000000000000000000000000000000000000;;		for i := 0; i < numGroups; i++ {
0000000000000000000000000000000000000000;;			pod := &v1.Pod{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					GenerateName: "sched-perf-node-affinity-pod-",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Spec: testutils.MakePodSpec(),
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pod.Spec.Affinity = &v1.Affinity{
0000000000000000000000000000000000000000;;				NodeAffinity: &v1.NodeAffinity{
0000000000000000000000000000000000000000;;					RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{
0000000000000000000000000000000000000000;;						NodeSelectorTerms: []v1.NodeSelectorTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								MatchExpressions: []v1.NodeSelectorRequirement{
0000000000000000000000000000000000000000;;									{
0000000000000000000000000000000000000000;;										Key:      nodeAffinityKey,
0000000000000000000000000000000000000000;;										Operator: v1.NodeSelectorOpIn,
0000000000000000000000000000000000000000;;										Values:   []string{fmt.Sprintf("%v", i)},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			podCreatorConfig.AddStrategy("sched-perf-node-affinity", config.numPods/numGroups,
0000000000000000000000000000000000000000;;				testutils.NewCustomCreatePodStrategy(pod),
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		config.podCreator = testutils.NewTestPodCreator(config.schedulerSupportFunctions.GetClient(), podCreatorConfig)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if min := schedulePods(config); min < threshold30K {
0000000000000000000000000000000000000000;;			t.Errorf("Too small pod scheduling throughput for 30k pods. Expected %v got %v", threshold30K, min)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			fmt.Printf("Minimal observed throughput for 30k pod test: %v\n", min)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestSchedule1000Node30KPods schedules 30k pods on 1000 nodes.
0000000000000000000000000000000000000000;;	func TestSchedule1000Node30KPods(t *testing.T) {
0000000000000000000000000000000000000000;;		if testing.Short() {
0000000000000000000000000000000000000000;;			t.Skip("Skipping because we want to run short tests")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		config := getBaseConfig(1000, 30000)
0000000000000000000000000000000000000000;;		writePodAndNodeTopologyToConfig(config)
0000000000000000000000000000000000000000;;		if min := schedulePods(config); min < threshold30K {
0000000000000000000000000000000000000000;;			t.Errorf("To small pod scheduling throughput for 30k pods. Expected %v got %v", threshold30K, min)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			fmt.Printf("Minimal observed throughput for 30k pod test: %v\n", min)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestSchedule2000Node60KPods schedules 60k pods on 2000 nodes.
0000000000000000000000000000000000000000;;	// This test won't fit in normal 10 minutes time window.
0000000000000000000000000000000000000000;;	// func TestSchedule2000Node60KPods(t *testing.T) {
0000000000000000000000000000000000000000;;	// 	if testing.Short() {
0000000000000000000000000000000000000000;;	// 		t.Skip("Skipping because we want to run short tests")
0000000000000000000000000000000000000000;;	// 	}
0000000000000000000000000000000000000000;;	// 	config := defaultSchedulerBenchmarkConfig(2000, 60000)
0000000000000000000000000000000000000000;;	// 	if min := schedulePods(config); min < threshold60K {
0000000000000000000000000000000000000000;;	// 		t.Errorf("To small pod scheduling throughput for 60k pods. Expected %v got %v", threshold60K, min)
0000000000000000000000000000000000000000;;	// 	} else {
0000000000000000000000000000000000000000;;	// 		fmt.Printf("Minimal observed throughput for 60k pod test: %v\n", min)
0000000000000000000000000000000000000000;;	// 	}
0000000000000000000000000000000000000000;;	// }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// testConfig contains the some input parameters needed for running test-suite
0000000000000000000000000000000000000000;;	type testConfig struct {
0000000000000000000000000000000000000000;;		// Note: We don't need numPods, numNodes anymore in this struct but keeping them for backward compatibility
0000000000000000000000000000000000000000;;		numPods                   int
0000000000000000000000000000000000000000;;		numNodes                  int
0000000000000000000000000000000000000000;;		nodePreparer              testutils.TestNodePreparer
0000000000000000000000000000000000000000;;		podCreator                *testutils.TestPodCreator
0000000000000000000000000000000000000000;;		schedulerSupportFunctions scheduler.Configurator
0000000000000000000000000000000000000000;;		destroyFunc               func()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	//  baseConfig returns a minimal testConfig to be customized for different tests.
0000000000000000000000000000000000000000;;	func baseConfig() *testConfig {
0000000000000000000000000000000000000000;;		schedulerConfigFactory, destroyFunc := mustSetupScheduler()
0000000000000000000000000000000000000000;;		return &testConfig{
0000000000000000000000000000000000000000;;			schedulerSupportFunctions: schedulerConfigFactory,
0000000000000000000000000000000000000000;;			destroyFunc:               destroyFunc,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getBaseConfig returns baseConfig after initializing number of nodes and pods.
0000000000000000000000000000000000000000;;	// We have to function for backward compatibility. We can combine this into baseConfig.
0000000000000000000000000000000000000000;;	// TODO: Remove this function once the backward compatibility is not needed.
0000000000000000000000000000000000000000;;	func getBaseConfig(nodes int, pods int) *testConfig {
0000000000000000000000000000000000000000;;		config := baseConfig()
0000000000000000000000000000000000000000;;		config.numNodes = nodes
0000000000000000000000000000000000000000;;		config.numPods = pods
0000000000000000000000000000000000000000;;		return config
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// schedulePods schedules specific number of pods on specific number of nodes.
0000000000000000000000000000000000000000;;	// This is used to learn the scheduling throughput on various
0000000000000000000000000000000000000000;;	// sizes of cluster and changes as more and more pods are scheduled.
0000000000000000000000000000000000000000;;	// It won't stop until all pods are scheduled.
0000000000000000000000000000000000000000;;	// It returns the minimum of throughput over whole run.
0000000000000000000000000000000000000000;;	func schedulePods(config *testConfig) int32 {
0000000000000000000000000000000000000000;;		defer config.destroyFunc()
0000000000000000000000000000000000000000;;		if err := config.nodePreparer.PrepareNodes(); err != nil {
0000000000000000000000000000000000000000;;			glog.Fatalf("%v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer config.nodePreparer.CleanupNodes()
0000000000000000000000000000000000000000;;		config.podCreator.CreatePods()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		prev := 0
0000000000000000000000000000000000000000;;		// On startup there may be a latent period where NO scheduling occurs (qps = 0).
0000000000000000000000000000000000000000;;		// We are interested in low scheduling rates (i.e. qps=2),
0000000000000000000000000000000000000000;;		minQps := int32(math.MaxInt32)
0000000000000000000000000000000000000000;;		start := time.Now()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Bake in time for the first pod scheduling event.
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			time.Sleep(50 * time.Millisecond)
0000000000000000000000000000000000000000;;			scheduled, err := config.schedulerSupportFunctions.GetScheduledPodLister().List(labels.Everything())
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Fatalf("%v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// 30,000 pods -> wait till @ least 300 are scheduled to start measuring.
0000000000000000000000000000000000000000;;			// TODO Find out why sometimes there may be scheduling blips in the beggining.
0000000000000000000000000000000000000000;;			if len(scheduled) > config.numPods/100 {
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// map minimum QPS entries in a counter, useful for debugging tests.
0000000000000000000000000000000000000000;;		qpsStats := map[int]int{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Now that scheduling has started, lets start taking the pulse on how many pods are happening per second.
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			// This can potentially affect performance of scheduler, since List() is done under mutex.
0000000000000000000000000000000000000000;;			// Listing 10000 pods is an expensive operation, so running it frequently may impact scheduler.
0000000000000000000000000000000000000000;;			// TODO: Setup watch on apiserver and wait until all pods scheduled.
0000000000000000000000000000000000000000;;			scheduled, err := config.schedulerSupportFunctions.GetScheduledPodLister().List(labels.Everything())
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Fatalf("%v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// We will be completed when all pods are done being scheduled.
0000000000000000000000000000000000000000;;			// return the worst-case-scenario interval that was seen during this time.
0000000000000000000000000000000000000000;;			// Note this should never be low due to cold-start, so allow bake in sched time if necessary.
0000000000000000000000000000000000000000;;			if len(scheduled) >= config.numPods {
0000000000000000000000000000000000000000;;				fmt.Printf("Scheduled %v Pods in %v seconds (%v per second on average). min QPS was %v\n",
0000000000000000000000000000000000000000;;					config.numPods, int(time.Since(start)/time.Second), config.numPods/int(time.Since(start)/time.Second), minQps)
0000000000000000000000000000000000000000;;				return minQps
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// There's no point in printing it for the last iteration, as the value is random
0000000000000000000000000000000000000000;;			qps := len(scheduled) - prev
0000000000000000000000000000000000000000;;			qpsStats[qps] += 1
0000000000000000000000000000000000000000;;			if int32(qps) < minQps {
0000000000000000000000000000000000000000;;				minQps = int32(qps)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			fmt.Printf("%ds\trate: %d\ttotal: %d (qps frequency: %v)\n", time.Since(start)/time.Second, qps, len(scheduled), qpsStats)
0000000000000000000000000000000000000000;;			prev = len(scheduled)
0000000000000000000000000000000000000000;;			time.Sleep(1 * time.Second)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// mutateNodeSpec returns the strategy needed for creation of nodes.
0000000000000000000000000000000000000000;;	// TODO: It should take the nodespec and return the modified version of it. As of now, returning the strategies for backward compatibilty.
0000000000000000000000000000000000000000;;	func (na nodeAffinity) mutateNodeSpec(numNodes int) []testutils.CountToStrategy {
0000000000000000000000000000000000000000;;		numGroups := na.numGroups
0000000000000000000000000000000000000000;;		nodeAffinityKey := na.nodeAffinityKey
0000000000000000000000000000000000000000;;		nodeStrategies := make([]testutils.CountToStrategy, 0, numGroups)
0000000000000000000000000000000000000000;;		for i := 0; i < numGroups; i++ {
0000000000000000000000000000000000000000;;			nodeStrategies = append(nodeStrategies, testutils.CountToStrategy{
0000000000000000000000000000000000000000;;				Count:    numNodes / numGroups,
0000000000000000000000000000000000000000;;				Strategy: testutils.NewLabelNodePrepareStrategy(nodeAffinityKey, fmt.Sprintf("%v", i)),
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nodeStrategies
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// mutatePodSpec returns the list of pods after mutating the pod spec based on predicates and priorities.
0000000000000000000000000000000000000000;;	// TODO: It should take the podspec and return the modified version of it. As of now, returning the podlist for backward compatibilty.
0000000000000000000000000000000000000000;;	func (na nodeAffinity) mutatePodSpec(numPods int, pod *v1.Pod) []*v1.Pod {
0000000000000000000000000000000000000000;;		numGroups := na.numGroups
0000000000000000000000000000000000000000;;		nodeAffinityKey := na.nodeAffinityKey
0000000000000000000000000000000000000000;;		podList := make([]*v1.Pod, 0, numGroups)
0000000000000000000000000000000000000000;;		for i := 0; i < numGroups; i++ {
0000000000000000000000000000000000000000;;			pod = &v1.Pod{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					GenerateName: "sched-perf-node-affinity-pod-",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Spec: testutils.MakePodSpec(),
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pod.Spec.Affinity = &v1.Affinity{
0000000000000000000000000000000000000000;;				NodeAffinity: &v1.NodeAffinity{
0000000000000000000000000000000000000000;;					RequiredDuringSchedulingIgnoredDuringExecution: &v1.NodeSelector{
0000000000000000000000000000000000000000;;						NodeSelectorTerms: []v1.NodeSelectorTerm{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								MatchExpressions: []v1.NodeSelectorRequirement{
0000000000000000000000000000000000000000;;									{
0000000000000000000000000000000000000000;;										Key:      nodeAffinityKey,
0000000000000000000000000000000000000000;;										Operator: v1.NodeSelectorOpIn,
0000000000000000000000000000000000000000;;										Values:   []string{fmt.Sprintf("%v", i)},
0000000000000000000000000000000000000000;;									},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			podList = append(podList, pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return podList
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// generatePodAndNodeTopology is the wrapper function for modifying both pods and node objects.
0000000000000000000000000000000000000000;;	func (inputConfig *schedulerPerfConfig) generatePodAndNodeTopology(config *testConfig) {
0000000000000000000000000000000000000000;;		nodeAffinity := inputConfig.NodeAffinity
0000000000000000000000000000000000000000;;		podCreatorConfig := testutils.NewTestPodCreatorConfig()
0000000000000000000000000000000000000000;;		var nodeStrategies []testutils.CountToStrategy
0000000000000000000000000000000000000000;;		var pod *v1.Pod
0000000000000000000000000000000000000000;;		var podList []*v1.Pod
0000000000000000000000000000000000000000;;		if nodeAffinity != nil {
0000000000000000000000000000000000000000;;			// Mutate Node
0000000000000000000000000000000000000000;;			nodeStrategies = nodeAffinity.mutateNodeSpec(config.numNodes)
0000000000000000000000000000000000000000;;			// Mutate Pod TODO: Make this to return to podSpec.
0000000000000000000000000000000000000000;;			podList = nodeAffinity.mutatePodSpec(config.numPods, pod)
0000000000000000000000000000000000000000;;			numGroups := nodeAffinity.numGroups
0000000000000000000000000000000000000000;;			for _, pod := range podList {
0000000000000000000000000000000000000000;;				podCreatorConfig.AddStrategy("sched-perf-node-affinity", config.numPods/numGroups,
0000000000000000000000000000000000000000;;					testutils.NewCustomCreatePodStrategy(pod),
0000000000000000000000000000000000000000;;				)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			config.nodePreparer = framework.NewIntegrationTestNodePreparer(
0000000000000000000000000000000000000000;;				config.schedulerSupportFunctions.GetClient(),
0000000000000000000000000000000000000000;;				nodeStrategies, "scheduler-perf-")
0000000000000000000000000000000000000000;;			config.podCreator = testutils.NewTestPodCreator(config.schedulerSupportFunctions.GetClient(), podCreatorConfig)
0000000000000000000000000000000000000000;;			// TODO: other predicates/priorities will be processed in subsequent if statements.
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// Default configuration.
0000000000000000000000000000000000000000;;			nodePreparer := framework.NewIntegrationTestNodePreparer(
0000000000000000000000000000000000000000;;				config.schedulerSupportFunctions.GetClient(),
0000000000000000000000000000000000000000;;				[]testutils.CountToStrategy{{Count: config.numNodes, Strategy: &testutils.TrivialNodePrepareStrategy{}}},
0000000000000000000000000000000000000000;;				"scheduler-perf-",
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			podConfig := testutils.NewTestPodCreatorConfig()
0000000000000000000000000000000000000000;;			podConfig.AddStrategy("sched-test", config.numPods, testutils.NewSimpleWithControllerCreatePodStrategy("rc1"))
0000000000000000000000000000000000000000;;			podCreator := testutils.NewTestPodCreator(config.schedulerSupportFunctions.GetClient(), podConfig)
0000000000000000000000000000000000000000;;			config.nodePreparer = nodePreparer
0000000000000000000000000000000000000000;;			config.podCreator = podCreator
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// writePodAndNodeTopologyToConfig reads a configuration and then applies it to a test configuration.
0000000000000000000000000000000000000000;;	//TODO: As of now, this function is not doing anything expect for reading input values to priority structs.
0000000000000000000000000000000000000000;;	func writePodAndNodeTopologyToConfig(config *testConfig) {
0000000000000000000000000000000000000000;;		// High Level structure that should be filled for every predicate or priority.
0000000000000000000000000000000000000000;;		inputConfig := &schedulerPerfConfig{
0000000000000000000000000000000000000000;;			NodeAffinity: &nodeAffinity{
0000000000000000000000000000000000000000;;				//number of Node-Pod sets with Pods NodeAffinity matching given Nodes.
0000000000000000000000000000000000000000;;				numGroups:       10,
0000000000000000000000000000000000000000;;				nodeAffinityKey: "kubernetes.io/sched-perf-node-affinity",
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		inputConfig.generatePodAndNodeTopology(config)
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
