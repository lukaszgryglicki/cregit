0000000000000000000000000000000000000000;;	# GCE Load-Balancer Controller (GLBC) Cluster Addon
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	This cluster addon is composed of:
0000000000000000000000000000000000000000;;	* A [Google L7 LoadBalancer Controller](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/gce)
0000000000000000000000000000000000000000;;	* A [404 default backend](https://github.com/kubernetes/contrib/tree/master/404-server) Service + RC
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	It relies on the [Ingress resource](https://kubernetes.io/docs/user-guide/ingress.md) only available in Kubernetes version 1.1 and beyond.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	## Prerequisites
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Before you can receive traffic through the GCE L7 Loadbalancer Controller you need:
0000000000000000000000000000000000000000;;	* A Working Kubernetes 1.1 cluster
0000000000000000000000000000000000000000;;	* At least 1 Kubernetes [NodePort Service](https://kubernetes.io/docs/user-guide/services.md#type-nodeport) (this is the endpoint for your Ingress)
0000000000000000000000000000000000000000;;	* Firewall-rules that allow traffic to the NodePort service, as indicated by `kubectl` at Service creation time
0000000000000000000000000000000000000000;;	* Adequate quota, as mentioned in the next section
0000000000000000000000000000000000000000;;	* A single instance of the L7 Loadbalancer Controller pod (if you're using the default GCE setup, this should already be running in the `kube-system` namespace)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	## Quota
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	GLBC is not aware of your GCE quota. As of this writing users get 3 [GCE Backend Services](https://cloud.google.com/compute/docs/load-balancing/http/backend-service) by default. If you plan on creating Ingresses for multiple Kubernetes Services, remember that each one requires a backend service, and request quota. Should you fail to do so the controller will poll periodically and grab the first free backend service slot it finds. You can view your quota:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	```console
0000000000000000000000000000000000000000;;	$ gcloud compute project-info describe --project myproject
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	See [GCE documentation](https://cloud.google.com/compute/docs/resource-quotas#checking_your_quota) for how to request more.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	## Latency
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	It takes ~1m to spin up a loadbalancer (this includes acquiring the public ip), and ~5-6m before the GCE api starts healthchecking backends. So as far as latency goes, here's what to expect:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Assume one creates the following simple Ingress:
0000000000000000000000000000000000000000;;	```yaml
0000000000000000000000000000000000000000;;	apiVersion: extensions/v1beta1
0000000000000000000000000000000000000000;;	kind: Ingress
0000000000000000000000000000000000000000;;	metadata:
0000000000000000000000000000000000000000;;	  name: test-ingress
0000000000000000000000000000000000000000;;	spec:
0000000000000000000000000000000000000000;;	  backend:
0000000000000000000000000000000000000000;;	    # This will just loopback to the default backend of GLBC
0000000000000000000000000000000000000000;;	    serviceName: default-http-backend
0000000000000000000000000000000000000000;;	    servicePort: 80
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	* time, t=0
0000000000000000000000000000000000000000;;	  ```console
0000000000000000000000000000000000000000;;	  $ kubectl get ing
0000000000000000000000000000000000000000;;	  NAME           RULE      BACKEND                   ADDRESS
0000000000000000000000000000000000000000;;	  test-ingress   -         default-http-backend:80
0000000000000000000000000000000000000000;;	  $ kubectl describe ing
0000000000000000000000000000000000000000;;	  No events.
0000000000000000000000000000000000000000;;	  ```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	* time, t=1m
0000000000000000000000000000000000000000;;	  ```console
0000000000000000000000000000000000000000;;	  $ kubectl get ing
0000000000000000000000000000000000000000;;	  NAME           RULE      BACKEND                   ADDRESS
0000000000000000000000000000000000000000;;	  test-ingress   -         default-http-backend:80   130.211.5.27
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  $ kubectl describe ing
0000000000000000000000000000000000000000;;	  target-proxy:		k8s-tp-default-test-ingress
0000000000000000000000000000000000000000;;	  url-map:		    k8s-um-default-test-ingress
0000000000000000000000000000000000000000;;	  backends:		    {"k8s-be-32342":"UNKNOWN"}
0000000000000000000000000000000000000000;;	  forwarding-rule:	k8s-fw-default-test-ingress
0000000000000000000000000000000000000000;;	  Events:
0000000000000000000000000000000000000000;;	    FirstSeen	LastSeen	Count	From				SubobjectPath	Reason	Message
0000000000000000000000000000000000000000;;	    ─────────	────────	─────	────				─────────────	──────	───────
0000000000000000000000000000000000000000;;	    46s		46s		1	{loadbalancer-controller }	Success	Created loadbalancer 130.211.5.27
0000000000000000000000000000000000000000;;	  ```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	* time, t=5m
0000000000000000000000000000000000000000;;	  ```console
0000000000000000000000000000000000000000;;	  $ kubectl describe ing
0000000000000000000000000000000000000000;;	  target-proxy:		k8s-tp-default-test-ingress
0000000000000000000000000000000000000000;;	  url-map:		    k8s-um-default-test-ingress
0000000000000000000000000000000000000000;;	  backends:		    {"k8s-be-32342":"HEALTHY"}
0000000000000000000000000000000000000000;;	  forwarding-rule:	k8s-fw-default-test-ingress
0000000000000000000000000000000000000000;;	  Events:
0000000000000000000000000000000000000000;;	    FirstSeen	LastSeen	Count	From				SubobjectPath	Reason	Message
0000000000000000000000000000000000000000;;	    ─────────	────────	─────	────				─────────────	──────	───────
0000000000000000000000000000000000000000;;	    46s		46s		1	{loadbalancer-controller }	Success	Created loadbalancer 130.211.5.27
0000000000000000000000000000000000000000;;	  ```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	## Disabling GLBC
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Since GLBC runs as a cluster addon, you cannot simply delete the RC. The easiest way to disable it is to do as follows:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	* IFF you want to tear down existing L7 loadbalancers, hit the /delete-all-and-quit endpoint on the pod:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ```console
0000000000000000000000000000000000000000;;	  $ kubectl get pods --namespace=kube-system
0000000000000000000000000000000000000000;;	  NAME                                               READY     STATUS    RESTARTS   AGE
0000000000000000000000000000000000000000;;	  l7-lb-controller-7bb21                             1/1       Running   0          1h
0000000000000000000000000000000000000000;;	  $ kubectl exec l7-lb-controller-7bb21 -c l7-lb-controller curl http://localhost:8081/delete-all-and-quit --namespace=kube-system
0000000000000000000000000000000000000000;;	  $ kubectl logs l7-lb-controller-7b221 -c l7-lb-controller --follow
0000000000000000000000000000000000000000;;	  ...
0000000000000000000000000000000000000000;;	  I1007 00:30:00.322528       1 main.go:160] Handled quit, awaiting pod deletion.
0000000000000000000000000000000000000000;;	  ```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	* Nullify the RC (but don't delete it or the addon controller will "fix" it for you)
0000000000000000000000000000000000000000;;	  ```console
0000000000000000000000000000000000000000;;	  $ kubectl scale rc l7-lb-controller --replicas=0 --namespace=kube-system
0000000000000000000000000000000000000000;;	  ```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	## Limitations
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	* This cluster addon is still in the Beta phase. It behooves you to read through the GLBC documentation mentioned above and make sure there are no surprises.
0000000000000000000000000000000000000000;;	* The recommended way to tear down a cluster with active Ingresses is to either delete each Ingress, or hit the /delete-all-and-quit endpoint on GLBC as described below, before invoking a cluster teardown script (eg: kube-down.sh). You will have to manually cleanup GCE resources through the [cloud console](https://cloud.google.com/compute/docs/console#access) or [gcloud CLI](https://cloud.google.com/compute/docs/gcloud-compute/) if you simply tear down the cluster with active Ingresses.
0000000000000000000000000000000000000000;;	* All L7 Loadbalancers created by GLBC have a default backend. If you don't specify one in your Ingress, GLBC will assign the 404 default backend mentioned above.
0000000000000000000000000000000000000000;;	* All Kubernetes services must serve a 200 page on '/', or whatever custom value you've specified through GLBC's `--health-check-path argument`.
0000000000000000000000000000000000000000;;	* GLBC is not built for performance. Creating many Ingresses at a time can overwhelm it. It won't fall over, but will take its own time to churn through the Ingress queue. It doesn't understand concepts like fairness or backoff just yet.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	[![Analytics](https://kubernetes-site.appspot.com/UA-36037335-10/GitHub/cluster/addons/cluster-loadbalancing/glbc/README.md?pixel)]()
