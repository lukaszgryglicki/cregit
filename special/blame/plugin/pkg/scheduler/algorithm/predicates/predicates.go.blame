0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2014 The Kubernetes Authors.
a3dc8d69e05351582f47f6135813ec671c39ea13;pkg/scheduler/firstfit.go[pkg/scheduler/firstfit.go][plugin/pkg/scheduler/algorithm/predicates/predicates.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package predicates
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"math/rand"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		apierrors "k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		utilruntime "k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		utilfeature "k8s.io/apiserver/pkg/util/feature"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/workqueue"
0000000000000000000000000000000000000000;;		v1helper "k8s.io/kubernetes/pkg/api/v1/helper"
0000000000000000000000000000000000000000;;		v1qos "k8s.io/kubernetes/pkg/api/v1/helper/qos"
0000000000000000000000000000000000000000;;		corelisters "k8s.io/kubernetes/pkg/client/listers/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/features"
0000000000000000000000000000000000000000;;		kubeletapis "k8s.io/kubernetes/pkg/kubelet/apis"
0000000000000000000000000000000000000000;;		volumeutil "k8s.io/kubernetes/pkg/volume/util"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm"
0000000000000000000000000000000000000000;;		priorityutil "k8s.io/kubernetes/plugin/pkg/scheduler/algorithm/priorities/util"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/schedulercache"
0000000000000000000000000000000000000000;;		schedutil "k8s.io/kubernetes/plugin/pkg/scheduler/util"
0000000000000000000000000000000000000000;;		"k8s.io/metrics/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PredicateMetadataModifier: Helper types/variables...
0000000000000000000000000000000000000000;;	type PredicateMetadataModifier func(pm *predicateMetadata)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var predicatePrecomputeRegisterLock sync.Mutex
0000000000000000000000000000000000000000;;	var predicatePrecomputations map[string]PredicateMetadataModifier = make(map[string]PredicateMetadataModifier)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func RegisterPredicatePrecomputation(predicateName string, precomp PredicateMetadataModifier) {
0000000000000000000000000000000000000000;;		predicatePrecomputeRegisterLock.Lock()
0000000000000000000000000000000000000000;;		defer predicatePrecomputeRegisterLock.Unlock()
0000000000000000000000000000000000000000;;		predicatePrecomputations[predicateName] = precomp
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NodeInfo: Other types for predicate functions...
0000000000000000000000000000000000000000;;	type NodeInfo interface {
0000000000000000000000000000000000000000;;		GetNodeInfo(nodeID string) (*v1.Node, error)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type PersistentVolumeInfo interface {
0000000000000000000000000000000000000000;;		GetPersistentVolumeInfo(pvID string) (*v1.PersistentVolume, error)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CachedPersistentVolumeInfo implements PersistentVolumeInfo
0000000000000000000000000000000000000000;;	type CachedPersistentVolumeInfo struct {
0000000000000000000000000000000000000000;;		corelisters.PersistentVolumeLister
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *CachedPersistentVolumeInfo) GetPersistentVolumeInfo(pvID string) (*v1.PersistentVolume, error) {
0000000000000000000000000000000000000000;;		return c.Get(pvID)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type PersistentVolumeClaimInfo interface {
0000000000000000000000000000000000000000;;		GetPersistentVolumeClaimInfo(namespace string, name string) (*v1.PersistentVolumeClaim, error)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CachedPersistentVolumeClaimInfo implements PersistentVolumeClaimInfo
0000000000000000000000000000000000000000;;	type CachedPersistentVolumeClaimInfo struct {
0000000000000000000000000000000000000000;;		corelisters.PersistentVolumeClaimLister
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetPersistentVolumeClaimInfo fetches the claim in specified namespace with specified name
0000000000000000000000000000000000000000;;	func (c *CachedPersistentVolumeClaimInfo) GetPersistentVolumeClaimInfo(namespace string, name string) (*v1.PersistentVolumeClaim, error) {
0000000000000000000000000000000000000000;;		return c.PersistentVolumeClaims(namespace).Get(name)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type CachedNodeInfo struct {
0000000000000000000000000000000000000000;;		corelisters.NodeLister
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetNodeInfo returns cached data for the node 'id'.
0000000000000000000000000000000000000000;;	func (c *CachedNodeInfo) GetNodeInfo(id string) (*v1.Node, error) {
0000000000000000000000000000000000000000;;		node, err := c.Get(id)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if apierrors.IsNotFound(err) {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("node '%v' not found", id)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("error retrieving node '%v' from cache: %v", id, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return node, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	//  Note that predicateMetadata and matchingPodAntiAffinityTerm need to be declared in the same file
0000000000000000000000000000000000000000;;	//  due to the way declarations are processed in predicate declaration unit tests.
0000000000000000000000000000000000000000;;	type matchingPodAntiAffinityTerm struct {
0000000000000000000000000000000000000000;;		term *v1.PodAffinityTerm
0000000000000000000000000000000000000000;;		node *v1.Node
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type predicateMetadata struct {
0000000000000000000000000000000000000000;;		pod                                *v1.Pod
0000000000000000000000000000000000000000;;		podBestEffort                      bool
0000000000000000000000000000000000000000;;		podRequest                         *schedulercache.Resource
0000000000000000000000000000000000000000;;		podPorts                           map[int]bool
0000000000000000000000000000000000000000;;		matchingAntiAffinityTerms          []matchingPodAntiAffinityTerm
0000000000000000000000000000000000000000;;		serviceAffinityMatchingPodList     []*v1.Pod
0000000000000000000000000000000000000000;;		serviceAffinityMatchingPodServices []*v1.Service
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func isVolumeConflict(volume v1.Volume, pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		// fast path if there is no conflict checking targets.
0000000000000000000000000000000000000000;;		if volume.GCEPersistentDisk == nil && volume.AWSElasticBlockStore == nil && volume.RBD == nil && volume.ISCSI == nil {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, existingVolume := range pod.Spec.Volumes {
0000000000000000000000000000000000000000;;			// Same GCE disk mounted by multiple pods conflicts unless all pods mount it read-only.
0000000000000000000000000000000000000000;;			if volume.GCEPersistentDisk != nil && existingVolume.GCEPersistentDisk != nil {
0000000000000000000000000000000000000000;;				disk, existingDisk := volume.GCEPersistentDisk, existingVolume.GCEPersistentDisk
0000000000000000000000000000000000000000;;				if disk.PDName == existingDisk.PDName && !(disk.ReadOnly && existingDisk.ReadOnly) {
0000000000000000000000000000000000000000;;					return true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if volume.AWSElasticBlockStore != nil && existingVolume.AWSElasticBlockStore != nil {
0000000000000000000000000000000000000000;;				if volume.AWSElasticBlockStore.VolumeID == existingVolume.AWSElasticBlockStore.VolumeID {
0000000000000000000000000000000000000000;;					return true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if volume.ISCSI != nil && existingVolume.ISCSI != nil {
0000000000000000000000000000000000000000;;				iqn := volume.ISCSI.IQN
0000000000000000000000000000000000000000;;				eiqn := existingVolume.ISCSI.IQN
0000000000000000000000000000000000000000;;				// two ISCSI volumes are same, if they share the same iqn. As iscsi volumes are of type
0000000000000000000000000000000000000000;;				// RWO or ROX, we could permit only one RW mount. Same iscsi volume mounted by multiple Pods
0000000000000000000000000000000000000000;;				// conflict unless all other pods mount as read only.
0000000000000000000000000000000000000000;;				if iqn == eiqn && !(volume.ISCSI.ReadOnly && existingVolume.ISCSI.ReadOnly) {
0000000000000000000000000000000000000000;;					return true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if volume.RBD != nil && existingVolume.RBD != nil {
0000000000000000000000000000000000000000;;				mon, pool, image := volume.RBD.CephMonitors, volume.RBD.RBDPool, volume.RBD.RBDImage
0000000000000000000000000000000000000000;;				emon, epool, eimage := existingVolume.RBD.CephMonitors, existingVolume.RBD.RBDPool, existingVolume.RBD.RBDImage
0000000000000000000000000000000000000000;;				// two RBDs images are the same if they share the same Ceph monitor, are in the same RADOS Pool, and have the same image name
0000000000000000000000000000000000000000;;				// only one read-write mount is permitted for the same RBD image.
0000000000000000000000000000000000000000;;				// same RBD image mounted by multiple Pods conflicts unless all Pods mount the image read-only
0000000000000000000000000000000000000000;;				if haveSame(mon, emon) && pool == epool && image == eimage && !(volume.RBD.ReadOnly && existingVolume.RBD.ReadOnly) {
0000000000000000000000000000000000000000;;					return true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NoDiskConflict evaluates if a pod can fit due to the volumes it requests, and those that
0000000000000000000000000000000000000000;;	// are already mounted. If there is already a volume mounted on that node, another pod that uses the same volume
0000000000000000000000000000000000000000;;	// can't be scheduled there.
0000000000000000000000000000000000000000;;	// This is GCE, Amazon EBS, and Ceph RBD specific for now:
0000000000000000000000000000000000000000;;	// - GCE PD allows multiple mounts as long as they're all read-only
0000000000000000000000000000000000000000;;	// - AWS EBS forbids any two pods mounting the same volume ID
0000000000000000000000000000000000000000;;	// - Ceph RBD forbids if any two pods share at least same monitor, and match pool and image.
0000000000000000000000000000000000000000;;	// - ISCSI forbids if any two pods share at least same IQN, LUN and Target
0000000000000000000000000000000000000000;;	// TODO: migrate this into some per-volume specific code?
0000000000000000000000000000000000000000;;	func NoDiskConflict(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		for _, v := range pod.Spec.Volumes {
0000000000000000000000000000000000000000;;			for _, ev := range nodeInfo.Pods() {
0000000000000000000000000000000000000000;;				if isVolumeConflict(v, ev) {
0000000000000000000000000000000000000000;;					return false, []algorithm.PredicateFailureReason{ErrDiskConflict}, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type MaxPDVolumeCountChecker struct {
0000000000000000000000000000000000000000;;		filter     VolumeFilter
0000000000000000000000000000000000000000;;		maxVolumes int
0000000000000000000000000000000000000000;;		pvInfo     PersistentVolumeInfo
0000000000000000000000000000000000000000;;		pvcInfo    PersistentVolumeClaimInfo
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// VolumeFilter contains information on how to filter PD Volumes when checking PD Volume caps
0000000000000000000000000000000000000000;;	type VolumeFilter struct {
0000000000000000000000000000000000000000;;		// Filter normal volumes
0000000000000000000000000000000000000000;;		FilterVolume           func(vol *v1.Volume) (id string, relevant bool)
0000000000000000000000000000000000000000;;		FilterPersistentVolume func(pv *v1.PersistentVolume) (id string, relevant bool)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewMaxPDVolumeCountPredicate creates a predicate which evaluates whether a pod can fit based on the
0000000000000000000000000000000000000000;;	// number of volumes which match a filter that it requests, and those that are already present.  The
0000000000000000000000000000000000000000;;	// maximum number is configurable to accommodate different systems.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// The predicate looks for both volumes used directly, as well as PVC volumes that are backed by relevant volume
0000000000000000000000000000000000000000;;	// types, counts the number of unique volumes, and rejects the new pod if it would place the total count over
0000000000000000000000000000000000000000;;	// the maximum.
0000000000000000000000000000000000000000;;	func NewMaxPDVolumeCountPredicate(filter VolumeFilter, maxVolumes int, pvInfo PersistentVolumeInfo, pvcInfo PersistentVolumeClaimInfo) algorithm.FitPredicate {
0000000000000000000000000000000000000000;;		c := &MaxPDVolumeCountChecker{
0000000000000000000000000000000000000000;;			filter:     filter,
0000000000000000000000000000000000000000;;			maxVolumes: maxVolumes,
0000000000000000000000000000000000000000;;			pvInfo:     pvInfo,
0000000000000000000000000000000000000000;;			pvcInfo:    pvcInfo,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return c.predicate
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *MaxPDVolumeCountChecker) filterVolumes(volumes []v1.Volume, namespace string, filteredVolumes map[string]bool) error {
0000000000000000000000000000000000000000;;		for i := range volumes {
0000000000000000000000000000000000000000;;			vol := &volumes[i]
0000000000000000000000000000000000000000;;			if id, ok := c.filter.FilterVolume(vol); ok {
0000000000000000000000000000000000000000;;				filteredVolumes[id] = true
0000000000000000000000000000000000000000;;			} else if vol.PersistentVolumeClaim != nil {
0000000000000000000000000000000000000000;;				pvcName := vol.PersistentVolumeClaim.ClaimName
0000000000000000000000000000000000000000;;				if pvcName == "" {
0000000000000000000000000000000000000000;;					return fmt.Errorf("PersistentVolumeClaim had no name")
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				pvc, err := c.pvcInfo.GetPersistentVolumeClaimInfo(namespace, pvcName)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					// if the PVC is not found, log the error and count the PV towards the PV limit
0000000000000000000000000000000000000000;;					// generate a random volume ID since its required for de-dup
0000000000000000000000000000000000000000;;					utilruntime.HandleError(fmt.Errorf("Unable to look up PVC info for %s/%s, assuming PVC matches predicate when counting limits: %v", namespace, pvcName, err))
0000000000000000000000000000000000000000;;					source := rand.NewSource(time.Now().UnixNano())
0000000000000000000000000000000000000000;;					generatedID := "missingPVC" + strconv.Itoa(rand.New(source).Intn(1000000))
0000000000000000000000000000000000000000;;					filteredVolumes[generatedID] = true
0000000000000000000000000000000000000000;;					return nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if pvc == nil {
0000000000000000000000000000000000000000;;					return fmt.Errorf("PersistentVolumeClaim not found: %q", pvcName)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				pvName := pvc.Spec.VolumeName
0000000000000000000000000000000000000000;;				if pvName == "" {
0000000000000000000000000000000000000000;;					return fmt.Errorf("PersistentVolumeClaim is not bound: %q", pvcName)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				pv, err := c.pvInfo.GetPersistentVolumeInfo(pvName)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					// if the PV is not found, log the error
0000000000000000000000000000000000000000;;					// and count the PV towards the PV limit
0000000000000000000000000000000000000000;;					// generate a random volume ID since it is required for de-dup
0000000000000000000000000000000000000000;;					utilruntime.HandleError(fmt.Errorf("Unable to look up PV info for %s/%s/%s, assuming PV matches predicate when counting limits: %v", namespace, pvcName, pvName, err))
0000000000000000000000000000000000000000;;					source := rand.NewSource(time.Now().UnixNano())
0000000000000000000000000000000000000000;;					generatedID := "missingPV" + strconv.Itoa(rand.New(source).Intn(1000000))
0000000000000000000000000000000000000000;;					filteredVolumes[generatedID] = true
0000000000000000000000000000000000000000;;					return nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if pv == nil {
0000000000000000000000000000000000000000;;					return fmt.Errorf("PersistentVolume not found: %q", pvName)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if id, ok := c.filter.FilterPersistentVolume(pv); ok {
0000000000000000000000000000000000000000;;					filteredVolumes[id] = true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *MaxPDVolumeCountChecker) predicate(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		// If a pod doesn't have any volume attached to it, the predicate will always be true.
0000000000000000000000000000000000000000;;		// Thus we make a fast path for it, to avoid unnecessary computations in this case.
0000000000000000000000000000000000000000;;		if len(pod.Spec.Volumes) == 0 {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newVolumes := make(map[string]bool)
0000000000000000000000000000000000000000;;		if err := c.filterVolumes(pod.Spec.Volumes, pod.Namespace, newVolumes); err != nil {
0000000000000000000000000000000000000000;;			return false, nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// quick return
0000000000000000000000000000000000000000;;		if len(newVolumes) == 0 {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// count unique volumes
0000000000000000000000000000000000000000;;		existingVolumes := make(map[string]bool)
0000000000000000000000000000000000000000;;		for _, existingPod := range nodeInfo.Pods() {
0000000000000000000000000000000000000000;;			if err := c.filterVolumes(existingPod.Spec.Volumes, existingPod.Namespace, existingVolumes); err != nil {
0000000000000000000000000000000000000000;;				return false, nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		numExistingVolumes := len(existingVolumes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// filter out already-mounted volumes
0000000000000000000000000000000000000000;;		for k := range existingVolumes {
0000000000000000000000000000000000000000;;			if _, ok := newVolumes[k]; ok {
0000000000000000000000000000000000000000;;				delete(newVolumes, k)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		numNewVolumes := len(newVolumes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if numExistingVolumes+numNewVolumes > c.maxVolumes {
0000000000000000000000000000000000000000;;			// violates MaxEBSVolumeCount or MaxGCEPDVolumeCount
0000000000000000000000000000000000000000;;			return false, []algorithm.PredicateFailureReason{ErrMaxVolumeCountExceeded}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// EBSVolumeFilter is a VolumeFilter for filtering AWS ElasticBlockStore Volumes
0000000000000000000000000000000000000000;;	var EBSVolumeFilter VolumeFilter = VolumeFilter{
0000000000000000000000000000000000000000;;		FilterVolume: func(vol *v1.Volume) (string, bool) {
0000000000000000000000000000000000000000;;			if vol.AWSElasticBlockStore != nil {
0000000000000000000000000000000000000000;;				return vol.AWSElasticBlockStore.VolumeID, true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return "", false
0000000000000000000000000000000000000000;;		},
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		FilterPersistentVolume: func(pv *v1.PersistentVolume) (string, bool) {
0000000000000000000000000000000000000000;;			if pv.Spec.AWSElasticBlockStore != nil {
0000000000000000000000000000000000000000;;				return pv.Spec.AWSElasticBlockStore.VolumeID, true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return "", false
0000000000000000000000000000000000000000;;		},
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GCEPDVolumeFilter is a VolumeFilter for filtering GCE PersistentDisk Volumes
0000000000000000000000000000000000000000;;	var GCEPDVolumeFilter VolumeFilter = VolumeFilter{
0000000000000000000000000000000000000000;;		FilterVolume: func(vol *v1.Volume) (string, bool) {
0000000000000000000000000000000000000000;;			if vol.GCEPersistentDisk != nil {
0000000000000000000000000000000000000000;;				return vol.GCEPersistentDisk.PDName, true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return "", false
0000000000000000000000000000000000000000;;		},
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		FilterPersistentVolume: func(pv *v1.PersistentVolume) (string, bool) {
0000000000000000000000000000000000000000;;			if pv.Spec.GCEPersistentDisk != nil {
0000000000000000000000000000000000000000;;				return pv.Spec.GCEPersistentDisk.PDName, true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return "", false
0000000000000000000000000000000000000000;;		},
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// AzureDiskVolumeFilter is a VolumeFilter for filtering Azure Disk Volumes
0000000000000000000000000000000000000000;;	var AzureDiskVolumeFilter VolumeFilter = VolumeFilter{
0000000000000000000000000000000000000000;;		FilterVolume: func(vol *v1.Volume) (string, bool) {
0000000000000000000000000000000000000000;;			if vol.AzureDisk != nil {
0000000000000000000000000000000000000000;;				return vol.AzureDisk.DiskName, true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return "", false
0000000000000000000000000000000000000000;;		},
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		FilterPersistentVolume: func(pv *v1.PersistentVolume) (string, bool) {
0000000000000000000000000000000000000000;;			if pv.Spec.AzureDisk != nil {
0000000000000000000000000000000000000000;;				return pv.Spec.AzureDisk.DiskName, true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return "", false
0000000000000000000000000000000000000000;;		},
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type VolumeZoneChecker struct {
0000000000000000000000000000000000000000;;		pvInfo  PersistentVolumeInfo
0000000000000000000000000000000000000000;;		pvcInfo PersistentVolumeClaimInfo
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewVolumeZonePredicate evaluates if a pod can fit due to the volumes it requests, given
0000000000000000000000000000000000000000;;	// that some volumes may have zone scheduling constraints.  The requirement is that any
0000000000000000000000000000000000000000;;	// volume zone-labels must match the equivalent zone-labels on the node.  It is OK for
0000000000000000000000000000000000000000;;	// the node to have more zone-label constraints (for example, a hypothetical replicated
0000000000000000000000000000000000000000;;	// volume might allow region-wide access)
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Currently this is only supported with PersistentVolumeClaims, and looks to the labels
0000000000000000000000000000000000000000;;	// only on the bound PersistentVolume.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Working with volumes declared inline in the pod specification (i.e. not
0000000000000000000000000000000000000000;;	// using a PersistentVolume) is likely to be harder, as it would require
0000000000000000000000000000000000000000;;	// determining the zone of a volume during scheduling, and that is likely to
0000000000000000000000000000000000000000;;	// require calling out to the cloud provider.  It seems that we are moving away
0000000000000000000000000000000000000000;;	// from inline volume declarations anyway.
0000000000000000000000000000000000000000;;	func NewVolumeZonePredicate(pvInfo PersistentVolumeInfo, pvcInfo PersistentVolumeClaimInfo) algorithm.FitPredicate {
0000000000000000000000000000000000000000;;		c := &VolumeZoneChecker{
0000000000000000000000000000000000000000;;			pvInfo:  pvInfo,
0000000000000000000000000000000000000000;;			pvcInfo: pvcInfo,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return c.predicate
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *VolumeZoneChecker) predicate(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		// If a pod doesn't have any volume attached to it, the predicate will always be true.
0000000000000000000000000000000000000000;;		// Thus we make a fast path for it, to avoid unnecessary computations in this case.
0000000000000000000000000000000000000000;;		if len(pod.Spec.Volumes) == 0 {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		node := nodeInfo.Node()
0000000000000000000000000000000000000000;;		if node == nil {
0000000000000000000000000000000000000000;;			return false, nil, fmt.Errorf("node not found")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nodeConstraints := make(map[string]string)
0000000000000000000000000000000000000000;;		for k, v := range node.ObjectMeta.Labels {
0000000000000000000000000000000000000000;;			if k != kubeletapis.LabelZoneFailureDomain && k != kubeletapis.LabelZoneRegion {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			nodeConstraints[k] = v
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(nodeConstraints) == 0 {
0000000000000000000000000000000000000000;;			// The node has no zone constraints, so we're OK to schedule.
0000000000000000000000000000000000000000;;			// In practice, when using zones, all nodes must be labeled with zone labels.
0000000000000000000000000000000000000000;;			// We want to fast-path this case though.
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		namespace := pod.Namespace
0000000000000000000000000000000000000000;;		manifest := &(pod.Spec)
0000000000000000000000000000000000000000;;		for i := range manifest.Volumes {
0000000000000000000000000000000000000000;;			volume := &manifest.Volumes[i]
0000000000000000000000000000000000000000;;			if volume.PersistentVolumeClaim != nil {
0000000000000000000000000000000000000000;;				pvcName := volume.PersistentVolumeClaim.ClaimName
0000000000000000000000000000000000000000;;				if pvcName == "" {
0000000000000000000000000000000000000000;;					return false, nil, fmt.Errorf("PersistentVolumeClaim had no name")
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				pvc, err := c.pvcInfo.GetPersistentVolumeClaimInfo(namespace, pvcName)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return false, nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if pvc == nil {
0000000000000000000000000000000000000000;;					return false, nil, fmt.Errorf("PersistentVolumeClaim was not found: %q", pvcName)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				pvName := pvc.Spec.VolumeName
0000000000000000000000000000000000000000;;				if pvName == "" {
0000000000000000000000000000000000000000;;					return false, nil, fmt.Errorf("PersistentVolumeClaim is not bound: %q", pvcName)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				pv, err := c.pvInfo.GetPersistentVolumeInfo(pvName)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return false, nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if pv == nil {
0000000000000000000000000000000000000000;;					return false, nil, fmt.Errorf("PersistentVolume not found: %q", pvName)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				for k, v := range pv.ObjectMeta.Labels {
0000000000000000000000000000000000000000;;					if k != kubeletapis.LabelZoneFailureDomain && k != kubeletapis.LabelZoneRegion {
0000000000000000000000000000000000000000;;						continue
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					nodeV, _ := nodeConstraints[k]
0000000000000000000000000000000000000000;;					if v != nodeV {
0000000000000000000000000000000000000000;;						glog.V(10).Infof("Won't schedule pod %q onto node %q due to volume %q (mismatch on %q)", pod.Name, node.Name, pvName, k)
0000000000000000000000000000000000000000;;						return false, []algorithm.PredicateFailureReason{ErrVolumeZoneConflict}, nil
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetResourceRequest returns a *schedulercache.Resource that covers the largest
0000000000000000000000000000000000000000;;	// width in each resource dimension. Because init-containers run sequentially, we collect
0000000000000000000000000000000000000000;;	// the max in each dimension iteratively. In contrast, we sum the resource vectors for
0000000000000000000000000000000000000000;;	// regular containers since they run simultaneously.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Example:
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Pod:
0000000000000000000000000000000000000000;;	//   InitContainers
0000000000000000000000000000000000000000;;	//     IC1:
0000000000000000000000000000000000000000;;	//       CPU: 2
0000000000000000000000000000000000000000;;	//       Memory: 1G
0000000000000000000000000000000000000000;;	//     IC2:
0000000000000000000000000000000000000000;;	//       CPU: 2
0000000000000000000000000000000000000000;;	//       Memory: 3G
0000000000000000000000000000000000000000;;	//   Containers
0000000000000000000000000000000000000000;;	//     C1:
0000000000000000000000000000000000000000;;	//       CPU: 2
0000000000000000000000000000000000000000;;	//       Memory: 1G
0000000000000000000000000000000000000000;;	//     C2:
0000000000000000000000000000000000000000;;	//       CPU: 1
0000000000000000000000000000000000000000;;	//       Memory: 1G
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Result: CPU: 3, Memory: 3G
0000000000000000000000000000000000000000;;	func GetResourceRequest(pod *v1.Pod) *schedulercache.Resource {
0000000000000000000000000000000000000000;;		result := &schedulercache.Resource{}
0000000000000000000000000000000000000000;;		for _, container := range pod.Spec.Containers {
0000000000000000000000000000000000000000;;			result.Add(container.Resources.Requests)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Account for storage requested by emptydir volumes
0000000000000000000000000000000000000000;;		// If the storage medium is memory, should exclude the size
0000000000000000000000000000000000000000;;		for _, vol := range pod.Spec.Volumes {
0000000000000000000000000000000000000000;;			if vol.EmptyDir != nil && vol.EmptyDir.Medium != v1.StorageMediumMemory {
0000000000000000000000000000000000000000;;				result.StorageScratch += vol.EmptyDir.SizeLimit.Value()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// take max_resource(sum_pod, any_init_container)
0000000000000000000000000000000000000000;;		for _, container := range pod.Spec.InitContainers {
0000000000000000000000000000000000000000;;			for rName, rQuantity := range container.Resources.Requests {
0000000000000000000000000000000000000000;;				switch rName {
0000000000000000000000000000000000000000;;				case v1.ResourceMemory:
0000000000000000000000000000000000000000;;					if mem := rQuantity.Value(); mem > result.Memory {
0000000000000000000000000000000000000000;;						result.Memory = mem
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				case v1.ResourceCPU:
0000000000000000000000000000000000000000;;					if cpu := rQuantity.MilliValue(); cpu > result.MilliCPU {
0000000000000000000000000000000000000000;;						result.MilliCPU = cpu
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				case v1.ResourceNvidiaGPU:
0000000000000000000000000000000000000000;;					if gpu := rQuantity.Value(); gpu > result.NvidiaGPU {
0000000000000000000000000000000000000000;;						result.NvidiaGPU = gpu
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				case v1.ResourceStorageOverlay:
0000000000000000000000000000000000000000;;					if overlay := rQuantity.Value(); overlay > result.StorageOverlay {
0000000000000000000000000000000000000000;;						result.StorageOverlay = overlay
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				default:
0000000000000000000000000000000000000000;;					if v1helper.IsOpaqueIntResourceName(rName) {
0000000000000000000000000000000000000000;;						value := rQuantity.Value()
0000000000000000000000000000000000000000;;						if value > result.OpaqueIntResources[rName] {
0000000000000000000000000000000000000000;;							result.SetOpaque(rName, value)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return result
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podName(pod *v1.Pod) string {
0000000000000000000000000000000000000000;;		return pod.Namespace + "/" + pod.Name
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodFitsResources checks if a node has sufficient resources, such as cpu, memory, gpu, opaque int resources etc to run a pod.
0000000000000000000000000000000000000000;;	// First return value indicates whether a node has sufficient resources to run a pod while the second return value indicates the
0000000000000000000000000000000000000000;;	// predicate failure reasons if the node has insufficient resources to run the pod.
0000000000000000000000000000000000000000;;	func PodFitsResources(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		node := nodeInfo.Node()
0000000000000000000000000000000000000000;;		if node == nil {
0000000000000000000000000000000000000000;;			return false, nil, fmt.Errorf("node not found")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var predicateFails []algorithm.PredicateFailureReason
0000000000000000000000000000000000000000;;		allowedPodNumber := nodeInfo.AllowedPodNumber()
0000000000000000000000000000000000000000;;		if len(nodeInfo.Pods())+1 > allowedPodNumber {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourcePods, 1, int64(len(nodeInfo.Pods())), int64(allowedPodNumber)))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var podRequest *schedulercache.Resource
0000000000000000000000000000000000000000;;		if predicateMeta, ok := meta.(*predicateMetadata); ok {
0000000000000000000000000000000000000000;;			podRequest = predicateMeta.podRequest
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// We couldn't parse metadata - fallback to computing it.
0000000000000000000000000000000000000000;;			podRequest = GetResourceRequest(pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if podRequest.MilliCPU == 0 && podRequest.Memory == 0 && podRequest.NvidiaGPU == 0 && podRequest.StorageOverlay == 0 && podRequest.StorageScratch == 0 && len(podRequest.OpaqueIntResources) == 0 {
0000000000000000000000000000000000000000;;			return len(predicateFails) == 0, predicateFails, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		allocatable := nodeInfo.AllocatableResource()
0000000000000000000000000000000000000000;;		if allocatable.MilliCPU < podRequest.MilliCPU+nodeInfo.RequestedResource().MilliCPU {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceCPU, podRequest.MilliCPU, nodeInfo.RequestedResource().MilliCPU, allocatable.MilliCPU))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if allocatable.Memory < podRequest.Memory+nodeInfo.RequestedResource().Memory {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceMemory, podRequest.Memory, nodeInfo.RequestedResource().Memory, allocatable.Memory))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if allocatable.NvidiaGPU < podRequest.NvidiaGPU+nodeInfo.RequestedResource().NvidiaGPU {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceNvidiaGPU, podRequest.NvidiaGPU, nodeInfo.RequestedResource().NvidiaGPU, allocatable.NvidiaGPU))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		scratchSpaceRequest := podRequest.StorageScratch
0000000000000000000000000000000000000000;;		if allocatable.StorageOverlay == 0 {
0000000000000000000000000000000000000000;;			scratchSpaceRequest += podRequest.StorageOverlay
0000000000000000000000000000000000000000;;			//scratchSpaceRequest += nodeInfo.RequestedResource().StorageOverlay
0000000000000000000000000000000000000000;;			nodeScratchRequest := nodeInfo.RequestedResource().StorageOverlay + nodeInfo.RequestedResource().StorageScratch
0000000000000000000000000000000000000000;;			if allocatable.StorageScratch < scratchSpaceRequest+nodeScratchRequest {
0000000000000000000000000000000000000000;;				predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceStorageScratch, scratchSpaceRequest, nodeScratchRequest, allocatable.StorageScratch))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		} else if allocatable.StorageScratch < scratchSpaceRequest+nodeInfo.RequestedResource().StorageScratch {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceStorageScratch, scratchSpaceRequest, nodeInfo.RequestedResource().StorageScratch, allocatable.StorageScratch))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if allocatable.StorageOverlay > 0 && allocatable.StorageOverlay < podRequest.StorageOverlay+nodeInfo.RequestedResource().StorageOverlay {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourceStorageOverlay, podRequest.StorageOverlay, nodeInfo.RequestedResource().StorageOverlay, allocatable.StorageOverlay))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for rName, rQuant := range podRequest.OpaqueIntResources {
0000000000000000000000000000000000000000;;			if allocatable.OpaqueIntResources[rName] < rQuant+nodeInfo.RequestedResource().OpaqueIntResources[rName] {
0000000000000000000000000000000000000000;;				predicateFails = append(predicateFails, NewInsufficientResourceError(rName, podRequest.OpaqueIntResources[rName], nodeInfo.RequestedResource().OpaqueIntResources[rName], allocatable.OpaqueIntResources[rName]))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if glog.V(10) {
0000000000000000000000000000000000000000;;			if len(predicateFails) == 0 {
0000000000000000000000000000000000000000;;				// We explicitly don't do glog.V(10).Infof() to avoid computing all the parameters if this is
0000000000000000000000000000000000000000;;				// not logged. There is visible performance gain from it.
0000000000000000000000000000000000000000;;				glog.Infof("Schedule Pod %+v on Node %+v is allowed, Node is running only %v out of %v Pods.",
0000000000000000000000000000000000000000;;					podName(pod), node.Name, len(nodeInfo.Pods()), allowedPodNumber)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return len(predicateFails) == 0, predicateFails, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// nodeMatchesNodeSelectorTerms checks if a node's labels satisfy a list of node selector terms,
0000000000000000000000000000000000000000;;	// terms are ORed, and an empty list of terms will match nothing.
0000000000000000000000000000000000000000;;	func nodeMatchesNodeSelectorTerms(node *v1.Node, nodeSelectorTerms []v1.NodeSelectorTerm) bool {
0000000000000000000000000000000000000000;;		for _, req := range nodeSelectorTerms {
0000000000000000000000000000000000000000;;			nodeSelector, err := v1helper.NodeSelectorRequirementsAsSelector(req.MatchExpressions)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.V(10).Infof("Failed to parse MatchExpressions: %+v, regarding as not match.", req.MatchExpressions)
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if nodeSelector.Matches(labels.Set(node.Labels)) {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// The pod can only schedule onto nodes that satisfy requirements in both NodeAffinity and nodeSelector.
0000000000000000000000000000000000000000;;	func podMatchesNodeLabels(pod *v1.Pod, node *v1.Node) bool {
0000000000000000000000000000000000000000;;		// Check if node.Labels match pod.Spec.NodeSelector.
0000000000000000000000000000000000000000;;		if len(pod.Spec.NodeSelector) > 0 {
0000000000000000000000000000000000000000;;			selector := labels.SelectorFromSet(pod.Spec.NodeSelector)
0000000000000000000000000000000000000000;;			if !selector.Matches(labels.Set(node.Labels)) {
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 1. nil NodeSelector matches all nodes (i.e. does not filter out any nodes)
0000000000000000000000000000000000000000;;		// 2. nil []NodeSelectorTerm (equivalent to non-nil empty NodeSelector) matches no nodes
0000000000000000000000000000000000000000;;		// 3. zero-length non-nil []NodeSelectorTerm matches no nodes also, just for simplicity
0000000000000000000000000000000000000000;;		// 4. nil []NodeSelectorRequirement (equivalent to non-nil empty NodeSelectorTerm) matches no nodes
0000000000000000000000000000000000000000;;		// 5. zero-length non-nil []NodeSelectorRequirement matches no nodes also, just for simplicity
0000000000000000000000000000000000000000;;		// 6. non-nil empty NodeSelectorRequirement is not allowed
0000000000000000000000000000000000000000;;		nodeAffinityMatches := true
0000000000000000000000000000000000000000;;		affinity := pod.Spec.Affinity
0000000000000000000000000000000000000000;;		if affinity != nil && affinity.NodeAffinity != nil {
0000000000000000000000000000000000000000;;			nodeAffinity := affinity.NodeAffinity
0000000000000000000000000000000000000000;;			// if no required NodeAffinity requirements, will do no-op, means select all nodes.
0000000000000000000000000000000000000000;;			// TODO: Replace next line with subsequent commented-out line when implement RequiredDuringSchedulingRequiredDuringExecution.
0000000000000000000000000000000000000000;;			if nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution == nil {
0000000000000000000000000000000000000000;;				// if nodeAffinity.RequiredDuringSchedulingRequiredDuringExecution == nil && nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution == nil {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Match node selector for requiredDuringSchedulingRequiredDuringExecution.
0000000000000000000000000000000000000000;;			// TODO: Uncomment this block when implement RequiredDuringSchedulingRequiredDuringExecution.
0000000000000000000000000000000000000000;;			// if nodeAffinity.RequiredDuringSchedulingRequiredDuringExecution != nil {
0000000000000000000000000000000000000000;;			// 	nodeSelectorTerms := nodeAffinity.RequiredDuringSchedulingRequiredDuringExecution.NodeSelectorTerms
0000000000000000000000000000000000000000;;			// 	glog.V(10).Infof("Match for RequiredDuringSchedulingRequiredDuringExecution node selector terms %+v", nodeSelectorTerms)
0000000000000000000000000000000000000000;;			// 	nodeAffinityMatches = nodeMatchesNodeSelectorTerms(node, nodeSelectorTerms)
0000000000000000000000000000000000000000;;			// }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Match node selector for requiredDuringSchedulingIgnoredDuringExecution.
0000000000000000000000000000000000000000;;			if nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution != nil {
0000000000000000000000000000000000000000;;				nodeSelectorTerms := nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms
0000000000000000000000000000000000000000;;				glog.V(10).Infof("Match for RequiredDuringSchedulingIgnoredDuringExecution node selector terms %+v", nodeSelectorTerms)
0000000000000000000000000000000000000000;;				nodeAffinityMatches = nodeAffinityMatches && nodeMatchesNodeSelectorTerms(node, nodeSelectorTerms)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nodeAffinityMatches
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodMatchNodeSelector checks if a pod node selector matches the node label.
0000000000000000000000000000000000000000;;	func PodMatchNodeSelector(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		node := nodeInfo.Node()
0000000000000000000000000000000000000000;;		if node == nil {
0000000000000000000000000000000000000000;;			return false, nil, fmt.Errorf("node not found")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if podMatchesNodeLabels(pod, node) {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false, []algorithm.PredicateFailureReason{ErrNodeSelectorNotMatch}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodFitsHost checks if a pod spec node name matches the current node.
0000000000000000000000000000000000000000;;	func PodFitsHost(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		if len(pod.Spec.NodeName) == 0 {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		node := nodeInfo.Node()
0000000000000000000000000000000000000000;;		if node == nil {
0000000000000000000000000000000000000000;;			return false, nil, fmt.Errorf("node not found")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if pod.Spec.NodeName == node.Name {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false, []algorithm.PredicateFailureReason{ErrPodNotMatchHostName}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type NodeLabelChecker struct {
0000000000000000000000000000000000000000;;		labels   []string
0000000000000000000000000000000000000000;;		presence bool
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewNodeLabelPredicate(labels []string, presence bool) algorithm.FitPredicate {
0000000000000000000000000000000000000000;;		labelChecker := &NodeLabelChecker{
0000000000000000000000000000000000000000;;			labels:   labels,
0000000000000000000000000000000000000000;;			presence: presence,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return labelChecker.CheckNodeLabelPresence
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CheckNodeLabelPresence checks whether all of the specified labels exists on a node or not, regardless of their value
0000000000000000000000000000000000000000;;	// If "presence" is false, then returns false if any of the requested labels matches any of the node's labels,
0000000000000000000000000000000000000000;;	// otherwise returns true.
0000000000000000000000000000000000000000;;	// If "presence" is true, then returns false if any of the requested labels does not match any of the node's labels,
0000000000000000000000000000000000000000;;	// otherwise returns true.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Consider the cases where the nodes are placed in regions/zones/racks and these are identified by labels
0000000000000000000000000000000000000000;;	// In some cases, it is required that only nodes that are part of ANY of the defined regions/zones/racks be selected
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Alternately, eliminating nodes that have a certain label, regardless of value, is also useful
0000000000000000000000000000000000000000;;	// A node may have a label with "retiring" as key and the date as the value
0000000000000000000000000000000000000000;;	// and it may be desirable to avoid scheduling new pods on this node
0000000000000000000000000000000000000000;;	func (n *NodeLabelChecker) CheckNodeLabelPresence(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		node := nodeInfo.Node()
0000000000000000000000000000000000000000;;		if node == nil {
0000000000000000000000000000000000000000;;			return false, nil, fmt.Errorf("node not found")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var exists bool
0000000000000000000000000000000000000000;;		nodeLabels := labels.Set(node.Labels)
0000000000000000000000000000000000000000;;		for _, label := range n.labels {
0000000000000000000000000000000000000000;;			exists = nodeLabels.Has(label)
0000000000000000000000000000000000000000;;			if (exists && !n.presence) || (!exists && n.presence) {
0000000000000000000000000000000000000000;;				return false, []algorithm.PredicateFailureReason{ErrNodeLabelPresenceViolated}, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type ServiceAffinity struct {
0000000000000000000000000000000000000000;;		podLister     algorithm.PodLister
0000000000000000000000000000000000000000;;		serviceLister algorithm.ServiceLister
0000000000000000000000000000000000000000;;		nodeInfo      NodeInfo
0000000000000000000000000000000000000000;;		labels        []string
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// serviceAffinityPrecomputation should be run once by the scheduler before looping through the Predicate.  It is a helper function that
0000000000000000000000000000000000000000;;	// only should be referenced by NewServiceAffinityPredicate.
0000000000000000000000000000000000000000;;	func (s *ServiceAffinity) serviceAffinityPrecomputation(pm *predicateMetadata) {
0000000000000000000000000000000000000000;;		if pm.pod == nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Cannot precompute service affinity, a pod is required to calculate service affinity.")
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var errSvc, errList error
0000000000000000000000000000000000000000;;		// Store services which match the pod.
0000000000000000000000000000000000000000;;		pm.serviceAffinityMatchingPodServices, errSvc = s.serviceLister.GetPodServices(pm.pod)
0000000000000000000000000000000000000000;;		selector := CreateSelectorFromLabels(pm.pod.Labels)
0000000000000000000000000000000000000000;;		// consider only the pods that belong to the same namespace
0000000000000000000000000000000000000000;;		allMatches, errList := s.podLister.List(selector)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// In the future maybe we will return them as part of the function.
0000000000000000000000000000000000000000;;		if errSvc != nil || errList != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Some Error were found while precomputing svc affinity: \nservices:%v , \npods:%v", errSvc, errList)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pm.serviceAffinityMatchingPodList = FilterPodsByNamespace(allMatches, pm.pod.Namespace)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewServiceAffinityPredicate(podLister algorithm.PodLister, serviceLister algorithm.ServiceLister, nodeInfo NodeInfo, labels []string) (algorithm.FitPredicate, PredicateMetadataModifier) {
0000000000000000000000000000000000000000;;		affinity := &ServiceAffinity{
0000000000000000000000000000000000000000;;			podLister:     podLister,
0000000000000000000000000000000000000000;;			serviceLister: serviceLister,
0000000000000000000000000000000000000000;;			nodeInfo:      nodeInfo,
0000000000000000000000000000000000000000;;			labels:        labels,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return affinity.checkServiceAffinity, affinity.serviceAffinityPrecomputation
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// checkServiceAffinity is a predicate which matches nodes in such a way to force that
0000000000000000000000000000000000000000;;	// ServiceAffinity.labels are homogenous for pods that are scheduled to a node.
0000000000000000000000000000000000000000;;	// (i.e. it returns true IFF this pod can be added to this node such that all other pods in
0000000000000000000000000000000000000000;;	// the same service are running on nodes with
0000000000000000000000000000000000000000;;	// the exact same ServiceAffinity.label values).
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// For example:
0000000000000000000000000000000000000000;;	// If the first pod of a service was scheduled to a node with label "region=foo",
0000000000000000000000000000000000000000;;	// all the other subsequent pods belong to the same service will be schedule on
0000000000000000000000000000000000000000;;	// nodes with the same "region=foo" label.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Details:
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// If (the svc affinity labels are not a subset of pod's label selectors )
0000000000000000000000000000000000000000;;	// 	The pod has all information necessary to check affinity, the pod's label selector is sufficient to calculate
0000000000000000000000000000000000000000;;	// 	the match.
0000000000000000000000000000000000000000;;	// Otherwise:
0000000000000000000000000000000000000000;;	// 	Create an "implicit selector" which guarantees pods will land on nodes with similar values
0000000000000000000000000000000000000000;;	// 	for the affinity labels.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// 	To do this, we "reverse engineer" a selector by introspecting existing pods running under the same service+namespace.
0000000000000000000000000000000000000000;;	//	These backfilled labels in the selector "L" are defined like so:
0000000000000000000000000000000000000000;;	// 		- L is a label that the ServiceAffinity object needs as a matching constraints.
0000000000000000000000000000000000000000;;	// 		- L is not defined in the pod itself already.
0000000000000000000000000000000000000000;;	// 		- and SOME pod, from a service, in the same namespace, ALREADY scheduled onto a node, has a matching value.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// WARNING: This Predicate is NOT guaranteed to work if some of the predicateMetadata data isn't precomputed...
0000000000000000000000000000000000000000;;	// For that reason it is not exported, i.e. it is highly coupled to the implementation of the FitPredicate construction.
0000000000000000000000000000000000000000;;	func (s *ServiceAffinity) checkServiceAffinity(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		var services []*v1.Service
0000000000000000000000000000000000000000;;		var pods []*v1.Pod
0000000000000000000000000000000000000000;;		if pm, ok := meta.(*predicateMetadata); ok && (pm.serviceAffinityMatchingPodList != nil || pm.serviceAffinityMatchingPodServices != nil) {
0000000000000000000000000000000000000000;;			services = pm.serviceAffinityMatchingPodServices
0000000000000000000000000000000000000000;;			pods = pm.serviceAffinityMatchingPodList
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// Make the predicate resilient in case metadata is missing.
0000000000000000000000000000000000000000;;			pm = &predicateMetadata{pod: pod}
0000000000000000000000000000000000000000;;			s.serviceAffinityPrecomputation(pm)
0000000000000000000000000000000000000000;;			pods, services = pm.serviceAffinityMatchingPodList, pm.serviceAffinityMatchingPodServices
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		node := nodeInfo.Node()
0000000000000000000000000000000000000000;;		if node == nil {
0000000000000000000000000000000000000000;;			return false, nil, fmt.Errorf("node not found")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// check if the pod being scheduled has the affinity labels specified in its NodeSelector
0000000000000000000000000000000000000000;;		affinityLabels := FindLabelsInSet(s.labels, labels.Set(pod.Spec.NodeSelector))
0000000000000000000000000000000000000000;;		// Step 1: If we don't have all constraints, introspect nodes to find the missing constraints.
0000000000000000000000000000000000000000;;		if len(s.labels) > len(affinityLabels) {
0000000000000000000000000000000000000000;;			if len(services) > 0 {
0000000000000000000000000000000000000000;;				if len(pods) > 0 {
0000000000000000000000000000000000000000;;					nodeWithAffinityLabels, err := s.nodeInfo.GetNodeInfo(pods[0].Spec.NodeName)
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						return false, nil, err
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					AddUnsetLabelsToMap(affinityLabels, s.labels, labels.Set(nodeWithAffinityLabels.Labels))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Step 2: Finally complete the affinity predicate based on whatever set of predicates we were able to find.
0000000000000000000000000000000000000000;;		if CreateSelectorFromLabels(affinityLabels).Matches(labels.Set(node.Labels)) {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false, []algorithm.PredicateFailureReason{ErrServiceAffinityViolated}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodFitsHostPorts checks if a node has free ports for the requested pod ports.
0000000000000000000000000000000000000000;;	func PodFitsHostPorts(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		var wantPorts map[int]bool
0000000000000000000000000000000000000000;;		if predicateMeta, ok := meta.(*predicateMetadata); ok {
0000000000000000000000000000000000000000;;			wantPorts = predicateMeta.podPorts
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// We couldn't parse metadata - fallback to computing it.
0000000000000000000000000000000000000000;;			wantPorts = schedutil.GetUsedPorts(pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(wantPorts) == 0 {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		existingPorts := nodeInfo.UsedPorts()
0000000000000000000000000000000000000000;;		for wport := range wantPorts {
0000000000000000000000000000000000000000;;			if wport != 0 && existingPorts[wport] {
0000000000000000000000000000000000000000;;				return false, []algorithm.PredicateFailureReason{ErrPodNotFitsHostPorts}, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// search two arrays and return true if they have at least one common element; return false otherwise
0000000000000000000000000000000000000000;;	func haveSame(a1, a2 []string) bool {
0000000000000000000000000000000000000000;;		m := map[string]int{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, val := range a1 {
0000000000000000000000000000000000000000;;			m[val] = 1
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, val := range a2 {
0000000000000000000000000000000000000000;;			m[val] = m[val] + 1
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, val := range m {
0000000000000000000000000000000000000000;;			if val > 1 {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GeneralPredicates checks whether noncriticalPredicates and EssentialPredicates pass. noncriticalPredicates are the predicates
0000000000000000000000000000000000000000;;	// that only non-critical pods need and EssentialPredicates are the predicates that all pods, including critical pods, need
0000000000000000000000000000000000000000;;	func GeneralPredicates(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		var predicateFails []algorithm.PredicateFailureReason
0000000000000000000000000000000000000000;;		fit, reasons, err := noncriticalPredicates(pod, meta, nodeInfo)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, predicateFails, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !fit {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, reasons...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fit, reasons, err = EssentialPredicates(pod, meta, nodeInfo)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, predicateFails, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !fit {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, reasons...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return len(predicateFails) == 0, predicateFails, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// noncriticalPredicates are the predicates that only non-critical pods need
0000000000000000000000000000000000000000;;	func noncriticalPredicates(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		var predicateFails []algorithm.PredicateFailureReason
0000000000000000000000000000000000000000;;		fit, reasons, err := PodFitsResources(pod, meta, nodeInfo)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, predicateFails, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !fit {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, reasons...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return len(predicateFails) == 0, predicateFails, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// EssentialPredicates are the predicates that all pods, including critical pods, need
0000000000000000000000000000000000000000;;	func EssentialPredicates(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		var predicateFails []algorithm.PredicateFailureReason
0000000000000000000000000000000000000000;;		fit, reasons, err := PodFitsHost(pod, meta, nodeInfo)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, predicateFails, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !fit {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, reasons...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: PodFitsHostPorts is essential for now, but kubelet should ideally
0000000000000000000000000000000000000000;;		//       preempt pods to free up host ports too
0000000000000000000000000000000000000000;;		fit, reasons, err = PodFitsHostPorts(pod, meta, nodeInfo)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, predicateFails, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !fit {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, reasons...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fit, reasons, err = PodMatchNodeSelector(pod, meta, nodeInfo)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, predicateFails, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !fit {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, reasons...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return len(predicateFails) == 0, predicateFails, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type PodAffinityChecker struct {
0000000000000000000000000000000000000000;;		info      NodeInfo
0000000000000000000000000000000000000000;;		podLister algorithm.PodLister
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewPodAffinityPredicate(info NodeInfo, podLister algorithm.PodLister) algorithm.FitPredicate {
0000000000000000000000000000000000000000;;		checker := &PodAffinityChecker{
0000000000000000000000000000000000000000;;			info:      info,
0000000000000000000000000000000000000000;;			podLister: podLister,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return checker.InterPodAffinityMatches
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// InterPodAffinityMatches checks if a pod can be scheduled on the specified node with pod affinity/anti-affinity configuration.
0000000000000000000000000000000000000000;;	// First return value indicates whether a pod can be scheduled on the specified node while the second return value indicates the
0000000000000000000000000000000000000000;;	// predicate failure reasons if the pod cannot be scheduled on the specified node.
0000000000000000000000000000000000000000;;	func (c *PodAffinityChecker) InterPodAffinityMatches(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		node := nodeInfo.Node()
0000000000000000000000000000000000000000;;		if node == nil {
0000000000000000000000000000000000000000;;			return false, nil, fmt.Errorf("node not found")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !c.satisfiesExistingPodsAntiAffinity(pod, meta, node) {
0000000000000000000000000000000000000000;;			return false, []algorithm.PredicateFailureReason{ErrPodAffinityNotMatch}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Now check if <pod> requirements will be satisfied on this node.
0000000000000000000000000000000000000000;;		affinity := pod.Spec.Affinity
0000000000000000000000000000000000000000;;		if affinity == nil || (affinity.PodAffinity == nil && affinity.PodAntiAffinity == nil) {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !c.satisfiesPodsAffinityAntiAffinity(pod, node, affinity) {
0000000000000000000000000000000000000000;;			return false, []algorithm.PredicateFailureReason{ErrPodAffinityNotMatch}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if glog.V(10) {
0000000000000000000000000000000000000000;;			// We explicitly don't do glog.V(10).Infof() to avoid computing all the parameters if this is
0000000000000000000000000000000000000000;;			// not logged. There is visible performance gain from it.
0000000000000000000000000000000000000000;;			glog.Infof("Schedule Pod %+v on Node %+v is allowed, pod (anti)affinity constraints satisfied",
0000000000000000000000000000000000000000;;				podName(pod), node.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// anyPodMatchesPodAffinityTerm checks if any of given pods can match the specific podAffinityTerm.
0000000000000000000000000000000000000000;;	// First return value indicates whether a matching pod exists on a node that matches the topology key,
0000000000000000000000000000000000000000;;	// while the second return value indicates whether a matching pod exists anywhere.
0000000000000000000000000000000000000000;;	// TODO: Do we really need any pod matching, or all pods matching? I think the latter.
0000000000000000000000000000000000000000;;	func (c *PodAffinityChecker) anyPodMatchesPodAffinityTerm(pod *v1.Pod, allPods []*v1.Pod, node *v1.Node, term *v1.PodAffinityTerm) (bool, bool, error) {
0000000000000000000000000000000000000000;;		if len(term.TopologyKey) == 0 {
0000000000000000000000000000000000000000;;			return false, false, fmt.Errorf("empty topologyKey is not allowed except for PreferredDuringScheduling pod anti-affinity")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		matchingPodExists := false
0000000000000000000000000000000000000000;;		namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(pod, term)
0000000000000000000000000000000000000000;;		selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, existingPod := range allPods {
0000000000000000000000000000000000000000;;			match := priorityutil.PodMatchesTermsNamespaceAndSelector(existingPod, namespaces, selector)
0000000000000000000000000000000000000000;;			if match {
0000000000000000000000000000000000000000;;				matchingPodExists = true
0000000000000000000000000000000000000000;;				existingPodNode, err := c.info.GetNodeInfo(existingPod.Spec.NodeName)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return false, matchingPodExists, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if priorityutil.NodesHaveSameTopologyKey(node, existingPodNode, term.TopologyKey) {
0000000000000000000000000000000000000000;;					return true, matchingPodExists, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false, matchingPodExists, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getPodAffinityTerms(podAffinity *v1.PodAffinity) (terms []v1.PodAffinityTerm) {
0000000000000000000000000000000000000000;;		if podAffinity != nil {
0000000000000000000000000000000000000000;;			if len(podAffinity.RequiredDuringSchedulingIgnoredDuringExecution) != 0 {
0000000000000000000000000000000000000000;;				terms = podAffinity.RequiredDuringSchedulingIgnoredDuringExecution
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// TODO: Uncomment this block when implement RequiredDuringSchedulingRequiredDuringExecution.
0000000000000000000000000000000000000000;;			//if len(podAffinity.RequiredDuringSchedulingRequiredDuringExecution) != 0 {
0000000000000000000000000000000000000000;;			//	terms = append(terms, podAffinity.RequiredDuringSchedulingRequiredDuringExecution...)
0000000000000000000000000000000000000000;;			//}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return terms
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getPodAntiAffinityTerms(podAntiAffinity *v1.PodAntiAffinity) (terms []v1.PodAffinityTerm) {
0000000000000000000000000000000000000000;;		if podAntiAffinity != nil {
0000000000000000000000000000000000000000;;			if len(podAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution) != 0 {
0000000000000000000000000000000000000000;;				terms = podAntiAffinity.RequiredDuringSchedulingIgnoredDuringExecution
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// TODO: Uncomment this block when implement RequiredDuringSchedulingRequiredDuringExecution.
0000000000000000000000000000000000000000;;			//if len(podAntiAffinity.RequiredDuringSchedulingRequiredDuringExecution) != 0 {
0000000000000000000000000000000000000000;;			//	terms = append(terms, podAntiAffinity.RequiredDuringSchedulingRequiredDuringExecution...)
0000000000000000000000000000000000000000;;			//}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return terms
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getMatchingAntiAffinityTerms(pod *v1.Pod, nodeInfoMap map[string]*schedulercache.NodeInfo) ([]matchingPodAntiAffinityTerm, error) {
0000000000000000000000000000000000000000;;		allNodeNames := make([]string, 0, len(nodeInfoMap))
0000000000000000000000000000000000000000;;		for name := range nodeInfoMap {
0000000000000000000000000000000000000000;;			allNodeNames = append(allNodeNames, name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var lock sync.Mutex
0000000000000000000000000000000000000000;;		var result []matchingPodAntiAffinityTerm
0000000000000000000000000000000000000000;;		var firstError error
0000000000000000000000000000000000000000;;		appendResult := func(toAppend []matchingPodAntiAffinityTerm) {
0000000000000000000000000000000000000000;;			lock.Lock()
0000000000000000000000000000000000000000;;			defer lock.Unlock()
0000000000000000000000000000000000000000;;			result = append(result, toAppend...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		catchError := func(err error) {
0000000000000000000000000000000000000000;;			lock.Lock()
0000000000000000000000000000000000000000;;			defer lock.Unlock()
0000000000000000000000000000000000000000;;			if firstError == nil {
0000000000000000000000000000000000000000;;				firstError = err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		processNode := func(i int) {
0000000000000000000000000000000000000000;;			nodeInfo := nodeInfoMap[allNodeNames[i]]
0000000000000000000000000000000000000000;;			node := nodeInfo.Node()
0000000000000000000000000000000000000000;;			if node == nil {
0000000000000000000000000000000000000000;;				catchError(fmt.Errorf("node not found"))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			var nodeResult []matchingPodAntiAffinityTerm
0000000000000000000000000000000000000000;;			for _, existingPod := range nodeInfo.PodsWithAffinity() {
0000000000000000000000000000000000000000;;				affinity := existingPod.Spec.Affinity
0000000000000000000000000000000000000000;;				if affinity == nil {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for _, term := range getPodAntiAffinityTerms(affinity.PodAntiAffinity) {
0000000000000000000000000000000000000000;;					namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(existingPod, &term)
0000000000000000000000000000000000000000;;					selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector)
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						catchError(err)
0000000000000000000000000000000000000000;;						return
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if priorityutil.PodMatchesTermsNamespaceAndSelector(pod, namespaces, selector) {
0000000000000000000000000000000000000000;;						nodeResult = append(nodeResult, matchingPodAntiAffinityTerm{term: &term, node: node})
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if len(nodeResult) > 0 {
0000000000000000000000000000000000000000;;				appendResult(nodeResult)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		workqueue.Parallelize(16, len(allNodeNames), processNode)
0000000000000000000000000000000000000000;;		return result, firstError
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *PodAffinityChecker) getMatchingAntiAffinityTerms(pod *v1.Pod, allPods []*v1.Pod) ([]matchingPodAntiAffinityTerm, error) {
0000000000000000000000000000000000000000;;		var result []matchingPodAntiAffinityTerm
0000000000000000000000000000000000000000;;		for _, existingPod := range allPods {
0000000000000000000000000000000000000000;;			affinity := existingPod.Spec.Affinity
0000000000000000000000000000000000000000;;			if affinity != nil && affinity.PodAntiAffinity != nil {
0000000000000000000000000000000000000000;;				existingPodNode, err := c.info.GetNodeInfo(existingPod.Spec.NodeName)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for _, term := range getPodAntiAffinityTerms(affinity.PodAntiAffinity) {
0000000000000000000000000000000000000000;;					namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(existingPod, &term)
0000000000000000000000000000000000000000;;					selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector)
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						return nil, err
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if priorityutil.PodMatchesTermsNamespaceAndSelector(pod, namespaces, selector) {
0000000000000000000000000000000000000000;;						result = append(result, matchingPodAntiAffinityTerm{term: &term, node: existingPodNode})
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Checks if scheduling the pod onto this node would break any anti-affinity
0000000000000000000000000000000000000000;;	// rules indicated by the existing pods.
0000000000000000000000000000000000000000;;	func (c *PodAffinityChecker) satisfiesExistingPodsAntiAffinity(pod *v1.Pod, meta interface{}, node *v1.Node) bool {
0000000000000000000000000000000000000000;;		var matchingTerms []matchingPodAntiAffinityTerm
0000000000000000000000000000000000000000;;		if predicateMeta, ok := meta.(*predicateMetadata); ok {
0000000000000000000000000000000000000000;;			matchingTerms = predicateMeta.matchingAntiAffinityTerms
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			allPods, err := c.podLister.List(labels.Everything())
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Failed to get all pods, %+v", err)
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if matchingTerms, err = c.getMatchingAntiAffinityTerms(pod, allPods); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Failed to get all terms that pod %+v matches, err: %+v", podName(pod), err)
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, term := range matchingTerms {
0000000000000000000000000000000000000000;;			if len(term.term.TopologyKey) == 0 {
0000000000000000000000000000000000000000;;				glog.Error("Empty topologyKey is not allowed except for PreferredDuringScheduling pod anti-affinity")
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if priorityutil.NodesHaveSameTopologyKey(node, term.node, term.term.TopologyKey) {
0000000000000000000000000000000000000000;;				glog.V(10).Infof("Cannot schedule pod %+v onto node %v,because of PodAntiAffinityTerm %v",
0000000000000000000000000000000000000000;;					podName(pod), node.Name, term.term)
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if glog.V(10) {
0000000000000000000000000000000000000000;;			// We explicitly don't do glog.V(10).Infof() to avoid computing all the parameters if this is
0000000000000000000000000000000000000000;;			// not logged. There is visible performance gain from it.
0000000000000000000000000000000000000000;;			glog.Infof("Schedule Pod %+v on Node %+v is allowed, existing pods anti-affinity rules satisfied.",
0000000000000000000000000000000000000000;;				podName(pod), node.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Checks if scheduling the pod onto this node would break any rules of this pod.
0000000000000000000000000000000000000000;;	func (c *PodAffinityChecker) satisfiesPodsAffinityAntiAffinity(pod *v1.Pod, node *v1.Node, affinity *v1.Affinity) bool {
0000000000000000000000000000000000000000;;		allPods, err := c.podLister.List(labels.Everything())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check all affinity terms.
0000000000000000000000000000000000000000;;		for _, term := range getPodAffinityTerms(affinity.PodAffinity) {
0000000000000000000000000000000000000000;;			termMatches, matchingPodExists, err := c.anyPodMatchesPodAffinityTerm(pod, allPods, node, &term)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Cannot schedule pod %+v onto node %v,because of PodAffinityTerm %v, err: %v",
0000000000000000000000000000000000000000;;					podName(pod), node.Name, term, err)
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if !termMatches {
0000000000000000000000000000000000000000;;				// If the requirement matches a pod's own labels are namespace, and there are
0000000000000000000000000000000000000000;;				// no other such pods, then disregard the requirement. This is necessary to
0000000000000000000000000000000000000000;;				// not block forever because the first pod of the collection can't be scheduled.
0000000000000000000000000000000000000000;;				if matchingPodExists {
0000000000000000000000000000000000000000;;					glog.V(10).Infof("Cannot schedule pod %+v onto node %v,because of PodAffinityTerm %v, err: %v",
0000000000000000000000000000000000000000;;						podName(pod), node.Name, term, err)
0000000000000000000000000000000000000000;;					return false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(pod, &term)
0000000000000000000000000000000000000000;;				selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					glog.Errorf("Cannot parse selector on term %v for pod %v. Details %v",
0000000000000000000000000000000000000000;;						term, podName(pod), err)
0000000000000000000000000000000000000000;;					return false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				match := priorityutil.PodMatchesTermsNamespaceAndSelector(pod, namespaces, selector)
0000000000000000000000000000000000000000;;				if !match {
0000000000000000000000000000000000000000;;					glog.V(10).Infof("Cannot schedule pod %+v onto node %v,because of PodAffinityTerm %v, err: %v",
0000000000000000000000000000000000000000;;						podName(pod), node.Name, term, err)
0000000000000000000000000000000000000000;;					return false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check all anti-affinity terms.
0000000000000000000000000000000000000000;;		for _, term := range getPodAntiAffinityTerms(affinity.PodAntiAffinity) {
0000000000000000000000000000000000000000;;			termMatches, _, err := c.anyPodMatchesPodAffinityTerm(pod, allPods, node, &term)
0000000000000000000000000000000000000000;;			if err != nil || termMatches {
0000000000000000000000000000000000000000;;				glog.V(10).Infof("Cannot schedule pod %+v onto node %v,because of PodAntiAffinityTerm %v, err: %v",
0000000000000000000000000000000000000000;;					podName(pod), node.Name, term, err)
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if glog.V(10) {
0000000000000000000000000000000000000000;;			// We explicitly don't do glog.V(10).Infof() to avoid computing all the parameters if this is
0000000000000000000000000000000000000000;;			// not logged. There is visible performance gain from it.
0000000000000000000000000000000000000000;;			glog.Infof("Schedule Pod %+v on Node %+v is allowed, pod afinnity/anti-affinity constraints satisfied.",
0000000000000000000000000000000000000000;;				podName(pod), node.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodToleratesNodeTaints checks if a pod tolertaions can tolerate the node taints
0000000000000000000000000000000000000000;;	func PodToleratesNodeTaints(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		return podToleratesNodeTaints(pod, nodeInfo, func(t *v1.Taint) bool {
0000000000000000000000000000000000000000;;			// PodToleratesNodeTaints is only interested in NoSchedule and NoExecute taints.
0000000000000000000000000000000000000000;;			return t.Effect == v1.TaintEffectNoSchedule || t.Effect == v1.TaintEffectNoExecute
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodToleratesNodeNoExecuteTaints checks if a pod tolertaions can tolerate the node's NoExecute taints
0000000000000000000000000000000000000000;;	func PodToleratesNodeNoExecuteTaints(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		return podToleratesNodeTaints(pod, nodeInfo, func(t *v1.Taint) bool {
0000000000000000000000000000000000000000;;			return t.Effect == v1.TaintEffectNoExecute
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podToleratesNodeTaints(pod *v1.Pod, nodeInfo *schedulercache.NodeInfo, filter func(t *v1.Taint) bool) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		taints, err := nodeInfo.Taints()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if v1helper.TolerationsTolerateTaintsWithFilter(pod.Spec.Tolerations, taints, filter) {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false, []algorithm.PredicateFailureReason{ErrTaintsTolerationsNotMatch}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// isPodBestEffort checks if pod is scheduled with best-effort QoS
0000000000000000000000000000000000000000;;	func isPodBestEffort(pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		return v1qos.GetPodQOS(pod) == v1.PodQOSBestEffort
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CheckNodeMemoryPressurePredicate checks if a pod can be scheduled on a node
0000000000000000000000000000000000000000;;	// reporting memory pressure condition.
0000000000000000000000000000000000000000;;	func CheckNodeMemoryPressurePredicate(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		var podBestEffort bool
0000000000000000000000000000000000000000;;		if predicateMeta, ok := meta.(*predicateMetadata); ok {
0000000000000000000000000000000000000000;;			podBestEffort = predicateMeta.podBestEffort
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// We couldn't parse metadata - fallback to computing it.
0000000000000000000000000000000000000000;;			podBestEffort = isPodBestEffort(pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// pod is not BestEffort pod
0000000000000000000000000000000000000000;;		if !podBestEffort {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// check if node is under memory preasure
0000000000000000000000000000000000000000;;		if nodeInfo.MemoryPressureCondition() == v1.ConditionTrue {
0000000000000000000000000000000000000000;;			return false, []algorithm.PredicateFailureReason{ErrNodeUnderMemoryPressure}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CheckNodeDiskPressurePredicate checks if a pod can be scheduled on a node
0000000000000000000000000000000000000000;;	// reporting disk pressure condition.
0000000000000000000000000000000000000000;;	func CheckNodeDiskPressurePredicate(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		// check if node is under disk preasure
0000000000000000000000000000000000000000;;		if nodeInfo.DiskPressureCondition() == v1.ConditionTrue {
0000000000000000000000000000000000000000;;			return false, []algorithm.PredicateFailureReason{ErrNodeUnderDiskPressure}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type VolumeNodeChecker struct {
0000000000000000000000000000000000000000;;		pvInfo  PersistentVolumeInfo
0000000000000000000000000000000000000000;;		pvcInfo PersistentVolumeClaimInfo
0000000000000000000000000000000000000000;;		client  clientset.Interface
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewVolumeNodePredicate evaluates if a pod can fit due to the volumes it requests, given
0000000000000000000000000000000000000000;;	// that some volumes have node topology constraints, particularly when using Local PVs.
0000000000000000000000000000000000000000;;	// The requirement is that any pod that uses a PVC that is bound to a PV with topology constraints
0000000000000000000000000000000000000000;;	// must be scheduled to a node that satisfies the PV's topology labels.
0000000000000000000000000000000000000000;;	func NewVolumeNodePredicate(pvInfo PersistentVolumeInfo, pvcInfo PersistentVolumeClaimInfo, client clientset.Interface) algorithm.FitPredicate {
0000000000000000000000000000000000000000;;		c := &VolumeNodeChecker{
0000000000000000000000000000000000000000;;			pvInfo:  pvInfo,
0000000000000000000000000000000000000000;;			pvcInfo: pvcInfo,
0000000000000000000000000000000000000000;;			client:  client,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return c.predicate
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *VolumeNodeChecker) predicate(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		if !utilfeature.DefaultFeatureGate.Enabled(features.PersistentLocalVolumes) {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If a pod doesn't have any volume attached to it, the predicate will always be true.
0000000000000000000000000000000000000000;;		// Thus we make a fast path for it, to avoid unnecessary computations in this case.
0000000000000000000000000000000000000000;;		if len(pod.Spec.Volumes) == 0 {
0000000000000000000000000000000000000000;;			return true, nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		node := nodeInfo.Node()
0000000000000000000000000000000000000000;;		if node == nil {
0000000000000000000000000000000000000000;;			return false, nil, fmt.Errorf("node not found")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(2).Infof("Checking for prebound volumes with node affinity")
0000000000000000000000000000000000000000;;		namespace := pod.Namespace
0000000000000000000000000000000000000000;;		manifest := &(pod.Spec)
0000000000000000000000000000000000000000;;		for i := range manifest.Volumes {
0000000000000000000000000000000000000000;;			volume := &manifest.Volumes[i]
0000000000000000000000000000000000000000;;			if volume.PersistentVolumeClaim == nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pvcName := volume.PersistentVolumeClaim.ClaimName
0000000000000000000000000000000000000000;;			if pvcName == "" {
0000000000000000000000000000000000000000;;				return false, nil, fmt.Errorf("PersistentVolumeClaim had no name")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pvc, err := c.pvcInfo.GetPersistentVolumeClaimInfo(namespace, pvcName)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if pvc == nil {
0000000000000000000000000000000000000000;;				return false, nil, fmt.Errorf("PersistentVolumeClaim was not found: %q", pvcName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pvName := pvc.Spec.VolumeName
0000000000000000000000000000000000000000;;			if pvName == "" {
0000000000000000000000000000000000000000;;				return false, nil, fmt.Errorf("PersistentVolumeClaim is not bound: %q", pvcName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			pv, err := c.pvInfo.GetPersistentVolumeInfo(pvName)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if pv == nil {
0000000000000000000000000000000000000000;;				return false, nil, fmt.Errorf("PersistentVolume not found: %q", pvName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			err = volumeutil.CheckNodeAffinity(pv, node.Labels)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.V(2).Infof("Won't schedule pod %q onto node %q due to volume %q node mismatch: %v", pod.Name, node.Name, pvName, err.Error())
0000000000000000000000000000000000000000;;				return false, []algorithm.PredicateFailureReason{ErrVolumeNodeConflict}, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("VolumeNode predicate allows node %q for pod %q due to volume %q", node.Name, pod.Name, pvName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true, nil, nil
0000000000000000000000000000000000000000;;	}
