0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2014 The Kubernetes Authors.
8e1c5b2581fcdddc9dc6fc4276c11b92b1fda222;pkg/scheduler/spreading_scheduler.go[pkg/scheduler/spreading_scheduler.go][plugin/pkg/scheduler/algorithm/priorities/selector_spreading.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package priorities
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/workqueue"
0000000000000000000000000000000000000000;;		utilnode "k8s.io/kubernetes/pkg/util/node"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm"
0000000000000000000000000000000000000000;;		schedulerapi "k8s.io/kubernetes/plugin/pkg/scheduler/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/schedulercache"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// The maximum priority value to give to a node
0000000000000000000000000000000000000000;;	// Priority values range from 0-maxPriority
0000000000000000000000000000000000000000;;	const maxPriority float32 = 10
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// When zone information is present, give 2/3 of the weighting to zone spreading, 1/3 to node spreading
0000000000000000000000000000000000000000;;	// TODO: Any way to justify this weighting?
0000000000000000000000000000000000000000;;	const zoneWeighting = 2.0 / 3.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type SelectorSpread struct {
0000000000000000000000000000000000000000;;		serviceLister     algorithm.ServiceLister
0000000000000000000000000000000000000000;;		controllerLister  algorithm.ControllerLister
0000000000000000000000000000000000000000;;		replicaSetLister  algorithm.ReplicaSetLister
0000000000000000000000000000000000000000;;		statefulSetLister algorithm.StatefulSetLister
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewSelectorSpreadPriority(
0000000000000000000000000000000000000000;;		serviceLister algorithm.ServiceLister,
0000000000000000000000000000000000000000;;		controllerLister algorithm.ControllerLister,
0000000000000000000000000000000000000000;;		replicaSetLister algorithm.ReplicaSetLister,
0000000000000000000000000000000000000000;;		statefulSetLister algorithm.StatefulSetLister) algorithm.PriorityFunction {
0000000000000000000000000000000000000000;;		selectorSpread := &SelectorSpread{
0000000000000000000000000000000000000000;;			serviceLister:     serviceLister,
0000000000000000000000000000000000000000;;			controllerLister:  controllerLister,
0000000000000000000000000000000000000000;;			replicaSetLister:  replicaSetLister,
0000000000000000000000000000000000000000;;			statefulSetLister: statefulSetLister,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return selectorSpread.CalculateSpreadPriority
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Returns selectors of services, RCs and RSs matching the given pod.
0000000000000000000000000000000000000000;;	func getSelectors(pod *v1.Pod, sl algorithm.ServiceLister, cl algorithm.ControllerLister, rsl algorithm.ReplicaSetLister, ssl algorithm.StatefulSetLister) []labels.Selector {
0000000000000000000000000000000000000000;;		var selectors []labels.Selector
0000000000000000000000000000000000000000;;		if services, err := sl.GetPodServices(pod); err == nil {
0000000000000000000000000000000000000000;;			for _, service := range services {
0000000000000000000000000000000000000000;;				selectors = append(selectors, labels.SelectorFromSet(service.Spec.Selector))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if rcs, err := cl.GetPodControllers(pod); err == nil {
0000000000000000000000000000000000000000;;			for _, rc := range rcs {
0000000000000000000000000000000000000000;;				selectors = append(selectors, labels.SelectorFromSet(rc.Spec.Selector))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if rss, err := rsl.GetPodReplicaSets(pod); err == nil {
0000000000000000000000000000000000000000;;			for _, rs := range rss {
0000000000000000000000000000000000000000;;				if selector, err := metav1.LabelSelectorAsSelector(rs.Spec.Selector); err == nil {
0000000000000000000000000000000000000000;;					selectors = append(selectors, selector)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if sss, err := ssl.GetPodStatefulSets(pod); err == nil {
0000000000000000000000000000000000000000;;			for _, ss := range sss {
0000000000000000000000000000000000000000;;				if selector, err := metav1.LabelSelectorAsSelector(ss.Spec.Selector); err == nil {
0000000000000000000000000000000000000000;;					selectors = append(selectors, selector)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return selectors
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (s *SelectorSpread) getSelectors(pod *v1.Pod) []labels.Selector {
0000000000000000000000000000000000000000;;		return getSelectors(pod, s.serviceLister, s.controllerLister, s.replicaSetLister, s.statefulSetLister)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CalculateSpreadPriority spreads pods across hosts and zones, considering pods belonging to the same service or replication controller.
0000000000000000000000000000000000000000;;	// When a pod is scheduled, it looks for services, RCs or RSs that match the pod, then finds existing pods that match those selectors.
0000000000000000000000000000000000000000;;	// It favors nodes that have fewer existing matching pods.
0000000000000000000000000000000000000000;;	// i.e. it pushes the scheduler towards a node where there's the smallest number of
0000000000000000000000000000000000000000;;	// pods which match the same service, RC or RS selectors as the pod being scheduled.
0000000000000000000000000000000000000000;;	// Where zone information is included on the nodes, it favors nodes in zones with fewer existing matching pods.
0000000000000000000000000000000000000000;;	func (s *SelectorSpread) CalculateSpreadPriority(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) {
0000000000000000000000000000000000000000;;		selectors := s.getSelectors(pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Count similar pods by node
0000000000000000000000000000000000000000;;		countsByNodeName := make(map[string]float32, len(nodes))
0000000000000000000000000000000000000000;;		countsByZone := make(map[string]float32, 10)
0000000000000000000000000000000000000000;;		maxCountByNodeName := float32(0)
0000000000000000000000000000000000000000;;		countsByNodeNameLock := sync.Mutex{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(selectors) > 0 {
0000000000000000000000000000000000000000;;			processNodeFunc := func(i int) {
0000000000000000000000000000000000000000;;				nodeName := nodes[i].Name
0000000000000000000000000000000000000000;;				count := float32(0)
0000000000000000000000000000000000000000;;				for _, nodePod := range nodeNameToInfo[nodeName].Pods() {
0000000000000000000000000000000000000000;;					if pod.Namespace != nodePod.Namespace {
0000000000000000000000000000000000000000;;						continue
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					// When we are replacing a failed pod, we often see the previous
0000000000000000000000000000000000000000;;					// deleted version while scheduling the replacement.
0000000000000000000000000000000000000000;;					// Ignore the previous deleted version for spreading purposes
0000000000000000000000000000000000000000;;					// (it can still be considered for resource restrictions etc.)
0000000000000000000000000000000000000000;;					if nodePod.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;						glog.V(4).Infof("skipping pending-deleted pod: %s/%s", nodePod.Namespace, nodePod.Name)
0000000000000000000000000000000000000000;;						continue
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					matches := false
0000000000000000000000000000000000000000;;					for _, selector := range selectors {
0000000000000000000000000000000000000000;;						if selector.Matches(labels.Set(nodePod.ObjectMeta.Labels)) {
0000000000000000000000000000000000000000;;							matches = true
0000000000000000000000000000000000000000;;							break
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if matches {
0000000000000000000000000000000000000000;;						count++
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				zoneId := utilnode.GetZoneKey(nodes[i])
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				countsByNodeNameLock.Lock()
0000000000000000000000000000000000000000;;				defer countsByNodeNameLock.Unlock()
0000000000000000000000000000000000000000;;				countsByNodeName[nodeName] = count
0000000000000000000000000000000000000000;;				if count > maxCountByNodeName {
0000000000000000000000000000000000000000;;					maxCountByNodeName = count
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if zoneId != "" {
0000000000000000000000000000000000000000;;					countsByZone[zoneId] += count
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			workqueue.Parallelize(16, len(nodes), processNodeFunc)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Aggregate by-zone information
0000000000000000000000000000000000000000;;		// Compute the maximum number of pods hosted in any zone
0000000000000000000000000000000000000000;;		haveZones := len(countsByZone) != 0
0000000000000000000000000000000000000000;;		maxCountByZone := float32(0)
0000000000000000000000000000000000000000;;		for _, count := range countsByZone {
0000000000000000000000000000000000000000;;			if count > maxCountByZone {
0000000000000000000000000000000000000000;;				maxCountByZone = count
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		result := make(schedulerapi.HostPriorityList, 0, len(nodes))
0000000000000000000000000000000000000000;;		//score int - scale of 0-maxPriority
0000000000000000000000000000000000000000;;		// 0 being the lowest priority and maxPriority being the highest
0000000000000000000000000000000000000000;;		for _, node := range nodes {
0000000000000000000000000000000000000000;;			// initializing to the default/max node score of maxPriority
0000000000000000000000000000000000000000;;			fScore := maxPriority
0000000000000000000000000000000000000000;;			if maxCountByNodeName > 0 {
0000000000000000000000000000000000000000;;				fScore = maxPriority * ((maxCountByNodeName - countsByNodeName[node.Name]) / maxCountByNodeName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// If there is zone information present, incorporate it
0000000000000000000000000000000000000000;;			if haveZones {
0000000000000000000000000000000000000000;;				zoneId := utilnode.GetZoneKey(node)
0000000000000000000000000000000000000000;;				if zoneId != "" {
0000000000000000000000000000000000000000;;					zoneScore := maxPriority * ((maxCountByZone - countsByZone[zoneId]) / maxCountByZone)
0000000000000000000000000000000000000000;;					fScore = (fScore * (1.0 - zoneWeighting)) + (zoneWeighting * zoneScore)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			result = append(result, schedulerapi.HostPriority{Host: node.Name, Score: int(fScore)})
0000000000000000000000000000000000000000;;			if glog.V(10) {
0000000000000000000000000000000000000000;;				// We explicitly don't do glog.V(10).Infof() to avoid computing all the parameters if this is
0000000000000000000000000000000000000000;;				// not logged. There is visible performance gain from it.
0000000000000000000000000000000000000000;;				glog.V(10).Infof(
0000000000000000000000000000000000000000;;					"%v -> %v: SelectorSpreadPriority, Score: (%d)", pod.Name, node.Name, int(fScore),
0000000000000000000000000000000000000000;;				)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type ServiceAntiAffinity struct {
0000000000000000000000000000000000000000;;		podLister     algorithm.PodLister
0000000000000000000000000000000000000000;;		serviceLister algorithm.ServiceLister
0000000000000000000000000000000000000000;;		label         string
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewServiceAntiAffinityPriority(podLister algorithm.PodLister, serviceLister algorithm.ServiceLister, label string) algorithm.PriorityFunction {
0000000000000000000000000000000000000000;;		antiAffinity := &ServiceAntiAffinity{
0000000000000000000000000000000000000000;;			podLister:     podLister,
0000000000000000000000000000000000000000;;			serviceLister: serviceLister,
0000000000000000000000000000000000000000;;			label:         label,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return antiAffinity.CalculateAntiAffinityPriority
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Classifies nodes into ones with labels and without labels.
0000000000000000000000000000000000000000;;	func (s *ServiceAntiAffinity) getNodeClassificationByLabels(nodes []*v1.Node) (map[string]string, []string) {
0000000000000000000000000000000000000000;;		labeledNodes := map[string]string{}
0000000000000000000000000000000000000000;;		nonLabeledNodes := []string{}
0000000000000000000000000000000000000000;;		for _, node := range nodes {
0000000000000000000000000000000000000000;;			if labels.Set(node.Labels).Has(s.label) {
0000000000000000000000000000000000000000;;				label := labels.Set(node.Labels).Get(s.label)
0000000000000000000000000000000000000000;;				labeledNodes[node.Name] = label
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				nonLabeledNodes = append(nonLabeledNodes, node.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return labeledNodes, nonLabeledNodes
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CalculateAntiAffinityPriority spreads pods by minimizing the number of pods belonging to the same service
0000000000000000000000000000000000000000;;	// on machines with the same value for a particular label.
0000000000000000000000000000000000000000;;	// The label to be considered is provided to the struct (ServiceAntiAffinity).
0000000000000000000000000000000000000000;;	func (s *ServiceAntiAffinity) CalculateAntiAffinityPriority(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) {
0000000000000000000000000000000000000000;;		var nsServicePods []*v1.Pod
0000000000000000000000000000000000000000;;		if services, err := s.serviceLister.GetPodServices(pod); err == nil && len(services) > 0 {
0000000000000000000000000000000000000000;;			// just use the first service and get the other pods within the service
0000000000000000000000000000000000000000;;			// TODO: a separate predicate can be created that tries to handle all services for the pod
0000000000000000000000000000000000000000;;			selector := labels.SelectorFromSet(services[0].Spec.Selector)
0000000000000000000000000000000000000000;;			pods, err := s.podLister.List(selector)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// consider only the pods that belong to the same namespace
0000000000000000000000000000000000000000;;			for _, nsPod := range pods {
0000000000000000000000000000000000000000;;				if nsPod.Namespace == pod.Namespace {
0000000000000000000000000000000000000000;;					nsServicePods = append(nsServicePods, nsPod)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// separate out the nodes that have the label from the ones that don't
0000000000000000000000000000000000000000;;		labeledNodes, nonLabeledNodes := s.getNodeClassificationByLabels(nodes)
0000000000000000000000000000000000000000;;		podCounts := map[string]int{}
0000000000000000000000000000000000000000;;		for _, pod := range nsServicePods {
0000000000000000000000000000000000000000;;			label, exists := labeledNodes[pod.Spec.NodeName]
0000000000000000000000000000000000000000;;			if !exists {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			podCounts[label]++
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		numServicePods := len(nsServicePods)
0000000000000000000000000000000000000000;;		result := []schedulerapi.HostPriority{}
0000000000000000000000000000000000000000;;		//score int - scale of 0-maxPriority
0000000000000000000000000000000000000000;;		// 0 being the lowest priority and maxPriority being the highest
0000000000000000000000000000000000000000;;		for node := range labeledNodes {
0000000000000000000000000000000000000000;;			// initializing to the default/max node score of maxPriority
0000000000000000000000000000000000000000;;			fScore := float32(maxPriority)
0000000000000000000000000000000000000000;;			if numServicePods > 0 {
0000000000000000000000000000000000000000;;				fScore = maxPriority * (float32(numServicePods-podCounts[labeledNodes[node]]) / float32(numServicePods))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			result = append(result, schedulerapi.HostPriority{Host: node, Score: int(fScore)})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// add the open nodes with a score of 0
0000000000000000000000000000000000000000;;		for _, node := range nonLabeledNodes {
0000000000000000000000000000000000000000;;			result = append(result, schedulerapi.HostPriority{Host: node, Score: 0})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result, nil
0000000000000000000000000000000000000000;;	}
