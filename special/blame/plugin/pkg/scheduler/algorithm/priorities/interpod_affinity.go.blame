0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
79cb5f148f9fbb125b5ed9b40f2a31f6f99e3405;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package priorities
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/workqueue"
0000000000000000000000000000000000000000;;		kubeletapis "k8s.io/kubernetes/pkg/kubelet/apis"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm/predicates"
0000000000000000000000000000000000000000;;		priorityutil "k8s.io/kubernetes/plugin/pkg/scheduler/algorithm/priorities/util"
0000000000000000000000000000000000000000;;		schedulerapi "k8s.io/kubernetes/plugin/pkg/scheduler/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/schedulercache"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type InterPodAffinity struct {
0000000000000000000000000000000000000000;;		info                  predicates.NodeInfo
0000000000000000000000000000000000000000;;		nodeLister            algorithm.NodeLister
0000000000000000000000000000000000000000;;		podLister             algorithm.PodLister
0000000000000000000000000000000000000000;;		hardPodAffinityWeight int
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewInterPodAffinityPriority(
0000000000000000000000000000000000000000;;		info predicates.NodeInfo,
0000000000000000000000000000000000000000;;		nodeLister algorithm.NodeLister,
0000000000000000000000000000000000000000;;		podLister algorithm.PodLister,
0000000000000000000000000000000000000000;;		hardPodAffinityWeight int) algorithm.PriorityFunction {
0000000000000000000000000000000000000000;;		interPodAffinity := &InterPodAffinity{
0000000000000000000000000000000000000000;;			info:                  info,
0000000000000000000000000000000000000000;;			nodeLister:            nodeLister,
0000000000000000000000000000000000000000;;			podLister:             podLister,
0000000000000000000000000000000000000000;;			hardPodAffinityWeight: hardPodAffinityWeight,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return interPodAffinity.CalculateInterPodAffinityPriority
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type podAffinityPriorityMap struct {
0000000000000000000000000000000000000000;;		sync.Mutex
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// nodes contain all nodes that should be considered
0000000000000000000000000000000000000000;;		nodes []*v1.Node
0000000000000000000000000000000000000000;;		// counts store the mapping from node name to so-far computed score of
0000000000000000000000000000000000000000;;		// the node.
0000000000000000000000000000000000000000;;		counts map[string]float64
0000000000000000000000000000000000000000;;		// failureDomains contain default failure domains keys
0000000000000000000000000000000000000000;;		failureDomains priorityutil.Topologies
0000000000000000000000000000000000000000;;		// The first error that we faced.
0000000000000000000000000000000000000000;;		firstError error
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newPodAffinityPriorityMap(nodes []*v1.Node) *podAffinityPriorityMap {
0000000000000000000000000000000000000000;;		return &podAffinityPriorityMap{
0000000000000000000000000000000000000000;;			nodes:          nodes,
0000000000000000000000000000000000000000;;			counts:         make(map[string]float64, len(nodes)),
0000000000000000000000000000000000000000;;			failureDomains: priorityutil.Topologies{DefaultKeys: strings.Split(kubeletapis.DefaultFailureDomains, ",")},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *podAffinityPriorityMap) setError(err error) {
0000000000000000000000000000000000000000;;		p.Lock()
0000000000000000000000000000000000000000;;		defer p.Unlock()
0000000000000000000000000000000000000000;;		if p.firstError == nil {
0000000000000000000000000000000000000000;;			p.firstError = err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *podAffinityPriorityMap) processTerm(term *v1.PodAffinityTerm, podDefiningAffinityTerm, podToCheck *v1.Pod, fixedNode *v1.Node, weight float64) {
0000000000000000000000000000000000000000;;		namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(podDefiningAffinityTerm, term)
0000000000000000000000000000000000000000;;		selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			p.setError(err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		match := priorityutil.PodMatchesTermsNamespaceAndSelector(podToCheck, namespaces, selector)
0000000000000000000000000000000000000000;;		if match {
0000000000000000000000000000000000000000;;			func() {
0000000000000000000000000000000000000000;;				p.Lock()
0000000000000000000000000000000000000000;;				defer p.Unlock()
0000000000000000000000000000000000000000;;				for _, node := range p.nodes {
0000000000000000000000000000000000000000;;					if p.failureDomains.NodesHaveSameTopologyKey(node, fixedNode, term.TopologyKey) {
0000000000000000000000000000000000000000;;						p.counts[node.Name] += weight
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *podAffinityPriorityMap) processTerms(terms []v1.WeightedPodAffinityTerm, podDefiningAffinityTerm, podToCheck *v1.Pod, fixedNode *v1.Node, multiplier int) {
0000000000000000000000000000000000000000;;		for i := range terms {
0000000000000000000000000000000000000000;;			term := &terms[i]
0000000000000000000000000000000000000000;;			p.processTerm(&term.PodAffinityTerm, podDefiningAffinityTerm, podToCheck, fixedNode, float64(term.Weight*int32(multiplier)))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// compute a sum by iterating through the elements of weightedPodAffinityTerm and adding
0000000000000000000000000000000000000000;;	// "weight" to the sum if the corresponding PodAffinityTerm is satisfied for
0000000000000000000000000000000000000000;;	// that node; the node(s) with the highest sum are the most preferred.
0000000000000000000000000000000000000000;;	// Symmetry need to be considered for preferredDuringSchedulingIgnoredDuringExecution from podAffinity & podAntiAffinity,
0000000000000000000000000000000000000000;;	// symmetry need to be considered for hard requirements from podAffinity
0000000000000000000000000000000000000000;;	func (ipa *InterPodAffinity) CalculateInterPodAffinityPriority(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) {
0000000000000000000000000000000000000000;;		affinity := pod.Spec.Affinity
0000000000000000000000000000000000000000;;		hasAffinityConstraints := affinity != nil && affinity.PodAffinity != nil
0000000000000000000000000000000000000000;;		hasAntiAffinityConstraints := affinity != nil && affinity.PodAntiAffinity != nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		allNodeNames := make([]string, 0, len(nodeNameToInfo))
0000000000000000000000000000000000000000;;		for name := range nodeNameToInfo {
0000000000000000000000000000000000000000;;			allNodeNames = append(allNodeNames, name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// convert the topology key based weights to the node name based weights
0000000000000000000000000000000000000000;;		var maxCount float64
0000000000000000000000000000000000000000;;		var minCount float64
0000000000000000000000000000000000000000;;		// priorityMap stores the mapping from node name to so-far computed score of
0000000000000000000000000000000000000000;;		// the node.
0000000000000000000000000000000000000000;;		pm := newPodAffinityPriorityMap(nodes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		processPod := func(existingPod *v1.Pod) error {
0000000000000000000000000000000000000000;;			existingPodNode, err := ipa.info.GetNodeInfo(existingPod.Spec.NodeName)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			existingPodAffinity := existingPod.Spec.Affinity
0000000000000000000000000000000000000000;;			existingHasAffinityConstraints := existingPodAffinity != nil && existingPodAffinity.PodAffinity != nil
0000000000000000000000000000000000000000;;			existingHasAntiAffinityConstraints := existingPodAffinity != nil && existingPodAffinity.PodAntiAffinity != nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if hasAffinityConstraints {
0000000000000000000000000000000000000000;;				// For every soft pod affinity term of <pod>, if <existingPod> matches the term,
0000000000000000000000000000000000000000;;				// increment <pm.counts> for every node in the cluster with the same <term.TopologyKey>
0000000000000000000000000000000000000000;;				// value as that of <existingPods>`s node by the term`s weight.
0000000000000000000000000000000000000000;;				terms := affinity.PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution
0000000000000000000000000000000000000000;;				pm.processTerms(terms, pod, existingPod, existingPodNode, 1)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if hasAntiAffinityConstraints {
0000000000000000000000000000000000000000;;				// For every soft pod anti-affinity term of <pod>, if <existingPod> matches the term,
0000000000000000000000000000000000000000;;				// decrement <pm.counts> for every node in the cluster with the same <term.TopologyKey>
0000000000000000000000000000000000000000;;				// value as that of <existingPod>`s node by the term`s weight.
0000000000000000000000000000000000000000;;				terms := affinity.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution
0000000000000000000000000000000000000000;;				pm.processTerms(terms, pod, existingPod, existingPodNode, -1)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if existingHasAffinityConstraints {
0000000000000000000000000000000000000000;;				// For every hard pod affinity term of <existingPod>, if <pod> matches the term,
0000000000000000000000000000000000000000;;				// increment <pm.counts> for every node in the cluster with the same <term.TopologyKey>
0000000000000000000000000000000000000000;;				// value as that of <existingPod>'s node by the constant <ipa.hardPodAffinityWeight>
0000000000000000000000000000000000000000;;				if ipa.hardPodAffinityWeight > 0 {
0000000000000000000000000000000000000000;;					terms := existingPodAffinity.PodAffinity.RequiredDuringSchedulingIgnoredDuringExecution
0000000000000000000000000000000000000000;;					// TODO: Uncomment this block when implement RequiredDuringSchedulingRequiredDuringExecution.
0000000000000000000000000000000000000000;;					//if len(existingPodAffinity.PodAffinity.RequiredDuringSchedulingRequiredDuringExecution) != 0 {
0000000000000000000000000000000000000000;;					//	terms = append(terms, existingPodAffinity.PodAffinity.RequiredDuringSchedulingRequiredDuringExecution...)
0000000000000000000000000000000000000000;;					//}
0000000000000000000000000000000000000000;;					for _, term := range terms {
0000000000000000000000000000000000000000;;						pm.processTerm(&term, existingPod, pod, existingPodNode, float64(ipa.hardPodAffinityWeight))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// For every soft pod affinity term of <existingPod>, if <pod> matches the term,
0000000000000000000000000000000000000000;;				// increment <pm.counts> for every node in the cluster with the same <term.TopologyKey>
0000000000000000000000000000000000000000;;				// value as that of <existingPod>'s node by the term's weight.
0000000000000000000000000000000000000000;;				terms := existingPodAffinity.PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution
0000000000000000000000000000000000000000;;				pm.processTerms(terms, existingPod, pod, existingPodNode, 1)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if existingHasAntiAffinityConstraints {
0000000000000000000000000000000000000000;;				// For every soft pod anti-affinity term of <existingPod>, if <pod> matches the term,
0000000000000000000000000000000000000000;;				// decrement <pm.counts> for every node in the cluster with the same <term.TopologyKey>
0000000000000000000000000000000000000000;;				// value as that of <existingPod>'s node by the term's weight.
0000000000000000000000000000000000000000;;				terms := existingPodAffinity.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution
0000000000000000000000000000000000000000;;				pm.processTerms(terms, existingPod, pod, existingPodNode, -1)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		processNode := func(i int) {
0000000000000000000000000000000000000000;;			nodeInfo := nodeNameToInfo[allNodeNames[i]]
0000000000000000000000000000000000000000;;			if hasAffinityConstraints || hasAntiAffinityConstraints {
0000000000000000000000000000000000000000;;				// We need to process all the nodes.
0000000000000000000000000000000000000000;;				for _, existingPod := range nodeInfo.Pods() {
0000000000000000000000000000000000000000;;					if err := processPod(existingPod); err != nil {
0000000000000000000000000000000000000000;;						pm.setError(err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				// The pod doesn't have any constraints - we need to check only existing
0000000000000000000000000000000000000000;;				// ones that have some.
0000000000000000000000000000000000000000;;				for _, existingPod := range nodeInfo.PodsWithAffinity() {
0000000000000000000000000000000000000000;;					if err := processPod(existingPod); err != nil {
0000000000000000000000000000000000000000;;						pm.setError(err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		workqueue.Parallelize(16, len(allNodeNames), processNode)
0000000000000000000000000000000000000000;;		if pm.firstError != nil {
0000000000000000000000000000000000000000;;			return nil, pm.firstError
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, node := range nodes {
0000000000000000000000000000000000000000;;			if pm.counts[node.Name] > maxCount {
0000000000000000000000000000000000000000;;				maxCount = pm.counts[node.Name]
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if pm.counts[node.Name] < minCount {
0000000000000000000000000000000000000000;;				minCount = pm.counts[node.Name]
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// calculate final priority score for each node
0000000000000000000000000000000000000000;;		result := make(schedulerapi.HostPriorityList, 0, len(nodes))
0000000000000000000000000000000000000000;;		for _, node := range nodes {
0000000000000000000000000000000000000000;;			fScore := float64(0)
0000000000000000000000000000000000000000;;			if (maxCount - minCount) > 0 {
0000000000000000000000000000000000000000;;				fScore = 10 * ((pm.counts[node.Name] - minCount) / (maxCount - minCount))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			result = append(result, schedulerapi.HostPriority{Host: node.Name, Score: int(fScore)})
0000000000000000000000000000000000000000;;			if glog.V(10) {
0000000000000000000000000000000000000000;;				// We explicitly don't do glog.V(10).Infof() to avoid computing all the parameters if this is
0000000000000000000000000000000000000000;;				// not logged. There is visible performance gain from it.
0000000000000000000000000000000000000000;;				glog.V(10).Infof("%v -> %v: InterPodAffinityPriority, Score: (%d)", pod.Name, node.Name, int(fScore))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result, nil
0000000000000000000000000000000000000000;;	}
