0000000000000000000000000000000000000000;;	package jmespath
7381c378252ae8f66befb472cf056371ba4be46a;Godeps/_workspace/src/github.com/jmespath/go-jmespath/lexer.go[Godeps/_workspace/src/github.com/jmespath/go-jmespath/lexer.go][vendor/github.com/jmespath/go-jmespath/lexer.go];	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"bytes"
0000000000000000000000000000000000000000;;		"encoding/json"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"unicode/utf8"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type token struct {
0000000000000000000000000000000000000000;;		tokenType tokType
0000000000000000000000000000000000000000;;		value     string
0000000000000000000000000000000000000000;;		position  int
0000000000000000000000000000000000000000;;		length    int
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type tokType int
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const eof = -1
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Lexer contains information about the expression being tokenized.
0000000000000000000000000000000000000000;;	type Lexer struct {
0000000000000000000000000000000000000000;;		expression string       // The expression provided by the user.
0000000000000000000000000000000000000000;;		currentPos int          // The current position in the string.
0000000000000000000000000000000000000000;;		lastWidth  int          // The width of the current rune.  This
0000000000000000000000000000000000000000;;		buf        bytes.Buffer // Internal buffer used for building up values.
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// SyntaxError is the main error used whenever a lexing or parsing error occurs.
0000000000000000000000000000000000000000;;	type SyntaxError struct {
0000000000000000000000000000000000000000;;		msg        string // Error message displayed to user
0000000000000000000000000000000000000000;;		Expression string // Expression that generated a SyntaxError
0000000000000000000000000000000000000000;;		Offset     int    // The location in the string where the error occurred
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (e SyntaxError) Error() string {
0000000000000000000000000000000000000000;;		// In the future, it would be good to underline the specific
0000000000000000000000000000000000000000;;		// location where the error occurred.
0000000000000000000000000000000000000000;;		return "SyntaxError: " + e.msg
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// HighlightLocation will show where the syntax error occurred.
0000000000000000000000000000000000000000;;	// It will place a "^" character on a line below the expression
0000000000000000000000000000000000000000;;	// at the point where the syntax error occurred.
0000000000000000000000000000000000000000;;	func (e SyntaxError) HighlightLocation() string {
0000000000000000000000000000000000000000;;		return e.Expression + "\n" + strings.Repeat(" ", e.Offset) + "^"
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	//go:generate stringer -type=tokType
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		tUnknown tokType = iota
0000000000000000000000000000000000000000;;		tStar
0000000000000000000000000000000000000000;;		tDot
0000000000000000000000000000000000000000;;		tFilter
0000000000000000000000000000000000000000;;		tFlatten
0000000000000000000000000000000000000000;;		tLparen
0000000000000000000000000000000000000000;;		tRparen
0000000000000000000000000000000000000000;;		tLbracket
0000000000000000000000000000000000000000;;		tRbracket
0000000000000000000000000000000000000000;;		tLbrace
0000000000000000000000000000000000000000;;		tRbrace
0000000000000000000000000000000000000000;;		tOr
0000000000000000000000000000000000000000;;		tPipe
0000000000000000000000000000000000000000;;		tNumber
0000000000000000000000000000000000000000;;		tUnquotedIdentifier
0000000000000000000000000000000000000000;;		tQuotedIdentifier
0000000000000000000000000000000000000000;;		tComma
0000000000000000000000000000000000000000;;		tColon
0000000000000000000000000000000000000000;;		tLT
0000000000000000000000000000000000000000;;		tLTE
0000000000000000000000000000000000000000;;		tGT
0000000000000000000000000000000000000000;;		tGTE
0000000000000000000000000000000000000000;;		tEQ
0000000000000000000000000000000000000000;;		tNE
0000000000000000000000000000000000000000;;		tJSONLiteral
0000000000000000000000000000000000000000;;		tStringLiteral
0000000000000000000000000000000000000000;;		tCurrent
0000000000000000000000000000000000000000;;		tExpref
0000000000000000000000000000000000000000;;		tAnd
0000000000000000000000000000000000000000;;		tNot
0000000000000000000000000000000000000000;;		tEOF
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var basicTokens = map[rune]tokType{
0000000000000000000000000000000000000000;;		'.': tDot,
0000000000000000000000000000000000000000;;		'*': tStar,
0000000000000000000000000000000000000000;;		',': tComma,
0000000000000000000000000000000000000000;;		':': tColon,
0000000000000000000000000000000000000000;;		'{': tLbrace,
0000000000000000000000000000000000000000;;		'}': tRbrace,
0000000000000000000000000000000000000000;;		']': tRbracket, // tLbracket not included because it could be "[]"
0000000000000000000000000000000000000000;;		'(': tLparen,
0000000000000000000000000000000000000000;;		')': tRparen,
0000000000000000000000000000000000000000;;		'@': tCurrent,
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Bit mask for [a-zA-Z_] shifted down 64 bits to fit in a single uint64.
0000000000000000000000000000000000000000;;	// When using this bitmask just be sure to shift the rune down 64 bits
0000000000000000000000000000000000000000;;	// before checking against identifierStartBits.
0000000000000000000000000000000000000000;;	const identifierStartBits uint64 = 576460745995190270
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Bit mask for [a-zA-Z0-9], 128 bits -> 2 uint64s.
0000000000000000000000000000000000000000;;	var identifierTrailingBits = [2]uint64{287948901175001088, 576460745995190270}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var whiteSpace = map[rune]bool{
0000000000000000000000000000000000000000;;		' ': true, '\t': true, '\n': true, '\r': true,
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (t token) String() string {
0000000000000000000000000000000000000000;;		return fmt.Sprintf("Token{%+v, %s, %d, %d}",
0000000000000000000000000000000000000000;;			t.tokenType, t.value, t.position, t.length)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewLexer creates a new JMESPath lexer.
0000000000000000000000000000000000000000;;	func NewLexer() *Lexer {
0000000000000000000000000000000000000000;;		lexer := Lexer{}
0000000000000000000000000000000000000000;;		return &lexer
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (lexer *Lexer) next() rune {
0000000000000000000000000000000000000000;;		if lexer.currentPos >= len(lexer.expression) {
0000000000000000000000000000000000000000;;			lexer.lastWidth = 0
0000000000000000000000000000000000000000;;			return eof
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		r, w := utf8.DecodeRuneInString(lexer.expression[lexer.currentPos:])
0000000000000000000000000000000000000000;;		lexer.lastWidth = w
0000000000000000000000000000000000000000;;		lexer.currentPos += w
0000000000000000000000000000000000000000;;		return r
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (lexer *Lexer) back() {
0000000000000000000000000000000000000000;;		lexer.currentPos -= lexer.lastWidth
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (lexer *Lexer) peek() rune {
0000000000000000000000000000000000000000;;		t := lexer.next()
0000000000000000000000000000000000000000;;		lexer.back()
0000000000000000000000000000000000000000;;		return t
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// tokenize takes an expression and returns corresponding tokens.
0000000000000000000000000000000000000000;;	func (lexer *Lexer) tokenize(expression string) ([]token, error) {
0000000000000000000000000000000000000000;;		var tokens []token
0000000000000000000000000000000000000000;;		lexer.expression = expression
0000000000000000000000000000000000000000;;		lexer.currentPos = 0
0000000000000000000000000000000000000000;;		lexer.lastWidth = 0
0000000000000000000000000000000000000000;;	loop:
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			r := lexer.next()
0000000000000000000000000000000000000000;;			if identifierStartBits&(1<<(uint64(r)-64)) > 0 {
0000000000000000000000000000000000000000;;				t := lexer.consumeUnquotedIdentifier()
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if val, ok := basicTokens[r]; ok {
0000000000000000000000000000000000000000;;				// Basic single char token.
0000000000000000000000000000000000000000;;				t := token{
0000000000000000000000000000000000000000;;					tokenType: val,
0000000000000000000000000000000000000000;;					value:     string(r),
0000000000000000000000000000000000000000;;					position:  lexer.currentPos - lexer.lastWidth,
0000000000000000000000000000000000000000;;					length:    1,
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '-' || (r >= '0' && r <= '9') {
0000000000000000000000000000000000000000;;				t := lexer.consumeNumber()
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '[' {
0000000000000000000000000000000000000000;;				t := lexer.consumeLBracket()
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '"' {
0000000000000000000000000000000000000000;;				t, err := lexer.consumeQuotedIdentifier()
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return tokens, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '\'' {
0000000000000000000000000000000000000000;;				t, err := lexer.consumeRawStringLiteral()
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return tokens, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '`' {
0000000000000000000000000000000000000000;;				t, err := lexer.consumeLiteral()
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return tokens, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '|' {
0000000000000000000000000000000000000000;;				t := lexer.matchOrElse(r, '|', tOr, tPipe)
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '<' {
0000000000000000000000000000000000000000;;				t := lexer.matchOrElse(r, '=', tLTE, tLT)
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '>' {
0000000000000000000000000000000000000000;;				t := lexer.matchOrElse(r, '=', tGTE, tGT)
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '!' {
0000000000000000000000000000000000000000;;				t := lexer.matchOrElse(r, '=', tNE, tNot)
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '=' {
0000000000000000000000000000000000000000;;				t := lexer.matchOrElse(r, '=', tEQ, tUnknown)
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == '&' {
0000000000000000000000000000000000000000;;				t := lexer.matchOrElse(r, '&', tAnd, tExpref)
0000000000000000000000000000000000000000;;				tokens = append(tokens, t)
0000000000000000000000000000000000000000;;			} else if r == eof {
0000000000000000000000000000000000000000;;				break loop
0000000000000000000000000000000000000000;;			} else if _, ok := whiteSpace[r]; ok {
0000000000000000000000000000000000000000;;				// Ignore whitespace
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				return tokens, lexer.syntaxError(fmt.Sprintf("Unknown char: %s", strconv.QuoteRuneToASCII(r)))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		tokens = append(tokens, token{tEOF, "", len(lexer.expression), 0})
0000000000000000000000000000000000000000;;		return tokens, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Consume characters until the ending rune "r" is reached.
0000000000000000000000000000000000000000;;	// If the end of the expression is reached before seeing the
0000000000000000000000000000000000000000;;	// terminating rune "r", then an error is returned.
0000000000000000000000000000000000000000;;	// If no error occurs then the matching substring is returned.
0000000000000000000000000000000000000000;;	// The returned string will not include the ending rune.
0000000000000000000000000000000000000000;;	func (lexer *Lexer) consumeUntil(end rune) (string, error) {
0000000000000000000000000000000000000000;;		start := lexer.currentPos
0000000000000000000000000000000000000000;;		current := lexer.next()
0000000000000000000000000000000000000000;;		for current != end && current != eof {
0000000000000000000000000000000000000000;;			if current == '\\' && lexer.peek() != eof {
0000000000000000000000000000000000000000;;				lexer.next()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			current = lexer.next()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if lexer.lastWidth == 0 {
0000000000000000000000000000000000000000;;			// Then we hit an EOF so we never reached the closing
0000000000000000000000000000000000000000;;			// delimiter.
0000000000000000000000000000000000000000;;			return "", SyntaxError{
0000000000000000000000000000000000000000;;				msg:        "Unclosed delimiter: " + string(end),
0000000000000000000000000000000000000000;;				Expression: lexer.expression,
0000000000000000000000000000000000000000;;				Offset:     len(lexer.expression),
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return lexer.expression[start : lexer.currentPos-lexer.lastWidth], nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (lexer *Lexer) consumeLiteral() (token, error) {
0000000000000000000000000000000000000000;;		start := lexer.currentPos
0000000000000000000000000000000000000000;;		value, err := lexer.consumeUntil('`')
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return token{}, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		value = strings.Replace(value, "\\`", "`", -1)
0000000000000000000000000000000000000000;;		return token{
0000000000000000000000000000000000000000;;			tokenType: tJSONLiteral,
0000000000000000000000000000000000000000;;			value:     value,
0000000000000000000000000000000000000000;;			position:  start,
0000000000000000000000000000000000000000;;			length:    len(value),
0000000000000000000000000000000000000000;;		}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (lexer *Lexer) consumeRawStringLiteral() (token, error) {
0000000000000000000000000000000000000000;;		start := lexer.currentPos
0000000000000000000000000000000000000000;;		currentIndex := start
0000000000000000000000000000000000000000;;		current := lexer.next()
0000000000000000000000000000000000000000;;		for current != '\'' && lexer.peek() != eof {
0000000000000000000000000000000000000000;;			if current == '\\' && lexer.peek() == '\'' {
0000000000000000000000000000000000000000;;				chunk := lexer.expression[currentIndex : lexer.currentPos-1]
0000000000000000000000000000000000000000;;				lexer.buf.WriteString(chunk)
0000000000000000000000000000000000000000;;				lexer.buf.WriteString("'")
0000000000000000000000000000000000000000;;				lexer.next()
0000000000000000000000000000000000000000;;				currentIndex = lexer.currentPos
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			current = lexer.next()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if lexer.lastWidth == 0 {
0000000000000000000000000000000000000000;;			// Then we hit an EOF so we never reached the closing
0000000000000000000000000000000000000000;;			// delimiter.
0000000000000000000000000000000000000000;;			return token{}, SyntaxError{
0000000000000000000000000000000000000000;;				msg:        "Unclosed delimiter: '",
0000000000000000000000000000000000000000;;				Expression: lexer.expression,
0000000000000000000000000000000000000000;;				Offset:     len(lexer.expression),
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if currentIndex < lexer.currentPos {
0000000000000000000000000000000000000000;;			lexer.buf.WriteString(lexer.expression[currentIndex : lexer.currentPos-1])
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		value := lexer.buf.String()
0000000000000000000000000000000000000000;;		// Reset the buffer so it can reused again.
0000000000000000000000000000000000000000;;		lexer.buf.Reset()
0000000000000000000000000000000000000000;;		return token{
0000000000000000000000000000000000000000;;			tokenType: tStringLiteral,
0000000000000000000000000000000000000000;;			value:     value,
0000000000000000000000000000000000000000;;			position:  start,
0000000000000000000000000000000000000000;;			length:    len(value),
0000000000000000000000000000000000000000;;		}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (lexer *Lexer) syntaxError(msg string) SyntaxError {
0000000000000000000000000000000000000000;;		return SyntaxError{
0000000000000000000000000000000000000000;;			msg:        msg,
0000000000000000000000000000000000000000;;			Expression: lexer.expression,
0000000000000000000000000000000000000000;;			Offset:     lexer.currentPos - 1,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Checks for a two char token, otherwise matches a single character
0000000000000000000000000000000000000000;;	// token. This is used whenever a two char token overlaps a single
0000000000000000000000000000000000000000;;	// char token, e.g. "||" -> tPipe, "|" -> tOr.
0000000000000000000000000000000000000000;;	func (lexer *Lexer) matchOrElse(first rune, second rune, matchedType tokType, singleCharType tokType) token {
0000000000000000000000000000000000000000;;		start := lexer.currentPos - lexer.lastWidth
0000000000000000000000000000000000000000;;		nextRune := lexer.next()
0000000000000000000000000000000000000000;;		var t token
0000000000000000000000000000000000000000;;		if nextRune == second {
0000000000000000000000000000000000000000;;			t = token{
0000000000000000000000000000000000000000;;				tokenType: matchedType,
0000000000000000000000000000000000000000;;				value:     string(first) + string(second),
0000000000000000000000000000000000000000;;				position:  start,
0000000000000000000000000000000000000000;;				length:    2,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			lexer.back()
0000000000000000000000000000000000000000;;			t = token{
0000000000000000000000000000000000000000;;				tokenType: singleCharType,
0000000000000000000000000000000000000000;;				value:     string(first),
0000000000000000000000000000000000000000;;				position:  start,
0000000000000000000000000000000000000000;;				length:    1,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return t
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (lexer *Lexer) consumeLBracket() token {
0000000000000000000000000000000000000000;;		// There's three options here:
0000000000000000000000000000000000000000;;		// 1. A filter expression "[?"
0000000000000000000000000000000000000000;;		// 2. A flatten operator "[]"
0000000000000000000000000000000000000000;;		// 3. A bare rbracket "["
0000000000000000000000000000000000000000;;		start := lexer.currentPos - lexer.lastWidth
0000000000000000000000000000000000000000;;		nextRune := lexer.next()
0000000000000000000000000000000000000000;;		var t token
0000000000000000000000000000000000000000;;		if nextRune == '?' {
0000000000000000000000000000000000000000;;			t = token{
0000000000000000000000000000000000000000;;				tokenType: tFilter,
0000000000000000000000000000000000000000;;				value:     "[?",
0000000000000000000000000000000000000000;;				position:  start,
0000000000000000000000000000000000000000;;				length:    2,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else if nextRune == ']' {
0000000000000000000000000000000000000000;;			t = token{
0000000000000000000000000000000000000000;;				tokenType: tFlatten,
0000000000000000000000000000000000000000;;				value:     "[]",
0000000000000000000000000000000000000000;;				position:  start,
0000000000000000000000000000000000000000;;				length:    2,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			t = token{
0000000000000000000000000000000000000000;;				tokenType: tLbracket,
0000000000000000000000000000000000000000;;				value:     "[",
0000000000000000000000000000000000000000;;				position:  start,
0000000000000000000000000000000000000000;;				length:    1,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			lexer.back()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return t
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (lexer *Lexer) consumeQuotedIdentifier() (token, error) {
0000000000000000000000000000000000000000;;		start := lexer.currentPos
0000000000000000000000000000000000000000;;		value, err := lexer.consumeUntil('"')
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return token{}, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var decoded string
0000000000000000000000000000000000000000;;		asJSON := []byte("\"" + value + "\"")
0000000000000000000000000000000000000000;;		if err := json.Unmarshal([]byte(asJSON), &decoded); err != nil {
0000000000000000000000000000000000000000;;			return token{}, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return token{
0000000000000000000000000000000000000000;;			tokenType: tQuotedIdentifier,
0000000000000000000000000000000000000000;;			value:     decoded,
0000000000000000000000000000000000000000;;			position:  start - 1,
0000000000000000000000000000000000000000;;			length:    len(decoded),
0000000000000000000000000000000000000000;;		}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (lexer *Lexer) consumeUnquotedIdentifier() token {
0000000000000000000000000000000000000000;;		// Consume runes until we reach the end of an unquoted
0000000000000000000000000000000000000000;;		// identifier.
0000000000000000000000000000000000000000;;		start := lexer.currentPos - lexer.lastWidth
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			r := lexer.next()
0000000000000000000000000000000000000000;;			if r < 0 || r > 128 || identifierTrailingBits[uint64(r)/64]&(1<<(uint64(r)%64)) == 0 {
0000000000000000000000000000000000000000;;				lexer.back()
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		value := lexer.expression[start:lexer.currentPos]
0000000000000000000000000000000000000000;;		return token{
0000000000000000000000000000000000000000;;			tokenType: tUnquotedIdentifier,
0000000000000000000000000000000000000000;;			value:     value,
0000000000000000000000000000000000000000;;			position:  start,
0000000000000000000000000000000000000000;;			length:    lexer.currentPos - start,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (lexer *Lexer) consumeNumber() token {
0000000000000000000000000000000000000000;;		// Consume runes until we reach something that's not a number.
0000000000000000000000000000000000000000;;		start := lexer.currentPos - lexer.lastWidth
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			r := lexer.next()
0000000000000000000000000000000000000000;;			if r < '0' || r > '9' {
0000000000000000000000000000000000000000;;				lexer.back()
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		value := lexer.expression[start:lexer.currentPos]
0000000000000000000000000000000000000000;;		return token{
0000000000000000000000000000000000000000;;			tokenType: tNumber,
0000000000000000000000000000000000000000;;			value:     value,
0000000000000000000000000000000000000000;;			position:  start,
0000000000000000000000000000000000000000;;			length:    lexer.currentPos - start,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
