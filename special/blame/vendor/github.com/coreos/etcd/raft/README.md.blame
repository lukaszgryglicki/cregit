0000000000000000000000000000000000000000;;	# Raft library
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Raft is a protocol with which a cluster of nodes can maintain a replicated state machine.
0000000000000000000000000000000000000000;;	The state machine is kept in sync through the use of a replicated log.
0000000000000000000000000000000000000000;;	For more details on Raft, see "In Search of an Understandable Consensus Algorithm"
0000000000000000000000000000000000000000;;	(https://ramcloud.stanford.edu/raft.pdf) by Diego Ongaro and John Ousterhout.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	This Raft library is stable and feature complete. As of 2016, it is **the most widely used** Raft library in production, serving tens of thousands clusters each day. It powers distributed systems such as etcd, Kubernetes, Docker Swarm, Cloud Foundry Diego, CockroachDB, TiDB, Project Calico, Flannel, and more.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Most Raft implementations have a monolithic design, including storage handling, messaging serialization, and network transport. This library instead follows a minimalistic design philosophy by only implementing the core raft algorithm. This minimalism buys flexibility, determinism, and performance.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To keep the codebase small as well as provide flexibility, the library only implements the Raft algorithm; both network and disk IO are left to the user. Library users must implement their own transportation layer for message passing between Raft peers over the wire. Similarly, users must implement their own storage layer to persist the Raft log and state.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	In order to easily test the Raft library, its behavior should be deterministic. To achieve this determinism, the library models Raft as a state machine.  The state machine takes a `Message` as input. A message can either be a local timer update or a network message sent from a remote peer. The state machine's output is a 3-tuple `{[]Messages, []LogEntries, NextState}` consisting of an array of `Messages`, `log entries`, and `Raft state changes`. For state machines with the same state, the same state machine input should always generate the same state machine output.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	A simple example application, _raftexample_, is also available to help illustrate
0000000000000000000000000000000000000000;;	how to use this package in practice:
0000000000000000000000000000000000000000;;	https://github.com/coreos/etcd/tree/master/contrib/raftexample
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Features
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	This raft implementation is a full feature implementation of Raft protocol. Features includes:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	- Leader election
0000000000000000000000000000000000000000;;	- Log replication
0000000000000000000000000000000000000000;;	- Log compaction 
0000000000000000000000000000000000000000;;	- Membership changes
0000000000000000000000000000000000000000;;	- Leadership transfer extension
0000000000000000000000000000000000000000;;	- Efficient linearizable read-only queries served by both the leader and followers
0000000000000000000000000000000000000000;;	 - leader checks with quorum and bypasses Raft log before processing read-only queries
0000000000000000000000000000000000000000;;	 - followers asks leader to get a safe read index before processing read-only queries
0000000000000000000000000000000000000000;;	- More efficient lease-based linearizable read-only queries served by both the leader and followers
0000000000000000000000000000000000000000;;	 - leader bypasses Raft log and processing read-only queries locally
0000000000000000000000000000000000000000;;	 - followers asks leader to get a safe read index before processing read-only queries
0000000000000000000000000000000000000000;;	 - this approach relies on the clock of the all the machines in raft group
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	This raft implementation also includes a few optional enhancements:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	- Optimistic pipelining to reduce log replication latency
0000000000000000000000000000000000000000;;	- Flow control for log replication
0000000000000000000000000000000000000000;;	- Batching Raft messages to reduce synchronized network I/O calls
0000000000000000000000000000000000000000;;	- Batching log entries to reduce disk synchronized I/O
0000000000000000000000000000000000000000;;	- Writing to leader's disk in parallel
0000000000000000000000000000000000000000;;	- Internal proposal redirection from followers to leader
0000000000000000000000000000000000000000;;	- Automatic stepping down when the leader loses quorum 
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	## Notable Users
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	- [cockroachdb](https://github.com/cockroachdb/cockroach) A Scalable, Survivable, Strongly-Consistent SQL Database
0000000000000000000000000000000000000000;;	- [dgraph](https://github.com/dgraph-io/dgraph) A Scalable, Distributed, Low Latency, High Throughput Graph Database
0000000000000000000000000000000000000000;;	- [etcd](https://github.com/coreos/etcd) A distributed reliable key-value store
0000000000000000000000000000000000000000;;	- [tikv](https://github.com/pingcap/tikv) A Distributed transactional key value database powered by Rust and Raft
0000000000000000000000000000000000000000;;	- [swarmkit](https://github.com/docker/swarmkit) A toolkit for orchestrating distributed systems at any scale.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	## Usage
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	The primary object in raft is a Node. You either start a Node from scratch
0000000000000000000000000000000000000000;;	using raft.StartNode or start a Node from some initial state using raft.RestartNode.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To start a three-node cluster
0000000000000000000000000000000000000000;;	```go
0000000000000000000000000000000000000000;;	  storage := raft.NewMemoryStorage()
0000000000000000000000000000000000000000;;	  c := &Config{
0000000000000000000000000000000000000000;;	    ID:              0x01,
0000000000000000000000000000000000000000;;	    ElectionTick:    10,
0000000000000000000000000000000000000000;;	    HeartbeatTick:   1,
0000000000000000000000000000000000000000;;	    Storage:         storage,
0000000000000000000000000000000000000000;;	    MaxSizePerMsg:   4096,
0000000000000000000000000000000000000000;;	    MaxInflightMsgs: 256,
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	  // Set peer list to the other nodes in the cluster.
0000000000000000000000000000000000000000;;	  // Note that they need to be started separately as well.
0000000000000000000000000000000000000000;;	  n := raft.StartNode(c, []raft.Peer{{ID: 0x02}, {ID: 0x03}})
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	You can start a single node cluster, like so:
0000000000000000000000000000000000000000;;	```go
0000000000000000000000000000000000000000;;	  // Create storage and config as shown above.
0000000000000000000000000000000000000000;;	  // Set peer list to itself, so this node can become the leader of this single-node cluster.
0000000000000000000000000000000000000000;;	  peers := []raft.Peer{{ID: 0x01}}
0000000000000000000000000000000000000000;;	  n := raft.StartNode(c, peers)
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To allow a new node to join this cluster, do not pass in any peers. First, you need add the node to the existing cluster by calling `ProposeConfChange` on any existing node inside the cluster. Then, you can start the node with empty peer list, like so:
0000000000000000000000000000000000000000;;	```go
0000000000000000000000000000000000000000;;	  // Create storage and config as shown above.
0000000000000000000000000000000000000000;;	  n := raft.StartNode(c, nil)
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To restart a node from previous state:
0000000000000000000000000000000000000000;;	```go
0000000000000000000000000000000000000000;;	  storage := raft.NewMemoryStorage()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  // Recover the in-memory storage from persistent snapshot, state and entries.
0000000000000000000000000000000000000000;;	  storage.ApplySnapshot(snapshot)
0000000000000000000000000000000000000000;;	  storage.SetHardState(state)
0000000000000000000000000000000000000000;;	  storage.Append(entries)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  c := &Config{
0000000000000000000000000000000000000000;;	    ID:              0x01,
0000000000000000000000000000000000000000;;	    ElectionTick:    10,
0000000000000000000000000000000000000000;;	    HeartbeatTick:   1,
0000000000000000000000000000000000000000;;	    Storage:         storage,
0000000000000000000000000000000000000000;;	    MaxSizePerMsg:   4096,
0000000000000000000000000000000000000000;;	    MaxInflightMsgs: 256,
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  // Restart raft without peer information.
0000000000000000000000000000000000000000;;	  // Peer information is already included in the storage.
0000000000000000000000000000000000000000;;	  n := raft.RestartNode(c)
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Now that you are holding onto a Node you have a few responsibilities:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	First, you must read from the Node.Ready() channel and process the updates
0000000000000000000000000000000000000000;;	it contains. These steps may be performed in parallel, except as noted in step
0000000000000000000000000000000000000000;;	2.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	1. Write HardState, Entries, and Snapshot to persistent storage if they are
0000000000000000000000000000000000000000;;	not empty. Note that when writing an Entry with Index i, any
0000000000000000000000000000000000000000;;	previously-persisted entries with Index >= i must be discarded.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	2. Send all Messages to the nodes named in the To field. It is important that
0000000000000000000000000000000000000000;;	no messages be sent until the latest HardState has been persisted to disk,
0000000000000000000000000000000000000000;;	and all Entries written by any previous Ready batch (Messages may be sent while
0000000000000000000000000000000000000000;;	entries from the same batch are being persisted). To reduce the I/O latency, an
0000000000000000000000000000000000000000;;	optimization can be applied to make leader write to disk in parallel with its
0000000000000000000000000000000000000000;;	followers (as explained at section 10.2.1 in Raft thesis). If any Message has type
0000000000000000000000000000000000000000;;	MsgSnap, call Node.ReportSnapshot() after it has been sent (these messages may be
0000000000000000000000000000000000000000;;	large). Note: Marshalling messages is not thread-safe; it is important that you
0000000000000000000000000000000000000000;;	make sure that no new entries are persisted while marshalling.
0000000000000000000000000000000000000000;;	The easiest way to achieve this is to serialise the messages directly inside
0000000000000000000000000000000000000000;;	your main raft loop.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	3. Apply Snapshot (if any) and CommittedEntries to the state machine.
0000000000000000000000000000000000000000;;	If any committed Entry has Type EntryConfChange, call Node.ApplyConfChange()
0000000000000000000000000000000000000000;;	to apply it to the node. The configuration change may be cancelled at this point
0000000000000000000000000000000000000000;;	by setting the NodeID field to zero before calling ApplyConfChange
0000000000000000000000000000000000000000;;	(but ApplyConfChange must be called one way or the other, and the decision to cancel
0000000000000000000000000000000000000000;;	must be based solely on the state machine and not external information such as
0000000000000000000000000000000000000000;;	the observed health of the node).
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	4. Call Node.Advance() to signal readiness for the next batch of updates.
0000000000000000000000000000000000000000;;	This may be done at any time after step 1, although all updates must be processed
0000000000000000000000000000000000000000;;	in the order they were returned by Ready.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Second, all persisted log entries must be made available via an
0000000000000000000000000000000000000000;;	implementation of the Storage interface. The provided MemoryStorage
0000000000000000000000000000000000000000;;	type can be used for this (if you repopulate its state upon a
0000000000000000000000000000000000000000;;	restart), or you can supply your own disk-backed implementation.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Third, when you receive a message from another node, pass it to Node.Step:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	```go
0000000000000000000000000000000000000000;;		func recvRaftRPC(ctx context.Context, m raftpb.Message) {
0000000000000000000000000000000000000000;;			n.Step(ctx, m)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Finally, you need to call `Node.Tick()` at regular intervals (probably
0000000000000000000000000000000000000000;;	via a `time.Ticker`). Raft has two important timeouts: heartbeat and the
0000000000000000000000000000000000000000;;	election timeout. However, internally to the raft package time is
0000000000000000000000000000000000000000;;	represented by an abstract "tick".
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	The total state machine handling loop will look something like this:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	```go
0000000000000000000000000000000000000000;;	  for {
0000000000000000000000000000000000000000;;	    select {
0000000000000000000000000000000000000000;;	    case <-s.Ticker:
0000000000000000000000000000000000000000;;	      n.Tick()
0000000000000000000000000000000000000000;;	    case rd := <-s.Node.Ready():
0000000000000000000000000000000000000000;;	      saveToStorage(rd.State, rd.Entries, rd.Snapshot)
0000000000000000000000000000000000000000;;	      send(rd.Messages)
0000000000000000000000000000000000000000;;	      if !raft.IsEmptySnap(rd.Snapshot) {
0000000000000000000000000000000000000000;;	        processSnapshot(rd.Snapshot)
0000000000000000000000000000000000000000;;	      }
0000000000000000000000000000000000000000;;	      for _, entry := range rd.CommittedEntries {
0000000000000000000000000000000000000000;;	        process(entry)
0000000000000000000000000000000000000000;;	        if entry.Type == raftpb.EntryConfChange {
0000000000000000000000000000000000000000;;	          var cc raftpb.ConfChange
0000000000000000000000000000000000000000;;	          cc.Unmarshal(entry.Data)
0000000000000000000000000000000000000000;;	          s.Node.ApplyConfChange(cc)
0000000000000000000000000000000000000000;;	        }
0000000000000000000000000000000000000000;;	      }
0000000000000000000000000000000000000000;;	      s.Node.Advance()
0000000000000000000000000000000000000000;;	    case <-s.done:
0000000000000000000000000000000000000000;;	      return
0000000000000000000000000000000000000000;;	    }
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To propose changes to the state machine from your node take your application
0000000000000000000000000000000000000000;;	data, serialize it into a byte slice and call:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	```go
0000000000000000000000000000000000000000;;		n.Propose(ctx, data)
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	If the proposal is committed, data will appear in committed entries with type
0000000000000000000000000000000000000000;;	raftpb.EntryNormal. There is no guarantee that a proposed command will be
0000000000000000000000000000000000000000;;	committed; you may have to re-propose after a timeout.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To add or remove node in a cluster, build ConfChange struct 'cc' and call:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	```go
0000000000000000000000000000000000000000;;		n.ProposeConfChange(ctx, cc)
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	After config change is committed, some committed entry with type
0000000000000000000000000000000000000000;;	raftpb.EntryConfChange will be returned. You must apply it to node through:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	```go
0000000000000000000000000000000000000000;;		var cc raftpb.ConfChange
0000000000000000000000000000000000000000;;		cc.Unmarshal(data)
0000000000000000000000000000000000000000;;		n.ApplyConfChange(cc)
0000000000000000000000000000000000000000;;	```
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Note: An ID represents a unique node in a cluster for all time. A
0000000000000000000000000000000000000000;;	given ID MUST be used only once even if the old node has been removed.
0000000000000000000000000000000000000000;;	This means that for example IP addresses make poor node IDs since they
0000000000000000000000000000000000000000;;	may be reused. Node IDs must be non-zero.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	## Implementation notes
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	This implementation is up to date with the final Raft thesis
0000000000000000000000000000000000000000;;	(https://ramcloud.stanford.edu/~ongaro/thesis.pdf), although our
0000000000000000000000000000000000000000;;	implementation of the membership change protocol differs somewhat from
0000000000000000000000000000000000000000;;	that described in chapter 4. The key invariant that membership changes
0000000000000000000000000000000000000000;;	happen one node at a time is preserved, but in our implementation the
0000000000000000000000000000000000000000;;	membership change takes effect when its entry is applied, not when it
0000000000000000000000000000000000000000;;	is added to the log (so the entry is committed under the old
0000000000000000000000000000000000000000;;	membership instead of the new). This is equivalent in terms of safety,
0000000000000000000000000000000000000000;;	since the old and new configurations are guaranteed to overlap.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To ensure that we do not attempt to commit two membership changes at
0000000000000000000000000000000000000000;;	once by matching log positions (which would be unsafe since they
0000000000000000000000000000000000000000;;	should have different quorum requirements), we simply disallow any
0000000000000000000000000000000000000000;;	proposed membership change while any uncommitted change appears in
0000000000000000000000000000000000000000;;	the leader's log.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	This approach introduces a problem when you try to remove a member
0000000000000000000000000000000000000000;;	from a two-member cluster: If one of the members dies before the
0000000000000000000000000000000000000000;;	other one receives the commit of the confchange entry, then the member
0000000000000000000000000000000000000000;;	cannot be removed any more since the cluster cannot make progress.
0000000000000000000000000000000000000000;;	For this reason it is highly recommended to use three or more nodes in
0000000000000000000000000000000000000000;;	every cluster.
