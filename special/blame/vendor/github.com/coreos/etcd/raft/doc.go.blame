0000000000000000000000000000000000000000;;	// Copyright 2015 The etcd Authors
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	// you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	// You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	//     http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	// distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	// See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	// limitations under the License.
5da4772d7eee0df4a5bf872fd32f72643fc2e5c9;Godeps/_workspace/src/github.com/coreos/etcd/raft/doc.go[Godeps/_workspace/src/github.com/coreos/etcd/raft/doc.go][vendor/github.com/coreos/etcd/raft/doc.go];	
0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Package raft sends and receives messages in the Protocol Buffer format
0000000000000000000000000000000000000000;;	defined in the raftpb package.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Raft is a protocol with which a cluster of nodes can maintain a replicated state machine.
0000000000000000000000000000000000000000;;	The state machine is kept in sync through the use of a replicated log.
0000000000000000000000000000000000000000;;	For more details on Raft, see "In Search of an Understandable Consensus Algorithm"
0000000000000000000000000000000000000000;;	(https://ramcloud.stanford.edu/raft.pdf) by Diego Ongaro and John Ousterhout.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	A simple example application, _raftexample_, is also available to help illustrate
0000000000000000000000000000000000000000;;	how to use this package in practice:
0000000000000000000000000000000000000000;;	https://github.com/coreos/etcd/tree/master/contrib/raftexample
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Usage
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	The primary object in raft is a Node. You either start a Node from scratch
0000000000000000000000000000000000000000;;	using raft.StartNode or start a Node from some initial state using raft.RestartNode.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To start a node from scratch:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  storage := raft.NewMemoryStorage()
0000000000000000000000000000000000000000;;	  c := &Config{
0000000000000000000000000000000000000000;;	    ID:              0x01,
0000000000000000000000000000000000000000;;	    ElectionTick:    10,
0000000000000000000000000000000000000000;;	    HeartbeatTick:   1,
0000000000000000000000000000000000000000;;	    Storage:         storage,
0000000000000000000000000000000000000000;;	    MaxSizePerMsg:   4096,
0000000000000000000000000000000000000000;;	    MaxInflightMsgs: 256,
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	  n := raft.StartNode(c, []raft.Peer{{ID: 0x02}, {ID: 0x03}})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To restart a node from previous state:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  storage := raft.NewMemoryStorage()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  // recover the in-memory storage from persistent
0000000000000000000000000000000000000000;;	  // snapshot, state and entries.
0000000000000000000000000000000000000000;;	  storage.ApplySnapshot(snapshot)
0000000000000000000000000000000000000000;;	  storage.SetHardState(state)
0000000000000000000000000000000000000000;;	  storage.Append(entries)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  c := &Config{
0000000000000000000000000000000000000000;;	    ID:              0x01,
0000000000000000000000000000000000000000;;	    ElectionTick:    10,
0000000000000000000000000000000000000000;;	    HeartbeatTick:   1,
0000000000000000000000000000000000000000;;	    Storage:         storage,
0000000000000000000000000000000000000000;;	    MaxSizePerMsg:   4096,
0000000000000000000000000000000000000000;;	    MaxInflightMsgs: 256,
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  // restart raft without peer information.
0000000000000000000000000000000000000000;;	  // peer information is already included in the storage.
0000000000000000000000000000000000000000;;	  n := raft.RestartNode(c)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Now that you are holding onto a Node you have a few responsibilities:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	First, you must read from the Node.Ready() channel and process the updates
0000000000000000000000000000000000000000;;	it contains. These steps may be performed in parallel, except as noted in step
0000000000000000000000000000000000000000;;	2.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	1. Write HardState, Entries, and Snapshot to persistent storage if they are
0000000000000000000000000000000000000000;;	not empty. Note that when writing an Entry with Index i, any
0000000000000000000000000000000000000000;;	previously-persisted entries with Index >= i must be discarded.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	2. Send all Messages to the nodes named in the To field. It is important that
0000000000000000000000000000000000000000;;	no messages be sent until the latest HardState has been persisted to disk,
0000000000000000000000000000000000000000;;	and all Entries written by any previous Ready batch (Messages may be sent while
0000000000000000000000000000000000000000;;	entries from the same batch are being persisted). To reduce the I/O latency, an
0000000000000000000000000000000000000000;;	optimization can be applied to make leader write to disk in parallel with its
0000000000000000000000000000000000000000;;	followers (as explained at section 10.2.1 in Raft thesis). If any Message has type
0000000000000000000000000000000000000000;;	MsgSnap, call Node.ReportSnapshot() after it has been sent (these messages may be
0000000000000000000000000000000000000000;;	large).
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Note: Marshalling messages is not thread-safe; it is important that you
0000000000000000000000000000000000000000;;	make sure that no new entries are persisted while marshalling.
0000000000000000000000000000000000000000;;	The easiest way to achieve this is to serialise the messages directly inside
0000000000000000000000000000000000000000;;	your main raft loop.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	3. Apply Snapshot (if any) and CommittedEntries to the state machine.
0000000000000000000000000000000000000000;;	If any committed Entry has Type EntryConfChange, call Node.ApplyConfChange()
0000000000000000000000000000000000000000;;	to apply it to the node. The configuration change may be cancelled at this point
0000000000000000000000000000000000000000;;	by setting the NodeID field to zero before calling ApplyConfChange
0000000000000000000000000000000000000000;;	(but ApplyConfChange must be called one way or the other, and the decision to cancel
0000000000000000000000000000000000000000;;	must be based solely on the state machine and not external information such as
0000000000000000000000000000000000000000;;	the observed health of the node).
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	4. Call Node.Advance() to signal readiness for the next batch of updates.
0000000000000000000000000000000000000000;;	This may be done at any time after step 1, although all updates must be processed
0000000000000000000000000000000000000000;;	in the order they were returned by Ready.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Second, all persisted log entries must be made available via an
0000000000000000000000000000000000000000;;	implementation of the Storage interface. The provided MemoryStorage
0000000000000000000000000000000000000000;;	type can be used for this (if you repopulate its state upon a
0000000000000000000000000000000000000000;;	restart), or you can supply your own disk-backed implementation.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Third, when you receive a message from another node, pass it to Node.Step:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		func recvRaftRPC(ctx context.Context, m raftpb.Message) {
0000000000000000000000000000000000000000;;			n.Step(ctx, m)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Finally, you need to call Node.Tick() at regular intervals (probably
0000000000000000000000000000000000000000;;	via a time.Ticker). Raft has two important timeouts: heartbeat and the
0000000000000000000000000000000000000000;;	election timeout. However, internally to the raft package time is
0000000000000000000000000000000000000000;;	represented by an abstract "tick".
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	The total state machine handling loop will look something like this:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  for {
0000000000000000000000000000000000000000;;	    select {
0000000000000000000000000000000000000000;;	    case <-s.Ticker:
0000000000000000000000000000000000000000;;	      n.Tick()
0000000000000000000000000000000000000000;;	    case rd := <-s.Node.Ready():
0000000000000000000000000000000000000000;;	      saveToStorage(rd.State, rd.Entries, rd.Snapshot)
0000000000000000000000000000000000000000;;	      send(rd.Messages)
0000000000000000000000000000000000000000;;	      if !raft.IsEmptySnap(rd.Snapshot) {
0000000000000000000000000000000000000000;;	        processSnapshot(rd.Snapshot)
0000000000000000000000000000000000000000;;	      }
0000000000000000000000000000000000000000;;	      for _, entry := range rd.CommittedEntries {
0000000000000000000000000000000000000000;;	        process(entry)
0000000000000000000000000000000000000000;;	        if entry.Type == raftpb.EntryConfChange {
0000000000000000000000000000000000000000;;	          var cc raftpb.ConfChange
0000000000000000000000000000000000000000;;	          cc.Unmarshal(entry.Data)
0000000000000000000000000000000000000000;;	          s.Node.ApplyConfChange(cc)
0000000000000000000000000000000000000000;;	        }
0000000000000000000000000000000000000000;;	      }
0000000000000000000000000000000000000000;;	      s.Node.Advance()
0000000000000000000000000000000000000000;;	    case <-s.done:
0000000000000000000000000000000000000000;;	      return
0000000000000000000000000000000000000000;;	    }
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To propose changes to the state machine from your node take your application
0000000000000000000000000000000000000000;;	data, serialize it into a byte slice and call:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		n.Propose(ctx, data)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	If the proposal is committed, data will appear in committed entries with type
0000000000000000000000000000000000000000;;	raftpb.EntryNormal. There is no guarantee that a proposed command will be
0000000000000000000000000000000000000000;;	committed; you may have to re-propose after a timeout.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To add or remove node in a cluster, build ConfChange struct 'cc' and call:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		n.ProposeConfChange(ctx, cc)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	After config change is committed, some committed entry with type
0000000000000000000000000000000000000000;;	raftpb.EntryConfChange will be returned. You must apply it to node through:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var cc raftpb.ConfChange
0000000000000000000000000000000000000000;;		cc.Unmarshal(data)
0000000000000000000000000000000000000000;;		n.ApplyConfChange(cc)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Note: An ID represents a unique node in a cluster for all time. A
0000000000000000000000000000000000000000;;	given ID MUST be used only once even if the old node has been removed.
0000000000000000000000000000000000000000;;	This means that for example IP addresses make poor node IDs since they
0000000000000000000000000000000000000000;;	may be reused. Node IDs must be non-zero.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Implementation notes
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	This implementation is up to date with the final Raft thesis
0000000000000000000000000000000000000000;;	(https://ramcloud.stanford.edu/~ongaro/thesis.pdf), although our
0000000000000000000000000000000000000000;;	implementation of the membership change protocol differs somewhat from
0000000000000000000000000000000000000000;;	that described in chapter 4. The key invariant that membership changes
0000000000000000000000000000000000000000;;	happen one node at a time is preserved, but in our implementation the
0000000000000000000000000000000000000000;;	membership change takes effect when its entry is applied, not when it
0000000000000000000000000000000000000000;;	is added to the log (so the entry is committed under the old
0000000000000000000000000000000000000000;;	membership instead of the new). This is equivalent in terms of safety,
0000000000000000000000000000000000000000;;	since the old and new configurations are guaranteed to overlap.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	To ensure that we do not attempt to commit two membership changes at
0000000000000000000000000000000000000000;;	once by matching log positions (which would be unsafe since they
0000000000000000000000000000000000000000;;	should have different quorum requirements), we simply disallow any
0000000000000000000000000000000000000000;;	proposed membership change while any uncommitted change appears in
0000000000000000000000000000000000000000;;	the leader's log.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	This approach introduces a problem when you try to remove a member
0000000000000000000000000000000000000000;;	from a two-member cluster: If one of the members dies before the
0000000000000000000000000000000000000000;;	other one receives the commit of the confchange entry, then the member
0000000000000000000000000000000000000000;;	cannot be removed any more since the cluster cannot make progress.
0000000000000000000000000000000000000000;;	For this reason it is highly recommended to use three or more nodes in
0000000000000000000000000000000000000000;;	every cluster.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	MessageType
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Package raft sends and receives message in Protocol Buffer format (defined
0000000000000000000000000000000000000000;;	in raftpb package). Each state (follower, candidate, leader) implements its
0000000000000000000000000000000000000000;;	own 'step' method ('stepFollower', 'stepCandidate', 'stepLeader') when
0000000000000000000000000000000000000000;;	advancing with the given raftpb.Message. Each step is determined by its
0000000000000000000000000000000000000000;;	raftpb.MessageType. Note that every step is checked by one common method
0000000000000000000000000000000000000000;;	'Step' that safety-checks the terms of node and incoming message to prevent
0000000000000000000000000000000000000000;;	stale log entries:
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgHup' is used for election. If a node is a follower or candidate, the
0000000000000000000000000000000000000000;;		'tick' function in 'raft' struct is set as 'tickElection'. If a follower or
0000000000000000000000000000000000000000;;		candidate has not received any heartbeat before the election timeout, it
0000000000000000000000000000000000000000;;		passes 'MsgHup' to its Step method and becomes (or remains) a candidate to
0000000000000000000000000000000000000000;;		start a new election.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgBeat' is an internal type that signals the leader to send a heartbeat of
0000000000000000000000000000000000000000;;		the 'MsgHeartbeat' type. If a node is a leader, the 'tick' function in
0000000000000000000000000000000000000000;;		the 'raft' struct is set as 'tickHeartbeat', and triggers the leader to
0000000000000000000000000000000000000000;;		send periodic 'MsgHeartbeat' messages to its followers.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgProp' proposes to append data to its log entries. This is a special
0000000000000000000000000000000000000000;;		type to redirect proposals to leader. Therefore, send method overwrites
0000000000000000000000000000000000000000;;		raftpb.Message's term with its HardState's term to avoid attaching its
0000000000000000000000000000000000000000;;		local term to 'MsgProp'. When 'MsgProp' is passed to the leader's 'Step'
0000000000000000000000000000000000000000;;		method, the leader first calls the 'appendEntry' method to append entries
0000000000000000000000000000000000000000;;		to its log, and then calls 'bcastAppend' method to send those entries to
0000000000000000000000000000000000000000;;		its peers. When passed to candidate, 'MsgProp' is dropped. When passed to
0000000000000000000000000000000000000000;;		follower, 'MsgProp' is stored in follower's mailbox(msgs) by the send
0000000000000000000000000000000000000000;;		method. It is stored with sender's ID and later forwarded to leader by
0000000000000000000000000000000000000000;;		rafthttp package.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgApp' contains log entries to replicate. A leader calls bcastAppend,
0000000000000000000000000000000000000000;;		which calls sendAppend, which sends soon-to-be-replicated logs in 'MsgApp'
0000000000000000000000000000000000000000;;		type. When 'MsgApp' is passed to candidate's Step method, candidate reverts
0000000000000000000000000000000000000000;;		back to follower, because it indicates that there is a valid leader sending
0000000000000000000000000000000000000000;;		'MsgApp' messages. Candidate and follower respond to this message in
0000000000000000000000000000000000000000;;		'MsgAppResp' type.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgAppResp' is response to log replication request('MsgApp'). When
0000000000000000000000000000000000000000;;		'MsgApp' is passed to candidate or follower's Step method, it responds by
0000000000000000000000000000000000000000;;		calling 'handleAppendEntries' method, which sends 'MsgAppResp' to raft
0000000000000000000000000000000000000000;;		mailbox.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgVote' requests votes for election. When a node is a follower or
0000000000000000000000000000000000000000;;		candidate and 'MsgHup' is passed to its Step method, then the node calls
0000000000000000000000000000000000000000;;		'campaign' method to campaign itself to become a leader. Once 'campaign'
0000000000000000000000000000000000000000;;		method is called, the node becomes candidate and sends 'MsgVote' to peers
0000000000000000000000000000000000000000;;		in cluster to request votes. When passed to leader or candidate's Step
0000000000000000000000000000000000000000;;		method and the message's Term is lower than leader's or candidate's,
0000000000000000000000000000000000000000;;		'MsgVote' will be rejected ('MsgVoteResp' is returned with Reject true).
0000000000000000000000000000000000000000;;		If leader or candidate receives 'MsgVote' with higher term, it will revert
0000000000000000000000000000000000000000;;		back to follower. When 'MsgVote' is passed to follower, it votes for the
0000000000000000000000000000000000000000;;		sender only when sender's last term is greater than MsgVote's term or
0000000000000000000000000000000000000000;;		sender's last term is equal to MsgVote's term but sender's last committed
0000000000000000000000000000000000000000;;		index is greater than or equal to follower's.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgVoteResp' contains responses from voting request. When 'MsgVoteResp' is
0000000000000000000000000000000000000000;;		passed to candidate, the candidate calculates how many votes it has won. If
0000000000000000000000000000000000000000;;		it's more than majority (quorum), it becomes leader and calls 'bcastAppend'.
0000000000000000000000000000000000000000;;		If candidate receives majority of votes of denials, it reverts back to
0000000000000000000000000000000000000000;;		follower.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgPreVote' and 'MsgPreVoteResp' are used in an optional two-phase election
0000000000000000000000000000000000000000;;		protocol. When Config.PreVote is true, a pre-election is carried out first
0000000000000000000000000000000000000000;;		(using the same rules as a regular election), and no node increases its term
0000000000000000000000000000000000000000;;		number unless the pre-election indicates that the campaigining node would win.
0000000000000000000000000000000000000000;;		This minimizes disruption when a partitioned node rejoins the cluster.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgSnap' requests to install a snapshot message. When a node has just
0000000000000000000000000000000000000000;;		become a leader or the leader receives 'MsgProp' message, it calls
0000000000000000000000000000000000000000;;		'bcastAppend' method, which then calls 'sendAppend' method to each
0000000000000000000000000000000000000000;;		follower. In 'sendAppend', if a leader fails to get term or entries,
0000000000000000000000000000000000000000;;		the leader requests snapshot by sending 'MsgSnap' type message.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgSnapStatus' tells the result of snapshot install message. When a
0000000000000000000000000000000000000000;;		follower rejected 'MsgSnap', it indicates the snapshot request with
0000000000000000000000000000000000000000;;		'MsgSnap' had failed from network issues which causes the network layer
0000000000000000000000000000000000000000;;		to fail to send out snapshots to its followers. Then leader considers
0000000000000000000000000000000000000000;;		follower's progress as probe. When 'MsgSnap' were not rejected, it
0000000000000000000000000000000000000000;;		indicates that the snapshot succeeded and the leader sets follower's
0000000000000000000000000000000000000000;;		progress to probe and resumes its log replication.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgHeartbeat' sends heartbeat from leader. When 'MsgHeartbeat' is passed
0000000000000000000000000000000000000000;;		to candidate and message's term is higher than candidate's, the candidate
0000000000000000000000000000000000000000;;		reverts back to follower and updates its committed index from the one in
0000000000000000000000000000000000000000;;		this heartbeat. And it sends the message to its mailbox. When
0000000000000000000000000000000000000000;;		'MsgHeartbeat' is passed to follower's Step method and message's term is
0000000000000000000000000000000000000000;;		higher than follower's, the follower updates its leaderID with the ID
0000000000000000000000000000000000000000;;		from the message.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgHeartbeatResp' is a response to 'MsgHeartbeat'. When 'MsgHeartbeatResp'
0000000000000000000000000000000000000000;;		is passed to leader's Step method, the leader knows which follower
0000000000000000000000000000000000000000;;		responded. And only when the leader's last committed index is greater than
0000000000000000000000000000000000000000;;		follower's Match index, the leader runs 'sendAppend` method.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		'MsgUnreachable' tells that request(message) wasn't delivered. When
0000000000000000000000000000000000000000;;		'MsgUnreachable' is passed to leader's Step method, the leader discovers
0000000000000000000000000000000000000000;;		that the follower that sent this 'MsgUnreachable' is not reachable, often
0000000000000000000000000000000000000000;;		indicating 'MsgApp' is lost. When follower's progress state is replicate,
0000000000000000000000000000000000000000;;		the leader sets it back to probe.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	package raft
