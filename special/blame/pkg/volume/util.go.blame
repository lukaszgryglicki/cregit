0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2014 The Kubernetes Authors.
e61a353adf010ce30e84ea28ccbd1a64c46f93bd;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package volume
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"reflect"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/fields"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/watch"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"hash/fnv"
0000000000000000000000000000000000000000;;		"math/rand"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		volutil "k8s.io/kubernetes/pkg/volume/util"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type RecycleEventRecorder func(eventtype, message string)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RecycleVolumeByWatchingPodUntilCompletion is intended for use with volume
0000000000000000000000000000000000000000;;	// Recyclers. This function will save the given Pod to the API and watch it
0000000000000000000000000000000000000000;;	// until it completes, fails, or the pod's ActiveDeadlineSeconds is exceeded,
0000000000000000000000000000000000000000;;	// whichever comes first. An attempt to delete a recycler pod is always
0000000000000000000000000000000000000000;;	// attempted before returning.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// In case there is a pod with the same namespace+name already running, this
0000000000000000000000000000000000000000;;	// function assumes it's an older instance of the recycler pod and watches
0000000000000000000000000000000000000000;;	// this old pod instead of starting a new one.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	//  pod - the pod designed by a volume plugin to recycle the volume. pod.Name
0000000000000000000000000000000000000000;;	//        will be overwritten with unique name based on PV.Name.
0000000000000000000000000000000000000000;;	//	client - kube client for API operations.
0000000000000000000000000000000000000000;;	func RecycleVolumeByWatchingPodUntilCompletion(pvName string, pod *v1.Pod, kubeClient clientset.Interface, recorder RecycleEventRecorder) error {
0000000000000000000000000000000000000000;;		return internalRecycleVolumeByWatchingPodUntilCompletion(pvName, pod, newRecyclerClient(kubeClient, recorder))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// same as above func comments, except 'recyclerClient' is a narrower pod API
0000000000000000000000000000000000000000;;	// interface to ease testing
0000000000000000000000000000000000000000;;	func internalRecycleVolumeByWatchingPodUntilCompletion(pvName string, pod *v1.Pod, recyclerClient recyclerClient) error {
0000000000000000000000000000000000000000;;		glog.V(5).Infof("creating recycler pod for volume %s\n", pod.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Generate unique name for the recycler pod - we need to get "already
0000000000000000000000000000000000000000;;		// exists" error when a previous controller has already started recycling
0000000000000000000000000000000000000000;;		// the volume. Here we assume that pv.Name is already unique.
0000000000000000000000000000000000000000;;		pod.Name = "recycler-for-" + pvName
0000000000000000000000000000000000000000;;		pod.GenerateName = ""
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		stopChannel := make(chan struct{})
0000000000000000000000000000000000000000;;		defer close(stopChannel)
0000000000000000000000000000000000000000;;		podCh, err := recyclerClient.WatchPod(pod.Name, pod.Namespace, stopChannel)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("cannot start watcher for pod %s/%s: %v", pod.Namespace, pod.Name, err)
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start the pod
0000000000000000000000000000000000000000;;		_, err = recyclerClient.CreatePod(pod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			if errors.IsAlreadyExists(err) {
0000000000000000000000000000000000000000;;				glog.V(5).Infof("old recycler pod %q found for volume", pod.Name)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				return fmt.Errorf("unexpected error creating recycler pod:  %+v\n", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer func(pod *v1.Pod) {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("deleting recycler pod %s/%s", pod.Namespace, pod.Name)
0000000000000000000000000000000000000000;;			if err := recyclerClient.DeletePod(pod.Name, pod.Namespace); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("failed to delete recycler pod %s/%s: %v", pod.Namespace, pod.Name, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}(pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Now only the old pod or the new pod run. Watch it until it finishes
0000000000000000000000000000000000000000;;		// and send all events on the pod to the PV
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			event, ok := <-podCh
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				return fmt.Errorf("recycler pod %q watch channel had been closed", pod.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			switch event.Object.(type) {
0000000000000000000000000000000000000000;;			case *v1.Pod:
0000000000000000000000000000000000000000;;				// POD changed
0000000000000000000000000000000000000000;;				pod := event.Object.(*v1.Pod)
0000000000000000000000000000000000000000;;				glog.V(4).Infof("recycler pod update received: %s %s/%s %s", event.Type, pod.Namespace, pod.Name, pod.Status.Phase)
0000000000000000000000000000000000000000;;				switch event.Type {
0000000000000000000000000000000000000000;;				case watch.Added, watch.Modified:
0000000000000000000000000000000000000000;;					if pod.Status.Phase == v1.PodSucceeded {
0000000000000000000000000000000000000000;;						// Recycle succeeded.
0000000000000000000000000000000000000000;;						return nil
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if pod.Status.Phase == v1.PodFailed {
0000000000000000000000000000000000000000;;						if pod.Status.Message != "" {
0000000000000000000000000000000000000000;;							return fmt.Errorf(pod.Status.Message)
0000000000000000000000000000000000000000;;						} else {
0000000000000000000000000000000000000000;;							return fmt.Errorf("pod failed, pod.Status.Message unknown.")
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				case watch.Deleted:
0000000000000000000000000000000000000000;;					return fmt.Errorf("recycler pod was deleted")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				case watch.Error:
0000000000000000000000000000000000000000;;					return fmt.Errorf("recycler pod watcher failed")
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			case *v1.Event:
0000000000000000000000000000000000000000;;				// Event received
0000000000000000000000000000000000000000;;				podEvent := event.Object.(*v1.Event)
0000000000000000000000000000000000000000;;				glog.V(4).Infof("recycler event received: %s %s/%s %s/%s %s", event.Type, podEvent.Namespace, podEvent.Name, podEvent.InvolvedObject.Namespace, podEvent.InvolvedObject.Name, podEvent.Message)
0000000000000000000000000000000000000000;;				if event.Type == watch.Added {
0000000000000000000000000000000000000000;;					recyclerClient.Event(podEvent.Type, podEvent.Message)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// recyclerClient abstracts access to a Pod by providing a narrower interface.
0000000000000000000000000000000000000000;;	// This makes it easier to mock a client for testing.
0000000000000000000000000000000000000000;;	type recyclerClient interface {
0000000000000000000000000000000000000000;;		CreatePod(pod *v1.Pod) (*v1.Pod, error)
0000000000000000000000000000000000000000;;		GetPod(name, namespace string) (*v1.Pod, error)
0000000000000000000000000000000000000000;;		DeletePod(name, namespace string) error
0000000000000000000000000000000000000000;;		// WatchPod returns a ListWatch for watching a pod.  The stopChannel is used
0000000000000000000000000000000000000000;;		// to close the reflector backing the watch.  The caller is responsible for
0000000000000000000000000000000000000000;;		// derring a close on the channel to stop the reflector.
0000000000000000000000000000000000000000;;		WatchPod(name, namespace string, stopChannel chan struct{}) (<-chan watch.Event, error)
0000000000000000000000000000000000000000;;		// Event sends an event to the volume that is being recycled.
0000000000000000000000000000000000000000;;		Event(eventtype, message string)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newRecyclerClient(client clientset.Interface, recorder RecycleEventRecorder) recyclerClient {
0000000000000000000000000000000000000000;;		return &realRecyclerClient{
0000000000000000000000000000000000000000;;			client,
0000000000000000000000000000000000000000;;			recorder,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type realRecyclerClient struct {
0000000000000000000000000000000000000000;;		client   clientset.Interface
0000000000000000000000000000000000000000;;		recorder RecycleEventRecorder
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *realRecyclerClient) CreatePod(pod *v1.Pod) (*v1.Pod, error) {
0000000000000000000000000000000000000000;;		return c.client.Core().Pods(pod.Namespace).Create(pod)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *realRecyclerClient) GetPod(name, namespace string) (*v1.Pod, error) {
0000000000000000000000000000000000000000;;		return c.client.Core().Pods(namespace).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *realRecyclerClient) DeletePod(name, namespace string) error {
0000000000000000000000000000000000000000;;		return c.client.Core().Pods(namespace).Delete(name, nil)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *realRecyclerClient) Event(eventtype, message string) {
0000000000000000000000000000000000000000;;		c.recorder(eventtype, message)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (c *realRecyclerClient) WatchPod(name, namespace string, stopChannel chan struct{}) (<-chan watch.Event, error) {
0000000000000000000000000000000000000000;;		podSelector, err := fields.ParseSelector("metadata.name=" + name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{
0000000000000000000000000000000000000000;;			FieldSelector: podSelector.String(),
0000000000000000000000000000000000000000;;			Watch:         true,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podWatch, err := c.client.Core().Pods(namespace).Watch(options)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		eventSelector, _ := fields.ParseSelector("involvedObject.name=" + name)
0000000000000000000000000000000000000000;;		eventWatch, err := c.client.Core().Events(namespace).Watch(metav1.ListOptions{
0000000000000000000000000000000000000000;;			FieldSelector: eventSelector.String(),
0000000000000000000000000000000000000000;;			Watch:         true,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			podWatch.Stop()
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		eventCh := make(chan watch.Event, 30)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		go func() {
0000000000000000000000000000000000000000;;			defer eventWatch.Stop()
0000000000000000000000000000000000000000;;			defer podWatch.Stop()
0000000000000000000000000000000000000000;;			defer close(eventCh)
0000000000000000000000000000000000000000;;			var podWatchChannelClosed bool
0000000000000000000000000000000000000000;;			var eventWatchChannelClosed bool
0000000000000000000000000000000000000000;;			for {
0000000000000000000000000000000000000000;;				select {
0000000000000000000000000000000000000000;;				case _ = <-stopChannel:
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				case podEvent, ok := <-podWatch.ResultChan():
0000000000000000000000000000000000000000;;					if !ok {
0000000000000000000000000000000000000000;;						podWatchChannelClosed = true
0000000000000000000000000000000000000000;;					} else {
0000000000000000000000000000000000000000;;						eventCh <- podEvent
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				case eventEvent, ok := <-eventWatch.ResultChan():
0000000000000000000000000000000000000000;;					if !ok {
0000000000000000000000000000000000000000;;						eventWatchChannelClosed = true
0000000000000000000000000000000000000000;;					} else {
0000000000000000000000000000000000000000;;						eventCh <- eventEvent
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if podWatchChannelClosed && eventWatchChannelClosed {
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return eventCh, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// CalculateTimeoutForVolume calculates time for a Recycler pod to complete a
0000000000000000000000000000000000000000;;	// recycle operation. The calculation and return value is either the
0000000000000000000000000000000000000000;;	// minimumTimeout or the timeoutIncrement per Gi of storage size, whichever is
0000000000000000000000000000000000000000;;	// greater.
0000000000000000000000000000000000000000;;	func CalculateTimeoutForVolume(minimumTimeout, timeoutIncrement int, pv *v1.PersistentVolume) int64 {
0000000000000000000000000000000000000000;;		giQty := resource.MustParse("1Gi")
0000000000000000000000000000000000000000;;		pvQty := pv.Spec.Capacity[v1.ResourceStorage]
0000000000000000000000000000000000000000;;		giSize := giQty.Value()
0000000000000000000000000000000000000000;;		pvSize := pvQty.Value()
0000000000000000000000000000000000000000;;		timeout := (pvSize / giSize) * int64(timeoutIncrement)
0000000000000000000000000000000000000000;;		if timeout < int64(minimumTimeout) {
0000000000000000000000000000000000000000;;			return int64(minimumTimeout)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			return timeout
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RoundUpSize calculates how many allocation units are needed to accommodate
0000000000000000000000000000000000000000;;	// a volume of given size. E.g. when user wants 1500MiB volume, while AWS EBS
0000000000000000000000000000000000000000;;	// allocates volumes in gibibyte-sized chunks,
0000000000000000000000000000000000000000;;	// RoundUpSize(1500 * 1024*1024, 1024*1024*1024) returns '2'
0000000000000000000000000000000000000000;;	// (2 GiB is the smallest allocatable volume that can hold 1500MiB)
0000000000000000000000000000000000000000;;	func RoundUpSize(volumeSizeBytes int64, allocationUnitBytes int64) int64 {
0000000000000000000000000000000000000000;;		return (volumeSizeBytes + allocationUnitBytes - 1) / allocationUnitBytes
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GenerateVolumeName returns a PV name with clusterName prefix. The function
0000000000000000000000000000000000000000;;	// should be used to generate a name of GCE PD or Cinder volume. It basically
0000000000000000000000000000000000000000;;	// adds "<clusterName>-dynamic-" before the PV name, making sure the resulting
0000000000000000000000000000000000000000;;	// string fits given length and cuts "dynamic" if not.
0000000000000000000000000000000000000000;;	func GenerateVolumeName(clusterName, pvName string, maxLength int) string {
0000000000000000000000000000000000000000;;		prefix := clusterName + "-dynamic"
0000000000000000000000000000000000000000;;		pvLen := len(pvName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// cut the "<clusterName>-dynamic" to fit full pvName into maxLength
0000000000000000000000000000000000000000;;		// +1 for the '-' dash
0000000000000000000000000000000000000000;;		if pvLen+1+len(prefix) > maxLength {
0000000000000000000000000000000000000000;;			prefix = prefix[:maxLength-pvLen-1]
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return prefix + "-" + pvName
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Check if the path from the mounter is empty.
0000000000000000000000000000000000000000;;	func GetPath(mounter Mounter) (string, error) {
0000000000000000000000000000000000000000;;		path := mounter.GetPath()
0000000000000000000000000000000000000000;;		if path == "" {
0000000000000000000000000000000000000000;;			return "", fmt.Errorf("Path is empty %s", reflect.TypeOf(mounter).String())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return path, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ChooseZone implements our heuristics for choosing a zone for volume creation based on the volume name
0000000000000000000000000000000000000000;;	// Volumes are generally round-robin-ed across all active zones, using the hash of the PVC Name.
0000000000000000000000000000000000000000;;	// However, if the PVCName ends with `-<integer>`, we will hash the prefix, and then add the integer to the hash.
0000000000000000000000000000000000000000;;	// This means that a StatefulSet's volumes (`claimname-statefulsetname-id`) will spread across available zones,
0000000000000000000000000000000000000000;;	// assuming the id values are consecutive.
0000000000000000000000000000000000000000;;	func ChooseZoneForVolume(zones sets.String, pvcName string) string {
0000000000000000000000000000000000000000;;		// We create the volume in a zone determined by the name
0000000000000000000000000000000000000000;;		// Eventually the scheduler will coordinate placement into an available zone
0000000000000000000000000000000000000000;;		var hash uint32
0000000000000000000000000000000000000000;;		var index uint32
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if pvcName == "" {
0000000000000000000000000000000000000000;;			// We should always be called with a name; this shouldn't happen
0000000000000000000000000000000000000000;;			glog.Warningf("No name defined during volume create; choosing random zone")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			hash = rand.Uint32()
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			hashString := pvcName
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Heuristic to make sure that volumes in a StatefulSet are spread across zones
0000000000000000000000000000000000000000;;			// StatefulSet PVCs are (currently) named ClaimName-StatefulSetName-Id,
0000000000000000000000000000000000000000;;			// where Id is an integer index.
0000000000000000000000000000000000000000;;			// Note though that if a StatefulSet pod has multiple claims, we need them to be
0000000000000000000000000000000000000000;;			// in the same zone, because otherwise the pod will be unable to mount both volumes,
0000000000000000000000000000000000000000;;			// and will be unschedulable.  So we hash _only_ the "StatefulSetName" portion when
0000000000000000000000000000000000000000;;			// it looks like `ClaimName-StatefulSetName-Id`.
0000000000000000000000000000000000000000;;			// We continue to round-robin volume names that look like `Name-Id` also; this is a useful
0000000000000000000000000000000000000000;;			// feature for users that are creating statefulset-like functionality without using statefulsets.
0000000000000000000000000000000000000000;;			lastDash := strings.LastIndexByte(pvcName, '-')
0000000000000000000000000000000000000000;;			if lastDash != -1 {
0000000000000000000000000000000000000000;;				statefulsetIDString := pvcName[lastDash+1:]
0000000000000000000000000000000000000000;;				statefulsetID, err := strconv.ParseUint(statefulsetIDString, 10, 32)
0000000000000000000000000000000000000000;;				if err == nil {
0000000000000000000000000000000000000000;;					// Offset by the statefulsetID, so we round-robin across zones
0000000000000000000000000000000000000000;;					index = uint32(statefulsetID)
0000000000000000000000000000000000000000;;					// We still hash the volume name, but only the prefix
0000000000000000000000000000000000000000;;					hashString = pvcName[:lastDash]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// In the special case where it looks like `ClaimName-StatefulSetName-Id`,
0000000000000000000000000000000000000000;;					// hash only the StatefulSetName, so that different claims on the same StatefulSet
0000000000000000000000000000000000000000;;					// member end up in the same zone.
0000000000000000000000000000000000000000;;					// Note that StatefulSetName (and ClaimName) might themselves both have dashes.
0000000000000000000000000000000000000000;;					// We actually just take the portion after the final - of ClaimName-StatefulSetName.
0000000000000000000000000000000000000000;;					// For our purposes it doesn't much matter (just suboptimal spreading).
0000000000000000000000000000000000000000;;					lastDash := strings.LastIndexByte(hashString, '-')
0000000000000000000000000000000000000000;;					if lastDash != -1 {
0000000000000000000000000000000000000000;;						hashString = hashString[lastDash+1:]
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					glog.V(2).Infof("Detected StatefulSet-style volume name %q; index=%d", pvcName, index)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// We hash the (base) volume name, so we don't bias towards the first N zones
0000000000000000000000000000000000000000;;			h := fnv.New32()
0000000000000000000000000000000000000000;;			h.Write([]byte(hashString))
0000000000000000000000000000000000000000;;			hash = h.Sum32()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Zones.List returns zones in a consistent order (sorted)
0000000000000000000000000000000000000000;;		// We do have a potential failure case where volumes will not be properly spread,
0000000000000000000000000000000000000000;;		// if the set of zones changes during StatefulSet volume creation.  However, this is
0000000000000000000000000000000000000000;;		// probably relatively unlikely because we expect the set of zones to be essentially
0000000000000000000000000000000000000000;;		// static for clusters.
0000000000000000000000000000000000000000;;		// Hopefully we can address this problem if/when we do full scheduler integration of
0000000000000000000000000000000000000000;;		// PVC placement (which could also e.g. avoid putting volumes in overloaded or
0000000000000000000000000000000000000000;;		// unhealthy zones)
0000000000000000000000000000000000000000;;		zoneSlice := zones.List()
0000000000000000000000000000000000000000;;		zone := zoneSlice[(hash+index)%uint32(len(zoneSlice))]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(2).Infof("Creating volume for PVC %q; chose zone=%q from zones=%q", pvcName, zone, zoneSlice)
0000000000000000000000000000000000000000;;		return zone
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// UnmountViaEmptyDir delegates the tear down operation for secret, configmap, git_repo and downwardapi
0000000000000000000000000000000000000000;;	// to empty_dir
0000000000000000000000000000000000000000;;	func UnmountViaEmptyDir(dir string, host VolumeHost, volName string, volSpec Spec, podUID types.UID) error {
0000000000000000000000000000000000000000;;		glog.V(3).Infof("Tearing down volume %v for pod %v at %v", volName, podUID, dir)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if pathExists, pathErr := volutil.PathExists(dir); pathErr != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Error checking if path exists: %v", pathErr)
0000000000000000000000000000000000000000;;		} else if !pathExists {
0000000000000000000000000000000000000000;;			glog.Warningf("Warning: Unmount skipped because path does not exist: %v", dir)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Wrap EmptyDir, let it do the teardown.
0000000000000000000000000000000000000000;;		wrapped, err := host.NewWrapperUnmounter(volName, volSpec, podUID)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return wrapped.TearDownAt(dir)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// MountOptionFromSpec extracts and joins mount options from volume spec with supplied options
0000000000000000000000000000000000000000;;	func MountOptionFromSpec(spec *Spec, options ...string) []string {
0000000000000000000000000000000000000000;;		pv := spec.PersistentVolume
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if pv != nil {
0000000000000000000000000000000000000000;;			if mo, ok := pv.Annotations[v1.MountOptionAnnotation]; ok {
0000000000000000000000000000000000000000;;				moList := strings.Split(mo, ",")
0000000000000000000000000000000000000000;;				return JoinMountOptions(moList, options)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return options
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// JoinMountOptions joins mount options eliminating duplicates
0000000000000000000000000000000000000000;;	func JoinMountOptions(userOptions []string, systemOptions []string) []string {
0000000000000000000000000000000000000000;;		allMountOptions := sets.NewString()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, mountOption := range userOptions {
0000000000000000000000000000000000000000;;			if len(mountOption) > 0 {
0000000000000000000000000000000000000000;;				allMountOptions.Insert(mountOption)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, mountOption := range systemOptions {
0000000000000000000000000000000000000000;;			allMountOptions.Insert(mountOption)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return allMountOptions.UnsortedList()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ZonesToSet converts a string containing a comma separated list of zones to set
0000000000000000000000000000000000000000;;	func ZonesToSet(zonesString string) (sets.String, error) {
0000000000000000000000000000000000000000;;		zonesSlice := strings.Split(zonesString, ",")
0000000000000000000000000000000000000000;;		zonesSet := make(sets.String)
0000000000000000000000000000000000000000;;		for _, zone := range zonesSlice {
0000000000000000000000000000000000000000;;			trimmedZone := strings.TrimSpace(zone)
0000000000000000000000000000000000000000;;			if trimmedZone == "" {
0000000000000000000000000000000000000000;;				return make(sets.String), fmt.Errorf("comma separated list of zones (%q) must not contain an empty zone", zonesString)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			zonesSet.Insert(trimmedZone)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return zonesSet, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ValidateZone returns:
0000000000000000000000000000000000000000;;	// - an error in case zone is an empty string or contains only any combination of spaces and tab characters
0000000000000000000000000000000000000000;;	// - nil otherwise
0000000000000000000000000000000000000000;;	func ValidateZone(zone string) error {
0000000000000000000000000000000000000000;;		if strings.TrimSpace(zone) == "" {
0000000000000000000000000000000000000000;;			return fmt.Errorf("the provided %q zone is not valid, it's an empty string or contains only spaces and tab characters", zone)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// AccessModesContains returns whether the requested mode is contained by modes
0000000000000000000000000000000000000000;;	func AccessModesContains(modes []v1.PersistentVolumeAccessMode, mode v1.PersistentVolumeAccessMode) bool {
0000000000000000000000000000000000000000;;		for _, m := range modes {
0000000000000000000000000000000000000000;;			if m == mode {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// AccessModesContainedInAll returns whether all of the requested modes are contained by modes
0000000000000000000000000000000000000000;;	func AccessModesContainedInAll(indexedModes []v1.PersistentVolumeAccessMode, requestedModes []v1.PersistentVolumeAccessMode) bool {
0000000000000000000000000000000000000000;;		for _, mode := range requestedModes {
0000000000000000000000000000000000000000;;			if !AccessModesContains(indexedModes, mode) {
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
