0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
c47674331a5a38e6753982ce760628167b2c6b0a;pkg/controller/daemon/manager.go[pkg/controller/daemon/manager.go][pkg/controller/daemon/daemoncontroller.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package daemon
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"reflect"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		apps "k8s.io/api/apps/v1beta1"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		clientv1 "k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		extensions "k8s.io/api/extensions/v1beta1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		utilerrors "k8s.io/apimachinery/pkg/util/errors"
0000000000000000000000000000000000000000;;		utilruntime "k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		utilfeature "k8s.io/apiserver/pkg/util/feature"
0000000000000000000000000000000000000000;;		v1core "k8s.io/client-go/kubernetes/typed/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/cache"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/workqueue"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		v1helper "k8s.io/kubernetes/pkg/api/v1/helper"
0000000000000000000000000000000000000000;;		podutil "k8s.io/kubernetes/pkg/api/v1/pod"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		unversionedextensions "k8s.io/kubernetes/pkg/client/clientset_generated/clientset/typed/extensions/v1beta1"
0000000000000000000000000000000000000000;;		appsinformers "k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/apps/v1beta1"
0000000000000000000000000000000000000000;;		coreinformers "k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/core/v1"
0000000000000000000000000000000000000000;;		extensionsinformers "k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/extensions/v1beta1"
0000000000000000000000000000000000000000;;		appslisters "k8s.io/kubernetes/pkg/client/listers/apps/v1beta1"
0000000000000000000000000000000000000000;;		corelisters "k8s.io/kubernetes/pkg/client/listers/core/v1"
0000000000000000000000000000000000000000;;		extensionslisters "k8s.io/kubernetes/pkg/client/listers/extensions/v1beta1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/controller"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/controller/daemon/util"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/features"
0000000000000000000000000000000000000000;;		kubelettypes "k8s.io/kubernetes/pkg/kubelet/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/metrics"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm/predicates"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/schedulercache"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// The value of 250 is chosen b/c values that are too high can cause registry DoS issues
0000000000000000000000000000000000000000;;		BurstReplicas = 250
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If sending a status upate to API server fails, we retry a finite number of times.
0000000000000000000000000000000000000000;;		StatusUpdateRetries = 1
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Reasons for DaemonSet events
0000000000000000000000000000000000000000;;		// SelectingAllReason is added to an event when a DaemonSet selects all Pods.
0000000000000000000000000000000000000000;;		SelectingAllReason = "SelectingAll"
0000000000000000000000000000000000000000;;		// FailedPlacementReason is added to an event when a DaemonSet can't schedule a Pod to a specified node.
0000000000000000000000000000000000000000;;		FailedPlacementReason = "FailedPlacement"
0000000000000000000000000000000000000000;;		// FailedDaemonPodReason is added to an event when the status of a Pod of a DaemonSet is 'Failed'.
0000000000000000000000000000000000000000;;		FailedDaemonPodReason = "FailedDaemonPod"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// controllerKind contains the schema.GroupVersionKind for this controller type.
0000000000000000000000000000000000000000;;	var controllerKind = extensions.SchemeGroupVersion.WithKind("DaemonSet")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// DaemonSetsController is responsible for synchronizing DaemonSet objects stored
0000000000000000000000000000000000000000;;	// in the system with actual running pods.
0000000000000000000000000000000000000000;;	type DaemonSetsController struct {
0000000000000000000000000000000000000000;;		kubeClient    clientset.Interface
0000000000000000000000000000000000000000;;		eventRecorder record.EventRecorder
0000000000000000000000000000000000000000;;		podControl    controller.PodControlInterface
0000000000000000000000000000000000000000;;		crControl     controller.ControllerRevisionControlInterface
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// An dsc is temporarily suspended after creating/deleting these many replicas.
0000000000000000000000000000000000000000;;		// It resumes normal action after observing the watch events for them.
0000000000000000000000000000000000000000;;		burstReplicas int
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// To allow injection of syncDaemonSet for testing.
0000000000000000000000000000000000000000;;		syncHandler func(dsKey string) error
0000000000000000000000000000000000000000;;		// used for unit testing
0000000000000000000000000000000000000000;;		enqueueDaemonSet func(ds *extensions.DaemonSet)
0000000000000000000000000000000000000000;;		// A TTLCache of pod creates/deletes each ds expects to see
0000000000000000000000000000000000000000;;		expectations controller.ControllerExpectationsInterface
0000000000000000000000000000000000000000;;		// dsLister can list/get daemonsets from the shared informer's store
0000000000000000000000000000000000000000;;		dsLister extensionslisters.DaemonSetLister
0000000000000000000000000000000000000000;;		// dsStoreSynced returns true if the daemonset store has been synced at least once.
0000000000000000000000000000000000000000;;		// Added as a member to the struct to allow injection for testing.
0000000000000000000000000000000000000000;;		dsStoreSynced cache.InformerSynced
0000000000000000000000000000000000000000;;		// historyLister get list/get history from the shared informers's store
0000000000000000000000000000000000000000;;		historyLister appslisters.ControllerRevisionLister
0000000000000000000000000000000000000000;;		// historyStoreSynced returns true if the history store has been synced at least once.
0000000000000000000000000000000000000000;;		// Added as a member to the struct to allow injection for testing.
0000000000000000000000000000000000000000;;		historyStoreSynced cache.InformerSynced
0000000000000000000000000000000000000000;;		// podLister get list/get pods from the shared informers's store
0000000000000000000000000000000000000000;;		podLister corelisters.PodLister
0000000000000000000000000000000000000000;;		// podStoreSynced returns true if the pod store has been synced at least once.
0000000000000000000000000000000000000000;;		// Added as a member to the struct to allow injection for testing.
0000000000000000000000000000000000000000;;		podStoreSynced cache.InformerSynced
0000000000000000000000000000000000000000;;		// nodeLister can list/get nodes from the shared informer's store
0000000000000000000000000000000000000000;;		nodeLister corelisters.NodeLister
0000000000000000000000000000000000000000;;		// nodeStoreSynced returns true if the node store has been synced at least once.
0000000000000000000000000000000000000000;;		// Added as a member to the struct to allow injection for testing.
0000000000000000000000000000000000000000;;		nodeStoreSynced cache.InformerSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// DaemonSet keys that need to be synced.
0000000000000000000000000000000000000000;;		queue workqueue.RateLimitingInterface
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewDaemonSetsController(daemonSetInformer extensionsinformers.DaemonSetInformer, historyInformer appsinformers.ControllerRevisionInformer, podInformer coreinformers.PodInformer, nodeInformer coreinformers.NodeInformer, kubeClient clientset.Interface) *DaemonSetsController {
0000000000000000000000000000000000000000;;		eventBroadcaster := record.NewBroadcaster()
0000000000000000000000000000000000000000;;		eventBroadcaster.StartLogging(glog.Infof)
0000000000000000000000000000000000000000;;		// TODO: remove the wrapper when every clients have moved to use the clientset.
0000000000000000000000000000000000000000;;		eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: v1core.New(kubeClient.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kubeClient != nil && kubeClient.Core().RESTClient().GetRateLimiter() != nil {
0000000000000000000000000000000000000000;;			metrics.RegisterMetricAndTrackRateLimiterUsage("daemon_controller", kubeClient.Core().RESTClient().GetRateLimiter())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		dsc := &DaemonSetsController{
0000000000000000000000000000000000000000;;			kubeClient:    kubeClient,
0000000000000000000000000000000000000000;;			eventRecorder: eventBroadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: "daemonset-controller"}),
0000000000000000000000000000000000000000;;			podControl: controller.RealPodControl{
0000000000000000000000000000000000000000;;				KubeClient: kubeClient,
0000000000000000000000000000000000000000;;				Recorder:   eventBroadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: "daemon-set"}),
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			crControl: controller.RealControllerRevisionControl{
0000000000000000000000000000000000000000;;				KubeClient: kubeClient,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			burstReplicas: BurstReplicas,
0000000000000000000000000000000000000000;;			expectations:  controller.NewControllerExpectations(),
0000000000000000000000000000000000000000;;			queue:         workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "daemonset"),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		daemonSetInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
0000000000000000000000000000000000000000;;			AddFunc: func(obj interface{}) {
0000000000000000000000000000000000000000;;				ds := obj.(*extensions.DaemonSet)
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Adding daemon set %s", ds.Name)
0000000000000000000000000000000000000000;;				dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			UpdateFunc: func(old, cur interface{}) {
0000000000000000000000000000000000000000;;				oldDS := old.(*extensions.DaemonSet)
0000000000000000000000000000000000000000;;				curDS := cur.(*extensions.DaemonSet)
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Updating daemon set %s", oldDS.Name)
0000000000000000000000000000000000000000;;				dsc.enqueueDaemonSet(curDS)
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			DeleteFunc: dsc.deleteDaemonset,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		dsc.dsLister = daemonSetInformer.Lister()
0000000000000000000000000000000000000000;;		dsc.dsStoreSynced = daemonSetInformer.Informer().HasSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		historyInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
0000000000000000000000000000000000000000;;			AddFunc:    dsc.addHistory,
0000000000000000000000000000000000000000;;			UpdateFunc: dsc.updateHistory,
0000000000000000000000000000000000000000;;			DeleteFunc: dsc.deleteHistory,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		dsc.historyLister = historyInformer.Lister()
0000000000000000000000000000000000000000;;		dsc.historyStoreSynced = historyInformer.Informer().HasSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Watch for creation/deletion of pods. The reason we watch is that we don't want a daemon set to create/delete
0000000000000000000000000000000000000000;;		// more pods until all the effects (expectations) of a daemon set's create/delete have been observed.
0000000000000000000000000000000000000000;;		podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
0000000000000000000000000000000000000000;;			AddFunc:    dsc.addPod,
0000000000000000000000000000000000000000;;			UpdateFunc: dsc.updatePod,
0000000000000000000000000000000000000000;;			DeleteFunc: dsc.deletePod,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		dsc.podLister = podInformer.Lister()
0000000000000000000000000000000000000000;;		dsc.podStoreSynced = podInformer.Informer().HasSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
0000000000000000000000000000000000000000;;			AddFunc:    dsc.addNode,
0000000000000000000000000000000000000000;;			UpdateFunc: dsc.updateNode,
0000000000000000000000000000000000000000;;		},
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		dsc.nodeStoreSynced = nodeInformer.Informer().HasSynced
0000000000000000000000000000000000000000;;		dsc.nodeLister = nodeInformer.Lister()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dsc.syncHandler = dsc.syncDaemonSet
0000000000000000000000000000000000000000;;		dsc.enqueueDaemonSet = dsc.enqueue
0000000000000000000000000000000000000000;;		return dsc
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) deleteDaemonset(obj interface{}) {
0000000000000000000000000000000000000000;;		ds, ok := obj.(*extensions.DaemonSet)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				utilruntime.HandleError(fmt.Errorf("Couldn't get object from tombstone %#v", obj))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			ds, ok = tombstone.Obj.(*extensions.DaemonSet)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				utilruntime.HandleError(fmt.Errorf("Tombstone contained object that is not a DaemonSet %#v", obj))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Deleting daemon set %s", ds.Name)
0000000000000000000000000000000000000000;;		dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Run begins watching and syncing daemon sets.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) Run(workers int, stopCh <-chan struct{}) {
0000000000000000000000000000000000000000;;		defer utilruntime.HandleCrash()
0000000000000000000000000000000000000000;;		defer dsc.queue.ShutDown()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.Infof("Starting daemon sets controller")
0000000000000000000000000000000000000000;;		defer glog.Infof("Shutting down daemon sets controller")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if !controller.WaitForCacheSync("daemon sets", stopCh, dsc.podStoreSynced, dsc.nodeStoreSynced, dsc.historyStoreSynced, dsc.dsStoreSynced) {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i := 0; i < workers; i++ {
0000000000000000000000000000000000000000;;			go wait.Until(dsc.runWorker, time.Second, stopCh)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		<-stopCh
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) runWorker() {
0000000000000000000000000000000000000000;;		for dsc.processNextWorkItem() {
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// processNextWorkItem deals with one key off the queue.  It returns false when it's time to quit.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) processNextWorkItem() bool {
0000000000000000000000000000000000000000;;		dsKey, quit := dsc.queue.Get()
0000000000000000000000000000000000000000;;		if quit {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer dsc.queue.Done(dsKey)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := dsc.syncHandler(dsKey.(string))
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			dsc.queue.Forget(dsKey)
0000000000000000000000000000000000000000;;			return true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		utilruntime.HandleError(fmt.Errorf("%v failed with : %v", dsKey, err))
0000000000000000000000000000000000000000;;		dsc.queue.AddRateLimited(dsKey)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) enqueue(ds *extensions.DaemonSet) {
0000000000000000000000000000000000000000;;		key, err := controller.KeyFunc(ds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			utilruntime.HandleError(fmt.Errorf("Couldn't get key for object %#v: %v", ds, err))
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Handle overlapping controllers better. See comment in ReplicationManager.
0000000000000000000000000000000000000000;;		dsc.queue.Add(key)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) enqueueDaemonSetAfter(obj interface{}, after time.Duration) {
0000000000000000000000000000000000000000;;		key, err := controller.KeyFunc(obj)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			utilruntime.HandleError(fmt.Errorf("Couldn't get key for object %+v: %v", obj, err))
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Handle overlapping controllers better. See comment in ReplicationManager.
0000000000000000000000000000000000000000;;		dsc.queue.AddAfter(key, after)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getPodDaemonSets returns a list of DaemonSets that potentially match the pod.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) getPodDaemonSets(pod *v1.Pod) []*extensions.DaemonSet {
0000000000000000000000000000000000000000;;		sets, err := dsc.dsLister.GetPodDaemonSets(pod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(sets) > 1 {
0000000000000000000000000000000000000000;;			// ControllerRef will ensure we don't do anything crazy, but more than one
0000000000000000000000000000000000000000;;			// item in this list nevertheless constitutes user error.
0000000000000000000000000000000000000000;;			utilruntime.HandleError(fmt.Errorf("user error! more than one daemon is selecting pods with labels: %+v", pod.Labels))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return sets
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getDaemonSetsForHistory returns a list of DaemonSets that potentially
0000000000000000000000000000000000000000;;	// match a ControllerRevision.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) getDaemonSetsForHistory(history *apps.ControllerRevision) []*extensions.DaemonSet {
0000000000000000000000000000000000000000;;		daemonSets, err := dsc.dsLister.GetHistoryDaemonSets(history)
0000000000000000000000000000000000000000;;		if err != nil || len(daemonSets) == 0 {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(daemonSets) > 1 {
0000000000000000000000000000000000000000;;			// ControllerRef will ensure we don't do anything crazy, but more than one
0000000000000000000000000000000000000000;;			// item in this list nevertheless constitutes user error.
0000000000000000000000000000000000000000;;			glog.V(4).Infof("User error! more than one DaemonSets is selecting ControllerRevision %s/%s with labels: %#v",
0000000000000000000000000000000000000000;;				history.Namespace, history.Name, history.Labels)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return daemonSets
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// addHistory enqueues the DaemonSet that manages a ControllerRevision when the ControllerRevision is created
0000000000000000000000000000000000000000;;	// or when the controller manager is restarted.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) addHistory(obj interface{}) {
0000000000000000000000000000000000000000;;		history := obj.(*apps.ControllerRevision)
0000000000000000000000000000000000000000;;		if history.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;			// On a restart of the controller manager, it's possible for an object to
0000000000000000000000000000000000000000;;			// show up in a state that is already pending deletion.
0000000000000000000000000000000000000000;;			dsc.deleteHistory(history)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If it has a ControllerRef, that's all that matters.
0000000000000000000000000000000000000000;;		if controllerRef := controller.GetControllerOf(history); controllerRef != nil {
0000000000000000000000000000000000000000;;			ds := dsc.resolveControllerRef(history.Namespace, controllerRef)
0000000000000000000000000000000000000000;;			if ds == nil {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("ControllerRevision %s added.", history.Name)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Otherwise, it's an orphan. Get a list of all matching DaemonSets and sync
0000000000000000000000000000000000000000;;		// them to see if anyone wants to adopt it.
0000000000000000000000000000000000000000;;		daemonSets := dsc.getDaemonSetsForHistory(history)
0000000000000000000000000000000000000000;;		if len(daemonSets) == 0 {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Orphan ControllerRevision %s added.", history.Name)
0000000000000000000000000000000000000000;;		for _, ds := range daemonSets {
0000000000000000000000000000000000000000;;			dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// updateHistory figures out what DaemonSet(s) manage a ControllerRevision when the ControllerRevision
0000000000000000000000000000000000000000;;	// is updated and wake them up. If the anything of the ControllerRevision have changed, we need to
0000000000000000000000000000000000000000;;	// awaken both the old and new DaemonSets.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) updateHistory(old, cur interface{}) {
0000000000000000000000000000000000000000;;		curHistory := cur.(*apps.ControllerRevision)
0000000000000000000000000000000000000000;;		oldHistory := old.(*apps.ControllerRevision)
0000000000000000000000000000000000000000;;		if curHistory.ResourceVersion == oldHistory.ResourceVersion {
0000000000000000000000000000000000000000;;			// Periodic resync will send update events for all known ControllerRevisions.
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		curControllerRef := controller.GetControllerOf(curHistory)
0000000000000000000000000000000000000000;;		oldControllerRef := controller.GetControllerOf(oldHistory)
0000000000000000000000000000000000000000;;		controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef)
0000000000000000000000000000000000000000;;		if controllerRefChanged && oldControllerRef != nil {
0000000000000000000000000000000000000000;;			// The ControllerRef was changed. Sync the old controller, if any.
0000000000000000000000000000000000000000;;			if ds := dsc.resolveControllerRef(oldHistory.Namespace, oldControllerRef); ds != nil {
0000000000000000000000000000000000000000;;				dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If it has a ControllerRef, that's all that matters.
0000000000000000000000000000000000000000;;		if curControllerRef != nil {
0000000000000000000000000000000000000000;;			ds := dsc.resolveControllerRef(curHistory.Namespace, curControllerRef)
0000000000000000000000000000000000000000;;			if ds == nil {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("ControllerRevision %s updated.", curHistory.Name)
0000000000000000000000000000000000000000;;			dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Otherwise, it's an orphan. If anything changed, sync matching controllers
0000000000000000000000000000000000000000;;		// to see if anyone wants to adopt it now.
0000000000000000000000000000000000000000;;		labelChanged := !reflect.DeepEqual(curHistory.Labels, oldHistory.Labels)
0000000000000000000000000000000000000000;;		if labelChanged || controllerRefChanged {
0000000000000000000000000000000000000000;;			daemonSets := dsc.getDaemonSetsForHistory(curHistory)
0000000000000000000000000000000000000000;;			if len(daemonSets) == 0 {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Orphan ControllerRevision %s updated.", curHistory.Name)
0000000000000000000000000000000000000000;;			for _, ds := range daemonSets {
0000000000000000000000000000000000000000;;				dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// deleteHistory enqueues the DaemonSet that manages a ControllerRevision when
0000000000000000000000000000000000000000;;	// the ControllerRevision is deleted. obj could be an *app.ControllerRevision, or
0000000000000000000000000000000000000000;;	// a DeletionFinalStateUnknown marker item.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) deleteHistory(obj interface{}) {
0000000000000000000000000000000000000000;;		history, ok := obj.(*apps.ControllerRevision)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// When a delete is dropped, the relist will notice a ControllerRevision in the store not
0000000000000000000000000000000000000000;;		// in the list, leading to the insertion of a tombstone object which contains
0000000000000000000000000000000000000000;;		// the deleted key/value. Note that this value might be stale. If the ControllerRevision
0000000000000000000000000000000000000000;;		// changed labels the new DaemonSet will not be woken up till the periodic resync.
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				utilruntime.HandleError(fmt.Errorf("Couldn't get object from tombstone %#v", obj))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			history, ok = tombstone.Obj.(*apps.ControllerRevision)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				utilruntime.HandleError(fmt.Errorf("Tombstone contained object that is not a ControllerRevision %#v", obj))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		controllerRef := controller.GetControllerOf(history)
0000000000000000000000000000000000000000;;		if controllerRef == nil {
0000000000000000000000000000000000000000;;			// No controller should care about orphans being deleted.
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ds := dsc.resolveControllerRef(history.Namespace, controllerRef)
0000000000000000000000000000000000000000;;		if ds == nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("ControllerRevision %s deleted.", history.Name)
0000000000000000000000000000000000000000;;		dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) addPod(obj interface{}) {
0000000000000000000000000000000000000000;;		pod := obj.(*v1.Pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if pod.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;			// on a restart of the controller manager, it's possible a new pod shows up in a state that
0000000000000000000000000000000000000000;;			// is already pending deletion. Prevent the pod from being a creation observation.
0000000000000000000000000000000000000000;;			dsc.deletePod(pod)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If it has a ControllerRef, that's all that matters.
0000000000000000000000000000000000000000;;		if controllerRef := controller.GetControllerOf(pod); controllerRef != nil {
0000000000000000000000000000000000000000;;			ds := dsc.resolveControllerRef(pod.Namespace, controllerRef)
0000000000000000000000000000000000000000;;			if ds == nil {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			dsKey, err := controller.KeyFunc(ds)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Pod %s added.", pod.Name)
0000000000000000000000000000000000000000;;			dsc.expectations.CreationObserved(dsKey)
0000000000000000000000000000000000000000;;			dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Otherwise, it's an orphan. Get a list of all matching DaemonSets and sync
0000000000000000000000000000000000000000;;		// them to see if anyone wants to adopt it.
0000000000000000000000000000000000000000;;		// DO NOT observe creation because no controller should be waiting for an
0000000000000000000000000000000000000000;;		// orphan.
0000000000000000000000000000000000000000;;		dss := dsc.getPodDaemonSets(pod)
0000000000000000000000000000000000000000;;		if len(dss) == 0 {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Orphan Pod %s added.", pod.Name)
0000000000000000000000000000000000000000;;		for _, ds := range dss {
0000000000000000000000000000000000000000;;			dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// When a pod is updated, figure out what sets manage it and wake them
0000000000000000000000000000000000000000;;	// up. If the labels of the pod have changed we need to awaken both the old
0000000000000000000000000000000000000000;;	// and new set. old and cur must be *v1.Pod types.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) updatePod(old, cur interface{}) {
0000000000000000000000000000000000000000;;		curPod := cur.(*v1.Pod)
0000000000000000000000000000000000000000;;		oldPod := old.(*v1.Pod)
0000000000000000000000000000000000000000;;		if curPod.ResourceVersion == oldPod.ResourceVersion {
0000000000000000000000000000000000000000;;			// Periodic resync will send update events for all known pods.
0000000000000000000000000000000000000000;;			// Two different versions of the same pod will always have different RVs.
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		changedToReady := !podutil.IsPodReady(oldPod) && podutil.IsPodReady(curPod)
0000000000000000000000000000000000000000;;		labelChanged := !reflect.DeepEqual(curPod.Labels, oldPod.Labels)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		curControllerRef := controller.GetControllerOf(curPod)
0000000000000000000000000000000000000000;;		oldControllerRef := controller.GetControllerOf(oldPod)
0000000000000000000000000000000000000000;;		controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef)
0000000000000000000000000000000000000000;;		if controllerRefChanged && oldControllerRef != nil {
0000000000000000000000000000000000000000;;			// The ControllerRef was changed. Sync the old controller, if any.
0000000000000000000000000000000000000000;;			if ds := dsc.resolveControllerRef(oldPod.Namespace, oldControllerRef); ds != nil {
0000000000000000000000000000000000000000;;				dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If it has a ControllerRef, that's all that matters.
0000000000000000000000000000000000000000;;		if curControllerRef != nil {
0000000000000000000000000000000000000000;;			ds := dsc.resolveControllerRef(curPod.Namespace, curControllerRef)
0000000000000000000000000000000000000000;;			if ds == nil {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Pod %s updated.", curPod.Name)
0000000000000000000000000000000000000000;;			dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;			// See https://github.com/kubernetes/kubernetes/pull/38076 for more details
0000000000000000000000000000000000000000;;			if changedToReady && ds.Spec.MinReadySeconds > 0 {
0000000000000000000000000000000000000000;;				// Add a second to avoid milliseconds skew in AddAfter.
0000000000000000000000000000000000000000;;				// See https://github.com/kubernetes/kubernetes/issues/39785#issuecomment-279959133 for more info.
0000000000000000000000000000000000000000;;				dsc.enqueueDaemonSetAfter(ds, (time.Duration(ds.Spec.MinReadySeconds)*time.Second)+time.Second)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Otherwise, it's an orphan. If anything changed, sync matching controllers
0000000000000000000000000000000000000000;;		// to see if anyone wants to adopt it now.
0000000000000000000000000000000000000000;;		dss := dsc.getPodDaemonSets(curPod)
0000000000000000000000000000000000000000;;		if len(dss) == 0 {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Orphan Pod %s updated.", curPod.Name)
0000000000000000000000000000000000000000;;		if labelChanged || controllerRefChanged {
0000000000000000000000000000000000000000;;			for _, ds := range dss {
0000000000000000000000000000000000000000;;				dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) deletePod(obj interface{}) {
0000000000000000000000000000000000000000;;		pod, ok := obj.(*v1.Pod)
0000000000000000000000000000000000000000;;		// When a delete is dropped, the relist will notice a pod in the store not
0000000000000000000000000000000000000000;;		// in the list, leading to the insertion of a tombstone object which contains
0000000000000000000000000000000000000000;;		// the deleted key/value. Note that this value might be stale. If the pod
0000000000000000000000000000000000000000;;		// changed labels the new daemonset will not be woken up till the periodic
0000000000000000000000000000000000000000;;		// resync.
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				utilruntime.HandleError(fmt.Errorf("couldn't get object from tombstone %#v", obj))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pod, ok = tombstone.Obj.(*v1.Pod)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				utilruntime.HandleError(fmt.Errorf("tombstone contained object that is not a pod %#v", obj))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		controllerRef := controller.GetControllerOf(pod)
0000000000000000000000000000000000000000;;		if controllerRef == nil {
0000000000000000000000000000000000000000;;			// No controller should care about orphans being deleted.
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ds := dsc.resolveControllerRef(pod.Namespace, controllerRef)
0000000000000000000000000000000000000000;;		if ds == nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		dsKey, err := controller.KeyFunc(ds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Pod %s deleted.", pod.Name)
0000000000000000000000000000000000000000;;		dsc.expectations.DeletionObserved(dsKey)
0000000000000000000000000000000000000000;;		dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) addNode(obj interface{}) {
0000000000000000000000000000000000000000;;		// TODO: it'd be nice to pass a hint with these enqueues, so that each ds would only examine the added node (unless it has other work to do, too).
0000000000000000000000000000000000000000;;		dsList, err := dsc.dsLister.List(labels.Everything())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Error enqueueing daemon sets: %v", err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		node := obj.(*v1.Node)
0000000000000000000000000000000000000000;;		for _, ds := range dsList {
0000000000000000000000000000000000000000;;			_, shouldSchedule, _, err := dsc.nodeShouldRunDaemonPod(node, ds)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if shouldSchedule {
0000000000000000000000000000000000000000;;				dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// nodeInSameCondition returns true if all effective types ("Status" is true) equals;
0000000000000000000000000000000000000000;;	// otherwise, returns false.
0000000000000000000000000000000000000000;;	func nodeInSameCondition(old []v1.NodeCondition, cur []v1.NodeCondition) bool {
0000000000000000000000000000000000000000;;		if len(old) == 0 && len(cur) == 0 {
0000000000000000000000000000000000000000;;			return true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		c1map := map[v1.NodeConditionType]v1.ConditionStatus{}
0000000000000000000000000000000000000000;;		for _, c := range old {
0000000000000000000000000000000000000000;;			if c.Status == v1.ConditionTrue {
0000000000000000000000000000000000000000;;				c1map[c.Type] = c.Status
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, c := range cur {
0000000000000000000000000000000000000000;;			if c.Status != v1.ConditionTrue {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if _, found := c1map[c.Type]; !found {
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			delete(c1map, c.Type)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return len(c1map) == 0
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) updateNode(old, cur interface{}) {
0000000000000000000000000000000000000000;;		oldNode := old.(*v1.Node)
0000000000000000000000000000000000000000;;		curNode := cur.(*v1.Node)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if reflect.DeepEqual(oldNode.Labels, curNode.Labels) &&
0000000000000000000000000000000000000000;;			reflect.DeepEqual(oldNode.Spec.Taints, curNode.Spec.Taints) &&
0000000000000000000000000000000000000000;;			nodeInSameCondition(oldNode.Status.Conditions, curNode.Status.Conditions) {
0000000000000000000000000000000000000000;;			// If node labels, taints and condition didn't change, we can ignore this update.
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dsList, err := dsc.dsLister.List(labels.Everything())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Error enqueueing daemon sets: %v", err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// TODO: it'd be nice to pass a hint with these enqueues, so that each ds would only examine the added node (unless it has other work to do, too).
0000000000000000000000000000000000000000;;		for _, ds := range dsList {
0000000000000000000000000000000000000000;;			_, oldShouldSchedule, oldShouldContinueRunning, err := dsc.nodeShouldRunDaemonPod(oldNode, ds)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			_, currentShouldSchedule, currentShouldContinueRunning, err := dsc.nodeShouldRunDaemonPod(curNode, ds)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if (oldShouldSchedule != currentShouldSchedule) || (oldShouldContinueRunning != currentShouldContinueRunning) {
0000000000000000000000000000000000000000;;				dsc.enqueueDaemonSet(ds)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getDaemonPods returns daemon pods owned by the given ds.
0000000000000000000000000000000000000000;;	// This also reconciles ControllerRef by adopting/orphaning.
0000000000000000000000000000000000000000;;	// Note that returned Pods are pointers to objects in the cache.
0000000000000000000000000000000000000000;;	// If you want to modify one, you need to deep-copy it first.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) getDaemonPods(ds *extensions.DaemonSet) ([]*v1.Pod, error) {
0000000000000000000000000000000000000000;;		selector, err := metav1.LabelSelectorAsSelector(ds.Spec.Selector)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// List all pods to include those that don't match the selector anymore but
0000000000000000000000000000000000000000;;		// have a ControllerRef pointing to this controller.
0000000000000000000000000000000000000000;;		pods, err := dsc.podLister.Pods(ds.Namespace).List(labels.Everything())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// If any adoptions are attempted, we should first recheck for deletion with
0000000000000000000000000000000000000000;;		// an uncached quorum read sometime after listing Pods (see #42639).
0000000000000000000000000000000000000000;;		canAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) {
0000000000000000000000000000000000000000;;			fresh, err := dsc.kubeClient.ExtensionsV1beta1().DaemonSets(ds.Namespace).Get(ds.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if fresh.UID != ds.UID {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("original DaemonSet %v/%v is gone: got uid %v, wanted %v", ds.Namespace, ds.Name, fresh.UID, ds.UID)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return fresh, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		// Use ControllerRefManager to adopt/orphan as needed.
0000000000000000000000000000000000000000;;		cm := controller.NewPodControllerRefManager(dsc.podControl, ds, selector, controllerKind, canAdoptFunc)
0000000000000000000000000000000000000000;;		return cm.ClaimPods(pods)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getNodesToDaemonPods returns a map from nodes to daemon pods (corresponding to ds) running on the nodes.
0000000000000000000000000000000000000000;;	// This also reconciles ControllerRef by adopting/orphaning.
0000000000000000000000000000000000000000;;	// Note that returned Pods are pointers to objects in the cache.
0000000000000000000000000000000000000000;;	// If you want to modify one, you need to deep-copy it first.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) getNodesToDaemonPods(ds *extensions.DaemonSet) (map[string][]*v1.Pod, error) {
0000000000000000000000000000000000000000;;		claimedPods, err := dsc.getDaemonPods(ds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Group Pods by Node name.
0000000000000000000000000000000000000000;;		nodeToDaemonPods := make(map[string][]*v1.Pod)
0000000000000000000000000000000000000000;;		for _, pod := range claimedPods {
0000000000000000000000000000000000000000;;			// Skip terminating pods
0000000000000000000000000000000000000000;;			if pod.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			nodeName := pod.Spec.NodeName
0000000000000000000000000000000000000000;;			nodeToDaemonPods[nodeName] = append(nodeToDaemonPods[nodeName], pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nodeToDaemonPods, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// resolveControllerRef returns the controller referenced by a ControllerRef,
0000000000000000000000000000000000000000;;	// or nil if the ControllerRef could not be resolved to a matching controller
0000000000000000000000000000000000000000;;	// of the corrrect Kind.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) resolveControllerRef(namespace string, controllerRef *metav1.OwnerReference) *extensions.DaemonSet {
0000000000000000000000000000000000000000;;		// We can't look up by UID, so look up by Name and then verify UID.
0000000000000000000000000000000000000000;;		// Don't even try to look up by Name if it's the wrong Kind.
0000000000000000000000000000000000000000;;		if controllerRef.Kind != controllerKind.Kind {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ds, err := dsc.dsLister.DaemonSets(namespace).Get(controllerRef.Name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if ds.UID != controllerRef.UID {
0000000000000000000000000000000000000000;;			// The controller we found with this Name is not the same one that the
0000000000000000000000000000000000000000;;			// ControllerRef points to.
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return ds
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) manage(ds *extensions.DaemonSet, hash string) error {
0000000000000000000000000000000000000000;;		// Find out which nodes are running the daemon pods controlled by ds.
0000000000000000000000000000000000000000;;		nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("couldn't get node to daemon pod mapping for daemon set %q: %v", ds.Name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// For each node, if the node is running the daemon pod but isn't supposed to, kill the daemon
0000000000000000000000000000000000000000;;		// pod. If the node is supposed to run the daemon pod, but isn't, create the daemon pod on the node.
0000000000000000000000000000000000000000;;		nodeList, err := dsc.nodeLister.List(labels.Everything())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("couldn't get list of nodes when syncing daemon set %#v: %v", ds, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var nodesNeedingDaemonPods, podsToDelete []string
0000000000000000000000000000000000000000;;		var failedPodsObserved int
0000000000000000000000000000000000000000;;		for _, node := range nodeList {
0000000000000000000000000000000000000000;;			_, shouldSchedule, shouldContinueRunning, err := dsc.nodeShouldRunDaemonPod(node, ds)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			daemonPods, exists := nodeToDaemonPods[node.Name]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case shouldSchedule && !exists:
0000000000000000000000000000000000000000;;				// If daemon pod is supposed to be running on node, but isn't, create daemon pod.
0000000000000000000000000000000000000000;;				nodesNeedingDaemonPods = append(nodesNeedingDaemonPods, node.Name)
0000000000000000000000000000000000000000;;			case shouldContinueRunning:
0000000000000000000000000000000000000000;;				// If a daemon pod failed, delete it
0000000000000000000000000000000000000000;;				// If there's no daemon pods left on this node, we will create it in the next sync loop
0000000000000000000000000000000000000000;;				var daemonPodsRunning []*v1.Pod
0000000000000000000000000000000000000000;;				for _, pod := range daemonPods {
0000000000000000000000000000000000000000;;					if pod.Status.Phase == v1.PodFailed {
0000000000000000000000000000000000000000;;						msg := fmt.Sprintf("Found failed daemon pod %s/%s on node %s, will try to kill it", pod.Namespace, node.Name, pod.Name)
0000000000000000000000000000000000000000;;						glog.V(2).Infof(msg)
0000000000000000000000000000000000000000;;						// Emit an event so that it's discoverable to users.
0000000000000000000000000000000000000000;;						dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, FailedDaemonPodReason, msg)
0000000000000000000000000000000000000000;;						podsToDelete = append(podsToDelete, pod.Name)
0000000000000000000000000000000000000000;;						failedPodsObserved++
0000000000000000000000000000000000000000;;					} else {
0000000000000000000000000000000000000000;;						daemonPodsRunning = append(daemonPodsRunning, pod)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// If daemon pod is supposed to be running on node, but more than 1 daemon pod is running, delete the excess daemon pods.
0000000000000000000000000000000000000000;;				// Sort the daemon pods by creation time, so the oldest is preserved.
0000000000000000000000000000000000000000;;				if len(daemonPodsRunning) > 1 {
0000000000000000000000000000000000000000;;					sort.Sort(podByCreationTimestamp(daemonPodsRunning))
0000000000000000000000000000000000000000;;					for i := 1; i < len(daemonPodsRunning); i++ {
0000000000000000000000000000000000000000;;						podsToDelete = append(podsToDelete, daemonPods[i].Name)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case !shouldContinueRunning && exists:
0000000000000000000000000000000000000000;;				// If daemon pod isn't supposed to run on node, but it is, delete all daemon pods on node.
0000000000000000000000000000000000000000;;				for _, pod := range daemonPods {
0000000000000000000000000000000000000000;;					podsToDelete = append(podsToDelete, pod.Name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Label new pods using the hash label value of the current history when creating them
0000000000000000000000000000000000000000;;		if err = dsc.syncNodes(ds, podsToDelete, nodesNeedingDaemonPods, hash); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Throw an error when the daemon pods fail, to use ratelimiter to prevent kill-recreate hot loop
0000000000000000000000000000000000000000;;		if failedPodsObserved > 0 {
0000000000000000000000000000000000000000;;			return fmt.Errorf("deleted %d failed pods of DaemonSet %s/%s", failedPodsObserved, ds.Namespace, ds.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncNodes deletes given pods and creates new daemon set pods on the given nodes
0000000000000000000000000000000000000000;;	// returns slice with erros if any
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) syncNodes(ds *extensions.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error {
0000000000000000000000000000000000000000;;		// We need to set expectations before creating/deleting pods to avoid race conditions.
0000000000000000000000000000000000000000;;		dsKey, err := controller.KeyFunc(ds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("couldn't get key for object %#v: %v", ds, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		createDiff := len(nodesNeedingDaemonPods)
0000000000000000000000000000000000000000;;		deleteDiff := len(podsToDelete)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if createDiff > dsc.burstReplicas {
0000000000000000000000000000000000000000;;			createDiff = dsc.burstReplicas
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if deleteDiff > dsc.burstReplicas {
0000000000000000000000000000000000000000;;			deleteDiff = dsc.burstReplicas
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dsc.expectations.SetExpectations(dsKey, createDiff, deleteDiff)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// error channel to communicate back failures.  make the buffer big enough to avoid any blocking
0000000000000000000000000000000000000000;;		errCh := make(chan error, createDiff+deleteDiff)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Nodes needing daemon pods for daemon set %s: %+v, creating %d", ds.Name, nodesNeedingDaemonPods, createDiff)
0000000000000000000000000000000000000000;;		createWait := sync.WaitGroup{}
0000000000000000000000000000000000000000;;		createWait.Add(createDiff)
0000000000000000000000000000000000000000;;		template := util.CreatePodTemplate(ds.Spec.Template, ds.Spec.TemplateGeneration, hash)
0000000000000000000000000000000000000000;;		for i := 0; i < createDiff; i++ {
0000000000000000000000000000000000000000;;			go func(ix int) {
0000000000000000000000000000000000000000;;				defer createWait.Done()
0000000000000000000000000000000000000000;;				if err := dsc.podControl.CreatePodsOnNode(nodesNeedingDaemonPods[ix], ds.Namespace, &template, ds, newControllerRef(ds)); err != nil {
0000000000000000000000000000000000000000;;					glog.V(2).Infof("Failed creation, decrementing expectations for set %q/%q", ds.Namespace, ds.Name)
0000000000000000000000000000000000000000;;					dsc.expectations.CreationObserved(dsKey)
0000000000000000000000000000000000000000;;					errCh <- err
0000000000000000000000000000000000000000;;					utilruntime.HandleError(err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}(i)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		createWait.Wait()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Pods to delete for daemon set %s: %+v, deleting %d", ds.Name, podsToDelete, deleteDiff)
0000000000000000000000000000000000000000;;		deleteWait := sync.WaitGroup{}
0000000000000000000000000000000000000000;;		deleteWait.Add(deleteDiff)
0000000000000000000000000000000000000000;;		for i := 0; i < deleteDiff; i++ {
0000000000000000000000000000000000000000;;			go func(ix int) {
0000000000000000000000000000000000000000;;				defer deleteWait.Done()
0000000000000000000000000000000000000000;;				if err := dsc.podControl.DeletePod(ds.Namespace, podsToDelete[ix], ds); err != nil {
0000000000000000000000000000000000000000;;					glog.V(2).Infof("Failed deletion, decrementing expectations for set %q/%q", ds.Namespace, ds.Name)
0000000000000000000000000000000000000000;;					dsc.expectations.DeletionObserved(dsKey)
0000000000000000000000000000000000000000;;					errCh <- err
0000000000000000000000000000000000000000;;					utilruntime.HandleError(err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}(i)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		deleteWait.Wait()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// collect errors if any for proper reporting/retry logic in the controller
0000000000000000000000000000000000000000;;		errors := []error{}
0000000000000000000000000000000000000000;;		close(errCh)
0000000000000000000000000000000000000000;;		for err := range errCh {
0000000000000000000000000000000000000000;;			errors = append(errors, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return utilerrors.NewAggregate(errors)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func storeDaemonSetStatus(dsClient unversionedextensions.DaemonSetInterface, ds *extensions.DaemonSet, desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable, numberUnavailable int) error {
0000000000000000000000000000000000000000;;		if int(ds.Status.DesiredNumberScheduled) == desiredNumberScheduled &&
0000000000000000000000000000000000000000;;			int(ds.Status.CurrentNumberScheduled) == currentNumberScheduled &&
0000000000000000000000000000000000000000;;			int(ds.Status.NumberMisscheduled) == numberMisscheduled &&
0000000000000000000000000000000000000000;;			int(ds.Status.NumberReady) == numberReady &&
0000000000000000000000000000000000000000;;			int(ds.Status.UpdatedNumberScheduled) == updatedNumberScheduled &&
0000000000000000000000000000000000000000;;			int(ds.Status.NumberAvailable) == numberAvailable &&
0000000000000000000000000000000000000000;;			int(ds.Status.NumberUnavailable) == numberUnavailable &&
0000000000000000000000000000000000000000;;			ds.Status.ObservedGeneration >= ds.Generation {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		clone, err := api.Scheme.DeepCopy(ds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		toUpdate := clone.(*extensions.DaemonSet)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var updateErr, getErr error
0000000000000000000000000000000000000000;;		for i := 0; i < StatusUpdateRetries; i++ {
0000000000000000000000000000000000000000;;			toUpdate.Status.ObservedGeneration = ds.Generation
0000000000000000000000000000000000000000;;			toUpdate.Status.DesiredNumberScheduled = int32(desiredNumberScheduled)
0000000000000000000000000000000000000000;;			toUpdate.Status.CurrentNumberScheduled = int32(currentNumberScheduled)
0000000000000000000000000000000000000000;;			toUpdate.Status.NumberMisscheduled = int32(numberMisscheduled)
0000000000000000000000000000000000000000;;			toUpdate.Status.NumberReady = int32(numberReady)
0000000000000000000000000000000000000000;;			toUpdate.Status.UpdatedNumberScheduled = int32(updatedNumberScheduled)
0000000000000000000000000000000000000000;;			toUpdate.Status.NumberAvailable = int32(numberAvailable)
0000000000000000000000000000000000000000;;			toUpdate.Status.NumberUnavailable = int32(numberUnavailable)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if _, updateErr = dsClient.UpdateStatus(toUpdate); updateErr == nil {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Update the set with the latest resource version for the next poll
0000000000000000000000000000000000000000;;			if toUpdate, getErr = dsClient.Get(ds.Name, metav1.GetOptions{}); getErr != nil {
0000000000000000000000000000000000000000;;				// If the GET fails we can't trust status.Replicas anymore. This error
0000000000000000000000000000000000000000;;				// is bound to be more interesting than the update failure.
0000000000000000000000000000000000000000;;				return getErr
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return updateErr
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) updateDaemonSetStatus(ds *extensions.DaemonSet, hash string) error {
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Updating daemon set status")
0000000000000000000000000000000000000000;;		nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("couldn't get node to daemon pod mapping for daemon set %q: %v", ds.Name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nodeList, err := dsc.nodeLister.List(labels.Everything())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("couldn't get list of nodes when updating daemon set %#v: %v", ds, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable int
0000000000000000000000000000000000000000;;		for _, node := range nodeList {
0000000000000000000000000000000000000000;;			wantToRun, _, _, err := dsc.nodeShouldRunDaemonPod(node, ds)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			scheduled := len(nodeToDaemonPods[node.Name]) > 0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if wantToRun {
0000000000000000000000000000000000000000;;				desiredNumberScheduled++
0000000000000000000000000000000000000000;;				if scheduled {
0000000000000000000000000000000000000000;;					currentNumberScheduled++
0000000000000000000000000000000000000000;;					// Sort the daemon pods by creation time, so that the oldest is first.
0000000000000000000000000000000000000000;;					daemonPods, _ := nodeToDaemonPods[node.Name]
0000000000000000000000000000000000000000;;					sort.Sort(podByCreationTimestamp(daemonPods))
0000000000000000000000000000000000000000;;					pod := daemonPods[0]
0000000000000000000000000000000000000000;;					if podutil.IsPodReady(pod) {
0000000000000000000000000000000000000000;;						numberReady++
0000000000000000000000000000000000000000;;						if podutil.IsPodAvailable(pod, ds.Spec.MinReadySeconds, metav1.Now()) {
0000000000000000000000000000000000000000;;							numberAvailable++
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if util.IsPodUpdated(ds.Spec.TemplateGeneration, pod, hash) {
0000000000000000000000000000000000000000;;						updatedNumberScheduled++
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				if scheduled {
0000000000000000000000000000000000000000;;					numberMisscheduled++
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		numberUnavailable := desiredNumberScheduled - numberAvailable
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err = storeDaemonSetStatus(dsc.kubeClient.Extensions().DaemonSets(ds.Namespace), ds, desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable, numberUnavailable)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error storing status for daemon set %#v: %v", ds, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) syncDaemonSet(key string) error {
0000000000000000000000000000000000000000;;		startTime := time.Now()
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Finished syncing daemon set %q (%v)", key, time.Now().Sub(startTime))
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		namespace, name, err := cache.SplitMetaNamespaceKey(key)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ds, err := dsc.dsLister.DaemonSets(namespace).Get(name)
0000000000000000000000000000000000000000;;		if errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;			glog.V(3).Infof("daemon set has been deleted %v", key)
0000000000000000000000000000000000000000;;			dsc.expectations.DeleteExpectations(key)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("unable to retrieve ds %v from store: %v", key, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		everything := metav1.LabelSelector{}
0000000000000000000000000000000000000000;;		if reflect.DeepEqual(ds.Spec.Selector, &everything) {
0000000000000000000000000000000000000000;;			dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, SelectingAllReason, "This daemon set is selecting all pods. A non-empty selector is required.")
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Don't process a daemon set until all its creations and deletions have been processed.
0000000000000000000000000000000000000000;;		// For example if daemon set foo asked for 3 new daemon pods in the previous call to manage,
0000000000000000000000000000000000000000;;		// then we do not want to call manage on foo until the daemon pods have been created.
0000000000000000000000000000000000000000;;		dsKey, err := controller.KeyFunc(ds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("couldn't get key for object %#v: %v", ds, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Construct histories of the DaemonSet, and get the hash of current history
0000000000000000000000000000000000000000;;		cur, old, err := dsc.constructHistory(ds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to construct revisions of DaemonSet: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		hash := cur.Labels[extensions.DefaultDaemonSetUniqueLabelKey]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if ds.DeletionTimestamp != nil || !dsc.expectations.SatisfiedExpectations(dsKey) {
0000000000000000000000000000000000000000;;			// Only update status.
0000000000000000000000000000000000000000;;			return dsc.updateDaemonSetStatus(ds, hash)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err = dsc.manage(ds, hash)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Process rolling updates if we're ready.
0000000000000000000000000000000000000000;;		if dsc.expectations.SatisfiedExpectations(dsKey) {
0000000000000000000000000000000000000000;;			switch ds.Spec.UpdateStrategy.Type {
0000000000000000000000000000000000000000;;			case extensions.OnDeleteDaemonSetStrategyType:
0000000000000000000000000000000000000000;;			case extensions.RollingUpdateDaemonSetStrategyType:
0000000000000000000000000000000000000000;;				err = dsc.rollingUpdate(ds, hash)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err = dsc.cleanupHistory(ds, old)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to clean up revisions of DaemonSet: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return dsc.updateDaemonSetStatus(ds, hash)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) simulate(newPod *v1.Pod, node *v1.Node, ds *extensions.DaemonSet) ([]algorithm.PredicateFailureReason, *schedulercache.NodeInfo, error) {
0000000000000000000000000000000000000000;;		// DaemonSet pods shouldn't be deleted by NodeController in case of node problems.
0000000000000000000000000000000000000000;;		// Add infinite toleration for taint notReady:NoExecute here
0000000000000000000000000000000000000000;;		// to survive taint-based eviction enforced by NodeController
0000000000000000000000000000000000000000;;		// when node turns not ready.
0000000000000000000000000000000000000000;;		v1helper.AddOrUpdateTolerationInPod(newPod, &v1.Toleration{
0000000000000000000000000000000000000000;;			Key:      algorithm.TaintNodeNotReady,
0000000000000000000000000000000000000000;;			Operator: v1.TolerationOpExists,
0000000000000000000000000000000000000000;;			Effect:   v1.TaintEffectNoExecute,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// DaemonSet pods shouldn't be deleted by NodeController in case of node problems.
0000000000000000000000000000000000000000;;		// Add infinite toleration for taint unreachable:NoExecute here
0000000000000000000000000000000000000000;;		// to survive taint-based eviction enforced by NodeController
0000000000000000000000000000000000000000;;		// when node turns unreachable.
0000000000000000000000000000000000000000;;		v1helper.AddOrUpdateTolerationInPod(newPod, &v1.Toleration{
0000000000000000000000000000000000000000;;			Key:      algorithm.TaintNodeUnreachable,
0000000000000000000000000000000000000000;;			Operator: v1.TolerationOpExists,
0000000000000000000000000000000000000000;;			Effect:   v1.TaintEffectNoExecute,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podList, err := dsc.podLister.List(labels.Everything())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pod := range podList {
0000000000000000000000000000000000000000;;			if pod.Spec.NodeName != node.Name {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// ignore pods that belong to the daemonset when taking into account whether
0000000000000000000000000000000000000000;;			// a daemonset should bind to a node.
0000000000000000000000000000000000000000;;			if controllerRef := controller.GetControllerOf(pod); controllerRef != nil && controllerRef.UID == ds.UID {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pods = append(pods, pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nodeInfo := schedulercache.NewNodeInfo(pods...)
0000000000000000000000000000000000000000;;		nodeInfo.SetNode(node)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		_, reasons, err := Predicates(newPod, nodeInfo)
0000000000000000000000000000000000000000;;		return reasons, nodeInfo, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// nodeShouldRunDaemonPod checks a set of preconditions against a (node,daemonset) and returns a
0000000000000000000000000000000000000000;;	// summary. Returned booleans are:
0000000000000000000000000000000000000000;;	// * wantToRun:
0000000000000000000000000000000000000000;;	//     Returns true when a user would expect a pod to run on this node and ignores conditions
0000000000000000000000000000000000000000;;	//     such as OutOfDisk or insufficient resource that would cause a daemonset pod not to schedule.
0000000000000000000000000000000000000000;;	//     This is primarily used to populate daemonset status.
0000000000000000000000000000000000000000;;	// * shouldSchedule:
0000000000000000000000000000000000000000;;	//     Returns true when a daemonset should be scheduled to a node if a daemonset pod is not already
0000000000000000000000000000000000000000;;	//     running on that node.
0000000000000000000000000000000000000000;;	// * shouldContinueRunning:
0000000000000000000000000000000000000000;;	//     Returns true when a daemonset should continue running on a node if a daemonset pod is already
0000000000000000000000000000000000000000;;	//     running on that node.
0000000000000000000000000000000000000000;;	func (dsc *DaemonSetsController) nodeShouldRunDaemonPod(node *v1.Node, ds *extensions.DaemonSet) (wantToRun, shouldSchedule, shouldContinueRunning bool, err error) {
0000000000000000000000000000000000000000;;		newPod := NewPod(ds, node.Name)
0000000000000000000000000000000000000000;;		critical := utilfeature.DefaultFeatureGate.Enabled(features.ExperimentalCriticalPodAnnotation) && kubelettypes.IsCriticalPod(newPod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Because these bools require an && of all their required conditions, we start
0000000000000000000000000000000000000000;;		// with all bools set to true and set a bool to false if a condition is not met.
0000000000000000000000000000000000000000;;		// A bool should probably not be set to true after this line.
0000000000000000000000000000000000000000;;		wantToRun, shouldSchedule, shouldContinueRunning = true, true, true
0000000000000000000000000000000000000000;;		// If the daemon set specifies a node name, check that it matches with node.Name.
0000000000000000000000000000000000000000;;		if !(ds.Spec.Template.Spec.NodeName == "" || ds.Spec.Template.Spec.NodeName == node.Name) {
0000000000000000000000000000000000000000;;			return false, false, false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Move it to the predicates
0000000000000000000000000000000000000000;;		for _, c := range node.Status.Conditions {
0000000000000000000000000000000000000000;;			if critical {
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// TODO: There are other node status that the DaemonSet should ideally respect too,
0000000000000000000000000000000000000000;;			//       e.g. MemoryPressure, and DiskPressure
0000000000000000000000000000000000000000;;			if c.Type == v1.NodeOutOfDisk && c.Status == v1.ConditionTrue {
0000000000000000000000000000000000000000;;				// the kubelet will evict this pod if it needs to. Let kubelet
0000000000000000000000000000000000000000;;				// decide whether to continue running this pod so leave shouldContinueRunning
0000000000000000000000000000000000000000;;				// set to true
0000000000000000000000000000000000000000;;				shouldSchedule = false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		reasons, nodeInfo, err := dsc.simulate(newPod, node, ds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Warningf("DaemonSet Predicates failed on node %s for ds '%s/%s' due to unexpected error: %v", node.Name, ds.ObjectMeta.Namespace, ds.ObjectMeta.Name, err)
0000000000000000000000000000000000000000;;			return false, false, false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var insufficientResourceErr error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, r := range reasons {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("DaemonSet Predicates failed on node %s for ds '%s/%s' for reason: %v", node.Name, ds.ObjectMeta.Namespace, ds.ObjectMeta.Name, r.GetReason())
0000000000000000000000000000000000000000;;			switch reason := r.(type) {
0000000000000000000000000000000000000000;;			case *predicates.InsufficientResourceError:
0000000000000000000000000000000000000000;;				insufficientResourceErr = reason
0000000000000000000000000000000000000000;;			case *predicates.PredicateFailureError:
0000000000000000000000000000000000000000;;				var emitEvent bool
0000000000000000000000000000000000000000;;				// we try to partition predicates into two partitions here: intentional on the part of the operator and not.
0000000000000000000000000000000000000000;;				switch reason {
0000000000000000000000000000000000000000;;				// intentional
0000000000000000000000000000000000000000;;				case
0000000000000000000000000000000000000000;;					predicates.ErrNodeSelectorNotMatch,
0000000000000000000000000000000000000000;;					predicates.ErrPodNotMatchHostName,
0000000000000000000000000000000000000000;;					predicates.ErrNodeLabelPresenceViolated,
0000000000000000000000000000000000000000;;					// this one is probably intentional since it's a workaround for not having
0000000000000000000000000000000000000000;;					// pod hard anti affinity.
0000000000000000000000000000000000000000;;					predicates.ErrPodNotFitsHostPorts:
0000000000000000000000000000000000000000;;					return false, false, false, nil
0000000000000000000000000000000000000000;;				case predicates.ErrTaintsTolerationsNotMatch:
0000000000000000000000000000000000000000;;					// DaemonSet is expected to respect taints and tolerations
0000000000000000000000000000000000000000;;					fitsNoExecute, _, err := predicates.PodToleratesNodeNoExecuteTaints(newPod, nil, nodeInfo)
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						return false, false, false, err
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if !fitsNoExecute {
0000000000000000000000000000000000000000;;						return false, false, false, nil
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					wantToRun, shouldSchedule = false, false
0000000000000000000000000000000000000000;;				// unintentional
0000000000000000000000000000000000000000;;				case
0000000000000000000000000000000000000000;;					predicates.ErrDiskConflict,
0000000000000000000000000000000000000000;;					predicates.ErrVolumeZoneConflict,
0000000000000000000000000000000000000000;;					predicates.ErrMaxVolumeCountExceeded,
0000000000000000000000000000000000000000;;					predicates.ErrNodeUnderMemoryPressure,
0000000000000000000000000000000000000000;;					predicates.ErrNodeUnderDiskPressure:
0000000000000000000000000000000000000000;;					// wantToRun and shouldContinueRunning are likely true here. They are
0000000000000000000000000000000000000000;;					// absolutely true at the time of writing the comment. See first comment
0000000000000000000000000000000000000000;;					// of this method.
0000000000000000000000000000000000000000;;					shouldSchedule = false
0000000000000000000000000000000000000000;;					emitEvent = true
0000000000000000000000000000000000000000;;				// unexpected
0000000000000000000000000000000000000000;;				case
0000000000000000000000000000000000000000;;					predicates.ErrPodAffinityNotMatch,
0000000000000000000000000000000000000000;;					predicates.ErrServiceAffinityViolated:
0000000000000000000000000000000000000000;;					glog.Warningf("unexpected predicate failure reason: %s", reason.GetReason())
0000000000000000000000000000000000000000;;					return false, false, false, fmt.Errorf("unexpected reason: DaemonSet Predicates should not return reason %s", reason.GetReason())
0000000000000000000000000000000000000000;;				default:
0000000000000000000000000000000000000000;;					glog.V(4).Infof("unknown predicate failure reason: %s", reason.GetReason())
0000000000000000000000000000000000000000;;					wantToRun, shouldSchedule, shouldContinueRunning = false, false, false
0000000000000000000000000000000000000000;;					emitEvent = true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if emitEvent {
0000000000000000000000000000000000000000;;					dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, FailedPlacementReason, "failed to place pod on %q: %s", node.ObjectMeta.Name, reason.GetReason())
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// only emit this event if insufficient resource is the only thing
0000000000000000000000000000000000000000;;		// preventing the daemon pod from scheduling
0000000000000000000000000000000000000000;;		if shouldSchedule && insufficientResourceErr != nil {
0000000000000000000000000000000000000000;;			dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, FailedPlacementReason, "failed to place pod on %q: %s", node.ObjectMeta.Name, insufficientResourceErr.Error())
0000000000000000000000000000000000000000;;			shouldSchedule = false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewPod(ds *extensions.DaemonSet, nodeName string) *v1.Pod {
0000000000000000000000000000000000000000;;		newPod := &v1.Pod{Spec: ds.Spec.Template.Spec, ObjectMeta: ds.Spec.Template.ObjectMeta}
0000000000000000000000000000000000000000;;		newPod.Namespace = ds.Namespace
0000000000000000000000000000000000000000;;		newPod.Spec.NodeName = nodeName
0000000000000000000000000000000000000000;;		return newPod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Predicates checks if a DaemonSet's pod can be scheduled on a node using GeneralPredicates
0000000000000000000000000000000000000000;;	// and PodToleratesNodeTaints predicate
0000000000000000000000000000000000000000;;	func Predicates(pod *v1.Pod, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) {
0000000000000000000000000000000000000000;;		var predicateFails []algorithm.PredicateFailureReason
0000000000000000000000000000000000000000;;		critical := utilfeature.DefaultFeatureGate.Enabled(features.ExperimentalCriticalPodAnnotation) && kubelettypes.IsCriticalPod(pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fit, reasons, err := predicates.PodToleratesNodeTaints(pod, nil, nodeInfo)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, predicateFails, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !fit {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, reasons...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if critical {
0000000000000000000000000000000000000000;;			// If the pod is marked as critical and support for critical pod annotations is enabled,
0000000000000000000000000000000000000000;;			// check predicates for critical pods only.
0000000000000000000000000000000000000000;;			fit, reasons, err = predicates.EssentialPredicates(pod, nil, nodeInfo)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			fit, reasons, err = predicates.GeneralPredicates(pod, nil, nodeInfo)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, predicateFails, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !fit {
0000000000000000000000000000000000000000;;			predicateFails = append(predicateFails, reasons...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return len(predicateFails) == 0, predicateFails, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// newControllerRef creates a ControllerRef pointing to the given DaemonSet.
0000000000000000000000000000000000000000;;	func newControllerRef(ds *extensions.DaemonSet) *metav1.OwnerReference {
0000000000000000000000000000000000000000;;		blockOwnerDeletion := true
0000000000000000000000000000000000000000;;		isController := true
0000000000000000000000000000000000000000;;		return &metav1.OwnerReference{
0000000000000000000000000000000000000000;;			APIVersion:         controllerKind.GroupVersion().String(),
0000000000000000000000000000000000000000;;			Kind:               controllerKind.Kind,
0000000000000000000000000000000000000000;;			Name:               ds.Name,
0000000000000000000000000000000000000000;;			UID:                ds.UID,
0000000000000000000000000000000000000000;;			BlockOwnerDeletion: &blockOwnerDeletion,
0000000000000000000000000000000000000000;;			Controller:         &isController,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// byCreationTimestamp sorts a list by creation timestamp, using their names as a tie breaker.
0000000000000000000000000000000000000000;;	type byCreationTimestamp []*extensions.DaemonSet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (o byCreationTimestamp) Len() int      { return len(o) }
0000000000000000000000000000000000000000;;	func (o byCreationTimestamp) Swap(i, j int) { o[i], o[j] = o[j], o[i] }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (o byCreationTimestamp) Less(i, j int) bool {
0000000000000000000000000000000000000000;;		if o[i].CreationTimestamp.Equal(o[j].CreationTimestamp) {
0000000000000000000000000000000000000000;;			return o[i].Name < o[j].Name
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return o[i].CreationTimestamp.Before(o[j].CreationTimestamp)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type podByCreationTimestamp []*v1.Pod
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (o podByCreationTimestamp) Len() int      { return len(o) }
0000000000000000000000000000000000000000;;	func (o podByCreationTimestamp) Swap(i, j int) { o[i], o[j] = o[j], o[i] }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (o podByCreationTimestamp) Less(i, j int) bool {
0000000000000000000000000000000000000000;;		if o[i].CreationTimestamp.Equal(o[j].CreationTimestamp) {
0000000000000000000000000000000000000000;;			return o[i].Name < o[j].Name
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return o[i].CreationTimestamp.Before(o[j].CreationTimestamp)
0000000000000000000000000000000000000000;;	}
