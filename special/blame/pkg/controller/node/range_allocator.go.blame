0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
48c9ce51c6af32b37ea1e581dfb2e8b1887eccc0;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package node
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"net"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		clientv1 "k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		apierrors "k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		v1core "k8s.io/client-go/kubernetes/typed/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO: figure out the good setting for those constants.
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// controls how many NodeSpec updates NC can process concurrently.
0000000000000000000000000000000000000000;;		cidrUpdateWorkers   = 10
0000000000000000000000000000000000000000;;		cidrUpdateQueueSize = 5000
0000000000000000000000000000000000000000;;		// podCIDRUpdateRetry controls the number of retries of writing Node.Spec.PodCIDR update.
0000000000000000000000000000000000000000;;		podCIDRUpdateRetry = 5
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type rangeAllocator struct {
0000000000000000000000000000000000000000;;		client      clientset.Interface
0000000000000000000000000000000000000000;;		cidrs       *cidrSet
0000000000000000000000000000000000000000;;		clusterCIDR *net.IPNet
0000000000000000000000000000000000000000;;		maxCIDRs    int
0000000000000000000000000000000000000000;;		// Channel that is used to pass updating Nodes with assigned CIDRs to the background
0000000000000000000000000000000000000000;;		// This increases a throughput of CIDR assignment by not blocking on long operations.
0000000000000000000000000000000000000000;;		nodeCIDRUpdateChannel chan nodeAndCIDR
0000000000000000000000000000000000000000;;		recorder              record.EventRecorder
0000000000000000000000000000000000000000;;		// Keep a set of nodes that are currectly being processed to avoid races in CIDR allocation
0000000000000000000000000000000000000000;;		sync.Mutex
0000000000000000000000000000000000000000;;		nodesInProcessing sets.String
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewCIDRRangeAllocator returns a CIDRAllocator to allocate CIDR for node
0000000000000000000000000000000000000000;;	// Caller must ensure subNetMaskSize is not less than cluster CIDR mask size.
0000000000000000000000000000000000000000;;	// Caller must always pass in a list of existing nodes so the new allocator
0000000000000000000000000000000000000000;;	// can initialize its CIDR map. NodeList is only nil in testing.
0000000000000000000000000000000000000000;;	func NewCIDRRangeAllocator(client clientset.Interface, clusterCIDR *net.IPNet, serviceCIDR *net.IPNet, subNetMaskSize int, nodeList *v1.NodeList) (CIDRAllocator, error) {
0000000000000000000000000000000000000000;;		eventBroadcaster := record.NewBroadcaster()
0000000000000000000000000000000000000000;;		recorder := eventBroadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: "cidrAllocator"})
0000000000000000000000000000000000000000;;		eventBroadcaster.StartLogging(glog.Infof)
0000000000000000000000000000000000000000;;		if client != nil {
0000000000000000000000000000000000000000;;			glog.V(0).Infof("Sending events to api server.")
0000000000000000000000000000000000000000;;			eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: v1core.New(client.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.Fatalf("kubeClient is nil when starting NodeController")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ra := &rangeAllocator{
0000000000000000000000000000000000000000;;			client:                client,
0000000000000000000000000000000000000000;;			cidrs:                 newCIDRSet(clusterCIDR, subNetMaskSize),
0000000000000000000000000000000000000000;;			clusterCIDR:           clusterCIDR,
0000000000000000000000000000000000000000;;			nodeCIDRUpdateChannel: make(chan nodeAndCIDR, cidrUpdateQueueSize),
0000000000000000000000000000000000000000;;			recorder:              recorder,
0000000000000000000000000000000000000000;;			nodesInProcessing:     sets.NewString(),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if serviceCIDR != nil {
0000000000000000000000000000000000000000;;			ra.filterOutServiceRange(serviceCIDR)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.V(0).Info("No Service CIDR provided. Skipping filtering out service addresses.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if nodeList != nil {
0000000000000000000000000000000000000000;;			for _, node := range nodeList.Items {
0000000000000000000000000000000000000000;;				if node.Spec.PodCIDR == "" {
0000000000000000000000000000000000000000;;					glog.Infof("Node %v has no CIDR, ignoring", node.Name)
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					glog.Infof("Node %v has CIDR %s, occupying it in CIDR map",
0000000000000000000000000000000000000000;;						node.Name, node.Spec.PodCIDR)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if err := ra.occupyCIDR(&node); err != nil {
0000000000000000000000000000000000000000;;					// This will happen if:
0000000000000000000000000000000000000000;;					// 1. We find garbage in the podCIDR field. Retrying is useless.
0000000000000000000000000000000000000000;;					// 2. CIDR out of range: This means a node CIDR has changed.
0000000000000000000000000000000000000000;;					// This error will keep crashing controller-manager.
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := 0; i < cidrUpdateWorkers; i++ {
0000000000000000000000000000000000000000;;			go func(stopChan <-chan struct{}) {
0000000000000000000000000000000000000000;;				for {
0000000000000000000000000000000000000000;;					select {
0000000000000000000000000000000000000000;;					case workItem, ok := <-ra.nodeCIDRUpdateChannel:
0000000000000000000000000000000000000000;;						if !ok {
0000000000000000000000000000000000000000;;							glog.Warning("NodeCIDRUpdateChannel read returned false.")
0000000000000000000000000000000000000000;;							return
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						ra.updateCIDRAllocation(workItem)
0000000000000000000000000000000000000000;;					case <-stopChan:
0000000000000000000000000000000000000000;;						return
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}(wait.NeverStop)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return ra, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (r *rangeAllocator) insertNodeToProcessing(nodeName string) bool {
0000000000000000000000000000000000000000;;		r.Lock()
0000000000000000000000000000000000000000;;		defer r.Unlock()
0000000000000000000000000000000000000000;;		if r.nodesInProcessing.Has(nodeName) {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		r.nodesInProcessing.Insert(nodeName)
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (r *rangeAllocator) removeNodeFromProcessing(nodeName string) {
0000000000000000000000000000000000000000;;		r.Lock()
0000000000000000000000000000000000000000;;		defer r.Unlock()
0000000000000000000000000000000000000000;;		r.nodesInProcessing.Delete(nodeName)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (r *rangeAllocator) occupyCIDR(node *v1.Node) error {
0000000000000000000000000000000000000000;;		defer r.removeNodeFromProcessing(node.Name)
0000000000000000000000000000000000000000;;		if node.Spec.PodCIDR == "" {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, podCIDR, err := net.ParseCIDR(node.Spec.PodCIDR)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to parse node %s, CIDR %s", node.Name, node.Spec.PodCIDR)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := r.cidrs.occupy(podCIDR); err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to mark cidr as occupied: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// WARNING: If you're adding any return calls or defer any more work from this
0000000000000000000000000000000000000000;;	// function you have to handle correctly nodesInProcessing.
0000000000000000000000000000000000000000;;	func (r *rangeAllocator) AllocateOrOccupyCIDR(node *v1.Node) error {
0000000000000000000000000000000000000000;;		if node == nil {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !r.insertNodeToProcessing(node.Name) {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("Node %v is already in a process of CIDR assignment.", node.Name)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if node.Spec.PodCIDR != "" {
0000000000000000000000000000000000000000;;			return r.occupyCIDR(node)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podCIDR, err := r.cidrs.allocateNext()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			r.removeNodeFromProcessing(node.Name)
0000000000000000000000000000000000000000;;			recordNodeStatusChange(r.recorder, node, "CIDRNotAvailable")
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to allocate cidr: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(10).Infof("Putting node %s with CIDR %s into the work queue", node.Name, podCIDR)
0000000000000000000000000000000000000000;;		r.nodeCIDRUpdateChannel <- nodeAndCIDR{
0000000000000000000000000000000000000000;;			nodeName: node.Name,
0000000000000000000000000000000000000000;;			cidr:     podCIDR,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (r *rangeAllocator) ReleaseCIDR(node *v1.Node) error {
0000000000000000000000000000000000000000;;		if node == nil || node.Spec.PodCIDR == "" {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, podCIDR, err := net.ParseCIDR(node.Spec.PodCIDR)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Failed to parse CIDR %s on Node %v: %v", node.Spec.PodCIDR, node.Name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(4).Infof("release CIDR %s", node.Spec.PodCIDR)
0000000000000000000000000000000000000000;;		if err = r.cidrs.release(podCIDR); err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Error when releasing CIDR %v: %v", node.Spec.PodCIDR, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Marks all CIDRs with subNetMaskSize that belongs to serviceCIDR as used,
0000000000000000000000000000000000000000;;	// so that they won't be assignable.
0000000000000000000000000000000000000000;;	func (r *rangeAllocator) filterOutServiceRange(serviceCIDR *net.IPNet) {
0000000000000000000000000000000000000000;;		// Checks if service CIDR has a nonempty intersection with cluster
0000000000000000000000000000000000000000;;		// CIDR. It is the case if either clusterCIDR contains serviceCIDR with
0000000000000000000000000000000000000000;;		// clusterCIDR's Mask applied (this means that clusterCIDR contains
0000000000000000000000000000000000000000;;		// serviceCIDR) or vice versa (which means that serviceCIDR contains
0000000000000000000000000000000000000000;;		// clusterCIDR).
0000000000000000000000000000000000000000;;		if !r.clusterCIDR.Contains(serviceCIDR.IP.Mask(r.clusterCIDR.Mask)) && !serviceCIDR.Contains(r.clusterCIDR.IP.Mask(serviceCIDR.Mask)) {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err := r.cidrs.occupy(serviceCIDR); err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Error filtering out service cidr %v: %v", serviceCIDR, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Assigns CIDR to Node and sends an update to the API server.
0000000000000000000000000000000000000000;;	func (r *rangeAllocator) updateCIDRAllocation(data nodeAndCIDR) error {
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		var node *v1.Node
0000000000000000000000000000000000000000;;		defer r.removeNodeFromProcessing(data.nodeName)
0000000000000000000000000000000000000000;;		for rep := 0; rep < podCIDRUpdateRetry; rep++ {
0000000000000000000000000000000000000000;;			// TODO: change it to using PATCH instead of full Node updates.
0000000000000000000000000000000000000000;;			node, err = r.client.Core().Nodes().Get(data.nodeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Failed while getting node %v to retry updating Node.Spec.PodCIDR: %v", data.nodeName, err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if node.Spec.PodCIDR != "" {
0000000000000000000000000000000000000000;;				glog.Errorf("Node %v already has allocated CIDR %v. Releasing assigned one if different.", node.Name, node.Spec.PodCIDR)
0000000000000000000000000000000000000000;;				if node.Spec.PodCIDR != data.cidr.String() {
0000000000000000000000000000000000000000;;					if err := r.cidrs.release(data.cidr); err != nil {
0000000000000000000000000000000000000000;;						glog.Errorf("Error when releasing CIDR %v", data.cidr.String())
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			node.Spec.PodCIDR = data.cidr.String()
0000000000000000000000000000000000000000;;			if _, err := r.client.Core().Nodes().Update(node); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Failed while updating Node.Spec.PodCIDR (%d retries left): %v", podCIDRUpdateRetry-rep-1, err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			recordNodeStatusChange(r.recorder, node, "CIDRAssignmentFailed")
0000000000000000000000000000000000000000;;			// We accept the fact that we may leek CIDRs here. This is safer than releasing
0000000000000000000000000000000000000000;;			// them in case when we don't know if request went through.
0000000000000000000000000000000000000000;;			// NodeController restart will return all falsely allocated CIDRs to the pool.
0000000000000000000000000000000000000000;;			if !apierrors.IsServerTimeout(err) {
0000000000000000000000000000000000000000;;				glog.Errorf("CIDR assignment for node %v failed: %v. Releasing allocated CIDR", data.nodeName, err)
0000000000000000000000000000000000000000;;				if releaseErr := r.cidrs.release(data.cidr); releaseErr != nil {
0000000000000000000000000000000000000000;;					glog.Errorf("Error releasing allocated CIDR for node %v: %v", data.nodeName, releaseErr)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
