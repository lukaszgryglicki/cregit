0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
fc9c25ec1edeff3780e90faa04791ac7766e789e;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package deployment
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"reflect"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		extensions "k8s.io/api/extensions/v1beta1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		utilerrors "k8s.io/apimachinery/pkg/util/errors"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/controller"
0000000000000000000000000000000000000000;;		deploymentutil "k8s.io/kubernetes/pkg/controller/deployment/util"
0000000000000000000000000000000000000000;;		labelsutil "k8s.io/kubernetes/pkg/util/labels"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncStatusOnly only updates Deployments Status and doesn't take any mutating actions.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) syncStatusOnly(d *extensions.Deployment, rsList []*extensions.ReplicaSet, podMap map[types.UID]*v1.PodList) error {
0000000000000000000000000000000000000000;;		newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, podMap, false)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		allRSs := append(oldRSs, newRS)
0000000000000000000000000000000000000000;;		return dc.syncDeploymentStatus(allRSs, newRS, d)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// sync is responsible for reconciling deployments on scaling events or when they
0000000000000000000000000000000000000000;;	// are paused.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) sync(d *extensions.Deployment, rsList []*extensions.ReplicaSet, podMap map[types.UID]*v1.PodList) error {
0000000000000000000000000000000000000000;;		newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, podMap, false)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := dc.scale(d, newRS, oldRSs); err != nil {
0000000000000000000000000000000000000000;;			// If we get an error while trying to scale, the deployment will be requeued
0000000000000000000000000000000000000000;;			// so we can abort this resync
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		allRSs := append(oldRSs, newRS)
0000000000000000000000000000000000000000;;		return dc.syncDeploymentStatus(allRSs, newRS, d)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// checkPausedConditions checks if the given deployment is paused or not and adds an appropriate condition.
0000000000000000000000000000000000000000;;	// These conditions are needed so that we won't accidentally report lack of progress for resumed deployments
0000000000000000000000000000000000000000;;	// that were paused for longer than progressDeadlineSeconds.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) checkPausedConditions(d *extensions.Deployment) error {
0000000000000000000000000000000000000000;;		if d.Spec.ProgressDeadlineSeconds == nil {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		cond := deploymentutil.GetDeploymentCondition(d.Status, extensions.DeploymentProgressing)
0000000000000000000000000000000000000000;;		if cond != nil && cond.Reason == deploymentutil.TimedOutReason {
0000000000000000000000000000000000000000;;			// If we have reported lack of progress, do not overwrite it with a paused condition.
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pausedCondExists := cond != nil && cond.Reason == deploymentutil.PausedDeployReason
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		needsUpdate := false
0000000000000000000000000000000000000000;;		if d.Spec.Paused && !pausedCondExists {
0000000000000000000000000000000000000000;;			condition := deploymentutil.NewDeploymentCondition(extensions.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.PausedDeployReason, "Deployment is paused")
0000000000000000000000000000000000000000;;			deploymentutil.SetDeploymentCondition(&d.Status, *condition)
0000000000000000000000000000000000000000;;			needsUpdate = true
0000000000000000000000000000000000000000;;		} else if !d.Spec.Paused && pausedCondExists {
0000000000000000000000000000000000000000;;			condition := deploymentutil.NewDeploymentCondition(extensions.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.ResumedDeployReason, "Deployment is resumed")
0000000000000000000000000000000000000000;;			deploymentutil.SetDeploymentCondition(&d.Status, *condition)
0000000000000000000000000000000000000000;;			needsUpdate = true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if !needsUpdate {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		d, err = dc.client.Extensions().Deployments(d.Namespace).UpdateStatus(d)
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getAllReplicaSetsAndSyncRevision returns all the replica sets for the provided deployment (new and all old), with new RS's and deployment's revision updated.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// rsList should come from getReplicaSetsForDeployment(d).
0000000000000000000000000000000000000000;;	// podMap should come from getPodMapForDeployment(d, rsList).
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// 1. Get all old RSes this deployment targets, and calculate the max revision number among them (maxOldV).
0000000000000000000000000000000000000000;;	// 2. Get new RS this deployment targets (whose pod template matches deployment's), and update new RS's revision number to (maxOldV + 1),
0000000000000000000000000000000000000000;;	//    only if its revision number is smaller than (maxOldV + 1). If this step failed, we'll update it in the next deployment sync loop.
0000000000000000000000000000000000000000;;	// 3. Copy new RS's revision number to deployment (update deployment's revision). If this step failed, we'll update it in the next deployment sync loop.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Note that currently the deployment controller is using caches to avoid querying the server for reads.
0000000000000000000000000000000000000000;;	// This may lead to stale reads of replica sets, thus incorrect deployment status.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) getAllReplicaSetsAndSyncRevision(d *extensions.Deployment, rsList []*extensions.ReplicaSet, podMap map[types.UID]*v1.PodList, createIfNotExisted bool) (*extensions.ReplicaSet, []*extensions.ReplicaSet, error) {
0000000000000000000000000000000000000000;;		// List the deployment's RSes & Pods and apply pod-template-hash info to deployment's adopted RSes/Pods
0000000000000000000000000000000000000000;;		rsList, err := dc.rsAndPodsWithHashKeySynced(d, rsList, podMap)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, nil, fmt.Errorf("error labeling replica sets and pods with pod-template-hash: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		_, allOldRSs, err := deploymentutil.FindOldReplicaSets(d, rsList)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Get new replica set with the updated revision number
0000000000000000000000000000000000000000;;		newRS, err := dc.getNewReplicaSet(d, rsList, allOldRSs, createIfNotExisted)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return newRS, allOldRSs, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// rsAndPodsWithHashKeySynced returns the RSes and pods the given deployment
0000000000000000000000000000000000000000;;	// targets, with pod-template-hash information synced.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// rsList should come from getReplicaSetsForDeployment(d).
0000000000000000000000000000000000000000;;	// podMap should come from getPodMapForDeployment(d, rsList).
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) rsAndPodsWithHashKeySynced(d *extensions.Deployment, rsList []*extensions.ReplicaSet, podMap map[types.UID]*v1.PodList) ([]*extensions.ReplicaSet, error) {
0000000000000000000000000000000000000000;;		var syncedRSList []*extensions.ReplicaSet
0000000000000000000000000000000000000000;;		for _, rs := range rsList {
0000000000000000000000000000000000000000;;			// Add pod-template-hash information if it's not in the RS.
0000000000000000000000000000000000000000;;			// Otherwise, new RS produced by Deployment will overlap with pre-existing ones
0000000000000000000000000000000000000000;;			// that aren't constrained by the pod-template-hash.
0000000000000000000000000000000000000000;;			syncedRS, err := dc.addHashKeyToRSAndPods(rs, podMap[rs.UID], d.Status.CollisionCount)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			syncedRSList = append(syncedRSList, syncedRS)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return syncedRSList, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// addHashKeyToRSAndPods adds pod-template-hash information to the given rs, if it's not already there, with the following steps:
0000000000000000000000000000000000000000;;	// 1. Add hash label to the rs's pod template, and make sure the controller sees this update so that no orphaned pods will be created
0000000000000000000000000000000000000000;;	// 2. Add hash label to all pods this rs owns, wait until replicaset controller reports rs.Status.FullyLabeledReplicas equal to the desired number of replicas
0000000000000000000000000000000000000000;;	// 3. Add hash label to the rs's label and selector
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) addHashKeyToRSAndPods(rs *extensions.ReplicaSet, podList *v1.PodList, collisionCount *int64) (*extensions.ReplicaSet, error) {
0000000000000000000000000000000000000000;;		// If the rs already has the new hash label in its selector, it's done syncing
0000000000000000000000000000000000000000;;		if labelsutil.SelectorHasLabel(rs.Spec.Selector, extensions.DefaultDeploymentUniqueLabelKey) {
0000000000000000000000000000000000000000;;			return rs, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		hash, err := deploymentutil.GetReplicaSetHash(rs, collisionCount)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// 1. Add hash template label to the rs. This ensures that any newly created pods will have the new label.
0000000000000000000000000000000000000000;;		updatedRS, err := deploymentutil.UpdateRSWithRetries(dc.client.Extensions().ReplicaSets(rs.Namespace), dc.rsLister, rs.Namespace, rs.Name,
0000000000000000000000000000000000000000;;			func(updated *extensions.ReplicaSet) error {
0000000000000000000000000000000000000000;;				// Precondition: the RS doesn't contain the new hash in its pod template label.
0000000000000000000000000000000000000000;;				if updated.Spec.Template.Labels[extensions.DefaultDeploymentUniqueLabelKey] == hash {
0000000000000000000000000000000000000000;;					return utilerrors.ErrPreconditionViolated
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				updated.Spec.Template.Labels = labelsutil.AddLabel(updated.Spec.Template.Labels, extensions.DefaultDeploymentUniqueLabelKey, hash)
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("error updating replica set %s/%s pod template label with template hash: %v", rs.Namespace, rs.Name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Make sure rs pod template is updated so that it won't create pods without the new label (orphaned pods).
0000000000000000000000000000000000000000;;		if updatedRS.Generation > updatedRS.Status.ObservedGeneration {
0000000000000000000000000000000000000000;;			// TODO: Revisit if we really need to wait here as opposed to returning and
0000000000000000000000000000000000000000;;			// potentially unblocking this worker (can wait up to 1min before timing out).
0000000000000000000000000000000000000000;;			if err = deploymentutil.WaitForReplicaSetUpdated(dc.rsLister, updatedRS.Generation, updatedRS.Namespace, updatedRS.Name); err != nil {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("error waiting for replica set %s/%s to be observed by controller: %v", updatedRS.Namespace, updatedRS.Name, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Observed the update of replica set %s/%s's pod template with hash %s.", rs.Namespace, rs.Name, hash)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 2. Update all pods managed by the rs to have the new hash label, so they will be correctly adopted.
0000000000000000000000000000000000000000;;		if err := deploymentutil.LabelPodsWithHash(podList, dc.client, dc.podLister, rs.Namespace, rs.Name, hash); err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("error in adding template hash label %s to pods %+v: %s", hash, podList, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// We need to wait for the replicaset controller to observe the pods being
0000000000000000000000000000000000000000;;		// labeled with pod template hash. Because previously we've called
0000000000000000000000000000000000000000;;		// WaitForReplicaSetUpdated, the replicaset controller should have dropped
0000000000000000000000000000000000000000;;		// FullyLabeledReplicas to 0 already, we only need to wait it to increase
0000000000000000000000000000000000000000;;		// back to the number of replicas in the spec.
0000000000000000000000000000000000000000;;		// TODO: Revisit if we really need to wait here as opposed to returning and
0000000000000000000000000000000000000000;;		// potentially unblocking this worker (can wait up to 1min before timing out).
0000000000000000000000000000000000000000;;		if err := deploymentutil.WaitForPodsHashPopulated(dc.rsLister, updatedRS.Generation, updatedRS.Namespace, updatedRS.Name); err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("Replica set %s/%s: error waiting for replicaset controller to observe pods being labeled with template hash: %v", updatedRS.Namespace, updatedRS.Name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 3. Update rs label and selector to include the new hash label
0000000000000000000000000000000000000000;;		// Copy the old selector, so that we can scrub out any orphaned pods
0000000000000000000000000000000000000000;;		updatedRS, err = deploymentutil.UpdateRSWithRetries(dc.client.Extensions().ReplicaSets(rs.Namespace), dc.rsLister, rs.Namespace, rs.Name, func(updated *extensions.ReplicaSet) error {
0000000000000000000000000000000000000000;;			// Precondition: the RS doesn't contain the new hash in its label and selector.
0000000000000000000000000000000000000000;;			if updated.Labels[extensions.DefaultDeploymentUniqueLabelKey] == hash && updated.Spec.Selector.MatchLabels[extensions.DefaultDeploymentUniqueLabelKey] == hash {
0000000000000000000000000000000000000000;;				return utilerrors.ErrPreconditionViolated
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			updated.Labels = labelsutil.AddLabel(updated.Labels, extensions.DefaultDeploymentUniqueLabelKey, hash)
0000000000000000000000000000000000000000;;			updated.Spec.Selector = labelsutil.AddLabelToSelector(updated.Spec.Selector, extensions.DefaultDeploymentUniqueLabelKey, hash)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		// If the RS isn't actually updated, that's okay, we'll retry in the
0000000000000000000000000000000000000000;;		// next sync loop since its selector isn't updated yet.
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("error updating ReplicaSet %s/%s label and selector with template hash: %v", updatedRS.Namespace, updatedRS.Name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: look for orphaned pods and label them in the background somewhere else periodically
0000000000000000000000000000000000000000;;		return updatedRS, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Returns a replica set that matches the intent of the given deployment. Returns nil if the new replica set doesn't exist yet.
0000000000000000000000000000000000000000;;	// 1. Get existing new RS (the RS that the given deployment targets, whose pod template is the same as deployment's).
0000000000000000000000000000000000000000;;	// 2. If there's existing new RS, update its revision number if it's smaller than (maxOldRevision + 1), where maxOldRevision is the max revision number among all old RSes.
0000000000000000000000000000000000000000;;	// 3. If there's no existing new RS and createIfNotExisted is true, create one with appropriate revision number (maxOldRevision + 1) and replicas.
0000000000000000000000000000000000000000;;	// Note that the pod-template-hash will be added to adopted RSes and pods.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) getNewReplicaSet(d *extensions.Deployment, rsList, oldRSs []*extensions.ReplicaSet, createIfNotExisted bool) (*extensions.ReplicaSet, error) {
0000000000000000000000000000000000000000;;		existingNewRS, err := deploymentutil.FindNewReplicaSet(d, rsList)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Calculate the max revision number among all old RSes
0000000000000000000000000000000000000000;;		maxOldRevision := deploymentutil.MaxRevision(oldRSs)
0000000000000000000000000000000000000000;;		// Calculate revision number for this new replica set
0000000000000000000000000000000000000000;;		newRevision := strconv.FormatInt(maxOldRevision+1, 10)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Latest replica set exists. We need to sync its annotations (includes copying all but
0000000000000000000000000000000000000000;;		// annotationsToSkip from the parent deployment, and update revision, desiredReplicas,
0000000000000000000000000000000000000000;;		// and maxReplicas) and also update the revision annotation in the deployment with the
0000000000000000000000000000000000000000;;		// latest revision.
0000000000000000000000000000000000000000;;		if existingNewRS != nil {
0000000000000000000000000000000000000000;;			objCopy, err := api.Scheme.Copy(existingNewRS)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			rsCopy := objCopy.(*extensions.ReplicaSet)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Set existing new replica set's annotation
0000000000000000000000000000000000000000;;			annotationsUpdated := deploymentutil.SetNewReplicaSetAnnotations(d, rsCopy, newRevision, true)
0000000000000000000000000000000000000000;;			minReadySecondsNeedsUpdate := rsCopy.Spec.MinReadySeconds != d.Spec.MinReadySeconds
0000000000000000000000000000000000000000;;			if annotationsUpdated || minReadySecondsNeedsUpdate {
0000000000000000000000000000000000000000;;				rsCopy.Spec.MinReadySeconds = d.Spec.MinReadySeconds
0000000000000000000000000000000000000000;;				return dc.client.Extensions().ReplicaSets(rsCopy.ObjectMeta.Namespace).Update(rsCopy)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Should use the revision in existingNewRS's annotation, since it set by before
0000000000000000000000000000000000000000;;			needsUpdate := deploymentutil.SetDeploymentRevision(d, rsCopy.Annotations[deploymentutil.RevisionAnnotation])
0000000000000000000000000000000000000000;;			// If no other Progressing condition has been recorded and we need to estimate the progress
0000000000000000000000000000000000000000;;			// of this deployment then it is likely that old users started caring about progress. In that
0000000000000000000000000000000000000000;;			// case we need to take into account the first time we noticed their new replica set.
0000000000000000000000000000000000000000;;			cond := deploymentutil.GetDeploymentCondition(d.Status, extensions.DeploymentProgressing)
0000000000000000000000000000000000000000;;			if d.Spec.ProgressDeadlineSeconds != nil && cond == nil {
0000000000000000000000000000000000000000;;				msg := fmt.Sprintf("Found new replica set %q", rsCopy.Name)
0000000000000000000000000000000000000000;;				condition := deploymentutil.NewDeploymentCondition(extensions.DeploymentProgressing, v1.ConditionTrue, deploymentutil.FoundNewRSReason, msg)
0000000000000000000000000000000000000000;;				deploymentutil.SetDeploymentCondition(&d.Status, *condition)
0000000000000000000000000000000000000000;;				needsUpdate = true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if needsUpdate {
0000000000000000000000000000000000000000;;				if d, err = dc.client.Extensions().Deployments(d.Namespace).UpdateStatus(d); err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return rsCopy, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if !createIfNotExisted {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// new ReplicaSet does not exist, create one.
0000000000000000000000000000000000000000;;		templateCopy, err := api.Scheme.DeepCopy(d.Spec.Template)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		newRSTemplate := templateCopy.(v1.PodTemplateSpec)
0000000000000000000000000000000000000000;;		podTemplateSpecHash := fmt.Sprintf("%d", controller.ComputeHash(&newRSTemplate, d.Status.CollisionCount))
0000000000000000000000000000000000000000;;		newRSTemplate.Labels = labelsutil.CloneAndAddLabel(d.Spec.Template.Labels, extensions.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash)
0000000000000000000000000000000000000000;;		// Add podTemplateHash label to selector.
0000000000000000000000000000000000000000;;		newRSSelector := labelsutil.CloneSelectorAndAddLabel(d.Spec.Selector, extensions.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Create new ReplicaSet
0000000000000000000000000000000000000000;;		newRS := extensions.ReplicaSet{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				// Make the name deterministic, to ensure idempotence
0000000000000000000000000000000000000000;;				Name:            d.Name + "-" + podTemplateSpecHash,
0000000000000000000000000000000000000000;;				Namespace:       d.Namespace,
0000000000000000000000000000000000000000;;				OwnerReferences: []metav1.OwnerReference{*newControllerRef(d)},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: extensions.ReplicaSetSpec{
0000000000000000000000000000000000000000;;				Replicas:        new(int32),
0000000000000000000000000000000000000000;;				MinReadySeconds: d.Spec.MinReadySeconds,
0000000000000000000000000000000000000000;;				Selector:        newRSSelector,
0000000000000000000000000000000000000000;;				Template:        newRSTemplate,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		allRSs := append(oldRSs, &newRS)
0000000000000000000000000000000000000000;;		newReplicasCount, err := deploymentutil.NewRSNewReplicas(d, allRSs, &newRS)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		*(newRS.Spec.Replicas) = newReplicasCount
0000000000000000000000000000000000000000;;		// Set new replica set's annotation
0000000000000000000000000000000000000000;;		deploymentutil.SetNewReplicaSetAnnotations(d, &newRS, newRevision, false)
0000000000000000000000000000000000000000;;		// Create the new ReplicaSet. If it already exists, then we need to check for possible
0000000000000000000000000000000000000000;;		// hash collisions. If there is any other error, we need to report it in the status of
0000000000000000000000000000000000000000;;		// the Deployment.
0000000000000000000000000000000000000000;;		alreadyExists := false
0000000000000000000000000000000000000000;;		createdRS, err := dc.client.Extensions().ReplicaSets(d.Namespace).Create(&newRS)
0000000000000000000000000000000000000000;;		switch {
0000000000000000000000000000000000000000;;		// We may end up hitting this due to a slow cache or a fast resync of the Deployment.
0000000000000000000000000000000000000000;;		// Fetch a copy of the ReplicaSet. If its PodTemplateSpec is semantically deep equal
0000000000000000000000000000000000000000;;		// with the PodTemplateSpec of the Deployment, then that is our new ReplicaSet. Otherwise,
0000000000000000000000000000000000000000;;		// this is a hash collision and we need to increment the collisionCount field in the
0000000000000000000000000000000000000000;;		// status of the Deployment and try the creation again.
0000000000000000000000000000000000000000;;		case errors.IsAlreadyExists(err):
0000000000000000000000000000000000000000;;			alreadyExists = true
0000000000000000000000000000000000000000;;			rs, rsErr := dc.rsLister.ReplicaSets(newRS.Namespace).Get(newRS.Name)
0000000000000000000000000000000000000000;;			if rsErr != nil {
0000000000000000000000000000000000000000;;				return nil, rsErr
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			isEqual, equalErr := deploymentutil.EqualIgnoreHash(&d.Spec.Template, &rs.Spec.Template)
0000000000000000000000000000000000000000;;			if equalErr != nil {
0000000000000000000000000000000000000000;;				return nil, equalErr
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Matching ReplicaSet is not equal - increment the collisionCount in the DeploymentStatus
0000000000000000000000000000000000000000;;			// and requeue the Deployment.
0000000000000000000000000000000000000000;;			if !isEqual {
0000000000000000000000000000000000000000;;				if d.Status.CollisionCount == nil {
0000000000000000000000000000000000000000;;					d.Status.CollisionCount = new(int64)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				preCollisionCount := *d.Status.CollisionCount
0000000000000000000000000000000000000000;;				*d.Status.CollisionCount++
0000000000000000000000000000000000000000;;				// Update the collisionCount for the Deployment and let it requeue by returning the original
0000000000000000000000000000000000000000;;				// error.
0000000000000000000000000000000000000000;;				_, dErr := dc.client.Extensions().Deployments(d.Namespace).UpdateStatus(d)
0000000000000000000000000000000000000000;;				if dErr == nil {
0000000000000000000000000000000000000000;;					glog.V(2).Infof("Found a hash collision for deployment %q - bumping collisionCount (%d->%d) to resolve it", d.Name, preCollisionCount, *d.Status.CollisionCount)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Pass through the matching ReplicaSet as the new ReplicaSet.
0000000000000000000000000000000000000000;;			createdRS = rs
0000000000000000000000000000000000000000;;			err = nil
0000000000000000000000000000000000000000;;		case err != nil:
0000000000000000000000000000000000000000;;			msg := fmt.Sprintf("Failed to create new replica set %q: %v", newRS.Name, err)
0000000000000000000000000000000000000000;;			if d.Spec.ProgressDeadlineSeconds != nil {
0000000000000000000000000000000000000000;;				cond := deploymentutil.NewDeploymentCondition(extensions.DeploymentProgressing, v1.ConditionFalse, deploymentutil.FailedRSCreateReason, msg)
0000000000000000000000000000000000000000;;				deploymentutil.SetDeploymentCondition(&d.Status, *cond)
0000000000000000000000000000000000000000;;				// We don't really care about this error at this point, since we have a bigger issue to report.
0000000000000000000000000000000000000000;;				// TODO: Identify which errors are permanent and switch DeploymentIsFailed to take into account
0000000000000000000000000000000000000000;;				// these reasons as well. Related issue: https://github.com/kubernetes/kubernetes/issues/18568
0000000000000000000000000000000000000000;;				_, _ = dc.client.Extensions().Deployments(d.Namespace).UpdateStatus(d)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			dc.eventRecorder.Eventf(d, v1.EventTypeWarning, deploymentutil.FailedRSCreateReason, msg)
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !alreadyExists && newReplicasCount > 0 {
0000000000000000000000000000000000000000;;			dc.eventRecorder.Eventf(d, v1.EventTypeNormal, "ScalingReplicaSet", "Scaled up replica set %s to %d", createdRS.Name, newReplicasCount)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		needsUpdate := deploymentutil.SetDeploymentRevision(d, newRevision)
0000000000000000000000000000000000000000;;		if !alreadyExists && d.Spec.ProgressDeadlineSeconds != nil {
0000000000000000000000000000000000000000;;			msg := fmt.Sprintf("Created new replica set %q", createdRS.Name)
0000000000000000000000000000000000000000;;			condition := deploymentutil.NewDeploymentCondition(extensions.DeploymentProgressing, v1.ConditionTrue, deploymentutil.NewReplicaSetReason, msg)
0000000000000000000000000000000000000000;;			deploymentutil.SetDeploymentCondition(&d.Status, *condition)
0000000000000000000000000000000000000000;;			needsUpdate = true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if needsUpdate {
0000000000000000000000000000000000000000;;			_, err = dc.client.Extensions().Deployments(d.Namespace).UpdateStatus(d)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return createdRS, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// scale scales proportionally in order to mitigate risk. Otherwise, scaling up can increase the size
0000000000000000000000000000000000000000;;	// of the new replica set and scaling down can decrease the sizes of the old ones, both of which would
0000000000000000000000000000000000000000;;	// have the effect of hastening the rollout progress, which could produce a higher proportion of unavailable
0000000000000000000000000000000000000000;;	// replicas in the event of a problem with the rolled out template. Should run only on scaling events or
0000000000000000000000000000000000000000;;	// when a deployment is paused and not during the normal rollout process.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) scale(deployment *extensions.Deployment, newRS *extensions.ReplicaSet, oldRSs []*extensions.ReplicaSet) error {
0000000000000000000000000000000000000000;;		// If there is only one active replica set then we should scale that up to the full count of the
0000000000000000000000000000000000000000;;		// deployment. If there is no active replica set, then we should scale up the newest replica set.
0000000000000000000000000000000000000000;;		if activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil {
0000000000000000000000000000000000000000;;			if *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			_, _, err := dc.scaleReplicaSetAndRecordEvent(activeOrLatest, *(deployment.Spec.Replicas), deployment)
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If the new replica set is saturated, old replica sets should be fully scaled down.
0000000000000000000000000000000000000000;;		// This case handles replica set adoption during a saturated new replica set.
0000000000000000000000000000000000000000;;		if deploymentutil.IsSaturated(deployment, newRS) {
0000000000000000000000000000000000000000;;			for _, old := range controller.FilterActiveReplicaSets(oldRSs) {
0000000000000000000000000000000000000000;;				if _, _, err := dc.scaleReplicaSetAndRecordEvent(old, 0, deployment); err != nil {
0000000000000000000000000000000000000000;;					return err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// There are old replica sets with pods and the new replica set is not saturated.
0000000000000000000000000000000000000000;;		// We need to proportionally scale all replica sets (new and old) in case of a
0000000000000000000000000000000000000000;;		// rolling deployment.
0000000000000000000000000000000000000000;;		if deploymentutil.IsRollingUpdate(deployment) {
0000000000000000000000000000000000000000;;			allRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS))
0000000000000000000000000000000000000000;;			allRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			allowedSize := int32(0)
0000000000000000000000000000000000000000;;			if *(deployment.Spec.Replicas) > 0 {
0000000000000000000000000000000000000000;;				allowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Number of additional replicas that can be either added or removed from the total
0000000000000000000000000000000000000000;;			// replicas count. These replicas should be distributed proportionally to the active
0000000000000000000000000000000000000000;;			// replica sets.
0000000000000000000000000000000000000000;;			deploymentReplicasToAdd := allowedSize - allRSsReplicas
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// The additional replicas should be distributed proportionally amongst the active
0000000000000000000000000000000000000000;;			// replica sets from the larger to the smaller in size replica set. Scaling direction
0000000000000000000000000000000000000000;;			// drives what happens in case we are trying to scale replica sets of the same size.
0000000000000000000000000000000000000000;;			// In such a case when scaling up, we should scale up newer replica sets first, and
0000000000000000000000000000000000000000;;			// when scaling down, we should scale down older replica sets first.
0000000000000000000000000000000000000000;;			var scalingOperation string
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case deploymentReplicasToAdd > 0:
0000000000000000000000000000000000000000;;				sort.Sort(controller.ReplicaSetsBySizeNewer(allRSs))
0000000000000000000000000000000000000000;;				scalingOperation = "up"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			case deploymentReplicasToAdd < 0:
0000000000000000000000000000000000000000;;				sort.Sort(controller.ReplicaSetsBySizeOlder(allRSs))
0000000000000000000000000000000000000000;;				scalingOperation = "down"
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Iterate over all active replica sets and estimate proportions for each of them.
0000000000000000000000000000000000000000;;			// The absolute value of deploymentReplicasAdded should never exceed the absolute
0000000000000000000000000000000000000000;;			// value of deploymentReplicasToAdd.
0000000000000000000000000000000000000000;;			deploymentReplicasAdded := int32(0)
0000000000000000000000000000000000000000;;			nameToSize := make(map[string]int32)
0000000000000000000000000000000000000000;;			for i := range allRSs {
0000000000000000000000000000000000000000;;				rs := allRSs[i]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Estimate proportions if we have replicas to add, otherwise simply populate
0000000000000000000000000000000000000000;;				// nameToSize with the current sizes for each replica set.
0000000000000000000000000000000000000000;;				if deploymentReplicasToAdd != 0 {
0000000000000000000000000000000000000000;;					proportion := deploymentutil.GetProportion(rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					nameToSize[rs.Name] = *(rs.Spec.Replicas) + proportion
0000000000000000000000000000000000000000;;					deploymentReplicasAdded += proportion
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					nameToSize[rs.Name] = *(rs.Spec.Replicas)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Update all replica sets
0000000000000000000000000000000000000000;;			for i := range allRSs {
0000000000000000000000000000000000000000;;				rs := allRSs[i]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Add/remove any leftovers to the largest replica set.
0000000000000000000000000000000000000000;;				if i == 0 && deploymentReplicasToAdd != 0 {
0000000000000000000000000000000000000000;;					leftover := deploymentReplicasToAdd - deploymentReplicasAdded
0000000000000000000000000000000000000000;;					nameToSize[rs.Name] = nameToSize[rs.Name] + leftover
0000000000000000000000000000000000000000;;					if nameToSize[rs.Name] < 0 {
0000000000000000000000000000000000000000;;						nameToSize[rs.Name] = 0
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// TODO: Use transactions when we have them.
0000000000000000000000000000000000000000;;				if _, _, err := dc.scaleReplicaSet(rs, nameToSize[rs.Name], deployment, scalingOperation); err != nil {
0000000000000000000000000000000000000000;;					// Return as soon as we fail, the deployment is requeued
0000000000000000000000000000000000000000;;					return err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) scaleReplicaSetAndRecordEvent(rs *extensions.ReplicaSet, newScale int32, deployment *extensions.Deployment) (bool, *extensions.ReplicaSet, error) {
0000000000000000000000000000000000000000;;		// No need to scale
0000000000000000000000000000000000000000;;		if *(rs.Spec.Replicas) == newScale {
0000000000000000000000000000000000000000;;			return false, rs, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var scalingOperation string
0000000000000000000000000000000000000000;;		if *(rs.Spec.Replicas) < newScale {
0000000000000000000000000000000000000000;;			scalingOperation = "up"
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			scalingOperation = "down"
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		scaled, newRS, err := dc.scaleReplicaSet(rs, newScale, deployment, scalingOperation)
0000000000000000000000000000000000000000;;		return scaled, newRS, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) scaleReplicaSet(rs *extensions.ReplicaSet, newScale int32, deployment *extensions.Deployment, scalingOperation string) (bool, *extensions.ReplicaSet, error) {
0000000000000000000000000000000000000000;;		objCopy, err := api.Scheme.Copy(rs)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rsCopy := objCopy.(*extensions.ReplicaSet)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sizeNeedsUpdate := *(rsCopy.Spec.Replicas) != newScale
0000000000000000000000000000000000000000;;		// TODO: Do not mutate the replica set here, instead simply compare the annotation and if they mismatch
0000000000000000000000000000000000000000;;		// call SetReplicasAnnotations inside the following if clause. Then we can also move the deep-copy from
0000000000000000000000000000000000000000;;		// above inside the if too.
0000000000000000000000000000000000000000;;		annotationsNeedUpdate := deploymentutil.SetReplicasAnnotations(rsCopy, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		scaled := false
0000000000000000000000000000000000000000;;		if sizeNeedsUpdate || annotationsNeedUpdate {
0000000000000000000000000000000000000000;;			*(rsCopy.Spec.Replicas) = newScale
0000000000000000000000000000000000000000;;			rs, err = dc.client.Extensions().ReplicaSets(rsCopy.Namespace).Update(rsCopy)
0000000000000000000000000000000000000000;;			if err == nil && sizeNeedsUpdate {
0000000000000000000000000000000000000000;;				scaled = true
0000000000000000000000000000000000000000;;				dc.eventRecorder.Eventf(deployment, v1.EventTypeNormal, "ScalingReplicaSet", "Scaled %s replica set %s to %d", scalingOperation, rs.Name, newScale)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return scaled, rs, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// cleanupDeployment is responsible for cleaning up a deployment ie. retains all but the latest N old replica sets
0000000000000000000000000000000000000000;;	// where N=d.Spec.RevisionHistoryLimit. Old replica sets are older versions of the podtemplate of a deployment kept
0000000000000000000000000000000000000000;;	// around by default 1) for historical reasons and 2) for the ability to rollback a deployment.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) cleanupDeployment(oldRSs []*extensions.ReplicaSet, deployment *extensions.Deployment) error {
0000000000000000000000000000000000000000;;		if deployment.Spec.RevisionHistoryLimit == nil {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Avoid deleting replica set with deletion timestamp set
0000000000000000000000000000000000000000;;		aliveFilter := func(rs *extensions.ReplicaSet) bool {
0000000000000000000000000000000000000000;;			return rs != nil && rs.ObjectMeta.DeletionTimestamp == nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		cleanableRSes := controller.FilterReplicaSets(oldRSs, aliveFilter)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		diff := int32(len(cleanableRSes)) - *deployment.Spec.RevisionHistoryLimit
0000000000000000000000000000000000000000;;		if diff <= 0 {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sort.Sort(controller.ReplicaSetsByCreationTimestamp(cleanableRSes))
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Looking to cleanup old replica sets for deployment %q", deployment.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var errList []error
0000000000000000000000000000000000000000;;		for i := int32(0); i < diff; i++ {
0000000000000000000000000000000000000000;;			rs := cleanableRSes[i]
0000000000000000000000000000000000000000;;			// Avoid delete replica set with non-zero replica counts
0000000000000000000000000000000000000000;;			if rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation > rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Trying to cleanup replica set %q for deployment %q", rs.Name, deployment.Name)
0000000000000000000000000000000000000000;;			if err := dc.client.Extensions().ReplicaSets(rs.Namespace).Delete(rs.Name, nil); err != nil && !errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;				glog.V(2).Infof("Failed deleting old replica set %v for deployment %v: %v", rs.Name, deployment.Name, err)
0000000000000000000000000000000000000000;;				errList = append(errList, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return utilerrors.NewAggregate(errList)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncDeploymentStatus checks if the status is up-to-date and sync it if necessary
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) syncDeploymentStatus(allRSs []*extensions.ReplicaSet, newRS *extensions.ReplicaSet, d *extensions.Deployment) error {
0000000000000000000000000000000000000000;;		newStatus := calculateStatus(allRSs, newRS, d)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if reflect.DeepEqual(d.Status, newStatus) {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newDeployment := d
0000000000000000000000000000000000000000;;		newDeployment.Status = newStatus
0000000000000000000000000000000000000000;;		_, err := dc.client.Extensions().Deployments(newDeployment.Namespace).UpdateStatus(newDeployment)
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// calculateStatus calculates the latest status for the provided deployment by looking into the provided replica sets.
0000000000000000000000000000000000000000;;	func calculateStatus(allRSs []*extensions.ReplicaSet, newRS *extensions.ReplicaSet, deployment *extensions.Deployment) extensions.DeploymentStatus {
0000000000000000000000000000000000000000;;		availableReplicas := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs)
0000000000000000000000000000000000000000;;		totalReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs)
0000000000000000000000000000000000000000;;		unavailableReplicas := totalReplicas - availableReplicas
0000000000000000000000000000000000000000;;		// If unavailableReplicas is negative, then that means the Deployment has more available replicas running than
0000000000000000000000000000000000000000;;		// desired, e.g. whenever it scales down. In such a case we should simply default unavailableReplicas to zero.
0000000000000000000000000000000000000000;;		if unavailableReplicas < 0 {
0000000000000000000000000000000000000000;;			unavailableReplicas = 0
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		status := extensions.DeploymentStatus{
0000000000000000000000000000000000000000;;			// TODO: Ensure that if we start retrying status updates, we won't pick up a new Generation value.
0000000000000000000000000000000000000000;;			ObservedGeneration:  deployment.Generation,
0000000000000000000000000000000000000000;;			Replicas:            deploymentutil.GetActualReplicaCountForReplicaSets(allRSs),
0000000000000000000000000000000000000000;;			UpdatedReplicas:     deploymentutil.GetActualReplicaCountForReplicaSets([]*extensions.ReplicaSet{newRS}),
0000000000000000000000000000000000000000;;			ReadyReplicas:       deploymentutil.GetReadyReplicaCountForReplicaSets(allRSs),
0000000000000000000000000000000000000000;;			AvailableReplicas:   availableReplicas,
0000000000000000000000000000000000000000;;			UnavailableReplicas: unavailableReplicas,
0000000000000000000000000000000000000000;;			CollisionCount:      deployment.Status.CollisionCount,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Copy conditions one by one so we won't mutate the original object.
0000000000000000000000000000000000000000;;		conditions := deployment.Status.Conditions
0000000000000000000000000000000000000000;;		for i := range conditions {
0000000000000000000000000000000000000000;;			status.Conditions = append(status.Conditions, conditions[i])
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if availableReplicas >= *(deployment.Spec.Replicas)-deploymentutil.MaxUnavailable(*deployment) {
0000000000000000000000000000000000000000;;			minAvailability := deploymentutil.NewDeploymentCondition(extensions.DeploymentAvailable, v1.ConditionTrue, deploymentutil.MinimumReplicasAvailable, "Deployment has minimum availability.")
0000000000000000000000000000000000000000;;			deploymentutil.SetDeploymentCondition(&status, *minAvailability)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			noMinAvailability := deploymentutil.NewDeploymentCondition(extensions.DeploymentAvailable, v1.ConditionFalse, deploymentutil.MinimumReplicasUnavailable, "Deployment does not have minimum availability.")
0000000000000000000000000000000000000000;;			deploymentutil.SetDeploymentCondition(&status, *noMinAvailability)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return status
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// isScalingEvent checks whether the provided deployment has been updated with a scaling event
0000000000000000000000000000000000000000;;	// by looking at the desired-replicas annotation in the active replica sets of the deployment.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// rsList should come from getReplicaSetsForDeployment(d).
0000000000000000000000000000000000000000;;	// podMap should come from getPodMapForDeployment(d, rsList).
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) isScalingEvent(d *extensions.Deployment, rsList []*extensions.ReplicaSet, podMap map[types.UID]*v1.PodList) (bool, error) {
0000000000000000000000000000000000000000;;		newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, podMap, false)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		allRSs := append(oldRSs, newRS)
0000000000000000000000000000000000000000;;		for _, rs := range controller.FilterActiveReplicaSets(allRSs) {
0000000000000000000000000000000000000000;;			desired, ok := deploymentutil.GetDesiredReplicasAnnotation(rs)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if desired != *(d.Spec.Replicas) {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// newControllerRef returns a ControllerRef pointing to the deployment.
0000000000000000000000000000000000000000;;	func newControllerRef(d *extensions.Deployment) *metav1.OwnerReference {
0000000000000000000000000000000000000000;;		blockOwnerDeletion := true
0000000000000000000000000000000000000000;;		isController := true
0000000000000000000000000000000000000000;;		return &metav1.OwnerReference{
0000000000000000000000000000000000000000;;			APIVersion:         controllerKind.GroupVersion().String(),
0000000000000000000000000000000000000000;;			Kind:               controllerKind.Kind,
0000000000000000000000000000000000000000;;			Name:               d.Name,
0000000000000000000000000000000000000000;;			UID:                d.UID,
0000000000000000000000000000000000000000;;			BlockOwnerDeletion: &blockOwnerDeletion,
0000000000000000000000000000000000000000;;			Controller:         &isController,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
