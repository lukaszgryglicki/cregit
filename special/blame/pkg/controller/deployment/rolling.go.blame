0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
fc9c25ec1edeff3780e90faa04791ac7766e789e;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package deployment
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		extensions "k8s.io/api/extensions/v1beta1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/integer"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/controller"
0000000000000000000000000000000000000000;;		deploymentutil "k8s.io/kubernetes/pkg/controller/deployment/util"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// rolloutRolling implements the logic for rolling a new replica set.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) rolloutRolling(d *extensions.Deployment, rsList []*extensions.ReplicaSet, podMap map[types.UID]*v1.PodList) error {
0000000000000000000000000000000000000000;;		newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, podMap, true)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		allRSs := append(oldRSs, newRS)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Scale up, if we can.
0000000000000000000000000000000000000000;;		scaledUp, err := dc.reconcileNewReplicaSet(allRSs, newRS, d)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if scaledUp {
0000000000000000000000000000000000000000;;			// Update DeploymentStatus
0000000000000000000000000000000000000000;;			return dc.syncRolloutStatus(allRSs, newRS, d)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Scale down, if we can.
0000000000000000000000000000000000000000;;		scaledDown, err := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if scaledDown {
0000000000000000000000000000000000000000;;			// Update DeploymentStatus
0000000000000000000000000000000000000000;;			return dc.syncRolloutStatus(allRSs, newRS, d)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Sync deployment status
0000000000000000000000000000000000000000;;		return dc.syncRolloutStatus(allRSs, newRS, d)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) reconcileNewReplicaSet(allRSs []*extensions.ReplicaSet, newRS *extensions.ReplicaSet, deployment *extensions.Deployment) (bool, error) {
0000000000000000000000000000000000000000;;		if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) {
0000000000000000000000000000000000000000;;			// Scaling not required.
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if *(newRS.Spec.Replicas) > *(deployment.Spec.Replicas) {
0000000000000000000000000000000000000000;;			// Scale down.
0000000000000000000000000000000000000000;;			scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, *(deployment.Spec.Replicas), deployment)
0000000000000000000000000000000000000000;;			return scaled, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		newReplicasCount, err := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, newReplicasCount, deployment)
0000000000000000000000000000000000000000;;		return scaled, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) reconcileOldReplicaSets(allRSs []*extensions.ReplicaSet, oldRSs []*extensions.ReplicaSet, newRS *extensions.ReplicaSet, deployment *extensions.Deployment) (bool, error) {
0000000000000000000000000000000000000000;;		oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs)
0000000000000000000000000000000000000000;;		if oldPodsCount == 0 {
0000000000000000000000000000000000000000;;			// Can't scale down further
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs)
0000000000000000000000000000000000000000;;		glog.V(4).Infof("New replica set %s/%s has %d available pods.", newRS.Namespace, newRS.Name, newRS.Status.AvailableReplicas)
0000000000000000000000000000000000000000;;		maxUnavailable := deploymentutil.MaxUnavailable(*deployment)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check if we can scale down. We can scale down in the following 2 cases:
0000000000000000000000000000000000000000;;		// * Some old replica sets have unhealthy replicas, we could safely scale down those unhealthy replicas since that won't further
0000000000000000000000000000000000000000;;		//  increase unavailability.
0000000000000000000000000000000000000000;;		// * New replica set has scaled up and it's replicas becomes ready, then we can scale down old replica sets in a further step.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// maxScaledDown := allPodsCount - minAvailable - newReplicaSetPodsUnavailable
0000000000000000000000000000000000000000;;		// take into account not only maxUnavailable and any surge pods that have been created, but also unavailable pods from
0000000000000000000000000000000000000000;;		// the newRS, so that the unavailable pods from the newRS would not make us scale down old replica sets in a further
0000000000000000000000000000000000000000;;		// step(that will increase unavailability).
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// Concrete example:
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// * 10 replicas
0000000000000000000000000000000000000000;;		// * 2 maxUnavailable (absolute number, not percent)
0000000000000000000000000000000000000000;;		// * 3 maxSurge (absolute number, not percent)
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// case 1:
0000000000000000000000000000000000000000;;		// * Deployment is updated, newRS is created with 3 replicas, oldRS is scaled down to 8, and newRS is scaled up to 5.
0000000000000000000000000000000000000000;;		// * The new replica set pods crashloop and never become available.
0000000000000000000000000000000000000000;;		// * allPodsCount is 13. minAvailable is 8. newRSPodsUnavailable is 5.
0000000000000000000000000000000000000000;;		// * A node fails and causes one of the oldRS pods to become unavailable. However, 13 - 8 - 5 = 0, so the oldRS won't be scaled down.
0000000000000000000000000000000000000000;;		// * The user notices the crashloop and does kubectl rollout undo to rollback.
0000000000000000000000000000000000000000;;		// * newRSPodsUnavailable is 1, since we rolled back to the good replica set, so maxScaledDown = 13 - 8 - 1 = 4. 4 of the crashlooping pods will be scaled down.
0000000000000000000000000000000000000000;;		// * The total number of pods will then be 9 and the newRS can be scaled up to 10.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// case 2:
0000000000000000000000000000000000000000;;		// Same example, but pushing a new pod template instead of rolling back (aka "roll over"):
0000000000000000000000000000000000000000;;		// * The new replica set created must start with 0 replicas because allPodsCount is already at 13.
0000000000000000000000000000000000000000;;		// * However, newRSPodsUnavailable would also be 0, so the 2 old replica sets could be scaled down by 5 (13 - 8 - 0), which would then
0000000000000000000000000000000000000000;;		// allow the new replica set to be scaled up by 5.
0000000000000000000000000000000000000000;;		minAvailable := *(deployment.Spec.Replicas) - maxUnavailable
0000000000000000000000000000000000000000;;		newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas
0000000000000000000000000000000000000000;;		maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount
0000000000000000000000000000000000000000;;		if maxScaledDown <= 0 {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Clean up unhealthy replicas first, otherwise unhealthy replicas will block deployment
0000000000000000000000000000000000000000;;		// and cause timeout. See https://github.com/kubernetes/kubernetes/issues/16737
0000000000000000000000000000000000000000;;		oldRSs, cleanupCount, err := dc.cleanupUnhealthyReplicas(oldRSs, deployment, maxScaledDown)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Cleaned up unhealthy replicas from old RSes by %d", cleanupCount)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Scale down old replica sets, need check maxUnavailable to ensure we can scale down
0000000000000000000000000000000000000000;;		allRSs = append(oldRSs, newRS)
0000000000000000000000000000000000000000;;		scaledDownCount, err := dc.scaleDownOldReplicaSetsForRollingUpdate(allRSs, oldRSs, deployment)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Scaled down old RSes of deployment %s by %d", deployment.Name, scaledDownCount)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		totalScaledDown := cleanupCount + scaledDownCount
0000000000000000000000000000000000000000;;		return totalScaledDown > 0, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// cleanupUnhealthyReplicas will scale down old replica sets with unhealthy replicas, so that all unhealthy replicas will be deleted.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) cleanupUnhealthyReplicas(oldRSs []*extensions.ReplicaSet, deployment *extensions.Deployment, maxCleanupCount int32) ([]*extensions.ReplicaSet, int32, error) {
0000000000000000000000000000000000000000;;		sort.Sort(controller.ReplicaSetsByCreationTimestamp(oldRSs))
0000000000000000000000000000000000000000;;		// Safely scale down all old replica sets with unhealthy replicas. Replica set will sort the pods in the order
0000000000000000000000000000000000000000;;		// such that not-ready < ready, unscheduled < scheduled, and pending < running. This ensures that unhealthy replicas will
0000000000000000000000000000000000000000;;		// been deleted first and won't increase unavailability.
0000000000000000000000000000000000000000;;		totalScaledDown := int32(0)
0000000000000000000000000000000000000000;;		for i, targetRS := range oldRSs {
0000000000000000000000000000000000000000;;			if totalScaledDown >= maxCleanupCount {
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if *(targetRS.Spec.Replicas) == 0 {
0000000000000000000000000000000000000000;;				// cannot scale down this replica set.
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Found %d available pods in old RS %s/%s", targetRS.Status.AvailableReplicas, targetRS.Namespace, targetRS.Name)
0000000000000000000000000000000000000000;;			if *(targetRS.Spec.Replicas) == targetRS.Status.AvailableReplicas {
0000000000000000000000000000000000000000;;				// no unhealthy replicas found, no scaling required.
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			scaledDownCount := int32(integer.IntMin(int(maxCleanupCount-totalScaledDown), int(*(targetRS.Spec.Replicas)-targetRS.Status.AvailableReplicas)))
0000000000000000000000000000000000000000;;			newReplicasCount := *(targetRS.Spec.Replicas) - scaledDownCount
0000000000000000000000000000000000000000;;			if newReplicasCount > *(targetRS.Spec.Replicas) {
0000000000000000000000000000000000000000;;				return nil, 0, fmt.Errorf("when cleaning up unhealthy replicas, got invalid request to scale down %s/%s %d -> %d", targetRS.Namespace, targetRS.Name, *(targetRS.Spec.Replicas), newReplicasCount)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			_, updatedOldRS, err := dc.scaleReplicaSetAndRecordEvent(targetRS, newReplicasCount, deployment)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, totalScaledDown, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			totalScaledDown += scaledDownCount
0000000000000000000000000000000000000000;;			oldRSs[i] = updatedOldRS
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return oldRSs, totalScaledDown, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// scaleDownOldReplicaSetsForRollingUpdate scales down old replica sets when deployment strategy is "RollingUpdate".
0000000000000000000000000000000000000000;;	// Need check maxUnavailable to ensure availability
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) scaleDownOldReplicaSetsForRollingUpdate(allRSs []*extensions.ReplicaSet, oldRSs []*extensions.ReplicaSet, deployment *extensions.Deployment) (int32, error) {
0000000000000000000000000000000000000000;;		maxUnavailable := deploymentutil.MaxUnavailable(*deployment)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check if we can scale down.
0000000000000000000000000000000000000000;;		minAvailable := *(deployment.Spec.Replicas) - maxUnavailable
0000000000000000000000000000000000000000;;		// Find the number of available pods.
0000000000000000000000000000000000000000;;		availablePodCount := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs)
0000000000000000000000000000000000000000;;		if availablePodCount <= minAvailable {
0000000000000000000000000000000000000000;;			// Cannot scale down.
0000000000000000000000000000000000000000;;			return 0, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Found %d available pods in deployment %s, scaling down old RSes", availablePodCount, deployment.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sort.Sort(controller.ReplicaSetsByCreationTimestamp(oldRSs))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		totalScaledDown := int32(0)
0000000000000000000000000000000000000000;;		totalScaleDownCount := availablePodCount - minAvailable
0000000000000000000000000000000000000000;;		for _, targetRS := range oldRSs {
0000000000000000000000000000000000000000;;			if totalScaledDown >= totalScaleDownCount {
0000000000000000000000000000000000000000;;				// No further scaling required.
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if *(targetRS.Spec.Replicas) == 0 {
0000000000000000000000000000000000000000;;				// cannot scale down this ReplicaSet.
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Scale down.
0000000000000000000000000000000000000000;;			scaleDownCount := int32(integer.IntMin(int(*(targetRS.Spec.Replicas)), int(totalScaleDownCount-totalScaledDown)))
0000000000000000000000000000000000000000;;			newReplicasCount := *(targetRS.Spec.Replicas) - scaleDownCount
0000000000000000000000000000000000000000;;			if newReplicasCount > *(targetRS.Spec.Replicas) {
0000000000000000000000000000000000000000;;				return 0, fmt.Errorf("when scaling down old RS, got invalid request to scale down %s/%s %d -> %d", targetRS.Namespace, targetRS.Name, *(targetRS.Spec.Replicas), newReplicasCount)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			_, _, err := dc.scaleReplicaSetAndRecordEvent(targetRS, newReplicasCount, deployment)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return totalScaledDown, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			totalScaledDown += scaleDownCount
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return totalScaledDown, nil
0000000000000000000000000000000000000000;;	}
