0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
9d3a1c468d36c47d8187b59c13d0d8cce0f49903;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package deployment
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"reflect"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		extensions "k8s.io/api/extensions/v1beta1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/controller/deployment/util"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// hasFailed determines if a deployment has failed or not by estimating its progress.
0000000000000000000000000000000000000000;;	// Progress for a deployment is considered when a new replica set is created or adopted,
0000000000000000000000000000000000000000;;	// and when new pods scale up or old pods scale down. Progress is not estimated for paused
0000000000000000000000000000000000000000;;	// deployments or when users don't really care about it ie. progressDeadlineSeconds is not
0000000000000000000000000000000000000000;;	// specified.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) hasFailed(d *extensions.Deployment, rsList []*extensions.ReplicaSet, podMap map[types.UID]*v1.PodList) (bool, error) {
0000000000000000000000000000000000000000;;		if d.Spec.ProgressDeadlineSeconds == nil || d.Spec.RollbackTo != nil || d.Spec.Paused {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, podMap, false)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// There is a template change so we don't need to check for any progress right now.
0000000000000000000000000000000000000000;;		if newRS == nil {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Look at the status of the deployment - if there is already a NewRSAvailableReason
0000000000000000000000000000000000000000;;		// then we don't need to estimate any progress. This is needed in order to avoid
0000000000000000000000000000000000000000;;		// estimating progress for scaling events after a rollout has finished.
0000000000000000000000000000000000000000;;		cond := util.GetDeploymentCondition(d.Status, extensions.DeploymentProgressing)
0000000000000000000000000000000000000000;;		if cond != nil && cond.Reason == util.NewRSAvailableReason {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Look for permanent failures here.
0000000000000000000000000000000000000000;;		// See https://github.com/kubernetes/kubernetes/issues/18568
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		allRSs := append(oldRSs, newRS)
0000000000000000000000000000000000000000;;		newStatus := calculateStatus(allRSs, newRS, d)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If the deployment is complete or it is progressing, there is no need to check if it
0000000000000000000000000000000000000000;;		// has timed out.
0000000000000000000000000000000000000000;;		if util.DeploymentComplete(d, &newStatus) || util.DeploymentProgressing(d, &newStatus) {
0000000000000000000000000000000000000000;;			return false, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check if the deployment has timed out.
0000000000000000000000000000000000000000;;		return util.DeploymentTimedOut(d, &newStatus), nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncRolloutStatus updates the status of a deployment during a rollout. There are
0000000000000000000000000000000000000000;;	// cases this helper will run that cannot be prevented from the scaling detection,
0000000000000000000000000000000000000000;;	// for example a resync of the deployment after it was scaled up. In those cases,
0000000000000000000000000000000000000000;;	// we shouldn't try to estimate any progress.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) syncRolloutStatus(allRSs []*extensions.ReplicaSet, newRS *extensions.ReplicaSet, d *extensions.Deployment) error {
0000000000000000000000000000000000000000;;		newStatus := calculateStatus(allRSs, newRS, d)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If there is no progressDeadlineSeconds set, remove any Progressing condition.
0000000000000000000000000000000000000000;;		if d.Spec.ProgressDeadlineSeconds == nil {
0000000000000000000000000000000000000000;;			util.RemoveDeploymentCondition(&newStatus, extensions.DeploymentProgressing)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If there is only one replica set that is active then that means we are not running
0000000000000000000000000000000000000000;;		// a new rollout and this is a resync where we don't need to estimate any progress.
0000000000000000000000000000000000000000;;		// In such a case, we should simply not estimate any progress for this deployment.
0000000000000000000000000000000000000000;;		currentCond := util.GetDeploymentCondition(d.Status, extensions.DeploymentProgressing)
0000000000000000000000000000000000000000;;		isCompleteDeployment := newStatus.Replicas == newStatus.UpdatedReplicas && currentCond != nil && currentCond.Reason == util.NewRSAvailableReason
0000000000000000000000000000000000000000;;		// Check for progress only if there is a progress deadline set and the latest rollout
0000000000000000000000000000000000000000;;		// hasn't completed yet.
0000000000000000000000000000000000000000;;		if d.Spec.ProgressDeadlineSeconds != nil && !isCompleteDeployment {
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case util.DeploymentComplete(d, &newStatus):
0000000000000000000000000000000000000000;;				// Update the deployment conditions with a message for the new replica set that
0000000000000000000000000000000000000000;;				// was successfully deployed. If the condition already exists, we ignore this update.
0000000000000000000000000000000000000000;;				msg := fmt.Sprintf("ReplicaSet %q has successfully progressed.", newRS.Name)
0000000000000000000000000000000000000000;;				condition := util.NewDeploymentCondition(extensions.DeploymentProgressing, v1.ConditionTrue, util.NewRSAvailableReason, msg)
0000000000000000000000000000000000000000;;				util.SetDeploymentCondition(&newStatus, *condition)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			case util.DeploymentProgressing(d, &newStatus):
0000000000000000000000000000000000000000;;				// If there is any progress made, continue by not checking if the deployment failed. This
0000000000000000000000000000000000000000;;				// behavior emulates the rolling updater progressDeadline check.
0000000000000000000000000000000000000000;;				msg := fmt.Sprintf("Deployment %q is progressing.", d.Name)
0000000000000000000000000000000000000000;;				if newRS != nil {
0000000000000000000000000000000000000000;;					msg = fmt.Sprintf("ReplicaSet %q is progressing.", newRS.Name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				condition := util.NewDeploymentCondition(extensions.DeploymentProgressing, v1.ConditionTrue, util.ReplicaSetUpdatedReason, msg)
0000000000000000000000000000000000000000;;				// Update the current Progressing condition or add a new one if it doesn't exist.
0000000000000000000000000000000000000000;;				// If a Progressing condition with status=true already exists, we should update
0000000000000000000000000000000000000000;;				// everything but lastTransitionTime. SetDeploymentCondition already does that but
0000000000000000000000000000000000000000;;				// it also is not updating conditions when the reason of the new condition is the
0000000000000000000000000000000000000000;;				// same as the old. The Progressing condition is a special case because we want to
0000000000000000000000000000000000000000;;				// update with the same reason and change just lastUpdateTime iff we notice any
0000000000000000000000000000000000000000;;				// progress. That's why we handle it here.
0000000000000000000000000000000000000000;;				if currentCond != nil {
0000000000000000000000000000000000000000;;					if currentCond.Status == v1.ConditionTrue {
0000000000000000000000000000000000000000;;						condition.LastTransitionTime = currentCond.LastTransitionTime
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					util.RemoveDeploymentCondition(&newStatus, extensions.DeploymentProgressing)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				util.SetDeploymentCondition(&newStatus, *condition)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			case util.DeploymentTimedOut(d, &newStatus):
0000000000000000000000000000000000000000;;				// Update the deployment with a timeout condition. If the condition already exists,
0000000000000000000000000000000000000000;;				// we ignore this update.
0000000000000000000000000000000000000000;;				msg := fmt.Sprintf("Deployment %q has timed out progressing.", d.Name)
0000000000000000000000000000000000000000;;				if newRS != nil {
0000000000000000000000000000000000000000;;					msg = fmt.Sprintf("ReplicaSet %q has timed out progressing.", newRS.Name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				condition := util.NewDeploymentCondition(extensions.DeploymentProgressing, v1.ConditionFalse, util.TimedOutReason, msg)
0000000000000000000000000000000000000000;;				util.SetDeploymentCondition(&newStatus, *condition)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Move failure conditions of all replica sets in deployment conditions. For now,
0000000000000000000000000000000000000000;;		// only one failure condition is returned from getReplicaFailures.
0000000000000000000000000000000000000000;;		if replicaFailureCond := dc.getReplicaFailures(allRSs, newRS); len(replicaFailureCond) > 0 {
0000000000000000000000000000000000000000;;			// There will be only one ReplicaFailure condition on the replica set.
0000000000000000000000000000000000000000;;			util.SetDeploymentCondition(&newStatus, replicaFailureCond[0])
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			util.RemoveDeploymentCondition(&newStatus, extensions.DeploymentReplicaFailure)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Do not update if there is nothing new to add.
0000000000000000000000000000000000000000;;		if reflect.DeepEqual(d.Status, newStatus) {
0000000000000000000000000000000000000000;;			// Requeue the deployment if required.
0000000000000000000000000000000000000000;;			dc.requeueStuckDeployment(d, newStatus)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newDeployment := d
0000000000000000000000000000000000000000;;		newDeployment.Status = newStatus
0000000000000000000000000000000000000000;;		_, err := dc.client.Extensions().Deployments(newDeployment.Namespace).UpdateStatus(newDeployment)
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getReplicaFailures will convert replica failure conditions from replica sets
0000000000000000000000000000000000000000;;	// to deployment conditions.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) getReplicaFailures(allRSs []*extensions.ReplicaSet, newRS *extensions.ReplicaSet) []extensions.DeploymentCondition {
0000000000000000000000000000000000000000;;		var conditions []extensions.DeploymentCondition
0000000000000000000000000000000000000000;;		if newRS != nil {
0000000000000000000000000000000000000000;;			for _, c := range newRS.Status.Conditions {
0000000000000000000000000000000000000000;;				if c.Type != extensions.ReplicaSetReplicaFailure {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				conditions = append(conditions, util.ReplicaSetToDeploymentCondition(c))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Return failures for the new replica set over failures from old replica sets.
0000000000000000000000000000000000000000;;		if len(conditions) > 0 {
0000000000000000000000000000000000000000;;			return conditions
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i := range allRSs {
0000000000000000000000000000000000000000;;			rs := allRSs[i]
0000000000000000000000000000000000000000;;			if rs == nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, c := range rs.Status.Conditions {
0000000000000000000000000000000000000000;;				if c.Type != extensions.ReplicaSetReplicaFailure {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				conditions = append(conditions, util.ReplicaSetToDeploymentCondition(c))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return conditions
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// used for unit testing
0000000000000000000000000000000000000000;;	var nowFn = func() time.Time { return time.Now() }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// requeueStuckDeployment checks whether the provided deployment needs to be synced for a progress
0000000000000000000000000000000000000000;;	// check. It returns the time after the deployment will be requeued for the progress check, 0 if it
0000000000000000000000000000000000000000;;	// will be requeued now, or -1 if it does not need to be requeued.
0000000000000000000000000000000000000000;;	func (dc *DeploymentController) requeueStuckDeployment(d *extensions.Deployment, newStatus extensions.DeploymentStatus) time.Duration {
0000000000000000000000000000000000000000;;		currentCond := util.GetDeploymentCondition(d.Status, extensions.DeploymentProgressing)
0000000000000000000000000000000000000000;;		// Can't estimate progress if there is no deadline in the spec or progressing condition in the current status.
0000000000000000000000000000000000000000;;		if d.Spec.ProgressDeadlineSeconds == nil || currentCond == nil {
0000000000000000000000000000000000000000;;			return time.Duration(-1)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// No need to estimate progress if the rollout is complete or already timed out.
0000000000000000000000000000000000000000;;		if util.DeploymentComplete(d, &newStatus) || currentCond.Reason == util.TimedOutReason {
0000000000000000000000000000000000000000;;			return time.Duration(-1)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// If there is no sign of progress at this point then there is a high chance that the
0000000000000000000000000000000000000000;;		// deployment is stuck. We should resync this deployment at some point in the future[1]
0000000000000000000000000000000000000000;;		// and check whether it has timed out. We definitely need this, otherwise we depend on the
0000000000000000000000000000000000000000;;		// controller resync interval. See https://github.com/kubernetes/kubernetes/issues/34458.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// [1] ProgressingCondition.LastUpdatedTime + progressDeadlineSeconds - time.Now()
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// For example, if a Deployment updated its Progressing condition 3 minutes ago and has a
0000000000000000000000000000000000000000;;		// deadline of 10 minutes, it would need to be resynced for a progress check after 7 minutes.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// lastUpdated: 			00:00:00
0000000000000000000000000000000000000000;;		// now: 					00:03:00
0000000000000000000000000000000000000000;;		// progressDeadlineSeconds: 600 (10 minutes)
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// lastUpdated + progressDeadlineSeconds - now => 00:00:00 + 00:10:00 - 00:03:00 => 07:00
0000000000000000000000000000000000000000;;		after := currentCond.LastUpdateTime.Time.Add(time.Duration(*d.Spec.ProgressDeadlineSeconds) * time.Second).Sub(nowFn())
0000000000000000000000000000000000000000;;		// If the remaining time is less than a second, then requeue the deployment immediately.
0000000000000000000000000000000000000000;;		// Make it ratelimited so we stay on the safe side, eventually the Deployment should
0000000000000000000000000000000000000000;;		// transition either to a Complete or to a TimedOut condition.
0000000000000000000000000000000000000000;;		if after < time.Second {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Queueing up deployment %q for a progress check now", d.Name)
0000000000000000000000000000000000000000;;			dc.enqueueRateLimited(d)
0000000000000000000000000000000000000000;;			return time.Duration(0)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Queueing up deployment %q for a progress check after %ds", d.Name, int(after.Seconds()))
0000000000000000000000000000000000000000;;		// Add a second to avoid milliseconds skew in AddAfter.
0000000000000000000000000000000000000000;;		// See https://github.com/kubernetes/kubernetes/issues/39785#issuecomment-279959133 for more info.
0000000000000000000000000000000000000000;;		dc.enqueueAfter(d, after+time.Second)
0000000000000000000000000000000000000000;;		return after
0000000000000000000000000000000000000000;;	}
