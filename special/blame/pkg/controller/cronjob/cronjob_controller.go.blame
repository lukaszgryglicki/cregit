0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
9109846b6b4280b9be5a8abbd7433b17fce70d25;pkg/controller/scheduledjob/controller.go[pkg/controller/scheduledjob/controller.go][pkg/controller/cronjob/cronjob_controller.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package cronjob
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	I did not use watch or expectations.  Those add a lot of corner cases, and we aren't
0000000000000000000000000000000000000000;;	expecting a large volume of jobs or scheduledJobs.  (We are favoring correctness
0000000000000000000000000000000000000000;;	over scalability.  If we find a single controller thread is too slow because
0000000000000000000000000000000000000000;;	there are a lot of Jobs or CronJobs, we we can parallelize by Namespace.
0000000000000000000000000000000000000000;;	If we find the load on the API server is too high, we can use a watch and
0000000000000000000000000000000000000000;;	UndeltaStore.)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Just periodically list jobs and SJs, and then reconcile them.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		batchv1 "k8s.io/api/batch/v1"
0000000000000000000000000000000000000000;;		batchv2alpha1 "k8s.io/api/batch/v2alpha1"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		clientv1 "k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		utilerrors "k8s.io/apimachinery/pkg/util/errors"
0000000000000000000000000000000000000000;;		utilruntime "k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		v1core "k8s.io/client-go/kubernetes/typed/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api/v1/ref"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/metrics"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Utilities for dealing with Jobs and CronJobs and time.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// controllerKind contains the schema.GroupVersionKind for this controller type.
0000000000000000000000000000000000000000;;	var controllerKind = batchv2alpha1.SchemeGroupVersion.WithKind("CronJob")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type CronJobController struct {
0000000000000000000000000000000000000000;;		kubeClient clientset.Interface
0000000000000000000000000000000000000000;;		jobControl jobControlInterface
0000000000000000000000000000000000000000;;		sjControl  sjControlInterface
0000000000000000000000000000000000000000;;		podControl podControlInterface
0000000000000000000000000000000000000000;;		recorder   record.EventRecorder
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewCronJobController(kubeClient clientset.Interface) *CronJobController {
0000000000000000000000000000000000000000;;		eventBroadcaster := record.NewBroadcaster()
0000000000000000000000000000000000000000;;		eventBroadcaster.StartLogging(glog.Infof)
0000000000000000000000000000000000000000;;		// TODO: remove the wrapper when every clients have moved to use the clientset.
0000000000000000000000000000000000000000;;		eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: v1core.New(kubeClient.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kubeClient != nil && kubeClient.Core().RESTClient().GetRateLimiter() != nil {
0000000000000000000000000000000000000000;;			metrics.RegisterMetricAndTrackRateLimiterUsage("cronjob_controller", kubeClient.Core().RESTClient().GetRateLimiter())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		jm := &CronJobController{
0000000000000000000000000000000000000000;;			kubeClient: kubeClient,
0000000000000000000000000000000000000000;;			jobControl: realJobControl{KubeClient: kubeClient},
0000000000000000000000000000000000000000;;			sjControl:  &realSJControl{KubeClient: kubeClient},
0000000000000000000000000000000000000000;;			podControl: &realPodControl{KubeClient: kubeClient},
0000000000000000000000000000000000000000;;			recorder:   eventBroadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: "cronjob-controller"}),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return jm
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewCronJobControllerFromClient(kubeClient clientset.Interface) *CronJobController {
0000000000000000000000000000000000000000;;		jm := NewCronJobController(kubeClient)
0000000000000000000000000000000000000000;;		return jm
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Run the main goroutine responsible for watching and syncing jobs.
0000000000000000000000000000000000000000;;	func (jm *CronJobController) Run(stopCh <-chan struct{}) {
0000000000000000000000000000000000000000;;		defer utilruntime.HandleCrash()
0000000000000000000000000000000000000000;;		glog.Infof("Starting CronJob Manager")
0000000000000000000000000000000000000000;;		// Check things every 10 second.
0000000000000000000000000000000000000000;;		go wait.Until(jm.syncAll, 10*time.Second, stopCh)
0000000000000000000000000000000000000000;;		<-stopCh
0000000000000000000000000000000000000000;;		glog.Infof("Shutting down CronJob Manager")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncAll lists all the CronJobs and Jobs and reconciles them.
0000000000000000000000000000000000000000;;	func (jm *CronJobController) syncAll() {
0000000000000000000000000000000000000000;;		// List children (Jobs) before parents (CronJob).
0000000000000000000000000000000000000000;;		// This guarantees that if we see any Job that got orphaned by the GC orphan finalizer,
0000000000000000000000000000000000000000;;		// we must also see that the parent CronJob has non-nil DeletionTimestamp (see #42639).
0000000000000000000000000000000000000000;;		// Note that this only works because we are NOT using any caches here.
0000000000000000000000000000000000000000;;		jl, err := jm.kubeClient.BatchV1().Jobs(metav1.NamespaceAll).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			utilruntime.HandleError(fmt.Errorf("can't list Jobs: %v", err))
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		js := jl.Items
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Found %d jobs", len(js))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sjl, err := jm.kubeClient.BatchV2alpha1().CronJobs(metav1.NamespaceAll).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			utilruntime.HandleError(fmt.Errorf("can't list CronJobs: %v", err))
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		sjs := sjl.Items
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Found %d cronjobs", len(sjs))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		jobsBySj := groupJobsByParent(js)
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Found %d groups", len(jobsBySj))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, sj := range sjs {
0000000000000000000000000000000000000000;;			syncOne(&sj, jobsBySj[sj.UID], time.Now(), jm.jobControl, jm.sjControl, jm.podControl, jm.recorder)
0000000000000000000000000000000000000000;;			cleanupFinishedJobs(&sj, jobsBySj[sj.UID], jm.jobControl, jm.sjControl, jm.podControl, jm.recorder)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// cleanupFinishedJobs cleanups finished jobs created by a CronJob
0000000000000000000000000000000000000000;;	func cleanupFinishedJobs(sj *batchv2alpha1.CronJob, js []batchv1.Job, jc jobControlInterface,
0000000000000000000000000000000000000000;;		sjc sjControlInterface, pc podControlInterface, recorder record.EventRecorder) {
0000000000000000000000000000000000000000;;		// If neither limits are active, there is no need to do anything.
0000000000000000000000000000000000000000;;		if sj.Spec.FailedJobsHistoryLimit == nil && sj.Spec.SuccessfulJobsHistoryLimit == nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		failedJobs := []batchv1.Job{}
0000000000000000000000000000000000000000;;		succesfulJobs := []batchv1.Job{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, job := range js {
0000000000000000000000000000000000000000;;			isFinished, finishedStatus := getFinishedStatus(&job)
0000000000000000000000000000000000000000;;			if isFinished && finishedStatus == batchv1.JobComplete {
0000000000000000000000000000000000000000;;				succesfulJobs = append(succesfulJobs, job)
0000000000000000000000000000000000000000;;			} else if isFinished && finishedStatus == batchv1.JobFailed {
0000000000000000000000000000000000000000;;				failedJobs = append(failedJobs, job)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if sj.Spec.SuccessfulJobsHistoryLimit != nil {
0000000000000000000000000000000000000000;;			removeOldestJobs(sj,
0000000000000000000000000000000000000000;;				succesfulJobs,
0000000000000000000000000000000000000000;;				jc,
0000000000000000000000000000000000000000;;				pc,
0000000000000000000000000000000000000000;;				*sj.Spec.SuccessfulJobsHistoryLimit,
0000000000000000000000000000000000000000;;				recorder)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if sj.Spec.FailedJobsHistoryLimit != nil {
0000000000000000000000000000000000000000;;			removeOldestJobs(sj,
0000000000000000000000000000000000000000;;				failedJobs,
0000000000000000000000000000000000000000;;				jc,
0000000000000000000000000000000000000000;;				pc,
0000000000000000000000000000000000000000;;				*sj.Spec.FailedJobsHistoryLimit,
0000000000000000000000000000000000000000;;				recorder)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Update the CronJob, in case jobs were removed from the list.
0000000000000000000000000000000000000000;;		if _, err := sjc.UpdateStatus(sj); err != nil {
0000000000000000000000000000000000000000;;			nameForLog := fmt.Sprintf("%s/%s", sj.Namespace, sj.Name)
0000000000000000000000000000000000000000;;			glog.Infof("Unable to update status for %s (rv = %s): %v", nameForLog, sj.ResourceVersion, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// removeOldestJobs removes the oldest jobs from a list of jobs
0000000000000000000000000000000000000000;;	func removeOldestJobs(sj *batchv2alpha1.CronJob, js []batchv1.Job, jc jobControlInterface,
0000000000000000000000000000000000000000;;		pc podControlInterface, maxJobs int32, recorder record.EventRecorder) {
0000000000000000000000000000000000000000;;		numToDelete := len(js) - int(maxJobs)
0000000000000000000000000000000000000000;;		if numToDelete <= 0 {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nameForLog := fmt.Sprintf("%s/%s", sj.Namespace, sj.Name)
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Cleaning up %d/%d jobs from %s", numToDelete, len(js), nameForLog)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sort.Sort(byJobStartTime(js))
0000000000000000000000000000000000000000;;		for i := 0; i < numToDelete; i++ {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Removing job %s from %s", js[i].Name, nameForLog)
0000000000000000000000000000000000000000;;			deleteJob(sj, &js[i], jc, pc, recorder, "history limit reached")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncOne reconciles a CronJob with a list of any Jobs that it created.
0000000000000000000000000000000000000000;;	// All known jobs created by "sj" should be included in "js".
0000000000000000000000000000000000000000;;	// The current time is passed in to facilitate testing.
0000000000000000000000000000000000000000;;	// It has no receiver, to facilitate testing.
0000000000000000000000000000000000000000;;	func syncOne(sj *batchv2alpha1.CronJob, js []batchv1.Job, now time.Time, jc jobControlInterface, sjc sjControlInterface, pc podControlInterface, recorder record.EventRecorder) {
0000000000000000000000000000000000000000;;		nameForLog := fmt.Sprintf("%s/%s", sj.Namespace, sj.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		childrenJobs := make(map[types.UID]bool)
0000000000000000000000000000000000000000;;		for _, j := range js {
0000000000000000000000000000000000000000;;			childrenJobs[j.ObjectMeta.UID] = true
0000000000000000000000000000000000000000;;			found := inActiveList(*sj, j.ObjectMeta.UID)
0000000000000000000000000000000000000000;;			if !found && !IsJobFinished(&j) {
0000000000000000000000000000000000000000;;				recorder.Eventf(sj, v1.EventTypeWarning, "UnexpectedJob", "Saw a job that the controller did not create or forgot: %v", j.Name)
0000000000000000000000000000000000000000;;				// We found an unfinished job that has us as the parent, but it is not in our Active list.
0000000000000000000000000000000000000000;;				// This could happen if we crashed right after creating the Job and before updating the status,
0000000000000000000000000000000000000000;;				// or if our jobs list is newer than our sj status after a relist, or if someone intentionally created
0000000000000000000000000000000000000000;;				// a job that they wanted us to adopt.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// TODO: maybe handle the adoption case?  Concurrency/suspend rules will not apply in that case, obviously, since we can't
0000000000000000000000000000000000000000;;				// stop users from creating jobs if they have permission.  It is assumed that if a
0000000000000000000000000000000000000000;;				// user has permission to create a job within a namespace, then they have permission to make any scheduledJob
0000000000000000000000000000000000000000;;				// in the same namespace "adopt" that job.  ReplicaSets and their Pods work the same way.
0000000000000000000000000000000000000000;;				// TBS: how to update sj.Status.LastScheduleTime if the adopted job is newer than any we knew about?
0000000000000000000000000000000000000000;;			} else if found && IsJobFinished(&j) {
0000000000000000000000000000000000000000;;				deleteFromActiveList(sj, j.ObjectMeta.UID)
0000000000000000000000000000000000000000;;				// TODO: event to call out failure vs success.
0000000000000000000000000000000000000000;;				recorder.Eventf(sj, v1.EventTypeNormal, "SawCompletedJob", "Saw completed job: %v", j.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Remove any job reference from the active list if the corresponding job does not exist any more.
0000000000000000000000000000000000000000;;		// Otherwise, the cronjob may be stuck in active mode forever even though there is no matching
0000000000000000000000000000000000000000;;		// job running.
0000000000000000000000000000000000000000;;		for _, j := range sj.Status.Active {
0000000000000000000000000000000000000000;;			if found := childrenJobs[j.UID]; !found {
0000000000000000000000000000000000000000;;				recorder.Eventf(sj, v1.EventTypeNormal, "MissingJob", "Active job went missing: %v", j.Name)
0000000000000000000000000000000000000000;;				deleteFromActiveList(sj, j.UID)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		updatedSJ, err := sjc.UpdateStatus(sj)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to update status for %s (rv = %s): %v", nameForLog, sj.ResourceVersion, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		*sj = *updatedSJ
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if sj.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;			// The CronJob is being deleted.
0000000000000000000000000000000000000000;;			// Don't do anything other than updating status.
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if sj.Spec.Suspend != nil && *sj.Spec.Suspend {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Not starting job for %s because it is suspended", nameForLog)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		times, err := getRecentUnmetScheduleTimes(*sj, now)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			recorder.Eventf(sj, v1.EventTypeWarning, "FailedNeedsStart", "Cannot determine if job needs to be started: %v", err)
0000000000000000000000000000000000000000;;			glog.Errorf("Cannot determine if %s needs to be started: %v", nameForLog, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// TODO: handle multiple unmet start times, from oldest to newest, updating status as needed.
0000000000000000000000000000000000000000;;		if len(times) == 0 {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("No unmet start times for %s", nameForLog)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(times) > 1 {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Multiple unmet start times for %s so only starting last one", nameForLog)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		scheduledTime := times[len(times)-1]
0000000000000000000000000000000000000000;;		tooLate := false
0000000000000000000000000000000000000000;;		if sj.Spec.StartingDeadlineSeconds != nil {
0000000000000000000000000000000000000000;;			tooLate = scheduledTime.Add(time.Second * time.Duration(*sj.Spec.StartingDeadlineSeconds)).Before(now)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if tooLate {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Missed starting window for %s", nameForLog)
0000000000000000000000000000000000000000;;			// TODO: generate an event for a miss.  Use a warning level event because it indicates a
0000000000000000000000000000000000000000;;			// problem with the controller (restart or long queue), and is not expected by user either.
0000000000000000000000000000000000000000;;			// Since we don't set LastScheduleTime when not scheduling, we are going to keep noticing
0000000000000000000000000000000000000000;;			// the miss every cycle.  In order to avoid sending multiple events, and to avoid processing
0000000000000000000000000000000000000000;;			// the sj again and again, we could set a Status.LastMissedTime when we notice a miss.
0000000000000000000000000000000000000000;;			// Then, when we call getRecentUnmetScheduleTimes, we can take max(creationTimestamp,
0000000000000000000000000000000000000000;;			// Status.LastScheduleTime, Status.LastMissedTime), and then so we won't generate
0000000000000000000000000000000000000000;;			// and event the next time we process it, and also so the user looking at the status
0000000000000000000000000000000000000000;;			// can see easily that there was a missed execution.
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if sj.Spec.ConcurrencyPolicy == batchv2alpha1.ForbidConcurrent && len(sj.Status.Active) > 0 {
0000000000000000000000000000000000000000;;			// Regardless which source of information we use for the set of active jobs,
0000000000000000000000000000000000000000;;			// there is some risk that we won't see an active job when there is one.
0000000000000000000000000000000000000000;;			// (because we haven't seen the status update to the SJ or the created pod).
0000000000000000000000000000000000000000;;			// So it is theoretically possible to have concurrency with Forbid.
0000000000000000000000000000000000000000;;			// As long the as the invokations are "far enough apart in time", this usually won't happen.
0000000000000000000000000000000000000000;;			//
0000000000000000000000000000000000000000;;			// TODO: for Forbid, we could use the same name for every execution, as a lock.
0000000000000000000000000000000000000000;;			// With replace, we could use a name that is deterministic per execution time.
0000000000000000000000000000000000000000;;			// But that would mean that you could not inspect prior successes or failures of Forbid jobs.
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Not starting job for %s because of prior execution still running and concurrency policy is Forbid", nameForLog)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if sj.Spec.ConcurrencyPolicy == batchv2alpha1.ReplaceConcurrent {
0000000000000000000000000000000000000000;;			for _, j := range sj.Status.Active {
0000000000000000000000000000000000000000;;				// TODO: this should be replaced with server side job deletion
0000000000000000000000000000000000000000;;				// currently this mimics JobReaper from pkg/kubectl/stop.go
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Deleting job %s of %s that was still running at next scheduled start time", j.Name, nameForLog)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				job, err := jc.GetJob(j.Namespace, j.Name)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					recorder.Eventf(sj, v1.EventTypeWarning, "FailedGet", "Get job: %v", err)
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if !deleteJob(sj, job, jc, pc, recorder, "") {
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		jobReq, err := getJobFromTemplate(sj, scheduledTime)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to make Job from template in %s: %v", nameForLog, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		jobResp, err := jc.CreateJob(sj.Namespace, jobReq)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			recorder.Eventf(sj, v1.EventTypeWarning, "FailedCreate", "Error creating job: %v", err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Created Job %s for %s", jobResp.Name, nameForLog)
0000000000000000000000000000000000000000;;		recorder.Eventf(sj, v1.EventTypeNormal, "SuccessfulCreate", "Created job %v", jobResp.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// ------------------------------------------------------------------ //
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If this process restarts at this point (after posting a job, but
0000000000000000000000000000000000000000;;		// before updating the status), then we might try to start the job on
0000000000000000000000000000000000000000;;		// the next time.  Actually, if we relist the SJs and Jobs on the next
0000000000000000000000000000000000000000;;		// iteration of syncAll, we might not see our own status update, and
0000000000000000000000000000000000000000;;		// then post one again.  So, we need to use the job name as a lock to
0000000000000000000000000000000000000000;;		// prevent us from making the job twice (name the job with hash of its
0000000000000000000000000000000000000000;;		// scheduled time).
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Add the just-started job to the status list.
0000000000000000000000000000000000000000;;		ref, err := getRef(jobResp)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("Unable to make object reference for job for %s", nameForLog)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			sj.Status.Active = append(sj.Status.Active, *ref)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		sj.Status.LastScheduleTime = &metav1.Time{Time: scheduledTime}
0000000000000000000000000000000000000000;;		if _, err := sjc.UpdateStatus(sj); err != nil {
0000000000000000000000000000000000000000;;			glog.Infof("Unable to update status for %s (rv = %s): %v", nameForLog, sj.ResourceVersion, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// deleteJob reaps a job, deleting the job, the pobs and the reference in the active list
0000000000000000000000000000000000000000;;	func deleteJob(sj *batchv2alpha1.CronJob, job *batchv1.Job, jc jobControlInterface,
0000000000000000000000000000000000000000;;		pc podControlInterface, recorder record.EventRecorder, reason string) bool {
0000000000000000000000000000000000000000;;		// TODO: this should be replaced with server side job deletion
0000000000000000000000000000000000000000;;		// currencontinuetly this mimics JobReaper from pkg/kubectl/stop.go
0000000000000000000000000000000000000000;;		nameForLog := fmt.Sprintf("%s/%s", sj.Namespace, sj.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// scale job down to 0
0000000000000000000000000000000000000000;;		if *job.Spec.Parallelism != 0 {
0000000000000000000000000000000000000000;;			zero := int32(0)
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			job.Spec.Parallelism = &zero
0000000000000000000000000000000000000000;;			job, err = jc.UpdateJob(job.Namespace, job)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				recorder.Eventf(sj, v1.EventTypeWarning, "FailedUpdate", "Update job: %v", err)
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// remove all pods...
0000000000000000000000000000000000000000;;		selector, _ := metav1.LabelSelectorAsSelector(job.Spec.Selector)
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{LabelSelector: selector.String()}
0000000000000000000000000000000000000000;;		podList, err := pc.ListPods(job.Namespace, options)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			recorder.Eventf(sj, v1.EventTypeWarning, "FailedList", "List job-pods: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		errList := []error{}
0000000000000000000000000000000000000000;;		for _, pod := range podList.Items {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("CronJob controller is deleting Pod %v/%v", pod.Namespace, pod.Name)
0000000000000000000000000000000000000000;;			if err := pc.DeletePod(pod.Namespace, pod.Name); err != nil {
0000000000000000000000000000000000000000;;				// ignores the error when the pod isn't found
0000000000000000000000000000000000000000;;				if !errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;					errList = append(errList, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(errList) != 0 {
0000000000000000000000000000000000000000;;			recorder.Eventf(sj, v1.EventTypeWarning, "FailedDelete", "Deleted job-pods: %v", utilerrors.NewAggregate(errList))
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// ... the job itself...
0000000000000000000000000000000000000000;;		if err := jc.DeleteJob(job.Namespace, job.Name); err != nil {
0000000000000000000000000000000000000000;;			recorder.Eventf(sj, v1.EventTypeWarning, "FailedDelete", "Deleted job: %v", err)
0000000000000000000000000000000000000000;;			glog.Errorf("Error deleting job %s from %s: %v", job.Name, nameForLog, err)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// ... and its reference from active list
0000000000000000000000000000000000000000;;		deleteFromActiveList(sj, job.ObjectMeta.UID)
0000000000000000000000000000000000000000;;		recorder.Eventf(sj, v1.EventTypeNormal, "SuccessfulDelete", "Deleted job %v", job.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getRef(object runtime.Object) (*v1.ObjectReference, error) {
0000000000000000000000000000000000000000;;		return ref.GetReference(api.Scheme, object)
0000000000000000000000000000000000000000;;	}
