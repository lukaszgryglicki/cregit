0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
ae083a94f29a34c1c70a3868e726d7102d66fd2a;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package disruption
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"reflect"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		apps "k8s.io/api/apps/v1beta1"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		clientv1 "k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/api/extensions/v1beta1"
0000000000000000000000000000000000000000;;		policy "k8s.io/api/policy/v1beta1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/intstr"
0000000000000000000000000000000000000000;;		utilruntime "k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		v1core "k8s.io/client-go/kubernetes/typed/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/cache"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/workqueue"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		podutil "k8s.io/kubernetes/pkg/api/v1/pod"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		policyclientset "k8s.io/kubernetes/pkg/client/clientset_generated/clientset/typed/policy/v1beta1"
0000000000000000000000000000000000000000;;		appsinformers "k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/apps/v1beta1"
0000000000000000000000000000000000000000;;		coreinformers "k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/core/v1"
0000000000000000000000000000000000000000;;		extensionsinformers "k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/extensions/v1beta1"
0000000000000000000000000000000000000000;;		policyinformers "k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/policy/v1beta1"
0000000000000000000000000000000000000000;;		appslisters "k8s.io/kubernetes/pkg/client/listers/apps/v1beta1"
0000000000000000000000000000000000000000;;		corelisters "k8s.io/kubernetes/pkg/client/listers/core/v1"
0000000000000000000000000000000000000000;;		extensionslisters "k8s.io/kubernetes/pkg/client/listers/extensions/v1beta1"
0000000000000000000000000000000000000000;;		policylisters "k8s.io/kubernetes/pkg/client/listers/policy/v1beta1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/controller"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const statusUpdateRetries = 2
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// DeletionTimeout sets maximum time from the moment a pod is added to DisruptedPods in PDB.Status
0000000000000000000000000000000000000000;;	// to the time when the pod is expected to be seen by PDB controller as having been marked for deletion.
0000000000000000000000000000000000000000;;	// If the pod was not marked for deletion during that time it is assumed that it won't be deleted at
0000000000000000000000000000000000000000;;	// all and the corresponding entry can be removed from pdb.Status.DisruptedPods. It is assumed that
0000000000000000000000000000000000000000;;	// pod/pdb apiserver to controller latency is relatively small (like 1-2sec) so the below value should
0000000000000000000000000000000000000000;;	// be more than enough.
0000000000000000000000000000000000000000;;	// If the controller is running on a different node it is important that the two nodes have synced
0000000000000000000000000000000000000000;;	// clock (via ntp for example). Otherwise PodDisruptionBudget controller may not provide enough
0000000000000000000000000000000000000000;;	// protection against unwanted pod disruptions.
0000000000000000000000000000000000000000;;	const DeletionTimeout = 2 * 60 * time.Second
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type updater func(*policy.PodDisruptionBudget) error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type DisruptionController struct {
0000000000000000000000000000000000000000;;		kubeClient clientset.Interface
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pdbLister       policylisters.PodDisruptionBudgetLister
0000000000000000000000000000000000000000;;		pdbListerSynced cache.InformerSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podLister       corelisters.PodLister
0000000000000000000000000000000000000000;;		podListerSynced cache.InformerSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rcLister       corelisters.ReplicationControllerLister
0000000000000000000000000000000000000000;;		rcListerSynced cache.InformerSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rsLister       extensionslisters.ReplicaSetLister
0000000000000000000000000000000000000000;;		rsListerSynced cache.InformerSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dLister       extensionslisters.DeploymentLister
0000000000000000000000000000000000000000;;		dListerSynced cache.InformerSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ssLister       appslisters.StatefulSetLister
0000000000000000000000000000000000000000;;		ssListerSynced cache.InformerSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// PodDisruptionBudget keys that need to be synced.
0000000000000000000000000000000000000000;;		queue        workqueue.RateLimitingInterface
0000000000000000000000000000000000000000;;		recheckQueue workqueue.DelayingInterface
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		broadcaster record.EventBroadcaster
0000000000000000000000000000000000000000;;		recorder    record.EventRecorder
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		getUpdater func() updater
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// controllerAndScale is used to return (controller, scale) pairs from the
0000000000000000000000000000000000000000;;	// controller finder functions.
0000000000000000000000000000000000000000;;	type controllerAndScale struct {
0000000000000000000000000000000000000000;;		types.UID
0000000000000000000000000000000000000000;;		scale int32
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// podControllerFinder is a function type that maps a pod to a list of
0000000000000000000000000000000000000000;;	// controllers and their scale.
0000000000000000000000000000000000000000;;	type podControllerFinder func(*v1.Pod) ([]controllerAndScale, error)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewDisruptionController(
0000000000000000000000000000000000000000;;		podInformer coreinformers.PodInformer,
0000000000000000000000000000000000000000;;		pdbInformer policyinformers.PodDisruptionBudgetInformer,
0000000000000000000000000000000000000000;;		rcInformer coreinformers.ReplicationControllerInformer,
0000000000000000000000000000000000000000;;		rsInformer extensionsinformers.ReplicaSetInformer,
0000000000000000000000000000000000000000;;		dInformer extensionsinformers.DeploymentInformer,
0000000000000000000000000000000000000000;;		ssInformer appsinformers.StatefulSetInformer,
0000000000000000000000000000000000000000;;		kubeClient clientset.Interface,
0000000000000000000000000000000000000000;;	) *DisruptionController {
0000000000000000000000000000000000000000;;		dc := &DisruptionController{
0000000000000000000000000000000000000000;;			kubeClient:   kubeClient,
0000000000000000000000000000000000000000;;			queue:        workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "disruption"),
0000000000000000000000000000000000000000;;			recheckQueue: workqueue.NewNamedDelayingQueue("disruption-recheck"),
0000000000000000000000000000000000000000;;			broadcaster:  record.NewBroadcaster(),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		dc.recorder = dc.broadcaster.NewRecorder(api.Scheme, clientv1.EventSource{Component: "controllermanager"})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dc.getUpdater = func() updater { return dc.writePdbStatus }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
0000000000000000000000000000000000000000;;			AddFunc:    dc.addPod,
0000000000000000000000000000000000000000;;			UpdateFunc: dc.updatePod,
0000000000000000000000000000000000000000;;			DeleteFunc: dc.deletePod,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		dc.podLister = podInformer.Lister()
0000000000000000000000000000000000000000;;		dc.podListerSynced = podInformer.Informer().HasSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pdbInformer.Informer().AddEventHandlerWithResyncPeriod(
0000000000000000000000000000000000000000;;			cache.ResourceEventHandlerFuncs{
0000000000000000000000000000000000000000;;				AddFunc:    dc.addDb,
0000000000000000000000000000000000000000;;				UpdateFunc: dc.updateDb,
0000000000000000000000000000000000000000;;				DeleteFunc: dc.removeDb,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			30*time.Second,
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		dc.pdbLister = pdbInformer.Lister()
0000000000000000000000000000000000000000;;		dc.pdbListerSynced = pdbInformer.Informer().HasSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dc.rcLister = rcInformer.Lister()
0000000000000000000000000000000000000000;;		dc.rcListerSynced = rcInformer.Informer().HasSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dc.rsLister = rsInformer.Lister()
0000000000000000000000000000000000000000;;		dc.rsListerSynced = rsInformer.Informer().HasSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dc.dLister = dInformer.Lister()
0000000000000000000000000000000000000000;;		dc.dListerSynced = dInformer.Informer().HasSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dc.ssLister = ssInformer.Lister()
0000000000000000000000000000000000000000;;		dc.ssListerSynced = ssInformer.Informer().HasSynced
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return dc
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO(mml): When controllerRef is implemented (#2210), we *could* simply
0000000000000000000000000000000000000000;;	// return controllers without their scales, and access scale type-generically
0000000000000000000000000000000000000000;;	// via the scale subresource.  That may not be as much of a win as it sounds,
0000000000000000000000000000000000000000;;	// however.  We are accessing everything through the pkg/client/cache API that
0000000000000000000000000000000000000000;;	// we have to set up and tune to the types we know we'll be accessing anyway,
0000000000000000000000000000000000000000;;	// and we may well need further tweaks just to be able to access scale
0000000000000000000000000000000000000000;;	// subresources.
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) finders() []podControllerFinder {
0000000000000000000000000000000000000000;;		return []podControllerFinder{dc.getPodReplicationControllers, dc.getPodDeployments, dc.getPodReplicaSets,
0000000000000000000000000000000000000000;;			dc.getPodStatefulSets}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var (
0000000000000000000000000000000000000000;;		controllerKindRS  = v1beta1.SchemeGroupVersion.WithKind("ReplicaSet")
0000000000000000000000000000000000000000;;		controllerKindSS  = apps.SchemeGroupVersion.WithKind("StatefulSet")
0000000000000000000000000000000000000000;;		controllerKindRC  = v1.SchemeGroupVersion.WithKind("ReplicationController")
0000000000000000000000000000000000000000;;		controllerKindDep = v1beta1.SchemeGroupVersion.WithKind("Deployment")
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getPodReplicaSets finds replicasets which have no matching deployments.
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) getPodReplicaSets(pod *v1.Pod) ([]controllerAndScale, error) {
0000000000000000000000000000000000000000;;		var casSlice []controllerAndScale
0000000000000000000000000000000000000000;;		controllerRef := controller.GetControllerOf(pod)
0000000000000000000000000000000000000000;;		if controllerRef == nil {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if controllerRef.Kind != controllerKindRS.Kind {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rs, err := dc.rsLister.ReplicaSets(pod.Namespace).Get(controllerRef.Name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			// The only possible error is NotFound, which is ok here.
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if rs.UID != controllerRef.UID {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		controllerRef = controller.GetControllerOf(rs)
0000000000000000000000000000000000000000;;		if controllerRef != nil && controllerRef.Kind == controllerKindDep.Kind {
0000000000000000000000000000000000000000;;			// Skip RS if it's controlled by a Deployment.
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		casSlice = append(casSlice, controllerAndScale{rs.UID, *(rs.Spec.Replicas)})
0000000000000000000000000000000000000000;;		return casSlice, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getPodStatefulSet returns the statefulset managing the given pod.
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) getPodStatefulSets(pod *v1.Pod) ([]controllerAndScale, error) {
0000000000000000000000000000000000000000;;		var casSlice []controllerAndScale
0000000000000000000000000000000000000000;;		controllerRef := controller.GetControllerOf(pod)
0000000000000000000000000000000000000000;;		if controllerRef == nil {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if controllerRef.Kind != controllerKindSS.Kind {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ss, err := dc.ssLister.StatefulSets(pod.Namespace).Get(controllerRef.Name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			// The only possible error is NotFound, which is ok here.
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if ss.UID != controllerRef.UID {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		casSlice = append(casSlice, controllerAndScale{ss.UID, *(ss.Spec.Replicas)})
0000000000000000000000000000000000000000;;		return casSlice, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getPodDeployments finds deployments for any replicasets which are being managed by deployments.
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) getPodDeployments(pod *v1.Pod) ([]controllerAndScale, error) {
0000000000000000000000000000000000000000;;		var casSlice []controllerAndScale
0000000000000000000000000000000000000000;;		controllerRef := controller.GetControllerOf(pod)
0000000000000000000000000000000000000000;;		if controllerRef == nil {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if controllerRef.Kind != controllerKindRS.Kind {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rs, err := dc.rsLister.ReplicaSets(pod.Namespace).Get(controllerRef.Name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			// The only possible error is NotFound, which is ok here.
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if rs.UID != controllerRef.UID {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		controllerRef = controller.GetControllerOf(rs)
0000000000000000000000000000000000000000;;		if controllerRef == nil {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if controllerRef.Kind != controllerKindDep.Kind {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		deployment, err := dc.dLister.Deployments(rs.Namespace).Get(controllerRef.Name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			// The only possible error is NotFound, which is ok here.
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if deployment.UID != controllerRef.UID {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		casSlice = append(casSlice, controllerAndScale{deployment.UID, *(deployment.Spec.Replicas)})
0000000000000000000000000000000000000000;;		return casSlice, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) getPodReplicationControllers(pod *v1.Pod) ([]controllerAndScale, error) {
0000000000000000000000000000000000000000;;		var casSlice []controllerAndScale
0000000000000000000000000000000000000000;;		controllerRef := controller.GetControllerOf(pod)
0000000000000000000000000000000000000000;;		if controllerRef == nil {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if controllerRef.Kind != controllerKindRC.Kind {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rc, err := dc.rcLister.ReplicationControllers(pod.Namespace).Get(controllerRef.Name)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			// The only possible error is NotFound, which is ok here.
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if rc.UID != controllerRef.UID {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		casSlice = append(casSlice, controllerAndScale{rc.UID, *(rc.Spec.Replicas)})
0000000000000000000000000000000000000000;;		return casSlice, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) Run(stopCh <-chan struct{}) {
0000000000000000000000000000000000000000;;		defer utilruntime.HandleCrash()
0000000000000000000000000000000000000000;;		defer dc.queue.ShutDown()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.Infof("Starting disruption controller")
0000000000000000000000000000000000000000;;		defer glog.Infof("Shutting down disruption controller")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if !controller.WaitForCacheSync("disruption", stopCh, dc.podListerSynced, dc.pdbListerSynced, dc.rcListerSynced, dc.rsListerSynced, dc.dListerSynced, dc.ssListerSynced) {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if dc.kubeClient != nil {
0000000000000000000000000000000000000000;;			glog.Infof("Sending events to api server.")
0000000000000000000000000000000000000000;;			dc.broadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: v1core.New(dc.kubeClient.Core().RESTClient()).Events("")})
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.Infof("No api server defined - no events will be sent to API server.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		go wait.Until(dc.worker, time.Second, stopCh)
0000000000000000000000000000000000000000;;		go wait.Until(dc.recheckWorker, time.Second, stopCh)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		<-stopCh
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) addDb(obj interface{}) {
0000000000000000000000000000000000000000;;		pdb := obj.(*policy.PodDisruptionBudget)
0000000000000000000000000000000000000000;;		glog.V(4).Infof("add DB %q", pdb.Name)
0000000000000000000000000000000000000000;;		dc.enqueuePdb(pdb)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) updateDb(old, cur interface{}) {
0000000000000000000000000000000000000000;;		// TODO(mml) ignore updates where 'old' is equivalent to 'cur'.
0000000000000000000000000000000000000000;;		pdb := cur.(*policy.PodDisruptionBudget)
0000000000000000000000000000000000000000;;		glog.V(4).Infof("update DB %q", pdb.Name)
0000000000000000000000000000000000000000;;		dc.enqueuePdb(pdb)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) removeDb(obj interface{}) {
0000000000000000000000000000000000000000;;		pdb := obj.(*policy.PodDisruptionBudget)
0000000000000000000000000000000000000000;;		glog.V(4).Infof("remove DB %q", pdb.Name)
0000000000000000000000000000000000000000;;		dc.enqueuePdb(pdb)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) addPod(obj interface{}) {
0000000000000000000000000000000000000000;;		pod := obj.(*v1.Pod)
0000000000000000000000000000000000000000;;		glog.V(4).Infof("addPod called on pod %q", pod.Name)
0000000000000000000000000000000000000000;;		pdb := dc.getPdbForPod(pod)
0000000000000000000000000000000000000000;;		if pdb == nil {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("No matching pdb for pod %q", pod.Name)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("addPod %q -> PDB %q", pod.Name, pdb.Name)
0000000000000000000000000000000000000000;;		dc.enqueuePdb(pdb)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) updatePod(old, cur interface{}) {
0000000000000000000000000000000000000000;;		pod := cur.(*v1.Pod)
0000000000000000000000000000000000000000;;		glog.V(4).Infof("updatePod called on pod %q", pod.Name)
0000000000000000000000000000000000000000;;		pdb := dc.getPdbForPod(pod)
0000000000000000000000000000000000000000;;		if pdb == nil {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("No matching pdb for pod %q", pod.Name)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("updatePod %q -> PDB %q", pod.Name, pdb.Name)
0000000000000000000000000000000000000000;;		dc.enqueuePdb(pdb)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) deletePod(obj interface{}) {
0000000000000000000000000000000000000000;;		pod, ok := obj.(*v1.Pod)
0000000000000000000000000000000000000000;;		// When a delete is dropped, the relist will notice a pod in the store not
0000000000000000000000000000000000000000;;		// in the list, leading to the insertion of a tombstone object which contains
0000000000000000000000000000000000000000;;		// the deleted key/value. Note that this value might be stale. If the pod
0000000000000000000000000000000000000000;;		// changed labels the new ReplicaSet will not be woken up till the periodic
0000000000000000000000000000000000000000;;		// resync.
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				glog.Errorf("Couldn't get object from tombstone %+v", obj)
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pod, ok = tombstone.Obj.(*v1.Pod)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				glog.Errorf("Tombstone contained object that is not a pod %+v", obj)
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("deletePod called on pod %q", pod.Name)
0000000000000000000000000000000000000000;;		pdb := dc.getPdbForPod(pod)
0000000000000000000000000000000000000000;;		if pdb == nil {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("No matching pdb for pod %q", pod.Name)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("deletePod %q -> PDB %q", pod.Name, pdb.Name)
0000000000000000000000000000000000000000;;		dc.enqueuePdb(pdb)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) enqueuePdb(pdb *policy.PodDisruptionBudget) {
0000000000000000000000000000000000000000;;		key, err := controller.KeyFunc(pdb)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Cound't get key for PodDisruptionBudget object %+v: %v", pdb, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		dc.queue.Add(key)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) enqueuePdbForRecheck(pdb *policy.PodDisruptionBudget, delay time.Duration) {
0000000000000000000000000000000000000000;;		key, err := controller.KeyFunc(pdb)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Cound't get key for PodDisruptionBudget object %+v: %v", pdb, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		dc.recheckQueue.AddAfter(key, delay)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) getPdbForPod(pod *v1.Pod) *policy.PodDisruptionBudget {
0000000000000000000000000000000000000000;;		// GetPodPodDisruptionBudgets returns an error only if no
0000000000000000000000000000000000000000;;		// PodDisruptionBudgets are found.  We don't return that as an error to the
0000000000000000000000000000000000000000;;		// caller.
0000000000000000000000000000000000000000;;		pdbs, err := dc.pdbLister.GetPodPodDisruptionBudgets(pod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("No PodDisruptionBudgets found for pod %v, PodDisruptionBudget controller will avoid syncing.", pod.Name)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(pdbs) > 1 {
0000000000000000000000000000000000000000;;			msg := fmt.Sprintf("Pod %q/%q matches multiple PodDisruptionBudgets.  Chose %q arbitrarily.", pod.Namespace, pod.Name, pdbs[0].Name)
0000000000000000000000000000000000000000;;			glog.Warning(msg)
0000000000000000000000000000000000000000;;			dc.recorder.Event(pod, v1.EventTypeWarning, "MultiplePodDisruptionBudgets", msg)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pdbs[0]
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// This function returns pods using the PodDisruptionBudget object.
0000000000000000000000000000000000000000;;	// IMPORTANT NOTE : the returned pods should NOT be modified.
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) getPodsForPdb(pdb *policy.PodDisruptionBudget) ([]*v1.Pod, error) {
0000000000000000000000000000000000000000;;		sel, err := metav1.LabelSelectorAsSelector(pdb.Spec.Selector)
0000000000000000000000000000000000000000;;		if sel.Empty() {
0000000000000000000000000000000000000000;;			return []*v1.Pod{}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return []*v1.Pod{}, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pods, err := dc.podLister.Pods(pdb.Namespace).List(sel)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return []*v1.Pod{}, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pods, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) worker() {
0000000000000000000000000000000000000000;;		for dc.processNextWorkItem() {
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) processNextWorkItem() bool {
0000000000000000000000000000000000000000;;		dKey, quit := dc.queue.Get()
0000000000000000000000000000000000000000;;		if quit {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer dc.queue.Done(dKey)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := dc.sync(dKey.(string))
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			dc.queue.Forget(dKey)
0000000000000000000000000000000000000000;;			return true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		utilruntime.HandleError(fmt.Errorf("Error syncing PodDisruptionBudget %v, requeuing: %v", dKey.(string), err))
0000000000000000000000000000000000000000;;		dc.queue.AddRateLimited(dKey)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) recheckWorker() {
0000000000000000000000000000000000000000;;		for dc.processNextRecheckWorkItem() {
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) processNextRecheckWorkItem() bool {
0000000000000000000000000000000000000000;;		dKey, quit := dc.recheckQueue.Get()
0000000000000000000000000000000000000000;;		if quit {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer dc.recheckQueue.Done(dKey)
0000000000000000000000000000000000000000;;		dc.queue.AddRateLimited(dKey)
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) sync(key string) error {
0000000000000000000000000000000000000000;;		startTime := time.Now()
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Finished syncing PodDisruptionBudget %q (%v)", key, time.Now().Sub(startTime))
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		namespace, name, err := cache.SplitMetaNamespaceKey(key)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pdb, err := dc.pdbLister.PodDisruptionBudgets(namespace).Get(name)
0000000000000000000000000000000000000000;;		if errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("PodDisruptionBudget %q has been deleted", key)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err := dc.trySync(pdb); err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Failed to sync pdb %s/%s: %v", pdb.Namespace, pdb.Name, err)
0000000000000000000000000000000000000000;;			return dc.failSafe(pdb)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) trySync(pdb *policy.PodDisruptionBudget) error {
0000000000000000000000000000000000000000;;		pods, err := dc.getPodsForPdb(pdb)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			dc.recorder.Eventf(pdb, v1.EventTypeWarning, "NoPods", "Failed to get pods: %v", err)
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(pods) == 0 {
0000000000000000000000000000000000000000;;			dc.recorder.Eventf(pdb, v1.EventTypeNormal, "NoPods", "No matching pods found")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		expectedCount, desiredHealthy, err := dc.getExpectedPodCount(pdb, pods)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			dc.recorder.Eventf(pdb, v1.EventTypeWarning, "CalculateExpectedPodCountFailed", "Failed to calculate the number of expected pods: %v", err)
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		currentTime := time.Now()
0000000000000000000000000000000000000000;;		disruptedPods, recheckTime := dc.buildDisruptedPodMap(pods, pdb, currentTime)
0000000000000000000000000000000000000000;;		currentHealthy := countHealthyPods(pods, disruptedPods, currentTime)
0000000000000000000000000000000000000000;;		err = dc.updatePdbStatus(pdb, currentHealthy, desiredHealthy, expectedCount, disruptedPods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err == nil && recheckTime != nil {
0000000000000000000000000000000000000000;;			// There is always at most one PDB waiting with a particular name in the queue,
0000000000000000000000000000000000000000;;			// and each PDB in the queue is associated with the lowest timestamp
0000000000000000000000000000000000000000;;			// that was supplied when a PDB with that name was added.
0000000000000000000000000000000000000000;;			dc.enqueuePdbForRecheck(pdb, recheckTime.Sub(currentTime))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) getExpectedPodCount(pdb *policy.PodDisruptionBudget, pods []*v1.Pod) (expectedCount, desiredHealthy int32, err error) {
0000000000000000000000000000000000000000;;		err = nil
0000000000000000000000000000000000000000;;		// TODO(davidopp): consider making the way expectedCount and rules about
0000000000000000000000000000000000000000;;		// permitted controller configurations (specifically, considering it an error
0000000000000000000000000000000000000000;;		// if a pod covered by a PDB has 0 controllers or > 1 controller) should be
0000000000000000000000000000000000000000;;		// handled the same way for integer and percentage minAvailable
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if pdb.Spec.MaxUnavailable != nil {
0000000000000000000000000000000000000000;;			expectedCount, err = dc.getExpectedScale(pdb, pods)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			var maxUnavailable int
0000000000000000000000000000000000000000;;			maxUnavailable, err = intstr.GetValueFromIntOrPercent(pdb.Spec.MaxUnavailable, int(expectedCount), true)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			desiredHealthy = expectedCount - int32(maxUnavailable)
0000000000000000000000000000000000000000;;			if desiredHealthy < 0 {
0000000000000000000000000000000000000000;;				desiredHealthy = 0
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else if pdb.Spec.MinAvailable != nil {
0000000000000000000000000000000000000000;;			if pdb.Spec.MinAvailable.Type == intstr.Int {
0000000000000000000000000000000000000000;;				desiredHealthy = pdb.Spec.MinAvailable.IntVal
0000000000000000000000000000000000000000;;				expectedCount = int32(len(pods))
0000000000000000000000000000000000000000;;			} else if pdb.Spec.MinAvailable.Type == intstr.String {
0000000000000000000000000000000000000000;;				expectedCount, err = dc.getExpectedScale(pdb, pods)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				var minAvailable int
0000000000000000000000000000000000000000;;				minAvailable, err = intstr.GetValueFromIntOrPercent(pdb.Spec.MinAvailable, int(expectedCount), true)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				desiredHealthy = int32(minAvailable)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) getExpectedScale(pdb *policy.PodDisruptionBudget, pods []*v1.Pod) (expectedCount int32, err error) {
0000000000000000000000000000000000000000;;		// When the user specifies a fraction of pods that must be available, we
0000000000000000000000000000000000000000;;		// use as the fraction's denominator
0000000000000000000000000000000000000000;;		// SUM_{all c in C} scale(c)
0000000000000000000000000000000000000000;;		// where C is the union of C_p1, C_p2, ..., C_pN
0000000000000000000000000000000000000000;;		// and each C_pi is the set of controllers controlling the pod pi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// k8s only defines what will happens when 0 or 1 controllers control a
0000000000000000000000000000000000000000;;		// given pod.  We explicitly exclude the 0 controllers case here, and we
0000000000000000000000000000000000000000;;		// report an error if we find a pod with more than 1 controller.  Thus in
0000000000000000000000000000000000000000;;		// practice each C_pi is a set of exactly 1 controller.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// A mapping from controllers to their scale.
0000000000000000000000000000000000000000;;		controllerScale := map[types.UID]int32{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 1. Find the controller(s) for each pod.  If any pod has 0 controllers,
0000000000000000000000000000000000000000;;		// that's an error.  If any pod has more than 1 controller, that's also an
0000000000000000000000000000000000000000;;		// error.
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			controllerCount := 0
0000000000000000000000000000000000000000;;			for _, finder := range dc.finders() {
0000000000000000000000000000000000000000;;				var controllers []controllerAndScale
0000000000000000000000000000000000000000;;				controllers, err = finder(pod)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for _, controller := range controllers {
0000000000000000000000000000000000000000;;					controllerScale[controller.UID] = controller.scale
0000000000000000000000000000000000000000;;					controllerCount++
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if controllerCount == 0 {
0000000000000000000000000000000000000000;;				err = fmt.Errorf("found no controllers for pod %q", pod.Name)
0000000000000000000000000000000000000000;;				dc.recorder.Event(pdb, v1.EventTypeWarning, "NoControllers", err.Error())
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			} else if controllerCount > 1 {
0000000000000000000000000000000000000000;;				err = fmt.Errorf("pod %q has %v>1 controllers", pod.Name, controllerCount)
0000000000000000000000000000000000000000;;				dc.recorder.Event(pdb, v1.EventTypeWarning, "TooManyControllers", err.Error())
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 2. Add up all the controllers.
0000000000000000000000000000000000000000;;		expectedCount = 0
0000000000000000000000000000000000000000;;		for _, count := range controllerScale {
0000000000000000000000000000000000000000;;			expectedCount += count
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func countHealthyPods(pods []*v1.Pod, disruptedPods map[string]metav1.Time, currentTime time.Time) (currentHealthy int32) {
0000000000000000000000000000000000000000;;	Pod:
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			// Pod is beeing deleted.
0000000000000000000000000000000000000000;;			if pod.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Pod is expected to be deleted soon.
0000000000000000000000000000000000000000;;			if disruptionTime, found := disruptedPods[pod.Name]; found && disruptionTime.Time.Add(DeletionTimeout).After(currentTime) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if podutil.IsPodReady(pod) {
0000000000000000000000000000000000000000;;				currentHealthy++
0000000000000000000000000000000000000000;;				continue Pod
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Builds new PodDisruption map, possibly removing items that refer to non-existing, already deleted
0000000000000000000000000000000000000000;;	// or not-deleted at all items. Also returns an information when this check should be repeated.
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) buildDisruptedPodMap(pods []*v1.Pod, pdb *policy.PodDisruptionBudget, currentTime time.Time) (map[string]metav1.Time, *time.Time) {
0000000000000000000000000000000000000000;;		disruptedPods := pdb.Status.DisruptedPods
0000000000000000000000000000000000000000;;		result := make(map[string]metav1.Time)
0000000000000000000000000000000000000000;;		var recheckTime *time.Time
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if disruptedPods == nil || len(disruptedPods) == 0 {
0000000000000000000000000000000000000000;;			return result, recheckTime
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			if pod.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;				// Already being deleted.
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			disruptionTime, found := disruptedPods[pod.Name]
0000000000000000000000000000000000000000;;			if !found {
0000000000000000000000000000000000000000;;				// Pod not on the list.
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			expectedDeletion := disruptionTime.Time.Add(DeletionTimeout)
0000000000000000000000000000000000000000;;			if expectedDeletion.Before(currentTime) {
0000000000000000000000000000000000000000;;				glog.V(1).Infof("Pod %s/%s was expected to be deleted at %s but it wasn't, updating pdb %s/%s",
0000000000000000000000000000000000000000;;					pod.Namespace, pod.Name, disruptionTime.String(), pdb.Namespace, pdb.Name)
0000000000000000000000000000000000000000;;				dc.recorder.Eventf(pod, v1.EventTypeWarning, "NotDeleted", "Pod was expected by PDB %s/%s to be deleted but it wasn't",
0000000000000000000000000000000000000000;;					pdb.Namespace, pdb.Namespace)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				if recheckTime == nil || expectedDeletion.Before(*recheckTime) {
0000000000000000000000000000000000000000;;					recheckTime = &expectedDeletion
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				result[pod.Name] = disruptionTime
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result, recheckTime
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// failSafe is an attempt to at least update the PodDisruptionsAllowed field to
0000000000000000000000000000000000000000;;	// 0 if everything else has failed.  This is one place we
0000000000000000000000000000000000000000;;	// implement the  "fail open" part of the design since if we manage to update
0000000000000000000000000000000000000000;;	// this field correctly, we will prevent the /evict handler from approving an
0000000000000000000000000000000000000000;;	// eviction when it may be unsafe to do so.
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) failSafe(pdb *policy.PodDisruptionBudget) error {
0000000000000000000000000000000000000000;;		obj, err := api.Scheme.DeepCopy(pdb)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		newPdb := obj.(*policy.PodDisruptionBudget)
0000000000000000000000000000000000000000;;		newPdb.Status.PodDisruptionsAllowed = 0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return dc.getUpdater()(newPdb)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) updatePdbStatus(pdb *policy.PodDisruptionBudget, currentHealthy, desiredHealthy, expectedCount int32,
0000000000000000000000000000000000000000;;		disruptedPods map[string]metav1.Time) error {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// We require expectedCount to be > 0 so that PDBs which currently match no
0000000000000000000000000000000000000000;;		// pods are in a safe state when their first pods appear but this controller
0000000000000000000000000000000000000000;;		// has not updated their status yet.  This isn't the only race, but it's a
0000000000000000000000000000000000000000;;		// common one that's easy to detect.
0000000000000000000000000000000000000000;;		disruptionsAllowed := currentHealthy - desiredHealthy
0000000000000000000000000000000000000000;;		if expectedCount <= 0 || disruptionsAllowed <= 0 {
0000000000000000000000000000000000000000;;			disruptionsAllowed = 0
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if pdb.Status.CurrentHealthy == currentHealthy &&
0000000000000000000000000000000000000000;;			pdb.Status.DesiredHealthy == desiredHealthy &&
0000000000000000000000000000000000000000;;			pdb.Status.ExpectedPods == expectedCount &&
0000000000000000000000000000000000000000;;			pdb.Status.PodDisruptionsAllowed == disruptionsAllowed &&
0000000000000000000000000000000000000000;;			reflect.DeepEqual(pdb.Status.DisruptedPods, disruptedPods) &&
0000000000000000000000000000000000000000;;			pdb.Status.ObservedGeneration == pdb.Generation {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		obj, err := api.Scheme.DeepCopy(pdb)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		newPdb := obj.(*policy.PodDisruptionBudget)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newPdb.Status = policy.PodDisruptionBudgetStatus{
0000000000000000000000000000000000000000;;			CurrentHealthy:        currentHealthy,
0000000000000000000000000000000000000000;;			DesiredHealthy:        desiredHealthy,
0000000000000000000000000000000000000000;;			ExpectedPods:          expectedCount,
0000000000000000000000000000000000000000;;			PodDisruptionsAllowed: disruptionsAllowed,
0000000000000000000000000000000000000000;;			DisruptedPods:         disruptedPods,
0000000000000000000000000000000000000000;;			ObservedGeneration:    pdb.Generation,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return dc.getUpdater()(newPdb)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// refresh tries to re-GET the given PDB.  If there are any errors, it just
0000000000000000000000000000000000000000;;	// returns the old PDB.  Intended to be used in a retry loop where it runs a
0000000000000000000000000000000000000000;;	// bounded number of times.
0000000000000000000000000000000000000000;;	func refresh(pdbClient policyclientset.PodDisruptionBudgetInterface, pdb *policy.PodDisruptionBudget) *policy.PodDisruptionBudget {
0000000000000000000000000000000000000000;;		newPdb, err := pdbClient.Get(pdb.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			return newPdb
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			return pdb
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (dc *DisruptionController) writePdbStatus(pdb *policy.PodDisruptionBudget) error {
0000000000000000000000000000000000000000;;		pdbClient := dc.kubeClient.Policy().PodDisruptionBudgets(pdb.Namespace)
0000000000000000000000000000000000000000;;		st := pdb.Status
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		for i, pdb := 0, pdb; i < statusUpdateRetries; i, pdb = i+1, refresh(pdbClient, pdb) {
0000000000000000000000000000000000000000;;			pdb.Status = st
0000000000000000000000000000000000000000;;			if _, err = pdbClient.UpdateStatus(pdb); err == nil {
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
