0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2014 The Kubernetes Authors.
9fbdd758c00a160e902805405146a779d3acf5d8;pkg/registry/replication_controller_test.go[pkg/registry/replication_controller_test.go][pkg/controller/replication/replication_controller_test.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// If you make changes to this file, you should also make the corresponding change in ReplicaSet.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package replication
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"errors"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"math/rand"
0000000000000000000000000000000000000000;;		"net/http/httptest"
0000000000000000000000000000000000000000;;		"net/url"
0000000000000000000000000000000000000000;;		"reflect"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"testing"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		apiequality "k8s.io/apimachinery/pkg/api/equality"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/watch"
0000000000000000000000000000000000000000;;		restclient "k8s.io/client-go/rest"
0000000000000000000000000000000000000000;;		core "k8s.io/client-go/testing"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/cache"
0000000000000000000000000000000000000000;;		utiltesting "k8s.io/client-go/util/testing"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/workqueue"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api/testapi"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset/fake"
0000000000000000000000000000000000000000;;		fakeclientset "k8s.io/kubernetes/pkg/client/clientset_generated/clientset/fake"
0000000000000000000000000000000000000000;;		informers "k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions"
0000000000000000000000000000000000000000;;		coreinformers "k8s.io/kubernetes/pkg/client/informers/informers_generated/externalversions/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/controller"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/securitycontext"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var alwaysReady = func() bool { return true }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getKey(rc *v1.ReplicationController, t *testing.T) string {
0000000000000000000000000000000000000000;;		if key, err := controller.KeyFunc(rc); err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Unexpected error getting key for rc %v: %v", rc.Name, err)
0000000000000000000000000000000000000000;;			return ""
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			return key
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newReplicationController(replicas int) *v1.ReplicationController {
0000000000000000000000000000000000000000;;		rc := &v1.ReplicationController{
0000000000000000000000000000000000000000;;			TypeMeta: metav1.TypeMeta{APIVersion: api.Registry.GroupOrDie(v1.GroupName).GroupVersion.String()},
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				UID:             uuid.NewUUID(),
0000000000000000000000000000000000000000;;				Name:            "foobar",
0000000000000000000000000000000000000000;;				Namespace:       metav1.NamespaceDefault,
0000000000000000000000000000000000000000;;				ResourceVersion: "18",
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.ReplicationControllerSpec{
0000000000000000000000000000000000000000;;				Replicas: func() *int32 { i := int32(replicas); return &i }(),
0000000000000000000000000000000000000000;;				Selector: map[string]string{"foo": "bar"},
0000000000000000000000000000000000000000;;				Template: &v1.PodTemplateSpec{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;						Labels: map[string]string{
0000000000000000000000000000000000000000;;							"name": "foo",
0000000000000000000000000000000000000000;;							"type": "production",
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;						Containers: []v1.Container{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Image: "foo/bar",
0000000000000000000000000000000000000000;;								TerminationMessagePath: v1.TerminationMessagePathDefault,
0000000000000000000000000000000000000000;;								ImagePullPolicy:        v1.PullIfNotPresent,
0000000000000000000000000000000000000000;;								SecurityContext:        securitycontext.ValidSecurityContextWithContainerDefaults(),
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						RestartPolicy: v1.RestartPolicyAlways,
0000000000000000000000000000000000000000;;						DNSPolicy:     v1.DNSDefault,
0000000000000000000000000000000000000000;;						NodeSelector: map[string]string{
0000000000000000000000000000000000000000;;							"baz": "blah",
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return rc
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// create a pod with the given phase for the given rc (same selectors and namespace).
0000000000000000000000000000000000000000;;	func newPod(name string, rc *v1.ReplicationController, status v1.PodPhase, lastTransitionTime *metav1.Time, properlyOwned bool) *v1.Pod {
0000000000000000000000000000000000000000;;		var conditions []v1.PodCondition
0000000000000000000000000000000000000000;;		if status == v1.PodRunning {
0000000000000000000000000000000000000000;;			condition := v1.PodCondition{Type: v1.PodReady, Status: v1.ConditionTrue}
0000000000000000000000000000000000000000;;			if lastTransitionTime != nil {
0000000000000000000000000000000000000000;;				condition.LastTransitionTime = *lastTransitionTime
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			conditions = append(conditions, condition)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var controllerReference metav1.OwnerReference
0000000000000000000000000000000000000000;;		if properlyOwned {
0000000000000000000000000000000000000000;;			var trueVar = true
0000000000000000000000000000000000000000;;			controllerReference = metav1.OwnerReference{UID: rc.UID, APIVersion: "v1beta1", Kind: "ReplicaSet", Name: rc.Name, Controller: &trueVar}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:            name,
0000000000000000000000000000000000000000;;				Labels:          rc.Spec.Selector,
0000000000000000000000000000000000000000;;				Namespace:       rc.Namespace,
0000000000000000000000000000000000000000;;				OwnerReferences: []metav1.OwnerReference{controllerReference},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Status: v1.PodStatus{Phase: status, Conditions: conditions},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// create count pods with the given phase for the given rc (same selectors and namespace), and add them to the store.
0000000000000000000000000000000000000000;;	func newPodList(store cache.Store, count int, status v1.PodPhase, rc *v1.ReplicationController, name string) *v1.PodList {
0000000000000000000000000000000000000000;;		pods := []v1.Pod{}
0000000000000000000000000000000000000000;;		var trueVar = true
0000000000000000000000000000000000000000;;		controllerReference := metav1.OwnerReference{UID: rc.UID, APIVersion: "v1", Kind: "ReplicationController", Name: rc.Name, Controller: &trueVar}
0000000000000000000000000000000000000000;;		for i := 0; i < count; i++ {
0000000000000000000000000000000000000000;;			pod := newPod(fmt.Sprintf("%s%d", name, i), rc, status, nil, false)
0000000000000000000000000000000000000000;;			pod.OwnerReferences = []metav1.OwnerReference{controllerReference}
0000000000000000000000000000000000000000;;			if store != nil {
0000000000000000000000000000000000000000;;				store.Add(pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pods = append(pods, *pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return &v1.PodList{
0000000000000000000000000000000000000000;;			Items: pods,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// processSync initiates a sync via processNextWorkItem() to test behavior that
0000000000000000000000000000000000000000;;	// depends on both functions (such as re-queueing on sync error).
0000000000000000000000000000000000000000;;	func processSync(rm *ReplicationManager, key string) error {
0000000000000000000000000000000000000000;;		// Save old syncHandler and replace with one that captures the error.
0000000000000000000000000000000000000000;;		oldSyncHandler := rm.syncHandler
0000000000000000000000000000000000000000;;		defer func() {
0000000000000000000000000000000000000000;;			rm.syncHandler = oldSyncHandler
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;		var syncErr error
0000000000000000000000000000000000000000;;		rm.syncHandler = func(key string) error {
0000000000000000000000000000000000000000;;			syncErr = oldSyncHandler(key)
0000000000000000000000000000000000000000;;			return syncErr
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rm.queue.Add(key)
0000000000000000000000000000000000000000;;		rm.processNextWorkItem()
0000000000000000000000000000000000000000;;		return syncErr
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func validateSyncReplication(t *testing.T, fakePodControl *controller.FakePodControl, expectedCreates, expectedDeletes, expectedPatches int) {
0000000000000000000000000000000000000000;;		if e, a := expectedCreates, len(fakePodControl.Templates); e != a {
0000000000000000000000000000000000000000;;			t.Errorf("Unexpected number of creates.  Expected %d, saw %d\n", e, a)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if e, a := expectedDeletes, len(fakePodControl.DeletePodName); e != a {
0000000000000000000000000000000000000000;;			t.Errorf("Unexpected number of deletes.  Expected %d, saw %d\n", e, a)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if e, a := expectedPatches, len(fakePodControl.Patches); e != a {
0000000000000000000000000000000000000000;;			t.Errorf("Unexpected number of patches.  Expected %d, saw %d\n", e, a)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func replicationControllerResourceName() string {
0000000000000000000000000000000000000000;;		return "replicationcontrollers"
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type serverResponse struct {
0000000000000000000000000000000000000000;;		statusCode int
0000000000000000000000000000000000000000;;		obj        interface{}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newReplicationManagerFromClient(kubeClient clientset.Interface, burstReplicas int) (*ReplicationManager, coreinformers.PodInformer, coreinformers.ReplicationControllerInformer) {
0000000000000000000000000000000000000000;;		informerFactory := informers.NewSharedInformerFactory(kubeClient, controller.NoResyncPeriodFunc())
0000000000000000000000000000000000000000;;		podInformer := informerFactory.Core().V1().Pods()
0000000000000000000000000000000000000000;;		rcInformer := informerFactory.Core().V1().ReplicationControllers()
0000000000000000000000000000000000000000;;		rm := NewReplicationManager(podInformer, rcInformer, kubeClient, burstReplicas)
0000000000000000000000000000000000000000;;		rm.podListerSynced = alwaysReady
0000000000000000000000000000000000000000;;		rm.rcListerSynced = alwaysReady
0000000000000000000000000000000000000000;;		return rm, podInformer, rcInformer
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSyncReplicationControllerDoesNothing(t *testing.T) {
0000000000000000000000000000000000000000;;		c := clientset.NewForConfigOrDie(&restclient.Config{Host: "", ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 2 running pods, a controller with 2 replicas, sync is a no-op
0000000000000000000000000000000000000000;;		controllerSpec := newReplicationController(2)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(controllerSpec)
0000000000000000000000000000000000000000;;		newPodList(podInformer.Informer().GetIndexer(), 2, v1.PodRunning, controllerSpec, "pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 0, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSyncReplicationControllerDeletes(t *testing.T) {
0000000000000000000000000000000000000000;;		controllerSpec := newReplicationController(1)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		c := fake.NewSimpleClientset(controllerSpec)
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 2 running pods and a controller with 1 replica, one pod delete expected
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(controllerSpec)
0000000000000000000000000000000000000000;;		newPodList(podInformer.Informer().GetIndexer(), 2, v1.PodRunning, controllerSpec, "pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := manager.syncReplicationController(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("syncReplicationController() error: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 0, 1, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDeleteFinalStateUnknown(t *testing.T) {
0000000000000000000000000000000000000000;;		c := clientset.NewForConfigOrDie(&restclient.Config{Host: "", ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager, _, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		received := make(chan string)
0000000000000000000000000000000000000000;;		manager.syncHandler = func(key string) error {
0000000000000000000000000000000000000000;;			received <- key
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The DeletedFinalStateUnknown object should cause the rc manager to insert
0000000000000000000000000000000000000000;;		// the controller matching the selectors of the deleted pod into the work queue.
0000000000000000000000000000000000000000;;		controllerSpec := newReplicationController(1)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(controllerSpec)
0000000000000000000000000000000000000000;;		pods := newPodList(nil, 1, v1.PodRunning, controllerSpec, "pod")
0000000000000000000000000000000000000000;;		manager.deletePod(cache.DeletedFinalStateUnknown{Key: "foo", Obj: &pods.Items[0]})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		go manager.worker()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		expected := getKey(controllerSpec, t)
0000000000000000000000000000000000000000;;		select {
0000000000000000000000000000000000000000;;		case key := <-received:
0000000000000000000000000000000000000000;;			if key != expected {
0000000000000000000000000000000000000000;;				t.Errorf("Unexpected sync all for rc %v, expected %v", key, expected)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		case <-time.After(wait.ForeverTestTimeout):
0000000000000000000000000000000000000000;;			t.Errorf("Processing DeleteFinalStateUnknown took longer than expected")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSyncReplicationControllerCreates(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		c := fake.NewSimpleClientset(rc)
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// A controller with 2 replicas and no active pods in the store.
0000000000000000000000000000000000000000;;		// Inactive pods should be ignored. 2 creates expected.
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		failedPod := newPod("failed-pod", rc, v1.PodFailed, nil, true)
0000000000000000000000000000000000000000;;		deletedPod := newPod("deleted-pod", rc, v1.PodRunning, nil, true)
0000000000000000000000000000000000000000;;		deletedPod.DeletionTimestamp = &metav1.Time{Time: time.Now()}
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(failedPod)
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(deletedPod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 2, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestStatusUpdatesWithoutReplicasChange(t *testing.T) {
0000000000000000000000000000000000000000;;		// Setup a fake server to listen for requests, and run the rc manager in steady state
0000000000000000000000000000000000000000;;		fakeHandler := utiltesting.FakeHandler{
0000000000000000000000000000000000000000;;			StatusCode:   200,
0000000000000000000000000000000000000000;;			ResponseBody: "",
0000000000000000000000000000000000000000;;			SkipRequestFn: func(verb string, url url.URL) bool {
0000000000000000000000000000000000000000;;				if verb == "GET" {
0000000000000000000000000000000000000000;;					// Ignore refetch to check DeletionTimestamp.
0000000000000000000000000000000000000000;;					return true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		testServer := httptest.NewServer(&fakeHandler)
0000000000000000000000000000000000000000;;		defer testServer.Close()
0000000000000000000000000000000000000000;;		c := clientset.NewForConfigOrDie(&restclient.Config{Host: testServer.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Steady state for the replication controller, no Status.Replicas updates expected
0000000000000000000000000000000000000000;;		activePods := 5
0000000000000000000000000000000000000000;;		rc := newReplicationController(activePods)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		rc.Status = v1.ReplicationControllerStatus{Replicas: int32(activePods), ReadyReplicas: int32(activePods), AvailableReplicas: int32(activePods)}
0000000000000000000000000000000000000000;;		newPodList(podInformer.Informer().GetIndexer(), activePods, v1.PodRunning, rc, "pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 0, 0, 0)
0000000000000000000000000000000000000000;;		if fakeHandler.RequestReceived != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Unexpected update when pods and rcs are in a steady state")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This response body is just so we don't err out decoding the http response, all
0000000000000000000000000000000000000000;;		// we care about is the request body sent below.
0000000000000000000000000000000000000000;;		response := runtime.EncodeOrDie(testapi.Default.Codec(), &v1.ReplicationController{})
0000000000000000000000000000000000000000;;		fakeHandler.ResponseBody = response
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rc.Generation = rc.Generation + 1
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rc.Status.ObservedGeneration = rc.Generation
0000000000000000000000000000000000000000;;		updatedRc := runtime.EncodeOrDie(testapi.Default.Codec(), rc)
0000000000000000000000000000000000000000;;		fakeHandler.ValidateRequest(t, testapi.Default.ResourcePath(replicationControllerResourceName(), rc.Namespace, rc.Name)+"/status", "PUT", &updatedRc)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestControllerUpdateReplicas(t *testing.T) {
0000000000000000000000000000000000000000;;		// This is a happy server just to record the PUT request we expect for status.Replicas
0000000000000000000000000000000000000000;;		fakeHandler := utiltesting.FakeHandler{
0000000000000000000000000000000000000000;;			StatusCode:   200,
0000000000000000000000000000000000000000;;			ResponseBody: "",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		testServer := httptest.NewServer(&fakeHandler)
0000000000000000000000000000000000000000;;		defer testServer.Close()
0000000000000000000000000000000000000000;;		c := clientset.NewForConfigOrDie(&restclient.Config{Host: testServer.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Insufficient number of pods in the system, and Status.Replicas is wrong;
0000000000000000000000000000000000000000;;		// Status.Replica should update to match number of pods in system, 1 new pod should be created.
0000000000000000000000000000000000000000;;		rc := newReplicationController(5)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		rc.Status = v1.ReplicationControllerStatus{Replicas: 2, FullyLabeledReplicas: 6, ReadyReplicas: 2, AvailableReplicas: 2, ObservedGeneration: 0}
0000000000000000000000000000000000000000;;		rc.Generation = 1
0000000000000000000000000000000000000000;;		newPodList(podInformer.Informer().GetIndexer(), 2, v1.PodRunning, rc, "pod")
0000000000000000000000000000000000000000;;		rcCopy := *rc
0000000000000000000000000000000000000000;;		extraLabelMap := map[string]string{"foo": "bar", "extraKey": "extraValue"}
0000000000000000000000000000000000000000;;		rcCopy.Spec.Selector = extraLabelMap
0000000000000000000000000000000000000000;;		newPodList(podInformer.Informer().GetIndexer(), 2, v1.PodRunning, &rcCopy, "podWithExtraLabel")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This response body is just so we don't err out decoding the http response
0000000000000000000000000000000000000000;;		response := runtime.EncodeOrDie(testapi.Default.Codec(), &v1.ReplicationController{})
0000000000000000000000000000000000000000;;		fakeHandler.ResponseBody = response
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// 1. Status.Replicas should go up from 2->4 even though we created 5-4=1 pod.
0000000000000000000000000000000000000000;;		// 2. Status.FullyLabeledReplicas should equal to the number of pods that
0000000000000000000000000000000000000000;;		// has the extra labels, i.e., 2.
0000000000000000000000000000000000000000;;		// 3. Every update to the status should include the Generation of the spec.
0000000000000000000000000000000000000000;;		rc.Status = v1.ReplicationControllerStatus{Replicas: 4, ReadyReplicas: 4, AvailableReplicas: 4, ObservedGeneration: 1}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		decRc := runtime.EncodeOrDie(testapi.Default.Codec(), rc)
0000000000000000000000000000000000000000;;		fakeHandler.ValidateRequest(t, testapi.Default.ResourcePath(replicationControllerResourceName(), rc.Namespace, rc.Name)+"/status", "PUT", &decRc)
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 1, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSyncReplicationControllerDormancy(t *testing.T) {
0000000000000000000000000000000000000000;;		controllerSpec := newReplicationController(2)
0000000000000000000000000000000000000000;;		c := fake.NewSimpleClientset(controllerSpec)
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(controllerSpec)
0000000000000000000000000000000000000000;;		newPodList(podInformer.Informer().GetIndexer(), 1, v1.PodRunning, controllerSpec, "pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Creates a replica and sets expectations
0000000000000000000000000000000000000000;;		controllerSpec.Status.Replicas = 1
0000000000000000000000000000000000000000;;		controllerSpec.Status.ReadyReplicas = 1
0000000000000000000000000000000000000000;;		controllerSpec.Status.AvailableReplicas = 1
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 1, 0, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Expectations prevents replicas but not an update on status
0000000000000000000000000000000000000000;;		controllerSpec.Status.Replicas = 0
0000000000000000000000000000000000000000;;		controllerSpec.Status.ReadyReplicas = 0
0000000000000000000000000000000000000000;;		controllerSpec.Status.AvailableReplicas = 0
0000000000000000000000000000000000000000;;		fakePodControl.Clear()
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 0, 0, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Get the key for the controller
0000000000000000000000000000000000000000;;		rcKey, err := controller.KeyFunc(controllerSpec)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Couldn't get key for object %#v: %v", controllerSpec, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Lowering expectations should lead to a sync that creates a replica, however the
0000000000000000000000000000000000000000;;		// fakePodControl error will prevent this, leaving expectations at 0, 0.
0000000000000000000000000000000000000000;;		manager.expectations.CreationObserved(rcKey)
0000000000000000000000000000000000000000;;		controllerSpec.Status.Replicas = 1
0000000000000000000000000000000000000000;;		controllerSpec.Status.ReadyReplicas = 1
0000000000000000000000000000000000000000;;		controllerSpec.Status.AvailableReplicas = 1
0000000000000000000000000000000000000000;;		fakePodControl.Clear()
0000000000000000000000000000000000000000;;		fakePodControl.Err = fmt.Errorf("Fake Error")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 1, 0, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This replica should not need a Lowering of expectations, since the previous create failed
0000000000000000000000000000000000000000;;		fakePodControl.Clear()
0000000000000000000000000000000000000000;;		fakePodControl.Err = nil
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 1, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestPodControllerLookup(t *testing.T) {
0000000000000000000000000000000000000000;;		manager, _, rcInformer := newReplicationManagerFromClient(clientset.NewForConfigOrDie(&restclient.Config{Host: "", ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}}), BurstReplicas)
0000000000000000000000000000000000000000;;		testCases := []struct {
0000000000000000000000000000000000000000;;			inRCs     []*v1.ReplicationController
0000000000000000000000000000000000000000;;			pod       *v1.Pod
0000000000000000000000000000000000000000;;			outRCName string
0000000000000000000000000000000000000000;;		}{
0000000000000000000000000000000000000000;;			// pods without labels don't match any rcs
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				inRCs: []*v1.ReplicationController{
0000000000000000000000000000000000000000;;					{ObjectMeta: metav1.ObjectMeta{Name: "basic"}}},
0000000000000000000000000000000000000000;;				pod:       &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "foo1", Namespace: metav1.NamespaceAll}},
0000000000000000000000000000000000000000;;				outRCName: "",
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			// Matching labels, not namespace
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				inRCs: []*v1.ReplicationController{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						ObjectMeta: metav1.ObjectMeta{Name: "foo"},
0000000000000000000000000000000000000000;;						Spec: v1.ReplicationControllerSpec{
0000000000000000000000000000000000000000;;							Selector: map[string]string{"foo": "bar"},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				pod: &v1.Pod{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;						Name: "foo2", Namespace: "ns", Labels: map[string]string{"foo": "bar"}}},
0000000000000000000000000000000000000000;;				outRCName: "",
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			// Matching ns and labels returns the key to the rc, not the rc name
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				inRCs: []*v1.ReplicationController{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						ObjectMeta: metav1.ObjectMeta{Name: "bar", Namespace: "ns"},
0000000000000000000000000000000000000000;;						Spec: v1.ReplicationControllerSpec{
0000000000000000000000000000000000000000;;							Selector: map[string]string{"foo": "bar"},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				pod: &v1.Pod{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;						Name: "foo3", Namespace: "ns", Labels: map[string]string{"foo": "bar"}}},
0000000000000000000000000000000000000000;;				outRCName: "bar",
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, c := range testCases {
0000000000000000000000000000000000000000;;			for _, r := range c.inRCs {
0000000000000000000000000000000000000000;;				rcInformer.Informer().GetIndexer().Add(r)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if rcs := manager.getPodControllers(c.pod); rcs != nil {
0000000000000000000000000000000000000000;;				if len(rcs) != 1 {
0000000000000000000000000000000000000000;;					t.Errorf("len(rcs) = %v, want %v", len(rcs), 1)
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				rc := rcs[0]
0000000000000000000000000000000000000000;;				if c.outRCName != rc.Name {
0000000000000000000000000000000000000000;;					t.Errorf("Got controller %+v expected %+v", rc.Name, c.outRCName)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			} else if c.outRCName != "" {
0000000000000000000000000000000000000000;;				t.Errorf("Expected a controller %v pod %v, found none", c.outRCName, c.pod.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestWatchControllers(t *testing.T) {
0000000000000000000000000000000000000000;;		fakeWatch := watch.NewFake()
0000000000000000000000000000000000000000;;		c := &fake.Clientset{}
0000000000000000000000000000000000000000;;		c.AddWatchReactor("replicationcontrollers", core.DefaultWatchReactor(fakeWatch, nil))
0000000000000000000000000000000000000000;;		stopCh := make(chan struct{})
0000000000000000000000000000000000000000;;		defer close(stopCh)
0000000000000000000000000000000000000000;;		informers := informers.NewSharedInformerFactory(c, controller.NoResyncPeriodFunc())
0000000000000000000000000000000000000000;;		podInformer := informers.Core().V1().Pods()
0000000000000000000000000000000000000000;;		rcInformer := informers.Core().V1().ReplicationControllers()
0000000000000000000000000000000000000000;;		manager := NewReplicationManager(podInformer, rcInformer, c, BurstReplicas)
0000000000000000000000000000000000000000;;		informers.Start(stopCh)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var testControllerSpec v1.ReplicationController
0000000000000000000000000000000000000000;;		received := make(chan string)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The update sent through the fakeWatcher should make its way into the workqueue,
0000000000000000000000000000000000000000;;		// and eventually into the syncHandler. The handler validates the received controller
0000000000000000000000000000000000000000;;		// and closes the received channel to indicate that the test can finish.
0000000000000000000000000000000000000000;;		manager.syncHandler = func(key string) error {
0000000000000000000000000000000000000000;;			obj, exists, err := rcInformer.Informer().GetIndexer().GetByKey(key)
0000000000000000000000000000000000000000;;			if !exists || err != nil {
0000000000000000000000000000000000000000;;				t.Errorf("Expected to find controller under key %v", key)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			controllerSpec := *obj.(*v1.ReplicationController)
0000000000000000000000000000000000000000;;			if !apiequality.Semantic.DeepDerivative(controllerSpec, testControllerSpec) {
0000000000000000000000000000000000000000;;				t.Errorf("Expected %#v, but got %#v", testControllerSpec, controllerSpec)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			close(received)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start only the rc watcher and the workqueue, send a watch event,
0000000000000000000000000000000000000000;;		// and make sure it hits the sync method.
0000000000000000000000000000000000000000;;		go wait.Until(manager.worker, 10*time.Millisecond, stopCh)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		testControllerSpec.Name = "foo"
0000000000000000000000000000000000000000;;		fakeWatch.Add(&testControllerSpec)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		select {
0000000000000000000000000000000000000000;;		case <-received:
0000000000000000000000000000000000000000;;		case <-time.After(wait.ForeverTestTimeout):
0000000000000000000000000000000000000000;;			t.Errorf("unexpected timeout from result channel")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestWatchPods(t *testing.T) {
0000000000000000000000000000000000000000;;		fakeWatch := watch.NewFake()
0000000000000000000000000000000000000000;;		c := &fake.Clientset{}
0000000000000000000000000000000000000000;;		c.AddWatchReactor("*", core.DefaultWatchReactor(fakeWatch, nil))
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Put one rc and one pod into the controller's stores
0000000000000000000000000000000000000000;;		testControllerSpec := newReplicationController(1)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(testControllerSpec)
0000000000000000000000000000000000000000;;		received := make(chan string)
0000000000000000000000000000000000000000;;		// The pod update sent through the fakeWatcher should figure out the managing rc and
0000000000000000000000000000000000000000;;		// send it into the syncHandler.
0000000000000000000000000000000000000000;;		manager.syncHandler = func(key string) error {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			obj, exists, err := rcInformer.Informer().GetIndexer().GetByKey(key)
0000000000000000000000000000000000000000;;			if !exists || err != nil {
0000000000000000000000000000000000000000;;				t.Errorf("Expected to find controller under key %v", key)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			controllerSpec := obj.(*v1.ReplicationController)
0000000000000000000000000000000000000000;;			if !apiequality.Semantic.DeepDerivative(controllerSpec, testControllerSpec) {
0000000000000000000000000000000000000000;;				t.Errorf("\nExpected %#v,\nbut got %#v", testControllerSpec, controllerSpec)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			close(received)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Start only the pod watcher and the workqueue, send a watch event,
0000000000000000000000000000000000000000;;		// and make sure it hits the sync method for the right rc.
0000000000000000000000000000000000000000;;		stopCh := make(chan struct{})
0000000000000000000000000000000000000000;;		defer close(stopCh)
0000000000000000000000000000000000000000;;		go podInformer.Informer().Run(stopCh)
0000000000000000000000000000000000000000;;		go wait.Until(manager.worker, 10*time.Millisecond, stopCh)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pods := newPodList(nil, 1, v1.PodRunning, testControllerSpec, "pod")
0000000000000000000000000000000000000000;;		testPod := pods.Items[0]
0000000000000000000000000000000000000000;;		testPod.Status.Phase = v1.PodFailed
0000000000000000000000000000000000000000;;		fakeWatch.Add(&testPod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		select {
0000000000000000000000000000000000000000;;		case <-received:
0000000000000000000000000000000000000000;;		case <-time.After(wait.ForeverTestTimeout):
0000000000000000000000000000000000000000;;			t.Errorf("unexpected timeout from result channel")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestUpdatePods(t *testing.T) {
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(fake.NewSimpleClientset(), BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		received := make(chan string)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		manager.syncHandler = func(key string) error {
0000000000000000000000000000000000000000;;			obj, exists, err := rcInformer.Informer().GetIndexer().GetByKey(key)
0000000000000000000000000000000000000000;;			if !exists || err != nil {
0000000000000000000000000000000000000000;;				t.Errorf("Expected to find controller under key %v", key)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			received <- obj.(*v1.ReplicationController).Name
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		stopCh := make(chan struct{})
0000000000000000000000000000000000000000;;		defer close(stopCh)
0000000000000000000000000000000000000000;;		go wait.Until(manager.worker, 10*time.Millisecond, stopCh)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Put 2 rcs and one pod into the controller's stores
0000000000000000000000000000000000000000;;		labelMap1 := map[string]string{"foo": "bar"}
0000000000000000000000000000000000000000;;		testControllerSpec1 := newReplicationController(1)
0000000000000000000000000000000000000000;;		testControllerSpec1.Spec.Selector = labelMap1
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(testControllerSpec1)
0000000000000000000000000000000000000000;;		labelMap2 := map[string]string{"bar": "foo"}
0000000000000000000000000000000000000000;;		testControllerSpec2 := *testControllerSpec1
0000000000000000000000000000000000000000;;		testControllerSpec2.Spec.Selector = labelMap2
0000000000000000000000000000000000000000;;		testControllerSpec2.Name = "barfoo"
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(&testControllerSpec2)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		isController := true
0000000000000000000000000000000000000000;;		controllerRef1 := metav1.OwnerReference{UID: testControllerSpec1.UID, APIVersion: "v1", Kind: "ReplicationController", Name: testControllerSpec1.Name, Controller: &isController}
0000000000000000000000000000000000000000;;		controllerRef2 := metav1.OwnerReference{UID: testControllerSpec2.UID, APIVersion: "v1", Kind: "ReplicationController", Name: testControllerSpec2.Name, Controller: &isController}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// case 1: Pod with a ControllerRef
0000000000000000000000000000000000000000;;		pod1 := newPodList(podInformer.Informer().GetIndexer(), 1, v1.PodRunning, testControllerSpec1, "pod").Items[0]
0000000000000000000000000000000000000000;;		pod1.OwnerReferences = []metav1.OwnerReference{controllerRef1}
0000000000000000000000000000000000000000;;		pod1.ResourceVersion = "1"
0000000000000000000000000000000000000000;;		pod2 := pod1
0000000000000000000000000000000000000000;;		pod2.Labels = labelMap2
0000000000000000000000000000000000000000;;		pod2.ResourceVersion = "2"
0000000000000000000000000000000000000000;;		manager.updatePod(&pod1, &pod2)
0000000000000000000000000000000000000000;;		expected := sets.NewString(testControllerSpec1.Name)
0000000000000000000000000000000000000000;;		for _, name := range expected.List() {
0000000000000000000000000000000000000000;;			t.Logf("Expecting update for %+v", name)
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case got := <-received:
0000000000000000000000000000000000000000;;				if !expected.Has(got) {
0000000000000000000000000000000000000000;;					t.Errorf("Expected keys %#v got %v", expected, got)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case <-time.After(wait.ForeverTestTimeout):
0000000000000000000000000000000000000000;;				t.Errorf("Expected update notifications for ReplicationControllers")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// case 2: Remove ControllerRef (orphan). Expect to sync label-matching RC.
0000000000000000000000000000000000000000;;		pod1 = newPodList(podInformer.Informer().GetIndexer(), 1, v1.PodRunning, testControllerSpec1, "pod").Items[0]
0000000000000000000000000000000000000000;;		pod1.ResourceVersion = "1"
0000000000000000000000000000000000000000;;		pod1.Labels = labelMap2
0000000000000000000000000000000000000000;;		pod1.OwnerReferences = []metav1.OwnerReference{controllerRef2}
0000000000000000000000000000000000000000;;		pod2 = pod1
0000000000000000000000000000000000000000;;		pod2.OwnerReferences = nil
0000000000000000000000000000000000000000;;		pod2.ResourceVersion = "2"
0000000000000000000000000000000000000000;;		manager.updatePod(&pod1, &pod2)
0000000000000000000000000000000000000000;;		expected = sets.NewString(testControllerSpec2.Name)
0000000000000000000000000000000000000000;;		for _, name := range expected.List() {
0000000000000000000000000000000000000000;;			t.Logf("Expecting update for %+v", name)
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case got := <-received:
0000000000000000000000000000000000000000;;				if !expected.Has(got) {
0000000000000000000000000000000000000000;;					t.Errorf("Expected keys %#v got %v", expected, got)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case <-time.After(wait.ForeverTestTimeout):
0000000000000000000000000000000000000000;;				t.Errorf("Expected update notifications for ReplicationControllers")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// case 2: Remove ControllerRef (orphan). Expect to sync both former owner and
0000000000000000000000000000000000000000;;		// any label-matching RC.
0000000000000000000000000000000000000000;;		pod1 = newPodList(podInformer.Informer().GetIndexer(), 1, v1.PodRunning, testControllerSpec1, "pod").Items[0]
0000000000000000000000000000000000000000;;		pod1.ResourceVersion = "1"
0000000000000000000000000000000000000000;;		pod1.Labels = labelMap2
0000000000000000000000000000000000000000;;		pod1.OwnerReferences = []metav1.OwnerReference{controllerRef1}
0000000000000000000000000000000000000000;;		pod2 = pod1
0000000000000000000000000000000000000000;;		pod2.OwnerReferences = nil
0000000000000000000000000000000000000000;;		pod2.ResourceVersion = "2"
0000000000000000000000000000000000000000;;		manager.updatePod(&pod1, &pod2)
0000000000000000000000000000000000000000;;		expected = sets.NewString(testControllerSpec1.Name, testControllerSpec2.Name)
0000000000000000000000000000000000000000;;		for _, name := range expected.List() {
0000000000000000000000000000000000000000;;			t.Logf("Expecting update for %+v", name)
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case got := <-received:
0000000000000000000000000000000000000000;;				if !expected.Has(got) {
0000000000000000000000000000000000000000;;					t.Errorf("Expected keys %#v got %v", expected, got)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case <-time.After(wait.ForeverTestTimeout):
0000000000000000000000000000000000000000;;				t.Errorf("Expected update notifications for ReplicationControllers")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// case 4: Keep ControllerRef, change labels. Expect to sync owning RC.
0000000000000000000000000000000000000000;;		pod1 = newPodList(podInformer.Informer().GetIndexer(), 1, v1.PodRunning, testControllerSpec1, "pod").Items[0]
0000000000000000000000000000000000000000;;		pod1.ResourceVersion = "1"
0000000000000000000000000000000000000000;;		pod1.Labels = labelMap1
0000000000000000000000000000000000000000;;		pod1.OwnerReferences = []metav1.OwnerReference{controllerRef2}
0000000000000000000000000000000000000000;;		pod2 = pod1
0000000000000000000000000000000000000000;;		pod2.Labels = labelMap2
0000000000000000000000000000000000000000;;		pod2.ResourceVersion = "2"
0000000000000000000000000000000000000000;;		manager.updatePod(&pod1, &pod2)
0000000000000000000000000000000000000000;;		expected = sets.NewString(testControllerSpec2.Name)
0000000000000000000000000000000000000000;;		for _, name := range expected.List() {
0000000000000000000000000000000000000000;;			t.Logf("Expecting update for %+v", name)
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case got := <-received:
0000000000000000000000000000000000000000;;				if !expected.Has(got) {
0000000000000000000000000000000000000000;;					t.Errorf("Expected keys %#v got %v", expected, got)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case <-time.After(wait.ForeverTestTimeout):
0000000000000000000000000000000000000000;;				t.Errorf("Expected update notifications for ReplicationControllers")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestControllerUpdateRequeue(t *testing.T) {
0000000000000000000000000000000000000000;;		// This server should force a requeue of the controller because it fails to update status.Replicas.
0000000000000000000000000000000000000000;;		rc := newReplicationController(1)
0000000000000000000000000000000000000000;;		c := fake.NewSimpleClientset(rc)
0000000000000000000000000000000000000000;;		c.PrependReactor("update", "replicationcontrollers",
0000000000000000000000000000000000000000;;			func(action core.Action) (bool, runtime.Object, error) {
0000000000000000000000000000000000000000;;				if action.GetSubresource() != "status" {
0000000000000000000000000000000000000000;;					return false, nil, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return true, nil, errors.New("failed to update status")
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		rc.Status = v1.ReplicationControllerStatus{Replicas: 2}
0000000000000000000000000000000000000000;;		newPodList(podInformer.Informer().GetIndexer(), 1, v1.PodRunning, rc, "pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Enqueue once. Then process it. Disable rate-limiting for this.
0000000000000000000000000000000000000000;;		manager.queue = workqueue.NewRateLimitingQueue(workqueue.NewMaxOfRateLimiter())
0000000000000000000000000000000000000000;;		manager.enqueueController(rc)
0000000000000000000000000000000000000000;;		manager.processNextWorkItem()
0000000000000000000000000000000000000000;;		// It should have been requeued.
0000000000000000000000000000000000000000;;		if got, want := manager.queue.Len(), 1; got != want {
0000000000000000000000000000000000000000;;			t.Errorf("queue.Len() = %v, want %v", got, want)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestControllerUpdateStatusWithFailure(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(1)
0000000000000000000000000000000000000000;;		c := &fake.Clientset{}
0000000000000000000000000000000000000000;;		c.AddReactor("get", "replicationcontrollers", func(action core.Action) (bool, runtime.Object, error) {
0000000000000000000000000000000000000000;;			return true, rc, nil
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		c.AddReactor("*", "*", func(action core.Action) (bool, runtime.Object, error) {
0000000000000000000000000000000000000000;;			return true, &v1.ReplicationController{}, fmt.Errorf("Fake error")
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		fakeRCClient := c.Core().ReplicationControllers("default")
0000000000000000000000000000000000000000;;		numReplicas := int32(10)
0000000000000000000000000000000000000000;;		status := v1.ReplicationControllerStatus{Replicas: numReplicas}
0000000000000000000000000000000000000000;;		updateReplicationControllerStatus(fakeRCClient, *rc, status)
0000000000000000000000000000000000000000;;		updates, gets := 0, 0
0000000000000000000000000000000000000000;;		for _, a := range c.Actions() {
0000000000000000000000000000000000000000;;			if a.GetResource().Resource != "replicationcontrollers" {
0000000000000000000000000000000000000000;;				t.Errorf("Unexpected action %+v", a)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			switch action := a.(type) {
0000000000000000000000000000000000000000;;			case core.GetAction:
0000000000000000000000000000000000000000;;				gets++
0000000000000000000000000000000000000000;;				// Make sure the get is for the right rc even though the update failed.
0000000000000000000000000000000000000000;;				if action.GetName() != rc.Name {
0000000000000000000000000000000000000000;;					t.Errorf("Expected get for rc %v, got %+v instead", rc.Name, action.GetName())
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case core.UpdateAction:
0000000000000000000000000000000000000000;;				updates++
0000000000000000000000000000000000000000;;				// Confirm that the update has the right status.Replicas even though the Get
0000000000000000000000000000000000000000;;				// returned an rc with replicas=1.
0000000000000000000000000000000000000000;;				if c, ok := action.GetObject().(*v1.ReplicationController); !ok {
0000000000000000000000000000000000000000;;					t.Errorf("Expected an rc as the argument to update, got %T", c)
0000000000000000000000000000000000000000;;				} else if c.Status.Replicas != numReplicas {
0000000000000000000000000000000000000000;;					t.Errorf("Expected update for rc to contain replicas %v, got %v instead",
0000000000000000000000000000000000000000;;						numReplicas, c.Status.Replicas)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				t.Errorf("Unexpected action %+v", a)
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if gets != 1 || updates != 2 {
0000000000000000000000000000000000000000;;			t.Errorf("Expected 1 get and 2 updates, got %d gets %d updates", gets, updates)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO: This test is too hairy for a unittest. It should be moved to an E2E suite.
0000000000000000000000000000000000000000;;	func doTestControllerBurstReplicas(t *testing.T, burstReplicas, numReplicas int) {
0000000000000000000000000000000000000000;;		controllerSpec := newReplicationController(numReplicas)
0000000000000000000000000000000000000000;;		c := fake.NewSimpleClientset(controllerSpec)
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, burstReplicas)
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(controllerSpec)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		expectedPods := 0
0000000000000000000000000000000000000000;;		pods := newPodList(nil, numReplicas, v1.PodPending, controllerSpec, "pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rcKey, err := controller.KeyFunc(controllerSpec)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Couldn't get key for object %#v: %v", controllerSpec, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Size up the controller, then size it down, and confirm the expected create/delete pattern
0000000000000000000000000000000000000000;;		for _, replicas := range []int{numReplicas, 0} {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			*(controllerSpec.Spec.Replicas) = int32(replicas)
0000000000000000000000000000000000000000;;			rcInformer.Informer().GetIndexer().Add(controllerSpec)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for i := 0; i < numReplicas; i += burstReplicas {
0000000000000000000000000000000000000000;;				manager.syncReplicationController(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// The store accrues active pods. It's also used by the rc to determine how many
0000000000000000000000000000000000000000;;				// replicas to create.
0000000000000000000000000000000000000000;;				activePods := len(podInformer.Informer().GetIndexer().List())
0000000000000000000000000000000000000000;;				if replicas != 0 {
0000000000000000000000000000000000000000;;					// This is the number of pods currently "in flight". They were created by the rc manager above,
0000000000000000000000000000000000000000;;					// which then puts the rc to sleep till all of them have been observed.
0000000000000000000000000000000000000000;;					expectedPods = replicas - activePods
0000000000000000000000000000000000000000;;					if expectedPods > burstReplicas {
0000000000000000000000000000000000000000;;						expectedPods = burstReplicas
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					// This validates the rc manager sync actually created pods
0000000000000000000000000000000000000000;;					validateSyncReplication(t, &fakePodControl, expectedPods, 0, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// This simulates the watch events for all but 1 of the expected pods.
0000000000000000000000000000000000000000;;					// None of these should wake the controller because it has expectations==BurstReplicas.
0000000000000000000000000000000000000000;;					for i := 0; i < expectedPods-1; i++ {
0000000000000000000000000000000000000000;;						podInformer.Informer().GetIndexer().Add(&pods.Items[i])
0000000000000000000000000000000000000000;;						manager.addPod(&pods.Items[i])
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					podExp, exists, err := manager.expectations.GetExpectations(rcKey)
0000000000000000000000000000000000000000;;					if !exists || err != nil {
0000000000000000000000000000000000000000;;						t.Fatalf("Did not find expectations for rc.")
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if add, _ := podExp.GetExpectations(); add != 1 {
0000000000000000000000000000000000000000;;						t.Fatalf("Expectations are wrong %v", podExp)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					expectedPods = (replicas - activePods) * -1
0000000000000000000000000000000000000000;;					if expectedPods > burstReplicas {
0000000000000000000000000000000000000000;;						expectedPods = burstReplicas
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					validateSyncReplication(t, &fakePodControl, 0, expectedPods, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// To accurately simulate a watch we must delete the exact pods
0000000000000000000000000000000000000000;;					// the rc is waiting for.
0000000000000000000000000000000000000000;;					expectedDels := manager.expectations.GetUIDs(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;					podsToDelete := []*v1.Pod{}
0000000000000000000000000000000000000000;;					isController := true
0000000000000000000000000000000000000000;;					for _, key := range expectedDels.List() {
0000000000000000000000000000000000000000;;						nsName := strings.Split(key, "/")
0000000000000000000000000000000000000000;;						podsToDelete = append(podsToDelete, &v1.Pod{
0000000000000000000000000000000000000000;;							ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;								Name:      nsName[1],
0000000000000000000000000000000000000000;;								Namespace: nsName[0],
0000000000000000000000000000000000000000;;								Labels:    controllerSpec.Spec.Selector,
0000000000000000000000000000000000000000;;								OwnerReferences: []metav1.OwnerReference{
0000000000000000000000000000000000000000;;									{UID: controllerSpec.UID, APIVersion: "v1", Kind: "ReplicationController", Name: controllerSpec.Name, Controller: &isController},
0000000000000000000000000000000000000000;;								},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						})
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					// Don't delete all pods because we confirm that the last pod
0000000000000000000000000000000000000000;;					// has exactly one expectation at the end, to verify that we
0000000000000000000000000000000000000000;;					// don't double delete.
0000000000000000000000000000000000000000;;					for i := range podsToDelete[1:] {
0000000000000000000000000000000000000000;;						podInformer.Informer().GetIndexer().Delete(podsToDelete[i])
0000000000000000000000000000000000000000;;						manager.deletePod(podsToDelete[i])
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					podExp, exists, err := manager.expectations.GetExpectations(rcKey)
0000000000000000000000000000000000000000;;					if !exists || err != nil {
0000000000000000000000000000000000000000;;						t.Fatalf("Did not find expectations for rc.")
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if _, del := podExp.GetExpectations(); del != 1 {
0000000000000000000000000000000000000000;;						t.Fatalf("Expectations are wrong %v", podExp)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Check that the rc didn't take any action for all the above pods
0000000000000000000000000000000000000000;;				fakePodControl.Clear()
0000000000000000000000000000000000000000;;				manager.syncReplicationController(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;				validateSyncReplication(t, &fakePodControl, 0, 0, 0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Create/Delete the last pod
0000000000000000000000000000000000000000;;				// The last add pod will decrease the expectation of the rc to 0,
0000000000000000000000000000000000000000;;				// which will cause it to create/delete the remaining replicas up to burstReplicas.
0000000000000000000000000000000000000000;;				if replicas != 0 {
0000000000000000000000000000000000000000;;					podInformer.Informer().GetIndexer().Add(&pods.Items[expectedPods-1])
0000000000000000000000000000000000000000;;					manager.addPod(&pods.Items[expectedPods-1])
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					expectedDel := manager.expectations.GetUIDs(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;					if expectedDel.Len() != 1 {
0000000000000000000000000000000000000000;;						t.Fatalf("Waiting on unexpected number of deletes.")
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					nsName := strings.Split(expectedDel.List()[0], "/")
0000000000000000000000000000000000000000;;					isController := true
0000000000000000000000000000000000000000;;					lastPod := &v1.Pod{
0000000000000000000000000000000000000000;;						ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;							Name:      nsName[1],
0000000000000000000000000000000000000000;;							Namespace: nsName[0],
0000000000000000000000000000000000000000;;							Labels:    controllerSpec.Spec.Selector,
0000000000000000000000000000000000000000;;							OwnerReferences: []metav1.OwnerReference{
0000000000000000000000000000000000000000;;								{UID: controllerSpec.UID, APIVersion: "v1", Kind: "ReplicationController", Name: controllerSpec.Name, Controller: &isController},
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					podInformer.Informer().GetIndexer().Delete(lastPod)
0000000000000000000000000000000000000000;;					manager.deletePod(lastPod)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				pods.Items = pods.Items[expectedPods:]
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Confirm that we've created the right number of replicas
0000000000000000000000000000000000000000;;			activePods := int32(len(podInformer.Informer().GetIndexer().List()))
0000000000000000000000000000000000000000;;			if activePods != *(controllerSpec.Spec.Replicas) {
0000000000000000000000000000000000000000;;				t.Fatalf("Unexpected number of active pods, expected %d, got %d", *(controllerSpec.Spec.Replicas), activePods)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Replenish the pod list, since we cut it down sizing up
0000000000000000000000000000000000000000;;			pods = newPodList(nil, replicas, v1.PodRunning, controllerSpec, "pod")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestControllerBurstReplicas(t *testing.T) {
0000000000000000000000000000000000000000;;		doTestControllerBurstReplicas(t, 5, 30)
0000000000000000000000000000000000000000;;		doTestControllerBurstReplicas(t, 5, 12)
0000000000000000000000000000000000000000;;		doTestControllerBurstReplicas(t, 3, 2)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type FakeRCExpectations struct {
0000000000000000000000000000000000000000;;		*controller.ControllerExpectations
0000000000000000000000000000000000000000;;		satisfied    bool
0000000000000000000000000000000000000000;;		expSatisfied func()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (fe FakeRCExpectations) SatisfiedExpectations(controllerKey string) bool {
0000000000000000000000000000000000000000;;		fe.expSatisfied()
0000000000000000000000000000000000000000;;		return fe.satisfied
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestRCSyncExpectations tests that a pod cannot sneak in between counting active pods
0000000000000000000000000000000000000000;;	// and checking expectations.
0000000000000000000000000000000000000000;;	func TestRCSyncExpectations(t *testing.T) {
0000000000000000000000000000000000000000;;		c := clientset.NewForConfigOrDie(&restclient.Config{Host: "", ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, 2)
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		controllerSpec := newReplicationController(2)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(controllerSpec)
0000000000000000000000000000000000000000;;		pods := newPodList(nil, 2, v1.PodPending, controllerSpec, "pod")
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(&pods.Items[0])
0000000000000000000000000000000000000000;;		postExpectationsPod := pods.Items[1]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		manager.expectations = controller.NewUIDTrackingControllerExpectations(FakeRCExpectations{
0000000000000000000000000000000000000000;;			controller.NewControllerExpectations(), true, func() {
0000000000000000000000000000000000000000;;				// If we check active pods before checking expectataions, the rc
0000000000000000000000000000000000000000;;				// will create a new replica because it doesn't see this pod, but
0000000000000000000000000000000000000000;;				// has fulfilled its expectations.
0000000000000000000000000000000000000000;;				podInformer.Informer().GetIndexer().Add(&postExpectationsPod)
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(controllerSpec, t))
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 0, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDeleteControllerAndExpectations(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(1)
0000000000000000000000000000000000000000;;		c := fake.NewSimpleClientset(rc)
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, 10)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This should set expectations for the rc
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 1, 0, 0)
0000000000000000000000000000000000000000;;		fakePodControl.Clear()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Get the RC key
0000000000000000000000000000000000000000;;		rcKey, err := controller.KeyFunc(rc)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Couldn't get key for object %#v: %v", rc, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This is to simulate a concurrent addPod, that has a handle on the expectations
0000000000000000000000000000000000000000;;		// as the controller deletes it.
0000000000000000000000000000000000000000;;		podExp, exists, err := manager.expectations.GetExpectations(rcKey)
0000000000000000000000000000000000000000;;		if !exists || err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("No expectations found for rc")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Delete(rc)
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if _, exists, err = manager.expectations.GetExpectations(rcKey); exists {
0000000000000000000000000000000000000000;;			t.Errorf("Found expectaions, expected none since the rc has been deleted.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This should have no effect, since we've deleted the rc.
0000000000000000000000000000000000000000;;		podExp.Add(-1, 0)
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Replace(make([]interface{}, 0), "0")
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 0, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// shuffle returns a new shuffled list of container controllers.
0000000000000000000000000000000000000000;;	func shuffle(controllers []*v1.ReplicationController) []*v1.ReplicationController {
0000000000000000000000000000000000000000;;		numControllers := len(controllers)
0000000000000000000000000000000000000000;;		randIndexes := rand.Perm(numControllers)
0000000000000000000000000000000000000000;;		shuffled := make([]*v1.ReplicationController, numControllers)
0000000000000000000000000000000000000000;;		for i := 0; i < numControllers; i++ {
0000000000000000000000000000000000000000;;			shuffled[i] = controllers[randIndexes[i]]
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return shuffled
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestOverlappingRCs(t *testing.T) {
0000000000000000000000000000000000000000;;		c := clientset.NewForConfigOrDie(&restclient.Config{Host: "", ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		manager, _, rcInformer := newReplicationManagerFromClient(c, 10)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Create 10 rcs, shuffled them randomly and insert them into the
0000000000000000000000000000000000000000;;		// rc manager's store.
0000000000000000000000000000000000000000;;		// All use the same CreationTimestamp since ControllerRef should be able
0000000000000000000000000000000000000000;;		// to handle that.
0000000000000000000000000000000000000000;;		var controllers []*v1.ReplicationController
0000000000000000000000000000000000000000;;		timestamp := metav1.Date(2014, time.December, 0, 0, 0, 0, 0, time.Local)
0000000000000000000000000000000000000000;;		for j := 1; j < 10; j++ {
0000000000000000000000000000000000000000;;			controllerSpec := newReplicationController(1)
0000000000000000000000000000000000000000;;			controllerSpec.CreationTimestamp = timestamp
0000000000000000000000000000000000000000;;			controllerSpec.Name = fmt.Sprintf("rc%d", j)
0000000000000000000000000000000000000000;;			controllers = append(controllers, controllerSpec)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		shuffledControllers := shuffle(controllers)
0000000000000000000000000000000000000000;;		for j := range shuffledControllers {
0000000000000000000000000000000000000000;;			rcInformer.Informer().GetIndexer().Add(shuffledControllers[j])
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Add a pod with a ControllerRef and make sure only the corresponding
0000000000000000000000000000000000000000;;		// ReplicationController is synced. Pick a RC in the middle since the old code
0000000000000000000000000000000000000000;;		// used to sort by name if all timestamps were equal.
0000000000000000000000000000000000000000;;		rc := controllers[3]
0000000000000000000000000000000000000000;;		pods := newPodList(nil, 1, v1.PodPending, rc, "pod")
0000000000000000000000000000000000000000;;		pod := &pods.Items[0]
0000000000000000000000000000000000000000;;		isController := true
0000000000000000000000000000000000000000;;		pod.OwnerReferences = []metav1.OwnerReference{
0000000000000000000000000000000000000000;;			{UID: rc.UID, APIVersion: "v1", Kind: "ReplicationController", Name: rc.Name, Controller: &isController},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rcKey := getKey(rc, t)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		manager.addPod(pod)
0000000000000000000000000000000000000000;;		queueRC, _ := manager.queue.Get()
0000000000000000000000000000000000000000;;		if queueRC != rcKey {
0000000000000000000000000000000000000000;;			t.Fatalf("Expected to find key %v in queue, found %v", rcKey, queueRC)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDeletionTimestamp(t *testing.T) {
0000000000000000000000000000000000000000;;		c := clientset.NewForConfigOrDie(&restclient.Config{Host: "", ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		manager, _, rcInformer := newReplicationManagerFromClient(c, 10)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		controllerSpec := newReplicationController(1)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(controllerSpec)
0000000000000000000000000000000000000000;;		rcKey, err := controller.KeyFunc(controllerSpec)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Errorf("Couldn't get key for object %#v: %v", controllerSpec, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pod := newPodList(nil, 1, v1.PodPending, controllerSpec, "pod").Items[0]
0000000000000000000000000000000000000000;;		pod.DeletionTimestamp = &metav1.Time{Time: time.Now()}
0000000000000000000000000000000000000000;;		pod.ResourceVersion = "1"
0000000000000000000000000000000000000000;;		manager.expectations.ExpectDeletions(rcKey, []string{controller.PodKey(&pod)})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// A pod added with a deletion timestamp should decrement deletions, not creations.
0000000000000000000000000000000000000000;;		manager.addPod(&pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		queueRC, _ := manager.queue.Get()
0000000000000000000000000000000000000000;;		if queueRC != rcKey {
0000000000000000000000000000000000000000;;			t.Fatalf("Expected to find key %v in queue, found %v", rcKey, queueRC)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		manager.queue.Done(rcKey)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podExp, exists, err := manager.expectations.GetExpectations(rcKey)
0000000000000000000000000000000000000000;;		if !exists || err != nil || !podExp.Fulfilled() {
0000000000000000000000000000000000000000;;			t.Fatalf("Wrong expectations %#v", podExp)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// An update from no deletion timestamp to having one should be treated
0000000000000000000000000000000000000000;;		// as a deletion.
0000000000000000000000000000000000000000;;		oldPod := newPodList(nil, 1, v1.PodPending, controllerSpec, "pod").Items[0]
0000000000000000000000000000000000000000;;		oldPod.ResourceVersion = "2"
0000000000000000000000000000000000000000;;		manager.expectations.ExpectDeletions(rcKey, []string{controller.PodKey(&pod)})
0000000000000000000000000000000000000000;;		manager.updatePod(&oldPod, &pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		queueRC, _ = manager.queue.Get()
0000000000000000000000000000000000000000;;		if queueRC != rcKey {
0000000000000000000000000000000000000000;;			t.Fatalf("Expected to find key %v in queue, found %v", rcKey, queueRC)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		manager.queue.Done(rcKey)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podExp, exists, err = manager.expectations.GetExpectations(rcKey)
0000000000000000000000000000000000000000;;		if !exists || err != nil || !podExp.Fulfilled() {
0000000000000000000000000000000000000000;;			t.Fatalf("Wrong expectations %#v", podExp)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// An update to the pod (including an update to the deletion timestamp)
0000000000000000000000000000000000000000;;		// should not be counted as a second delete.
0000000000000000000000000000000000000000;;		isController := true
0000000000000000000000000000000000000000;;		secondPod := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Namespace: pod.Namespace,
0000000000000000000000000000000000000000;;				Name:      "secondPod",
0000000000000000000000000000000000000000;;				Labels:    pod.Labels,
0000000000000000000000000000000000000000;;				OwnerReferences: []metav1.OwnerReference{
0000000000000000000000000000000000000000;;					{UID: controllerSpec.UID, APIVersion: "v1", Kind: "ReplicationController", Name: controllerSpec.Name, Controller: &isController},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		manager.expectations.ExpectDeletions(rcKey, []string{controller.PodKey(secondPod)})
0000000000000000000000000000000000000000;;		oldPod.DeletionTimestamp = &metav1.Time{Time: time.Now()}
0000000000000000000000000000000000000000;;		oldPod.ResourceVersion = "2"
0000000000000000000000000000000000000000;;		manager.updatePod(&oldPod, &pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podExp, exists, err = manager.expectations.GetExpectations(rcKey)
0000000000000000000000000000000000000000;;		if !exists || err != nil || podExp.Fulfilled() {
0000000000000000000000000000000000000000;;			t.Fatalf("Wrong expectations %#v", podExp)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// A pod with a non-nil deletion timestamp should also be ignored by the
0000000000000000000000000000000000000000;;		// delete handler, because it's already been counted in the update.
0000000000000000000000000000000000000000;;		manager.deletePod(&pod)
0000000000000000000000000000000000000000;;		podExp, exists, err = manager.expectations.GetExpectations(rcKey)
0000000000000000000000000000000000000000;;		if !exists || err != nil || podExp.Fulfilled() {
0000000000000000000000000000000000000000;;			t.Fatalf("Wrong expectations %#v", podExp)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Deleting the second pod should clear expectations.
0000000000000000000000000000000000000000;;		manager.deletePod(secondPod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		queueRC, _ = manager.queue.Get()
0000000000000000000000000000000000000000;;		if queueRC != rcKey {
0000000000000000000000000000000000000000;;			t.Fatalf("Expected to find key %v in queue, found %v", rcKey, queueRC)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		manager.queue.Done(rcKey)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podExp, exists, err = manager.expectations.GetExpectations(rcKey)
0000000000000000000000000000000000000000;;		if !exists || err != nil || !podExp.Fulfilled() {
0000000000000000000000000000000000000000;;			t.Fatalf("Wrong expectations %#v", podExp)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func BenchmarkGetPodControllerMultiNS(b *testing.B) {
0000000000000000000000000000000000000000;;		client := clientset.NewForConfigOrDie(&restclient.Config{Host: "", ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		manager, _, rcInformer := newReplicationManagerFromClient(client, BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		const nsNum = 1000
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pods := []v1.Pod{}
0000000000000000000000000000000000000000;;		for i := 0; i < nsNum; i++ {
0000000000000000000000000000000000000000;;			ns := fmt.Sprintf("ns-%d", i)
0000000000000000000000000000000000000000;;			for j := 0; j < 10; j++ {
0000000000000000000000000000000000000000;;				rcName := fmt.Sprintf("rc-%d", j)
0000000000000000000000000000000000000000;;				for k := 0; k < 10; k++ {
0000000000000000000000000000000000000000;;					podName := fmt.Sprintf("pod-%d-%d", j, k)
0000000000000000000000000000000000000000;;					pods = append(pods, v1.Pod{
0000000000000000000000000000000000000000;;						ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;							Name:      podName,
0000000000000000000000000000000000000000;;							Namespace: ns,
0000000000000000000000000000000000000000;;							Labels:    map[string]string{"rcName": rcName},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					})
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i := 0; i < nsNum; i++ {
0000000000000000000000000000000000000000;;			ns := fmt.Sprintf("ns-%d", i)
0000000000000000000000000000000000000000;;			for j := 0; j < 10; j++ {
0000000000000000000000000000000000000000;;				rcName := fmt.Sprintf("rc-%d", j)
0000000000000000000000000000000000000000;;				rcInformer.Informer().GetIndexer().Add(&v1.ReplicationController{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{Name: rcName, Namespace: ns},
0000000000000000000000000000000000000000;;					Spec: v1.ReplicationControllerSpec{
0000000000000000000000000000000000000000;;						Selector: map[string]string{"rcName": rcName},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		b.ResetTimer()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i := 0; i < b.N; i++ {
0000000000000000000000000000000000000000;;			for _, pod := range pods {
0000000000000000000000000000000000000000;;				manager.getPodControllers(&pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func BenchmarkGetPodControllerSingleNS(b *testing.B) {
0000000000000000000000000000000000000000;;		client := clientset.NewForConfigOrDie(&restclient.Config{Host: "", ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		manager, _, rcInformer := newReplicationManagerFromClient(client, BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		const rcNum = 1000
0000000000000000000000000000000000000000;;		const replicaNum = 3
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pods := []v1.Pod{}
0000000000000000000000000000000000000000;;		for i := 0; i < rcNum; i++ {
0000000000000000000000000000000000000000;;			rcName := fmt.Sprintf("rc-%d", i)
0000000000000000000000000000000000000000;;			for j := 0; j < replicaNum; j++ {
0000000000000000000000000000000000000000;;				podName := fmt.Sprintf("pod-%d-%d", i, j)
0000000000000000000000000000000000000000;;				pods = append(pods, v1.Pod{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;						Name:      podName,
0000000000000000000000000000000000000000;;						Namespace: "foo",
0000000000000000000000000000000000000000;;						Labels:    map[string]string{"rcName": rcName},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i := 0; i < rcNum; i++ {
0000000000000000000000000000000000000000;;			rcName := fmt.Sprintf("rc-%d", i)
0000000000000000000000000000000000000000;;			rcInformer.Informer().GetIndexer().Add(&v1.ReplicationController{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{Name: rcName, Namespace: "foo"},
0000000000000000000000000000000000000000;;				Spec: v1.ReplicationControllerSpec{
0000000000000000000000000000000000000000;;					Selector: map[string]string{"rcName": rcName},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		b.ResetTimer()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i := 0; i < b.N; i++ {
0000000000000000000000000000000000000000;;			for _, pod := range pods {
0000000000000000000000000000000000000000;;				manager.getPodControllers(&pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// setupManagerWithGCEnabled creates a RC manager with a fakePodControl
0000000000000000000000000000000000000000;;	func setupManagerWithGCEnabled(objs ...runtime.Object) (manager *ReplicationManager, fakePodControl *controller.FakePodControl, podInformer coreinformers.PodInformer, rcInformer coreinformers.ReplicationControllerInformer) {
0000000000000000000000000000000000000000;;		c := fakeclientset.NewSimpleClientset(objs...)
0000000000000000000000000000000000000000;;		fakePodControl = &controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer = newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;		manager.podControl = fakePodControl
0000000000000000000000000000000000000000;;		return manager, fakePodControl, podInformer, rcInformer
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDoNotPatchPodWithOtherControlRef(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		manager, fakePodControl, podInformer, rcInformer := setupManagerWithGCEnabled(rc)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		var trueVar = true
0000000000000000000000000000000000000000;;		otherControllerReference := metav1.OwnerReference{UID: uuid.NewUUID(), APIVersion: "v1", Kind: "ReplicationController", Name: "AnotherRC", Controller: &trueVar}
0000000000000000000000000000000000000000;;		// add to podLister a matching Pod controlled by another controller. Expect no patch.
0000000000000000000000000000000000000000;;		pod := newPod("pod", rc, v1.PodRunning, nil, false)
0000000000000000000000000000000000000000;;		pod.OwnerReferences = []metav1.OwnerReference{otherControllerReference}
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(pod)
0000000000000000000000000000000000000000;;		err := manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatal(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// because the matching pod already has a controller, so 2 pods should be created.
0000000000000000000000000000000000000000;;		validateSyncReplication(t, fakePodControl, 2, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestPatchPodWithOtherOwnerRef(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		manager, fakePodControl, podInformer, rcInformer := setupManagerWithGCEnabled(rc)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		// add to podLister one more matching pod that doesn't have a controller
0000000000000000000000000000000000000000;;		// ref, but has an owner ref pointing to other object. Expect a patch to
0000000000000000000000000000000000000000;;		// take control of it.
0000000000000000000000000000000000000000;;		unrelatedOwnerReference := metav1.OwnerReference{UID: uuid.NewUUID(), APIVersion: "batch/v1", Kind: "Job", Name: "Job"}
0000000000000000000000000000000000000000;;		pod := newPod("pod", rc, v1.PodRunning, nil, false)
0000000000000000000000000000000000000000;;		pod.OwnerReferences = []metav1.OwnerReference{unrelatedOwnerReference}
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatal(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// 1 patch to take control of pod, and 1 create of new pod.
0000000000000000000000000000000000000000;;		validateSyncReplication(t, fakePodControl, 1, 0, 1)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestPatchPodWithCorrectOwnerRef(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		manager, fakePodControl, podInformer, rcInformer := setupManagerWithGCEnabled(rc)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		// add to podLister a matching pod that has an ownerRef pointing to the rc,
0000000000000000000000000000000000000000;;		// but ownerRef.Controller is false. Expect a patch to take control it.
0000000000000000000000000000000000000000;;		rcOwnerReference := metav1.OwnerReference{UID: rc.UID, APIVersion: "v1", Kind: "ReplicationController", Name: rc.Name}
0000000000000000000000000000000000000000;;		pod := newPod("pod", rc, v1.PodRunning, nil, false)
0000000000000000000000000000000000000000;;		pod.OwnerReferences = []metav1.OwnerReference{rcOwnerReference}
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatal(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// 1 patch to take control of pod, and 1 create of new pod.
0000000000000000000000000000000000000000;;		validateSyncReplication(t, fakePodControl, 1, 0, 1)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestPatchPodFails(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		manager, fakePodControl, podInformer, rcInformer := setupManagerWithGCEnabled(rc)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		// add to podLister two matching pods. Expect two patches to take control
0000000000000000000000000000000000000000;;		// them.
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(newPod("pod1", rc, v1.PodRunning, nil, false))
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(newPod("pod2", rc, v1.PodRunning, nil, false))
0000000000000000000000000000000000000000;;		// let both patches fail. The rc manager will assume it fails to take
0000000000000000000000000000000000000000;;		// control of the pods and requeue to try again.
0000000000000000000000000000000000000000;;		fakePodControl.Err = fmt.Errorf("Fake Error")
0000000000000000000000000000000000000000;;		rcKey := getKey(rc, t)
0000000000000000000000000000000000000000;;		err := processSync(manager, rcKey)
0000000000000000000000000000000000000000;;		if err == nil || !strings.Contains(err.Error(), "Fake Error") {
0000000000000000000000000000000000000000;;			t.Fatalf("expected Fake Error, got %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// 2 patches to take control of pod1 and pod2 (both fail).
0000000000000000000000000000000000000000;;		validateSyncReplication(t, fakePodControl, 0, 0, 2)
0000000000000000000000000000000000000000;;		// RC should requeue itself.
0000000000000000000000000000000000000000;;		queueRC, _ := manager.queue.Get()
0000000000000000000000000000000000000000;;		if queueRC != rcKey {
0000000000000000000000000000000000000000;;			t.Fatalf("Expected to find key %v in queue, found %v", rcKey, queueRC)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestPatchExtraPodsThenDelete(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		manager, fakePodControl, podInformer, rcInformer := setupManagerWithGCEnabled(rc)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		// add to podLister three matching pods. Expect three patches to take control
0000000000000000000000000000000000000000;;		// them, and later delete one of them.
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(newPod("pod1", rc, v1.PodRunning, nil, false))
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(newPod("pod2", rc, v1.PodRunning, nil, false))
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(newPod("pod3", rc, v1.PodRunning, nil, false))
0000000000000000000000000000000000000000;;		err := manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatal(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// 3 patches to take control of the pods, and 1 deletion because there is an extra pod.
0000000000000000000000000000000000000000;;		validateSyncReplication(t, fakePodControl, 0, 1, 3)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestUpdateLabelsRemoveControllerRef(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		manager, fakePodControl, podInformer, rcInformer := setupManagerWithGCEnabled(rc)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		// put one pod in the podLister
0000000000000000000000000000000000000000;;		pod := newPod("pod", rc, v1.PodRunning, nil, false)
0000000000000000000000000000000000000000;;		pod.ResourceVersion = "1"
0000000000000000000000000000000000000000;;		var trueVar = true
0000000000000000000000000000000000000000;;		rcOwnerReference := metav1.OwnerReference{UID: rc.UID, APIVersion: "v1", Kind: "ReplicationController", Name: rc.Name, Controller: &trueVar}
0000000000000000000000000000000000000000;;		pod.OwnerReferences = []metav1.OwnerReference{rcOwnerReference}
0000000000000000000000000000000000000000;;		updatedPod := *pod
0000000000000000000000000000000000000000;;		// reset the labels
0000000000000000000000000000000000000000;;		updatedPod.Labels = make(map[string]string)
0000000000000000000000000000000000000000;;		updatedPod.ResourceVersion = "2"
0000000000000000000000000000000000000000;;		// add the updatedPod to the store. This is consistent with the behavior of
0000000000000000000000000000000000000000;;		// the Informer: Informer updates the store before call the handler
0000000000000000000000000000000000000000;;		// (updatePod() in this case).
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(&updatedPod)
0000000000000000000000000000000000000000;;		// send a update of the same pod with modified labels
0000000000000000000000000000000000000000;;		manager.updatePod(pod, &updatedPod)
0000000000000000000000000000000000000000;;		// verifies that rc is added to the queue
0000000000000000000000000000000000000000;;		rcKey := getKey(rc, t)
0000000000000000000000000000000000000000;;		queueRC, _ := manager.queue.Get()
0000000000000000000000000000000000000000;;		if queueRC != rcKey {
0000000000000000000000000000000000000000;;			t.Fatalf("Expected to find key %v in queue, found %v", rcKey, queueRC)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		manager.queue.Done(queueRC)
0000000000000000000000000000000000000000;;		err := manager.syncReplicationController(rcKey)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatal(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// expect 1 patch to be sent to remove the controllerRef for the pod.
0000000000000000000000000000000000000000;;		// expect 2 creates because the *(rc.Spec.Replicas)=2 and there exists no
0000000000000000000000000000000000000000;;		// matching pod.
0000000000000000000000000000000000000000;;		validateSyncReplication(t, fakePodControl, 2, 0, 1)
0000000000000000000000000000000000000000;;		fakePodControl.Clear()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestUpdateSelectorControllerRef(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		manager, fakePodControl, podInformer, rcInformer := setupManagerWithGCEnabled(rc)
0000000000000000000000000000000000000000;;		// put 2 pods in the podLister
0000000000000000000000000000000000000000;;		newPodList(podInformer.Informer().GetIndexer(), 2, v1.PodRunning, rc, "pod")
0000000000000000000000000000000000000000;;		// update the RC so that its selector no longer matches the pods
0000000000000000000000000000000000000000;;		updatedRC := *rc
0000000000000000000000000000000000000000;;		updatedRC.Spec.Selector = map[string]string{"foo": "baz"}
0000000000000000000000000000000000000000;;		// put the updatedRC into the store. This is consistent with the behavior of
0000000000000000000000000000000000000000;;		// the Informer: Informer updates the store before call the handler
0000000000000000000000000000000000000000;;		// (updateRC() in this case).
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(&updatedRC)
0000000000000000000000000000000000000000;;		manager.updateRC(rc, &updatedRC)
0000000000000000000000000000000000000000;;		// verifies that the rc is added to the queue
0000000000000000000000000000000000000000;;		rcKey := getKey(rc, t)
0000000000000000000000000000000000000000;;		queueRC, _ := manager.queue.Get()
0000000000000000000000000000000000000000;;		if queueRC != rcKey {
0000000000000000000000000000000000000000;;			t.Fatalf("Expected to find key %v in queue, found %v", rcKey, queueRC)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		manager.queue.Done(queueRC)
0000000000000000000000000000000000000000;;		err := manager.syncReplicationController(rcKey)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatal(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// expect 2 patches to be sent to remove the controllerRef for the pods.
0000000000000000000000000000000000000000;;		// expect 2 creates because the *(rc.Spec.Replicas)=2 and there exists no
0000000000000000000000000000000000000000;;		// matching pod.
0000000000000000000000000000000000000000;;		validateSyncReplication(t, fakePodControl, 2, 0, 2)
0000000000000000000000000000000000000000;;		fakePodControl.Clear()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RC manager shouldn't adopt or create more pods if the rc is about to be
0000000000000000000000000000000000000000;;	// deleted.
0000000000000000000000000000000000000000;;	func TestDoNotAdoptOrCreateIfBeingDeleted(t *testing.T) {
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		now := metav1.Now()
0000000000000000000000000000000000000000;;		rc.DeletionTimestamp = &now
0000000000000000000000000000000000000000;;		manager, fakePodControl, podInformer, rcInformer := setupManagerWithGCEnabled(rc)
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;		pod1 := newPod("pod1", rc, v1.PodRunning, nil, false)
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(pod1)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// no patch, no create
0000000000000000000000000000000000000000;;		err := manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatal(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		validateSyncReplication(t, fakePodControl, 0, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDoNotAdoptOrCreateIfBeingDeletedRace(t *testing.T) {
0000000000000000000000000000000000000000;;		// Bare client says it IS deleted.
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		now := metav1.Now()
0000000000000000000000000000000000000000;;		rc.DeletionTimestamp = &now
0000000000000000000000000000000000000000;;		manager, fakePodControl, podInformer, rcInformer := setupManagerWithGCEnabled(rc)
0000000000000000000000000000000000000000;;		// Lister (cache) says it's NOT deleted.
0000000000000000000000000000000000000000;;		rc2 := *rc
0000000000000000000000000000000000000000;;		rc2.DeletionTimestamp = nil
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(&rc2)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Recheck occurs if a matching orphan is present.
0000000000000000000000000000000000000000;;		pod1 := newPod("pod1", rc, v1.PodRunning, nil, false)
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(pod1)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// sync should abort.
0000000000000000000000000000000000000000;;		err := manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			t.Error("syncReplicationController() err = nil, expected non-nil")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// no patch, no create.
0000000000000000000000000000000000000000;;		validateSyncReplication(t, fakePodControl, 0, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestReadyReplicas(t *testing.T) {
0000000000000000000000000000000000000000;;		// This is a happy server just to record the PUT request we expect for status.Replicas
0000000000000000000000000000000000000000;;		fakeHandler := utiltesting.FakeHandler{
0000000000000000000000000000000000000000;;			StatusCode:   200,
0000000000000000000000000000000000000000;;			ResponseBody: "{}",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		testServer := httptest.NewServer(&fakeHandler)
0000000000000000000000000000000000000000;;		defer testServer.Close()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		c := clientset.NewForConfigOrDie(&restclient.Config{Host: testServer.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Status.Replica should update to match number of pods in system, 1 new pod should be created.
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		rc.Status = v1.ReplicationControllerStatus{Replicas: 2, ReadyReplicas: 0, AvailableReplicas: 0, ObservedGeneration: 1}
0000000000000000000000000000000000000000;;		rc.Generation = 1
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newPodList(podInformer.Informer().GetIndexer(), 2, v1.PodPending, rc, "pod")
0000000000000000000000000000000000000000;;		newPodList(podInformer.Informer().GetIndexer(), 2, v1.PodRunning, rc, "pod")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This response body is just so we don't err out decoding the http response
0000000000000000000000000000000000000000;;		response := runtime.EncodeOrDie(testapi.Default.Codec(), &v1.ReplicationController{})
0000000000000000000000000000000000000000;;		fakeHandler.ResponseBody = response
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// ReadyReplicas should go from 0 to 2.
0000000000000000000000000000000000000000;;		rc.Status = v1.ReplicationControllerStatus{Replicas: 2, ReadyReplicas: 2, AvailableReplicas: 2, ObservedGeneration: 1}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		decRc := runtime.EncodeOrDie(testapi.Default.Codec(), rc)
0000000000000000000000000000000000000000;;		fakeHandler.ValidateRequest(t, testapi.Default.ResourcePath(replicationControllerResourceName(), rc.Namespace, rc.Name)+"/status", "PUT", &decRc)
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 0, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestAvailableReplicas(t *testing.T) {
0000000000000000000000000000000000000000;;		// This is a happy server just to record the PUT request we expect for status.Replicas
0000000000000000000000000000000000000000;;		fakeHandler := utiltesting.FakeHandler{
0000000000000000000000000000000000000000;;			StatusCode:   200,
0000000000000000000000000000000000000000;;			ResponseBody: "{}",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		testServer := httptest.NewServer(&fakeHandler)
0000000000000000000000000000000000000000;;		defer testServer.Close()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		c := clientset.NewForConfigOrDie(&restclient.Config{Host: testServer.URL, ContentConfig: restclient.ContentConfig{GroupVersion: &api.Registry.GroupOrDie(v1.GroupName).GroupVersion}})
0000000000000000000000000000000000000000;;		manager, podInformer, rcInformer := newReplicationManagerFromClient(c, BurstReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Status.Replica should update to match number of pods in system, 1 new pod should be created.
0000000000000000000000000000000000000000;;		rc := newReplicationController(2)
0000000000000000000000000000000000000000;;		rc.Status = v1.ReplicationControllerStatus{Replicas: 2, ReadyReplicas: 0, ObservedGeneration: 1}
0000000000000000000000000000000000000000;;		rc.Generation = 1
0000000000000000000000000000000000000000;;		// minReadySeconds set to 15s
0000000000000000000000000000000000000000;;		rc.Spec.MinReadySeconds = 15
0000000000000000000000000000000000000000;;		rcInformer.Informer().GetIndexer().Add(rc)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// First pod becomes ready 20s ago
0000000000000000000000000000000000000000;;		moment := metav1.Time{Time: time.Now().Add(-2e10)}
0000000000000000000000000000000000000000;;		pod := newPod("pod", rc, v1.PodRunning, &moment, true)
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Second pod becomes ready now
0000000000000000000000000000000000000000;;		otherMoment := metav1.Now()
0000000000000000000000000000000000000000;;		otherPod := newPod("otherPod", rc, v1.PodRunning, &otherMoment, true)
0000000000000000000000000000000000000000;;		podInformer.Informer().GetIndexer().Add(otherPod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This response body is just so we don't err out decoding the http response
0000000000000000000000000000000000000000;;		response := runtime.EncodeOrDie(testapi.Default.Codec(), &v1.ReplicationController{})
0000000000000000000000000000000000000000;;		fakeHandler.ResponseBody = response
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakePodControl := controller.FakePodControl{}
0000000000000000000000000000000000000000;;		manager.podControl = &fakePodControl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The controller should see only one available pod.
0000000000000000000000000000000000000000;;		manager.syncReplicationController(getKey(rc, t))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rc.Status = v1.ReplicationControllerStatus{Replicas: 2, ReadyReplicas: 2, AvailableReplicas: 1, ObservedGeneration: 1}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		decRc := runtime.EncodeOrDie(testapi.Default.Codec(), rc)
0000000000000000000000000000000000000000;;		fakeHandler.ValidateRequest(t, testapi.Default.ResourcePath(replicationControllerResourceName(), rc.Namespace, rc.Name)+"/status", "PUT", &decRc)
0000000000000000000000000000000000000000;;		validateSyncReplication(t, &fakePodControl, 0, 0, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var (
0000000000000000000000000000000000000000;;		imagePullBackOff v1.ReplicationControllerConditionType = "ImagePullBackOff"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		condImagePullBackOff = func() v1.ReplicationControllerCondition {
0000000000000000000000000000000000000000;;			return v1.ReplicationControllerCondition{
0000000000000000000000000000000000000000;;				Type:   imagePullBackOff,
0000000000000000000000000000000000000000;;				Status: v1.ConditionTrue,
0000000000000000000000000000000000000000;;				Reason: "NonExistentImage",
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		condReplicaFailure = func() v1.ReplicationControllerCondition {
0000000000000000000000000000000000000000;;			return v1.ReplicationControllerCondition{
0000000000000000000000000000000000000000;;				Type:   v1.ReplicationControllerReplicaFailure,
0000000000000000000000000000000000000000;;				Status: v1.ConditionTrue,
0000000000000000000000000000000000000000;;				Reason: "OtherFailure",
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		condReplicaFailure2 = func() v1.ReplicationControllerCondition {
0000000000000000000000000000000000000000;;			return v1.ReplicationControllerCondition{
0000000000000000000000000000000000000000;;				Type:   v1.ReplicationControllerReplicaFailure,
0000000000000000000000000000000000000000;;				Status: v1.ConditionTrue,
0000000000000000000000000000000000000000;;				Reason: "AnotherFailure",
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		status = func() *v1.ReplicationControllerStatus {
0000000000000000000000000000000000000000;;			return &v1.ReplicationControllerStatus{
0000000000000000000000000000000000000000;;				Conditions: []v1.ReplicationControllerCondition{condReplicaFailure()},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestGetCondition(t *testing.T) {
0000000000000000000000000000000000000000;;		exampleStatus := status()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		tests := []struct {
0000000000000000000000000000000000000000;;			name string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			status     v1.ReplicationControllerStatus
0000000000000000000000000000000000000000;;			condType   v1.ReplicationControllerConditionType
0000000000000000000000000000000000000000;;			condStatus v1.ConditionStatus
0000000000000000000000000000000000000000;;			condReason string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			expected bool
0000000000000000000000000000000000000000;;		}{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				name: "condition exists",
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				status:   *exampleStatus,
0000000000000000000000000000000000000000;;				condType: v1.ReplicationControllerReplicaFailure,
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				expected: true,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				name: "condition does not exist",
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				status:   *exampleStatus,
0000000000000000000000000000000000000000;;				condType: imagePullBackOff,
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				expected: false,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, test := range tests {
0000000000000000000000000000000000000000;;			cond := GetCondition(test.status, test.condType)
0000000000000000000000000000000000000000;;			exists := cond != nil
0000000000000000000000000000000000000000;;			if exists != test.expected {
0000000000000000000000000000000000000000;;				t.Errorf("%s: expected condition to exist: %t, got: %t", test.name, test.expected, exists)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSetCondition(t *testing.T) {
0000000000000000000000000000000000000000;;		tests := []struct {
0000000000000000000000000000000000000000;;			name string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			status *v1.ReplicationControllerStatus
0000000000000000000000000000000000000000;;			cond   v1.ReplicationControllerCondition
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			expectedStatus *v1.ReplicationControllerStatus
0000000000000000000000000000000000000000;;		}{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				name: "set for the first time",
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				status: &v1.ReplicationControllerStatus{},
0000000000000000000000000000000000000000;;				cond:   condReplicaFailure(),
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				expectedStatus: &v1.ReplicationControllerStatus{Conditions: []v1.ReplicationControllerCondition{condReplicaFailure()}},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				name: "simple set",
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				status: &v1.ReplicationControllerStatus{Conditions: []v1.ReplicationControllerCondition{condImagePullBackOff()}},
0000000000000000000000000000000000000000;;				cond:   condReplicaFailure(),
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				expectedStatus: &v1.ReplicationControllerStatus{Conditions: []v1.ReplicationControllerCondition{condImagePullBackOff(), condReplicaFailure()}},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				name: "overwrite",
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				status: &v1.ReplicationControllerStatus{Conditions: []v1.ReplicationControllerCondition{condReplicaFailure()}},
0000000000000000000000000000000000000000;;				cond:   condReplicaFailure2(),
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				expectedStatus: &v1.ReplicationControllerStatus{Conditions: []v1.ReplicationControllerCondition{condReplicaFailure2()}},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, test := range tests {
0000000000000000000000000000000000000000;;			SetCondition(test.status, test.cond)
0000000000000000000000000000000000000000;;			if !reflect.DeepEqual(test.status, test.expectedStatus) {
0000000000000000000000000000000000000000;;				t.Errorf("%s: expected status: %v, got: %v", test.name, test.expectedStatus, test.status)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestRemoveCondition(t *testing.T) {
0000000000000000000000000000000000000000;;		tests := []struct {
0000000000000000000000000000000000000000;;			name string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			status   *v1.ReplicationControllerStatus
0000000000000000000000000000000000000000;;			condType v1.ReplicationControllerConditionType
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			expectedStatus *v1.ReplicationControllerStatus
0000000000000000000000000000000000000000;;		}{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				name: "remove from empty status",
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				status:   &v1.ReplicationControllerStatus{},
0000000000000000000000000000000000000000;;				condType: v1.ReplicationControllerReplicaFailure,
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				expectedStatus: &v1.ReplicationControllerStatus{},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				name: "simple remove",
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				status:   &v1.ReplicationControllerStatus{Conditions: []v1.ReplicationControllerCondition{condReplicaFailure()}},
0000000000000000000000000000000000000000;;				condType: v1.ReplicationControllerReplicaFailure,
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				expectedStatus: &v1.ReplicationControllerStatus{},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				name: "doesn't remove anything",
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				status:   status(),
0000000000000000000000000000000000000000;;				condType: imagePullBackOff,
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				expectedStatus: status(),
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, test := range tests {
0000000000000000000000000000000000000000;;			RemoveCondition(test.status, test.condType)
0000000000000000000000000000000000000000;;			if !reflect.DeepEqual(test.status, test.expectedStatus) {
0000000000000000000000000000000000000000;;				t.Errorf("%s: expected status: %v, got: %v", test.name, test.expectedStatus, test.status)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
