0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2014 The Kubernetes Authors.
ce7a06715f84202018cc0482403baee6061bdade;pkg/kubelet/status_manager.go[pkg/kubelet/status_manager.go][pkg/kubelet/status/status_manager.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package status
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		apiequality "k8s.io/apimachinery/pkg/api/equality"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/diff"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		podutil "k8s.io/kubernetes/pkg/api/v1/pod"
0000000000000000000000000000000000000000;;		kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
0000000000000000000000000000000000000000;;		kubepod "k8s.io/kubernetes/pkg/kubelet/pod"
0000000000000000000000000000000000000000;;		kubetypes "k8s.io/kubernetes/pkg/kubelet/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/format"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// A wrapper around v1.PodStatus that includes a version to enforce that stale pod statuses are
0000000000000000000000000000000000000000;;	// not sent to the API server.
0000000000000000000000000000000000000000;;	type versionedPodStatus struct {
0000000000000000000000000000000000000000;;		status v1.PodStatus
0000000000000000000000000000000000000000;;		// Monotonically increasing version number (per pod).
0000000000000000000000000000000000000000;;		version uint64
0000000000000000000000000000000000000000;;		// Pod name & namespace, for sending updates to API server.
0000000000000000000000000000000000000000;;		podName      string
0000000000000000000000000000000000000000;;		podNamespace string
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type podStatusSyncRequest struct {
0000000000000000000000000000000000000000;;		podUID types.UID
0000000000000000000000000000000000000000;;		status versionedPodStatus
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Updates pod statuses in apiserver. Writes only when new status has changed.
0000000000000000000000000000000000000000;;	// All methods are thread-safe.
0000000000000000000000000000000000000000;;	type manager struct {
0000000000000000000000000000000000000000;;		kubeClient clientset.Interface
0000000000000000000000000000000000000000;;		podManager kubepod.Manager
0000000000000000000000000000000000000000;;		// Map from pod UID to sync status of the corresponding pod.
0000000000000000000000000000000000000000;;		podStatuses      map[types.UID]versionedPodStatus
0000000000000000000000000000000000000000;;		podStatusesLock  sync.RWMutex
0000000000000000000000000000000000000000;;		podStatusChannel chan podStatusSyncRequest
0000000000000000000000000000000000000000;;		// Map from (mirror) pod UID to latest status version successfully sent to the API server.
0000000000000000000000000000000000000000;;		// apiStatusVersions must only be accessed from the sync thread.
0000000000000000000000000000000000000000;;		apiStatusVersions map[types.UID]uint64
0000000000000000000000000000000000000000;;		podDeletionSafety PodDeletionSafetyProvider
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodStatusProvider knows how to provide status for a pod.  It's intended to be used by other components
0000000000000000000000000000000000000000;;	// that need to introspect status.
0000000000000000000000000000000000000000;;	type PodStatusProvider interface {
0000000000000000000000000000000000000000;;		// GetPodStatus returns the cached status for the provided pod UID, as well as whether it
0000000000000000000000000000000000000000;;		// was a cache hit.
0000000000000000000000000000000000000000;;		GetPodStatus(uid types.UID) (v1.PodStatus, bool)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// An object which provides guarantees that a pod can be saftely deleted.
0000000000000000000000000000000000000000;;	type PodDeletionSafetyProvider interface {
0000000000000000000000000000000000000000;;		// A function which returns true if the pod can safely be deleted
0000000000000000000000000000000000000000;;		PodResourcesAreReclaimed(pod *v1.Pod, status v1.PodStatus) bool
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Manager is the Source of truth for kubelet pod status, and should be kept up-to-date with
0000000000000000000000000000000000000000;;	// the latest v1.PodStatus. It also syncs updates back to the API server.
0000000000000000000000000000000000000000;;	type Manager interface {
0000000000000000000000000000000000000000;;		PodStatusProvider
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start the API server status sync loop.
0000000000000000000000000000000000000000;;		Start()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// SetPodStatus caches updates the cached status for the given pod, and triggers a status update.
0000000000000000000000000000000000000000;;		SetPodStatus(pod *v1.Pod, status v1.PodStatus)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// SetContainerReadiness updates the cached container status with the given readiness, and
0000000000000000000000000000000000000000;;		// triggers a status update.
0000000000000000000000000000000000000000;;		SetContainerReadiness(podUID types.UID, containerID kubecontainer.ContainerID, ready bool)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TerminatePod resets the container status for the provided pod to terminated and triggers
0000000000000000000000000000000000000000;;		// a status update.
0000000000000000000000000000000000000000;;		TerminatePod(pod *v1.Pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// RemoveOrphanedStatuses scans the status cache and removes any entries for pods not included in
0000000000000000000000000000000000000000;;		// the provided podUIDs.
0000000000000000000000000000000000000000;;		RemoveOrphanedStatuses(podUIDs map[types.UID]bool)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const syncPeriod = 10 * time.Second
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewManager(kubeClient clientset.Interface, podManager kubepod.Manager, podDeletionSafety PodDeletionSafetyProvider) Manager {
0000000000000000000000000000000000000000;;		return &manager{
0000000000000000000000000000000000000000;;			kubeClient:        kubeClient,
0000000000000000000000000000000000000000;;			podManager:        podManager,
0000000000000000000000000000000000000000;;			podStatuses:       make(map[types.UID]versionedPodStatus),
0000000000000000000000000000000000000000;;			podStatusChannel:  make(chan podStatusSyncRequest, 1000), // Buffer up to 1000 statuses
0000000000000000000000000000000000000000;;			apiStatusVersions: make(map[types.UID]uint64),
0000000000000000000000000000000000000000;;			podDeletionSafety: podDeletionSafety,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// isStatusEqual returns true if the given pod statuses are equal, false otherwise.
0000000000000000000000000000000000000000;;	// This method normalizes the status before comparing so as to make sure that meaningless
0000000000000000000000000000000000000000;;	// changes will be ignored.
0000000000000000000000000000000000000000;;	func isStatusEqual(oldStatus, status *v1.PodStatus) bool {
0000000000000000000000000000000000000000;;		return apiequality.Semantic.DeepEqual(status, oldStatus)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *manager) Start() {
0000000000000000000000000000000000000000;;		// Don't start the status manager if we don't have a client. This will happen
0000000000000000000000000000000000000000;;		// on the master, where the kubelet is responsible for bootstrapping the pods
0000000000000000000000000000000000000000;;		// of the master components.
0000000000000000000000000000000000000000;;		if m.kubeClient == nil {
0000000000000000000000000000000000000000;;			glog.Infof("Kubernetes client is nil, not starting status manager.")
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.Info("Starting to sync pod status with apiserver")
0000000000000000000000000000000000000000;;		syncTicker := time.Tick(syncPeriod)
0000000000000000000000000000000000000000;;		// syncPod and syncBatch share the same go routine to avoid sync races.
0000000000000000000000000000000000000000;;		go wait.Forever(func() {
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case syncRequest := <-m.podStatusChannel:
0000000000000000000000000000000000000000;;				glog.V(5).Infof("Status Manager: syncing pod: %q, with status: (%d, %v) from podStatusChannel",
0000000000000000000000000000000000000000;;					syncRequest.podUID, syncRequest.status.version, syncRequest.status.status)
0000000000000000000000000000000000000000;;				m.syncPod(syncRequest.podUID, syncRequest.status)
0000000000000000000000000000000000000000;;			case <-syncTicker:
0000000000000000000000000000000000000000;;				m.syncBatch()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *manager) GetPodStatus(uid types.UID) (v1.PodStatus, bool) {
0000000000000000000000000000000000000000;;		m.podStatusesLock.RLock()
0000000000000000000000000000000000000000;;		defer m.podStatusesLock.RUnlock()
0000000000000000000000000000000000000000;;		status, ok := m.podStatuses[m.podManager.TranslatePodUID(uid)]
0000000000000000000000000000000000000000;;		return status.status, ok
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *manager) SetPodStatus(pod *v1.Pod, status v1.PodStatus) {
0000000000000000000000000000000000000000;;		m.podStatusesLock.Lock()
0000000000000000000000000000000000000000;;		defer m.podStatusesLock.Unlock()
0000000000000000000000000000000000000000;;		// Make sure we're caching a deep copy.
0000000000000000000000000000000000000000;;		status, err := copyStatus(&status)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Force a status update if deletion timestamp is set. This is necessary
0000000000000000000000000000000000000000;;		// because if the pod is in the non-running state, the pod worker still
0000000000000000000000000000000000000000;;		// needs to be able to trigger an update and/or deletion.
0000000000000000000000000000000000000000;;		m.updateStatusInternal(pod, status, pod.DeletionTimestamp != nil)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *manager) SetContainerReadiness(podUID types.UID, containerID kubecontainer.ContainerID, ready bool) {
0000000000000000000000000000000000000000;;		m.podStatusesLock.Lock()
0000000000000000000000000000000000000000;;		defer m.podStatusesLock.Unlock()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pod, ok := m.podManager.GetPodByUID(podUID)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Pod %q has been deleted, no need to update readiness", string(podUID))
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		oldStatus, found := m.podStatuses[pod.UID]
0000000000000000000000000000000000000000;;		if !found {
0000000000000000000000000000000000000000;;			glog.Warningf("Container readiness changed before pod has synced: %q - %q",
0000000000000000000000000000000000000000;;				format.Pod(pod), containerID.String())
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Find the container to update.
0000000000000000000000000000000000000000;;		containerStatus, _, ok := findContainerStatus(&oldStatus.status, containerID.String())
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			glog.Warningf("Container readiness changed for unknown container: %q - %q",
0000000000000000000000000000000000000000;;				format.Pod(pod), containerID.String())
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if containerStatus.Ready == ready {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Container readiness unchanged (%v): %q - %q", ready,
0000000000000000000000000000000000000000;;				format.Pod(pod), containerID.String())
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Make sure we're not updating the cached version.
0000000000000000000000000000000000000000;;		status, err := copyStatus(&oldStatus.status)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		containerStatus, _, _ = findContainerStatus(&status, containerID.String())
0000000000000000000000000000000000000000;;		containerStatus.Ready = ready
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Update pod condition.
0000000000000000000000000000000000000000;;		readyConditionIndex := -1
0000000000000000000000000000000000000000;;		for i, condition := range status.Conditions {
0000000000000000000000000000000000000000;;			if condition.Type == v1.PodReady {
0000000000000000000000000000000000000000;;				readyConditionIndex = i
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		readyCondition := GeneratePodReadyCondition(&pod.Spec, status.ContainerStatuses, status.Phase)
0000000000000000000000000000000000000000;;		if readyConditionIndex != -1 {
0000000000000000000000000000000000000000;;			status.Conditions[readyConditionIndex] = readyCondition
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.Warningf("PodStatus missing PodReady condition: %+v", status)
0000000000000000000000000000000000000000;;			status.Conditions = append(status.Conditions, readyCondition)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		m.updateStatusInternal(pod, status, false)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func findContainerStatus(status *v1.PodStatus, containerID string) (containerStatus *v1.ContainerStatus, init bool, ok bool) {
0000000000000000000000000000000000000000;;		// Find the container to update.
0000000000000000000000000000000000000000;;		for i, c := range status.ContainerStatuses {
0000000000000000000000000000000000000000;;			if c.ContainerID == containerID {
0000000000000000000000000000000000000000;;				return &status.ContainerStatuses[i], false, true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i, c := range status.InitContainerStatuses {
0000000000000000000000000000000000000000;;			if c.ContainerID == containerID {
0000000000000000000000000000000000000000;;				return &status.InitContainerStatuses[i], true, true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil, false, false
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *manager) TerminatePod(pod *v1.Pod) {
0000000000000000000000000000000000000000;;		m.podStatusesLock.Lock()
0000000000000000000000000000000000000000;;		defer m.podStatusesLock.Unlock()
0000000000000000000000000000000000000000;;		oldStatus := &pod.Status
0000000000000000000000000000000000000000;;		if cachedStatus, ok := m.podStatuses[pod.UID]; ok {
0000000000000000000000000000000000000000;;			oldStatus = &cachedStatus.status
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		status, err := copyStatus(oldStatus)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := range status.ContainerStatuses {
0000000000000000000000000000000000000000;;			status.ContainerStatuses[i].State = v1.ContainerState{
0000000000000000000000000000000000000000;;				Terminated: &v1.ContainerStateTerminated{},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := range status.InitContainerStatuses {
0000000000000000000000000000000000000000;;			status.InitContainerStatuses[i].State = v1.ContainerState{
0000000000000000000000000000000000000000;;				Terminated: &v1.ContainerStateTerminated{},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		m.updateStatusInternal(pod, status, true)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// updateStatusInternal updates the internal status cache, and queues an update to the api server if
0000000000000000000000000000000000000000;;	// necessary. Returns whether an update was triggered.
0000000000000000000000000000000000000000;;	// This method IS NOT THREAD SAFE and must be called from a locked function.
0000000000000000000000000000000000000000;;	func (m *manager) updateStatusInternal(pod *v1.Pod, status v1.PodStatus, forceUpdate bool) bool {
0000000000000000000000000000000000000000;;		var oldStatus v1.PodStatus
0000000000000000000000000000000000000000;;		cachedStatus, isCached := m.podStatuses[pod.UID]
0000000000000000000000000000000000000000;;		if isCached {
0000000000000000000000000000000000000000;;			oldStatus = cachedStatus.status
0000000000000000000000000000000000000000;;		} else if mirrorPod, ok := m.podManager.GetMirrorPodByPod(pod); ok {
0000000000000000000000000000000000000000;;			oldStatus = mirrorPod.Status
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			oldStatus = pod.Status
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Set ReadyCondition.LastTransitionTime.
0000000000000000000000000000000000000000;;		if _, readyCondition := podutil.GetPodCondition(&status, v1.PodReady); readyCondition != nil {
0000000000000000000000000000000000000000;;			// Need to set LastTransitionTime.
0000000000000000000000000000000000000000;;			lastTransitionTime := metav1.Now()
0000000000000000000000000000000000000000;;			_, oldReadyCondition := podutil.GetPodCondition(&oldStatus, v1.PodReady)
0000000000000000000000000000000000000000;;			if oldReadyCondition != nil && readyCondition.Status == oldReadyCondition.Status {
0000000000000000000000000000000000000000;;				lastTransitionTime = oldReadyCondition.LastTransitionTime
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			readyCondition.LastTransitionTime = lastTransitionTime
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Set InitializedCondition.LastTransitionTime.
0000000000000000000000000000000000000000;;		if _, initCondition := podutil.GetPodCondition(&status, v1.PodInitialized); initCondition != nil {
0000000000000000000000000000000000000000;;			// Need to set LastTransitionTime.
0000000000000000000000000000000000000000;;			lastTransitionTime := metav1.Now()
0000000000000000000000000000000000000000;;			_, oldInitCondition := podutil.GetPodCondition(&oldStatus, v1.PodInitialized)
0000000000000000000000000000000000000000;;			if oldInitCondition != nil && initCondition.Status == oldInitCondition.Status {
0000000000000000000000000000000000000000;;				lastTransitionTime = oldInitCondition.LastTransitionTime
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			initCondition.LastTransitionTime = lastTransitionTime
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// ensure that the start time does not change across updates.
0000000000000000000000000000000000000000;;		if oldStatus.StartTime != nil && !oldStatus.StartTime.IsZero() {
0000000000000000000000000000000000000000;;			status.StartTime = oldStatus.StartTime
0000000000000000000000000000000000000000;;		} else if status.StartTime.IsZero() {
0000000000000000000000000000000000000000;;			// if the status has no start time, we need to set an initial time
0000000000000000000000000000000000000000;;			now := metav1.Now()
0000000000000000000000000000000000000000;;			status.StartTime = &now
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		normalizeStatus(pod, &status)
0000000000000000000000000000000000000000;;		// The intent here is to prevent concurrent updates to a pod's status from
0000000000000000000000000000000000000000;;		// clobbering each other so the phase of a pod progresses monotonically.
0000000000000000000000000000000000000000;;		if isCached && isStatusEqual(&cachedStatus.status, &status) && !forceUpdate {
0000000000000000000000000000000000000000;;			glog.V(3).Infof("Ignoring same status for pod %q, status: %+v", format.Pod(pod), status)
0000000000000000000000000000000000000000;;			return false // No new status.
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newStatus := versionedPodStatus{
0000000000000000000000000000000000000000;;			status:       status,
0000000000000000000000000000000000000000;;			version:      cachedStatus.version + 1,
0000000000000000000000000000000000000000;;			podName:      pod.Name,
0000000000000000000000000000000000000000;;			podNamespace: pod.Namespace,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		m.podStatuses[pod.UID] = newStatus
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		select {
0000000000000000000000000000000000000000;;		case m.podStatusChannel <- podStatusSyncRequest{pod.UID, newStatus}:
0000000000000000000000000000000000000000;;			glog.V(5).Infof("Status Manager: adding pod: %q, with status: (%q, %v) to podStatusChannel",
0000000000000000000000000000000000000000;;				pod.UID, newStatus.version, newStatus.status)
0000000000000000000000000000000000000000;;			return true
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			// Let the periodic syncBatch handle the update if the channel is full.
0000000000000000000000000000000000000000;;			// We can't block, since we hold the mutex lock.
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Skpping the status update for pod %q for now because the channel is full; status: %+v",
0000000000000000000000000000000000000000;;				format.Pod(pod), status)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// deletePodStatus simply removes the given pod from the status cache.
0000000000000000000000000000000000000000;;	func (m *manager) deletePodStatus(uid types.UID) {
0000000000000000000000000000000000000000;;		m.podStatusesLock.Lock()
0000000000000000000000000000000000000000;;		defer m.podStatusesLock.Unlock()
0000000000000000000000000000000000000000;;		delete(m.podStatuses, uid)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO(filipg): It'd be cleaner if we can do this without signal from user.
0000000000000000000000000000000000000000;;	func (m *manager) RemoveOrphanedStatuses(podUIDs map[types.UID]bool) {
0000000000000000000000000000000000000000;;		m.podStatusesLock.Lock()
0000000000000000000000000000000000000000;;		defer m.podStatusesLock.Unlock()
0000000000000000000000000000000000000000;;		for key := range m.podStatuses {
0000000000000000000000000000000000000000;;			if _, ok := podUIDs[key]; !ok {
0000000000000000000000000000000000000000;;				glog.V(5).Infof("Removing %q from status map.", key)
0000000000000000000000000000000000000000;;				delete(m.podStatuses, key)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncBatch syncs pods statuses with the apiserver.
0000000000000000000000000000000000000000;;	func (m *manager) syncBatch() {
0000000000000000000000000000000000000000;;		var updatedStatuses []podStatusSyncRequest
0000000000000000000000000000000000000000;;		podToMirror, mirrorToPod := m.podManager.GetUIDTranslations()
0000000000000000000000000000000000000000;;		func() { // Critical section
0000000000000000000000000000000000000000;;			m.podStatusesLock.RLock()
0000000000000000000000000000000000000000;;			defer m.podStatusesLock.RUnlock()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Clean up orphaned versions.
0000000000000000000000000000000000000000;;			for uid := range m.apiStatusVersions {
0000000000000000000000000000000000000000;;				_, hasPod := m.podStatuses[uid]
0000000000000000000000000000000000000000;;				_, hasMirror := mirrorToPod[uid]
0000000000000000000000000000000000000000;;				if !hasPod && !hasMirror {
0000000000000000000000000000000000000000;;					delete(m.apiStatusVersions, uid)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for uid, status := range m.podStatuses {
0000000000000000000000000000000000000000;;				syncedUID := uid
0000000000000000000000000000000000000000;;				if mirrorUID, ok := podToMirror[uid]; ok {
0000000000000000000000000000000000000000;;					if mirrorUID == "" {
0000000000000000000000000000000000000000;;						glog.V(5).Infof("Static pod %q (%s/%s) does not have a corresponding mirror pod; skipping", uid, status.podName, status.podNamespace)
0000000000000000000000000000000000000000;;						continue
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					syncedUID = mirrorUID
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if m.needsUpdate(syncedUID, status) {
0000000000000000000000000000000000000000;;					updatedStatuses = append(updatedStatuses, podStatusSyncRequest{uid, status})
0000000000000000000000000000000000000000;;				} else if m.needsReconcile(uid, status.status) {
0000000000000000000000000000000000000000;;					// Delete the apiStatusVersions here to force an update on the pod status
0000000000000000000000000000000000000000;;					// In most cases the deleted apiStatusVersions here should be filled
0000000000000000000000000000000000000000;;					// soon after the following syncPod() [If the syncPod() sync an update
0000000000000000000000000000000000000000;;					// successfully].
0000000000000000000000000000000000000000;;					delete(m.apiStatusVersions, syncedUID)
0000000000000000000000000000000000000000;;					updatedStatuses = append(updatedStatuses, podStatusSyncRequest{uid, status})
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, update := range updatedStatuses {
0000000000000000000000000000000000000000;;			glog.V(5).Infof("Status Manager: syncPod in syncbatch. pod UID: %q", update.podUID)
0000000000000000000000000000000000000000;;			m.syncPod(update.podUID, update.status)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncPod syncs the given status with the API server. The caller must not hold the lock.
0000000000000000000000000000000000000000;;	func (m *manager) syncPod(uid types.UID, status versionedPodStatus) {
0000000000000000000000000000000000000000;;		if !m.needsUpdate(uid, status) {
0000000000000000000000000000000000000000;;			glog.V(1).Infof("Status for pod %q is up-to-date; skipping", uid)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: make me easier to express from client code
0000000000000000000000000000000000000000;;		pod, err := m.kubeClient.Core().Pods(status.podNamespace).Get(status.podName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;			glog.V(3).Infof("Pod %q (%s) does not exist on the server", status.podName, uid)
0000000000000000000000000000000000000000;;			// If the Pod is deleted the status will be cleared in
0000000000000000000000000000000000000000;;			// RemoveOrphanedStatuses, so we just ignore the update here.
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Warningf("Failed to get status for pod %q: %v", format.PodDesc(status.podName, status.podNamespace, uid), err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		translatedUID := m.podManager.TranslatePodUID(pod.UID)
0000000000000000000000000000000000000000;;		if len(translatedUID) > 0 && translatedUID != uid {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("Pod %q was deleted and then recreated, skipping status update; old UID %q, new UID %q", format.Pod(pod), uid, translatedUID)
0000000000000000000000000000000000000000;;			m.deletePodStatus(uid)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pod.Status = status.status
0000000000000000000000000000000000000000;;		if err := podutil.SetInitContainersStatusesAnnotations(pod); err != nil {
0000000000000000000000000000000000000000;;			glog.Error(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// TODO: handle conflict as a retry, make that easier too.
0000000000000000000000000000000000000000;;		newPod, err := m.kubeClient.Core().Pods(pod.Namespace).UpdateStatus(pod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Warningf("Failed to update status for pod %q: %v", format.Pod(pod), err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pod = newPod
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(3).Infof("Status for pod %q updated successfully: (%d, %+v)", format.Pod(pod), status.version, status.status)
0000000000000000000000000000000000000000;;		m.apiStatusVersions[pod.UID] = status.version
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// We don't handle graceful deletion of mirror pods.
0000000000000000000000000000000000000000;;		if m.canBeDeleted(pod, status.status) {
0000000000000000000000000000000000000000;;			deleteOptions := metav1.NewDeleteOptions(0)
0000000000000000000000000000000000000000;;			// Use the pod UID as the precondition for deletion to prevent deleting a newly created pod with the same name and namespace.
0000000000000000000000000000000000000000;;			deleteOptions.Preconditions = metav1.NewUIDPreconditions(string(pod.UID))
0000000000000000000000000000000000000000;;			err = m.kubeClient.Core().Pods(pod.Namespace).Delete(pod.Name, deleteOptions)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Warningf("Failed to delete status for pod %q: %v", format.Pod(pod), err)
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(3).Infof("Pod %q fully terminated and removed from etcd", format.Pod(pod))
0000000000000000000000000000000000000000;;			m.deletePodStatus(uid)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// needsUpdate returns whether the status is stale for the given pod UID.
0000000000000000000000000000000000000000;;	// This method is not thread safe, and most only be accessed by the sync thread.
0000000000000000000000000000000000000000;;	func (m *manager) needsUpdate(uid types.UID, status versionedPodStatus) bool {
0000000000000000000000000000000000000000;;		latest, ok := m.apiStatusVersions[uid]
0000000000000000000000000000000000000000;;		if !ok || latest < status.version {
0000000000000000000000000000000000000000;;			return true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pod, ok := m.podManager.GetPodByUID(uid)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return m.canBeDeleted(pod, status.status)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *manager) canBeDeleted(pod *v1.Pod, status v1.PodStatus) bool {
0000000000000000000000000000000000000000;;		if pod.DeletionTimestamp == nil || kubepod.IsMirrorPod(pod) {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return m.podDeletionSafety.PodResourcesAreReclaimed(pod, status)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// needsReconcile compares the given status with the status in the pod manager (which
0000000000000000000000000000000000000000;;	// in fact comes from apiserver), returns whether the status needs to be reconciled with
0000000000000000000000000000000000000000;;	// the apiserver. Now when pod status is inconsistent between apiserver and kubelet,
0000000000000000000000000000000000000000;;	// kubelet should forcibly send an update to reconcile the inconsistence, because kubelet
0000000000000000000000000000000000000000;;	// should be the source of truth of pod status.
0000000000000000000000000000000000000000;;	// NOTE(random-liu): It's simpler to pass in mirror pod uid and get mirror pod by uid, but
0000000000000000000000000000000000000000;;	// now the pod manager only supports getting mirror pod by static pod, so we have to pass
0000000000000000000000000000000000000000;;	// static pod uid here.
0000000000000000000000000000000000000000;;	// TODO(random-liu): Simplify the logic when mirror pod manager is added.
0000000000000000000000000000000000000000;;	func (m *manager) needsReconcile(uid types.UID, status v1.PodStatus) bool {
0000000000000000000000000000000000000000;;		// The pod could be a static pod, so we should translate first.
0000000000000000000000000000000000000000;;		pod, ok := m.podManager.GetPodByUID(uid)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Pod %q has been deleted, no need to reconcile", string(uid))
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// If the pod is a static pod, we should check its mirror pod, because only status in mirror pod is meaningful to us.
0000000000000000000000000000000000000000;;		if kubepod.IsStaticPod(pod) {
0000000000000000000000000000000000000000;;			mirrorPod, ok := m.podManager.GetMirrorPodByPod(pod)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Static pod %q has no corresponding mirror pod, no need to reconcile", format.Pod(pod))
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pod = mirrorPod
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podStatus, err := copyStatus(&pod.Status)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		normalizeStatus(pod, &podStatus)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if isStatusEqual(&podStatus, &status) {
0000000000000000000000000000000000000000;;			// If the status from the source is the same with the cached status,
0000000000000000000000000000000000000000;;			// reconcile is not needed. Just return.
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(3).Infof("Pod status is inconsistent with cached status for pod %q, a reconciliation should be triggered:\n %+v", format.Pod(pod),
0000000000000000000000000000000000000000;;			diff.ObjectDiff(podStatus, status))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// We add this function, because apiserver only supports *RFC3339* now, which means that the timestamp returned by
0000000000000000000000000000000000000000;;	// apiserver has no nanosecond information. However, the timestamp returned by metav1.Now() contains nanosecond,
0000000000000000000000000000000000000000;;	// so when we do comparison between status from apiserver and cached status, isStatusEqual() will always return false.
0000000000000000000000000000000000000000;;	// There is related issue #15262 and PR #15263 about this.
0000000000000000000000000000000000000000;;	// In fact, the best way to solve this is to do it on api side. However, for now, we normalize the status locally in
0000000000000000000000000000000000000000;;	// kubelet temporarily.
0000000000000000000000000000000000000000;;	// TODO(random-liu): Remove timestamp related logic after apiserver supports nanosecond or makes it consistent.
0000000000000000000000000000000000000000;;	func normalizeStatus(pod *v1.Pod, status *v1.PodStatus) *v1.PodStatus {
0000000000000000000000000000000000000000;;		bytesPerStatus := kubecontainer.MaxPodTerminationMessageLogLength
0000000000000000000000000000000000000000;;		if containers := len(pod.Spec.Containers) + len(pod.Spec.InitContainers); containers > 0 {
0000000000000000000000000000000000000000;;			bytesPerStatus = bytesPerStatus / containers
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		normalizeTimeStamp := func(t *metav1.Time) {
0000000000000000000000000000000000000000;;			*t = t.Rfc3339Copy()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		normalizeContainerState := func(c *v1.ContainerState) {
0000000000000000000000000000000000000000;;			if c.Running != nil {
0000000000000000000000000000000000000000;;				normalizeTimeStamp(&c.Running.StartedAt)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if c.Terminated != nil {
0000000000000000000000000000000000000000;;				normalizeTimeStamp(&c.Terminated.StartedAt)
0000000000000000000000000000000000000000;;				normalizeTimeStamp(&c.Terminated.FinishedAt)
0000000000000000000000000000000000000000;;				if len(c.Terminated.Message) > bytesPerStatus {
0000000000000000000000000000000000000000;;					c.Terminated.Message = c.Terminated.Message[:bytesPerStatus]
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if status.StartTime != nil {
0000000000000000000000000000000000000000;;			normalizeTimeStamp(status.StartTime)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := range status.Conditions {
0000000000000000000000000000000000000000;;			condition := &status.Conditions[i]
0000000000000000000000000000000000000000;;			normalizeTimeStamp(&condition.LastProbeTime)
0000000000000000000000000000000000000000;;			normalizeTimeStamp(&condition.LastTransitionTime)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// update container statuses
0000000000000000000000000000000000000000;;		for i := range status.ContainerStatuses {
0000000000000000000000000000000000000000;;			cstatus := &status.ContainerStatuses[i]
0000000000000000000000000000000000000000;;			normalizeContainerState(&cstatus.State)
0000000000000000000000000000000000000000;;			normalizeContainerState(&cstatus.LastTerminationState)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Sort the container statuses, so that the order won't affect the result of comparison
0000000000000000000000000000000000000000;;		sort.Sort(kubetypes.SortedContainerStatuses(status.ContainerStatuses))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// update init container statuses
0000000000000000000000000000000000000000;;		for i := range status.InitContainerStatuses {
0000000000000000000000000000000000000000;;			cstatus := &status.InitContainerStatuses[i]
0000000000000000000000000000000000000000;;			normalizeContainerState(&cstatus.State)
0000000000000000000000000000000000000000;;			normalizeContainerState(&cstatus.LastTerminationState)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Sort the container statuses, so that the order won't affect the result of comparison
0000000000000000000000000000000000000000;;		kubetypes.SortInitContainerStatuses(pod, status.InitContainerStatuses)
0000000000000000000000000000000000000000;;		return status
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func copyStatus(source *v1.PodStatus) (v1.PodStatus, error) {
0000000000000000000000000000000000000000;;		clone, err := api.Scheme.DeepCopy(source)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Failed to clone status %+v: %v", source, err)
0000000000000000000000000000000000000000;;			return v1.PodStatus{}, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		status := *clone.(*v1.PodStatus)
0000000000000000000000000000000000000000;;		return status, nil
0000000000000000000000000000000000000000;;	}
