0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
9fbdd758c00a160e902805405146a779d3acf5d8;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"crypto/tls"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"net"
0000000000000000000000000000000000000000;;		"net/http"
0000000000000000000000000000000000000000;;		"net/url"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"path"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"sync/atomic"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		clientgoclientset "k8s.io/client-go/kubernetes"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cadvisorapi "github.com/google/cadvisor/info/v1"
0000000000000000000000000000000000000000;;		cadvisorapiv2 "github.com/google/cadvisor/info/v2"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		clientv1 "k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/fields"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/clock"
0000000000000000000000000000000000000000;;		utilruntime "k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		utilfeature "k8s.io/apiserver/pkg/util/feature"
0000000000000000000000000000000000000000;;		v1core "k8s.io/client-go/kubernetes/typed/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/cache"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/flowcontrol"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/integer"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/cmd/kubelet/app/options"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/componentconfig"
0000000000000000000000000000000000000000;;		componentconfigv1alpha1 "k8s.io/kubernetes/pkg/apis/componentconfig/v1alpha1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
0000000000000000000000000000000000000000;;		corelisters "k8s.io/kubernetes/pkg/client/listers/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/cloudprovider"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/features"
0000000000000000000000000000000000000000;;		internalapi "k8s.io/kubernetes/pkg/kubelet/apis/cri"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/cadvisor"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/certificate"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/cm"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/config"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/configmap"
0000000000000000000000000000000000000000;;		kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/dockershim"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/dockershim/libdocker"
0000000000000000000000000000000000000000;;		dockerremote "k8s.io/kubernetes/pkg/kubelet/dockershim/remote"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/events"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/eviction"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/gpu"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/gpu/nvidia"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/images"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/kuberuntime"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/lifecycle"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/metrics"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/network"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/pleg"
0000000000000000000000000000000000000000;;		kubepod "k8s.io/kubernetes/pkg/kubelet/pod"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/preemption"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/prober"
0000000000000000000000000000000000000000;;		proberesults "k8s.io/kubernetes/pkg/kubelet/prober/results"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/remote"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/rkt"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/secret"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/server"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/server/stats"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/server/streaming"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/status"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/sysctl"
0000000000000000000000000000000000000000;;		kubetypes "k8s.io/kubernetes/pkg/kubelet/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/format"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/queue"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/sliceutils"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/volumemanager"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/security/apparmor"
0000000000000000000000000000000000000000;;		utildbus "k8s.io/kubernetes/pkg/util/dbus"
0000000000000000000000000000000000000000;;		utilexec "k8s.io/kubernetes/pkg/util/exec"
0000000000000000000000000000000000000000;;		kubeio "k8s.io/kubernetes/pkg/util/io"
0000000000000000000000000000000000000000;;		utilipt "k8s.io/kubernetes/pkg/util/iptables"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/mount"
0000000000000000000000000000000000000000;;		nodeutil "k8s.io/kubernetes/pkg/util/node"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/oom"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/volume"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm/predicates"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// Max amount of time to wait for the container runtime to come up.
0000000000000000000000000000000000000000;;		maxWaitForContainerRuntime = 30 * time.Second
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed.
0000000000000000000000000000000000000000;;		nodeStatusUpdateRetry = 5
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Location of container logs.
0000000000000000000000000000000000000000;;		ContainerLogsDir = "/var/log/containers"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// max backoff period, exported for the e2e test
0000000000000000000000000000000000000000;;		MaxContainerBackOff = 300 * time.Second
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Capacity of the channel for storing pods to kill. A small number should
0000000000000000000000000000000000000000;;		// suffice because a goroutine is dedicated to check the channel and does
0000000000000000000000000000000000000000;;		// not block on anything else.
0000000000000000000000000000000000000000;;		podKillingChannelCapacity = 50
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Period for performing global cleanup tasks.
0000000000000000000000000000000000000000;;		housekeepingPeriod = time.Second * 2
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Period for performing eviction monitoring.
0000000000000000000000000000000000000000;;		// TODO ensure this is in sync with internal cadvisor housekeeping.
0000000000000000000000000000000000000000;;		evictionMonitoringPeriod = time.Second * 10
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The path in containers' filesystems where the hosts file is mounted.
0000000000000000000000000000000000000000;;		etcHostsPath = "/etc/hosts"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Capacity of the channel for receiving pod lifecycle events. This number
0000000000000000000000000000000000000000;;		// is a bit arbitrary and may be adjusted in the future.
0000000000000000000000000000000000000000;;		plegChannelCapacity = 1000
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Generic PLEG relies on relisting for discovering container events.
0000000000000000000000000000000000000000;;		// A longer period means that kubelet will take longer to detect container
0000000000000000000000000000000000000000;;		// changes and to update pod status. On the other hand, a shorter period
0000000000000000000000000000000000000000;;		// will cause more frequent relisting (e.g., container runtime operations),
0000000000000000000000000000000000000000;;		// leading to higher cpu usage.
0000000000000000000000000000000000000000;;		// Note that even though we set the period to 1s, the relisting itself can
0000000000000000000000000000000000000000;;		// take more than 1s to finish if the container runtime responds slowly
0000000000000000000000000000000000000000;;		// and/or when there are many container changes in one cycle.
0000000000000000000000000000000000000000;;		plegRelistPeriod = time.Second * 1
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// backOffPeriod is the period to back off when pod syncing results in an
0000000000000000000000000000000000000000;;		// error. It is also used as the base period for the exponential backoff
0000000000000000000000000000000000000000;;		// container restarts and image pulls.
0000000000000000000000000000000000000000;;		backOffPeriod = time.Second * 10
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Period for performing container garbage collection.
0000000000000000000000000000000000000000;;		ContainerGCPeriod = time.Minute
0000000000000000000000000000000000000000;;		// Period for performing image garbage collection.
0000000000000000000000000000000000000000;;		ImageGCPeriod = 5 * time.Minute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Minimum number of dead containers to keep in a pod
0000000000000000000000000000000000000000;;		minDeadContainerInPod = 1
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// SyncHandler is an interface implemented by Kubelet, for testability
0000000000000000000000000000000000000000;;	type SyncHandler interface {
0000000000000000000000000000000000000000;;		HandlePodAdditions(pods []*v1.Pod)
0000000000000000000000000000000000000000;;		HandlePodUpdates(pods []*v1.Pod)
0000000000000000000000000000000000000000;;		HandlePodRemoves(pods []*v1.Pod)
0000000000000000000000000000000000000000;;		HandlePodReconcile(pods []*v1.Pod)
0000000000000000000000000000000000000000;;		HandlePodSyncs(pods []*v1.Pod)
0000000000000000000000000000000000000000;;		HandlePodCleanups() error
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Option is a functional option type for Kubelet
0000000000000000000000000000000000000000;;	type Option func(*Kubelet)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// bootstrapping interface for kubelet, targets the initialization protocol
0000000000000000000000000000000000000000;;	type KubeletBootstrap interface {
0000000000000000000000000000000000000000;;		GetConfiguration() componentconfig.KubeletConfiguration
0000000000000000000000000000000000000000;;		BirthCry()
0000000000000000000000000000000000000000;;		StartGarbageCollection()
0000000000000000000000000000000000000000;;		ListenAndServe(address net.IP, port uint, tlsOptions *server.TLSOptions, auth server.AuthInterface, enableDebuggingHandlers, enableContentionProfiling bool)
0000000000000000000000000000000000000000;;		ListenAndServeReadOnly(address net.IP, port uint)
0000000000000000000000000000000000000000;;		Run(<-chan kubetypes.PodUpdate)
0000000000000000000000000000000000000000;;		RunOnce(<-chan kubetypes.PodUpdate) ([]RunPodResult, error)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// create and initialize a Kubelet instance
0000000000000000000000000000000000000000;;	type KubeletBuilder func(kubeCfg *componentconfig.KubeletConfiguration, kubeDeps *KubeletDeps, crOptions *options.ContainerRuntimeOptions, standaloneMode bool, hostnameOverride, nodeIP, providerID string) (KubeletBootstrap, error)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// KubeletDeps is a bin for things we might consider "injected dependencies" -- objects constructed
0000000000000000000000000000000000000000;;	// at runtime that are necessary for running the Kubelet. This is a temporary solution for grouping
0000000000000000000000000000000000000000;;	// these objects while we figure out a more comprehensive dependency injection story for the Kubelet.
0000000000000000000000000000000000000000;;	type KubeletDeps struct {
0000000000000000000000000000000000000000;;		// TODO(mtaufen): KubeletBuilder:
0000000000000000000000000000000000000000;;		//                Mesos currently uses this as a hook to let them make their own call to
0000000000000000000000000000000000000000;;		//                let them wrap the KubeletBootstrap that CreateAndInitKubelet returns with
0000000000000000000000000000000000000000;;		//                their own KubeletBootstrap. It's a useful hook. I need to think about what
0000000000000000000000000000000000000000;;		//                a nice home for it would be. There seems to be a trend, between this and
0000000000000000000000000000000000000000;;		//                the Options fields below, of providing hooks where you can add extra functionality
0000000000000000000000000000000000000000;;		//                to the Kubelet for your solution. Maybe we should centralize these sorts of things?
0000000000000000000000000000000000000000;;		Builder KubeletBuilder
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO(mtaufen): ContainerRuntimeOptions and Options:
0000000000000000000000000000000000000000;;		//                Arrays of functions that can do arbitrary things to the Kubelet and the Runtime
0000000000000000000000000000000000000000;;		//                seem like a difficult path to trace when it's time to debug something.
0000000000000000000000000000000000000000;;		//                I'm leaving these fields here for now, but there is likely an easier-to-follow
0000000000000000000000000000000000000000;;		//                way to support their intended use cases. E.g. ContainerRuntimeOptions
0000000000000000000000000000000000000000;;		//                is used by Mesos to set an environment variable in containers which has
0000000000000000000000000000000000000000;;		//                some connection to their container GC. It seems that Mesos intends to use
0000000000000000000000000000000000000000;;		//                Options to add additional node conditions that are updated as part of the
0000000000000000000000000000000000000000;;		//                Kubelet lifecycle (see https://github.com/kubernetes/kubernetes/pull/21521).
0000000000000000000000000000000000000000;;		//                We should think about providing more explicit ways of doing these things.
0000000000000000000000000000000000000000;;		ContainerRuntimeOptions []kubecontainer.Option
0000000000000000000000000000000000000000;;		Options                 []Option
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Injected Dependencies
0000000000000000000000000000000000000000;;		Auth               server.AuthInterface
0000000000000000000000000000000000000000;;		CAdvisorInterface  cadvisor.Interface
0000000000000000000000000000000000000000;;		Cloud              cloudprovider.Interface
0000000000000000000000000000000000000000;;		ContainerManager   cm.ContainerManager
0000000000000000000000000000000000000000;;		DockerClient       libdocker.Interface
0000000000000000000000000000000000000000;;		EventClient        v1core.EventsGetter
0000000000000000000000000000000000000000;;		KubeClient         clientset.Interface
0000000000000000000000000000000000000000;;		ExternalKubeClient clientgoclientset.Interface
0000000000000000000000000000000000000000;;		Mounter            mount.Interface
0000000000000000000000000000000000000000;;		NetworkPlugins     []network.NetworkPlugin
0000000000000000000000000000000000000000;;		OOMAdjuster        *oom.OOMAdjuster
0000000000000000000000000000000000000000;;		OSInterface        kubecontainer.OSInterface
0000000000000000000000000000000000000000;;		PodConfig          *config.PodConfig
0000000000000000000000000000000000000000;;		Recorder           record.EventRecorder
0000000000000000000000000000000000000000;;		Writer             kubeio.Writer
0000000000000000000000000000000000000000;;		VolumePlugins      []volume.VolumePlugin
0000000000000000000000000000000000000000;;		TLSOptions         *server.TLSOptions
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// makePodSourceConfig creates a config.PodConfig from the given
0000000000000000000000000000000000000000;;	// KubeletConfiguration or returns an error.
0000000000000000000000000000000000000000;;	func makePodSourceConfig(kubeCfg *componentconfig.KubeletConfiguration, kubeDeps *KubeletDeps, nodeName types.NodeName) (*config.PodConfig, error) {
0000000000000000000000000000000000000000;;		manifestURLHeader := make(http.Header)
0000000000000000000000000000000000000000;;		if kubeCfg.ManifestURLHeader != "" {
0000000000000000000000000000000000000000;;			pieces := strings.Split(kubeCfg.ManifestURLHeader, ":")
0000000000000000000000000000000000000000;;			if len(pieces) != 2 {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("manifest-url-header must have a single ':' key-value separator, got %q", kubeCfg.ManifestURLHeader)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			manifestURLHeader.Set(pieces[0], pieces[1])
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// source of all configuration
0000000000000000000000000000000000000000;;		cfg := config.NewPodConfig(config.PodConfigNotificationIncremental, kubeDeps.Recorder)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// define file config source
0000000000000000000000000000000000000000;;		if kubeCfg.PodManifestPath != "" {
0000000000000000000000000000000000000000;;			glog.Infof("Adding manifest file: %v", kubeCfg.PodManifestPath)
0000000000000000000000000000000000000000;;			config.NewSourceFile(kubeCfg.PodManifestPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(kubetypes.FileSource))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// define url config source
0000000000000000000000000000000000000000;;		if kubeCfg.ManifestURL != "" {
0000000000000000000000000000000000000000;;			glog.Infof("Adding manifest url %q with HTTP header %v", kubeCfg.ManifestURL, manifestURLHeader)
0000000000000000000000000000000000000000;;			config.NewSourceURL(kubeCfg.ManifestURL, manifestURLHeader, nodeName, kubeCfg.HTTPCheckFrequency.Duration, cfg.Channel(kubetypes.HTTPSource))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kubeDeps.KubeClient != nil {
0000000000000000000000000000000000000000;;			glog.Infof("Watching apiserver")
0000000000000000000000000000000000000000;;			config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, cfg.Channel(kubetypes.ApiserverSource))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return cfg, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getRuntimeAndImageServices(config *componentconfig.KubeletConfiguration) (internalapi.RuntimeService, internalapi.ImageManagerService, error) {
0000000000000000000000000000000000000000;;		rs, err := remote.NewRemoteRuntimeService(config.RemoteRuntimeEndpoint, config.RuntimeRequestTimeout.Duration)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		is, err := remote.NewRemoteImageService(config.RemoteImageEndpoint, config.RuntimeRequestTimeout.Duration)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return rs, is, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewMainKubelet instantiates a new Kubelet object along with all the required internal modules.
0000000000000000000000000000000000000000;;	// No initialization of Kubelet and its modules should happen here.
0000000000000000000000000000000000000000;;	func NewMainKubelet(kubeCfg *componentconfig.KubeletConfiguration, kubeDeps *KubeletDeps, crOptions *options.ContainerRuntimeOptions, standaloneMode bool, hostnameOverride, nodeIP, providerID string) (*Kubelet, error) {
0000000000000000000000000000000000000000;;		if kubeCfg.RootDirectory == "" {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("invalid root directory %q", kubeCfg.RootDirectory)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kubeCfg.SyncFrequency.Duration <= 0 {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("invalid sync frequency %d", kubeCfg.SyncFrequency.Duration)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kubeCfg.MakeIPTablesUtilChains {
0000000000000000000000000000000000000000;;			if kubeCfg.IPTablesMasqueradeBit > 31 || kubeCfg.IPTablesMasqueradeBit < 0 {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("iptables-masquerade-bit is not valid. Must be within [0, 31]")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if kubeCfg.IPTablesDropBit > 31 || kubeCfg.IPTablesDropBit < 0 {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("iptables-drop-bit is not valid. Must be within [0, 31]")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if kubeCfg.IPTablesDropBit == kubeCfg.IPTablesMasqueradeBit {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("iptables-masquerade-bit and iptables-drop-bit must be different")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		hostname := nodeutil.GetHostname(hostnameOverride)
0000000000000000000000000000000000000000;;		// Query the cloud provider for our node name, default to hostname
0000000000000000000000000000000000000000;;		nodeName := types.NodeName(hostname)
0000000000000000000000000000000000000000;;		cloudIPs := []net.IP{}
0000000000000000000000000000000000000000;;		cloudNames := []string{}
0000000000000000000000000000000000000000;;		if kubeDeps.Cloud != nil {
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			instances, ok := kubeDeps.Cloud.Instances()
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("failed to get instances from cloud provider")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodeName, err = instances.CurrentNodeName(hostname)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("error fetching current instance name from cloud provider: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			glog.V(2).Infof("cloud provider determined current node name to be %s", nodeName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if utilfeature.DefaultFeatureGate.Enabled(features.RotateKubeletServerCertificate) {
0000000000000000000000000000000000000000;;				nodeAddresses, err := instances.NodeAddresses(nodeName)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return nil, fmt.Errorf("failed to get the addresses of the current instance from the cloud provider: %v", err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				for _, nodeAddress := range nodeAddresses {
0000000000000000000000000000000000000000;;					switch nodeAddress.Type {
0000000000000000000000000000000000000000;;					case v1.NodeExternalIP, v1.NodeInternalIP:
0000000000000000000000000000000000000000;;						ip := net.ParseIP(nodeAddress.Address)
0000000000000000000000000000000000000000;;						if ip != nil && !ip.IsLoopback() {
0000000000000000000000000000000000000000;;							cloudIPs = append(cloudIPs, ip)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					case v1.NodeExternalDNS, v1.NodeInternalDNS, v1.NodeHostName:
0000000000000000000000000000000000000000;;						cloudNames = append(cloudNames, nodeAddress.Address)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kubeDeps.PodConfig == nil {
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		containerGCPolicy := kubecontainer.ContainerGCPolicy{
0000000000000000000000000000000000000000;;			MinAge:             kubeCfg.MinimumGCAge.Duration,
0000000000000000000000000000000000000000;;			MaxPerPodContainer: int(kubeCfg.MaxPerPodContainerCount),
0000000000000000000000000000000000000000;;			MaxContainers:      int(kubeCfg.MaxContainerCount),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		daemonEndpoints := &v1.NodeDaemonEndpoints{
0000000000000000000000000000000000000000;;			KubeletEndpoint: v1.DaemonEndpoint{Port: kubeCfg.Port},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		imageGCPolicy := images.ImageGCPolicy{
0000000000000000000000000000000000000000;;			MinAge:               kubeCfg.ImageMinimumGCAge.Duration,
0000000000000000000000000000000000000000;;			HighThresholdPercent: int(kubeCfg.ImageGCHighThresholdPercent),
0000000000000000000000000000000000000000;;			LowThresholdPercent:  int(kubeCfg.ImageGCLowThresholdPercent),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		diskSpacePolicy := DiskSpacePolicy{
0000000000000000000000000000000000000000;;			DockerFreeDiskMB: int(kubeCfg.LowDiskSpaceThresholdMB),
0000000000000000000000000000000000000000;;			RootFreeDiskMB:   int(kubeCfg.LowDiskSpaceThresholdMB),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		enforceNodeAllocatable := kubeCfg.EnforceNodeAllocatable
0000000000000000000000000000000000000000;;		if kubeCfg.ExperimentalNodeAllocatableIgnoreEvictionThreshold {
0000000000000000000000000000000000000000;;			// Do not provide kubeCfg.EnforceNodeAllocatable to eviction threshold parsing if we are not enforcing Evictions
0000000000000000000000000000000000000000;;			enforceNodeAllocatable = []string{}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		thresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		evictionConfig := eviction.Config{
0000000000000000000000000000000000000000;;			PressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,
0000000000000000000000000000000000000000;;			MaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),
0000000000000000000000000000000000000000;;			Thresholds:               thresholds,
0000000000000000000000000000000000000000;;			KernelMemcgNotification:  kubeCfg.ExperimentalKernelMemcgNotification,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})
0000000000000000000000000000000000000000;;		if kubeDeps.KubeClient != nil {
0000000000000000000000000000000000000000;;			serviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.Core().RESTClient(), "services", metav1.NamespaceAll, fields.Everything())
0000000000000000000000000000000000000000;;			cache.NewReflector(serviceLW, &v1.Service{}, serviceIndexer, 0).Run()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		serviceLister := corelisters.NewServiceLister(serviceIndexer)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{})
0000000000000000000000000000000000000000;;		if kubeDeps.KubeClient != nil {
0000000000000000000000000000000000000000;;			fieldSelector := fields.Set{api.ObjectNameField: string(nodeName)}.AsSelector()
0000000000000000000000000000000000000000;;			nodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.Core().RESTClient(), "nodes", metav1.NamespaceAll, fieldSelector)
0000000000000000000000000000000000000000;;			cache.NewReflector(nodeLW, &v1.Node{}, nodeIndexer, 0).Run()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		nodeInfo := &predicates.CachedNodeInfo{NodeLister: corelisters.NewNodeLister(nodeIndexer)}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: get the real node object of ourself,
0000000000000000000000000000000000000000;;		// and use the real node name and UID.
0000000000000000000000000000000000000000;;		// TODO: what is namespace for node?
0000000000000000000000000000000000000000;;		nodeRef := &clientv1.ObjectReference{
0000000000000000000000000000000000000000;;			Kind:      "Node",
0000000000000000000000000000000000000000;;			Name:      string(nodeName),
0000000000000000000000000000000000000000;;			UID:       types.UID(nodeName),
0000000000000000000000000000000000000000;;			Namespace: "",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		diskSpaceManager, err := newDiskSpaceManager(kubeDeps.CAdvisorInterface, diskSpacePolicy)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed to initialize disk manager: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		containerRefManager := kubecontainer.NewRefManager()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		oomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		clusterDNS := make([]net.IP, 0, len(kubeCfg.ClusterDNS))
0000000000000000000000000000000000000000;;		for _, ipEntry := range kubeCfg.ClusterDNS {
0000000000000000000000000000000000000000;;			ip := net.ParseIP(ipEntry)
0000000000000000000000000000000000000000;;			if ip == nil {
0000000000000000000000000000000000000000;;				glog.Warningf("Invalid clusterDNS ip '%q'", ipEntry)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				clusterDNS = append(clusterDNS, ip)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		httpClient := &http.Client{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		klet := &Kubelet{
0000000000000000000000000000000000000000;;			hostname:                       hostname,
0000000000000000000000000000000000000000;;			nodeName:                       nodeName,
0000000000000000000000000000000000000000;;			kubeClient:                     kubeDeps.KubeClient,
0000000000000000000000000000000000000000;;			rootDirectory:                  kubeCfg.RootDirectory,
0000000000000000000000000000000000000000;;			resyncInterval:                 kubeCfg.SyncFrequency.Duration,
0000000000000000000000000000000000000000;;			sourcesReady:                   config.NewSourcesReady(kubeDeps.PodConfig.SeenAllSources),
0000000000000000000000000000000000000000;;			registerNode:                   kubeCfg.RegisterNode,
0000000000000000000000000000000000000000;;			registerSchedulable:            kubeCfg.RegisterSchedulable,
0000000000000000000000000000000000000000;;			standaloneMode:                 standaloneMode,
0000000000000000000000000000000000000000;;			clusterDomain:                  kubeCfg.ClusterDomain,
0000000000000000000000000000000000000000;;			clusterDNS:                     clusterDNS,
0000000000000000000000000000000000000000;;			serviceLister:                  serviceLister,
0000000000000000000000000000000000000000;;			nodeInfo:                       nodeInfo,
0000000000000000000000000000000000000000;;			masterServiceNamespace:         kubeCfg.MasterServiceNamespace,
0000000000000000000000000000000000000000;;			streamingConnectionIdleTimeout: kubeCfg.StreamingConnectionIdleTimeout.Duration,
0000000000000000000000000000000000000000;;			recorder:                       kubeDeps.Recorder,
0000000000000000000000000000000000000000;;			cadvisor:                       kubeDeps.CAdvisorInterface,
0000000000000000000000000000000000000000;;			diskSpaceManager:               diskSpaceManager,
0000000000000000000000000000000000000000;;			cloud:                          kubeDeps.Cloud,
0000000000000000000000000000000000000000;;			autoDetectCloudProvider:   (componentconfigv1alpha1.AutoDetectCloudProvider == kubeCfg.CloudProvider),
0000000000000000000000000000000000000000;;			externalCloudProvider:     cloudprovider.IsExternal(kubeCfg.CloudProvider),
0000000000000000000000000000000000000000;;			providerID:                providerID,
0000000000000000000000000000000000000000;;			nodeRef:                   nodeRef,
0000000000000000000000000000000000000000;;			nodeLabels:                kubeCfg.NodeLabels,
0000000000000000000000000000000000000000;;			nodeStatusUpdateFrequency: kubeCfg.NodeStatusUpdateFrequency.Duration,
0000000000000000000000000000000000000000;;			os:               kubeDeps.OSInterface,
0000000000000000000000000000000000000000;;			oomWatcher:       oomWatcher,
0000000000000000000000000000000000000000;;			cgroupsPerQOS:    kubeCfg.CgroupsPerQOS,
0000000000000000000000000000000000000000;;			cgroupRoot:       kubeCfg.CgroupRoot,
0000000000000000000000000000000000000000;;			mounter:          kubeDeps.Mounter,
0000000000000000000000000000000000000000;;			writer:           kubeDeps.Writer,
0000000000000000000000000000000000000000;;			maxPods:          int(kubeCfg.MaxPods),
0000000000000000000000000000000000000000;;			podsPerCore:      int(kubeCfg.PodsPerCore),
0000000000000000000000000000000000000000;;			syncLoopMonitor:  atomic.Value{},
0000000000000000000000000000000000000000;;			resolverConfig:   kubeCfg.ResolverConfig,
0000000000000000000000000000000000000000;;			daemonEndpoints:  daemonEndpoints,
0000000000000000000000000000000000000000;;			containerManager: kubeDeps.ContainerManager,
0000000000000000000000000000000000000000;;			nodeIP:           net.ParseIP(nodeIP),
0000000000000000000000000000000000000000;;			clock:            clock.RealClock{},
0000000000000000000000000000000000000000;;			outOfDiskTransitionFrequency:            kubeCfg.OutOfDiskTransitionFrequency.Duration,
0000000000000000000000000000000000000000;;			enableControllerAttachDetach:            kubeCfg.EnableControllerAttachDetach,
0000000000000000000000000000000000000000;;			iptClient:                               utilipt.New(utilexec.New(), utildbus.New(), utilipt.ProtocolIpv4),
0000000000000000000000000000000000000000;;			makeIPTablesUtilChains:                  kubeCfg.MakeIPTablesUtilChains,
0000000000000000000000000000000000000000;;			iptablesMasqueradeBit:                   int(kubeCfg.IPTablesMasqueradeBit),
0000000000000000000000000000000000000000;;			iptablesDropBit:                         int(kubeCfg.IPTablesDropBit),
0000000000000000000000000000000000000000;;			experimentalHostUserNamespaceDefaulting: utilfeature.DefaultFeatureGate.Enabled(features.ExperimentalHostUserNamespaceDefaultingGate),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		secretManager := secret.NewCachingSecretManager(
0000000000000000000000000000000000000000;;			kubeDeps.KubeClient, secret.GetObjectTTLFromNodeFunc(klet.GetNode))
0000000000000000000000000000000000000000;;		klet.secretManager = secretManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		configMapManager := configmap.NewCachingConfigMapManager(
0000000000000000000000000000000000000000;;			kubeDeps.KubeClient, configmap.GetObjectTTLFromNodeFunc(klet.GetNode))
0000000000000000000000000000000000000000;;		klet.configMapManager = configMapManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if klet.experimentalHostUserNamespaceDefaulting {
0000000000000000000000000000000000000000;;			glog.Infof("Experimental host user namespace defaulting is enabled.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		hairpinMode, err := effectiveHairpinMode(componentconfig.HairpinMode(kubeCfg.HairpinMode), kubeCfg.ContainerRuntime, crOptions.NetworkPluginName)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			// This is a non-recoverable error. Returning it up the callstack will just
0000000000000000000000000000000000000000;;			// lead to retries of the same failure, so just fail hard.
0000000000000000000000000000000000000000;;			glog.Fatalf("Invalid hairpin mode: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.Infof("Hairpin mode set to %q", hairpinMode)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO(#36485) Remove this workaround once we fix the init-container issue.
0000000000000000000000000000000000000000;;		// Touch iptables lock file, which will be shared among all processes accessing
0000000000000000000000000000000000000000;;		// the iptables.
0000000000000000000000000000000000000000;;		f, err := os.OpenFile(utilipt.LockfilePath16x, os.O_CREATE, 0600)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Warningf("Failed to open iptables lock file: %v", err)
0000000000000000000000000000000000000000;;		} else if err = f.Close(); err != nil {
0000000000000000000000000000000000000000;;			glog.Warningf("Failed to close iptables lock file: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if plug, err := network.InitNetworkPlugin(kubeDeps.NetworkPlugins, crOptions.NetworkPluginName, &criNetworkHost{&networkHost{klet}, &network.NoopPortMappingGetter{}}, hairpinMode, kubeCfg.NonMasqueradeCIDR, int(crOptions.NetworkPluginMTU)); err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			klet.networkPlugin = plug
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		machineInfo, err := klet.GetCachedMachineInfo()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		imageBackOff := flowcontrol.NewBackOff(backOffPeriod, MaxContainerBackOff)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		klet.livenessManager = proberesults.NewManager()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		klet.podCache = kubecontainer.NewCache()
0000000000000000000000000000000000000000;;		// podManager is also responsible for keeping secretManager and configMapManager contents up-to-date.
0000000000000000000000000000000000000000;;		klet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kubeCfg.RemoteRuntimeEndpoint != "" {
0000000000000000000000000000000000000000;;			// kubeCfg.RemoteImageEndpoint is same as kubeCfg.RemoteRuntimeEndpoint if not explicitly specified
0000000000000000000000000000000000000000;;			if kubeCfg.RemoteImageEndpoint == "" {
0000000000000000000000000000000000000000;;				kubeCfg.RemoteImageEndpoint = kubeCfg.RemoteRuntimeEndpoint
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: These need to become arguments to a standalone docker shim.
0000000000000000000000000000000000000000;;		binDir := crOptions.CNIBinDir
0000000000000000000000000000000000000000;;		if binDir == "" {
0000000000000000000000000000000000000000;;			binDir = crOptions.NetworkPluginDir
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pluginSettings := dockershim.NetworkPluginSettings{
0000000000000000000000000000000000000000;;			HairpinMode:       hairpinMode,
0000000000000000000000000000000000000000;;			NonMasqueradeCIDR: kubeCfg.NonMasqueradeCIDR,
0000000000000000000000000000000000000000;;			PluginName:        crOptions.NetworkPluginName,
0000000000000000000000000000000000000000;;			PluginConfDir:     crOptions.CNIConfDir,
0000000000000000000000000000000000000000;;			PluginBinDir:      binDir,
0000000000000000000000000000000000000000;;			MTU:               int(crOptions.NetworkPluginMTU),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Remote runtime shim just cannot talk back to kubelet, so it doesn't
0000000000000000000000000000000000000000;;		// support bandwidth shaping or hostports till #35457. To enable legacy
0000000000000000000000000000000000000000;;		// features, replace with networkHost.
0000000000000000000000000000000000000000;;		var nl *NoOpLegacyHost
0000000000000000000000000000000000000000;;		pluginSettings.LegacyRuntimeHost = nl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// rktnetes cannot be run with CRI.
0000000000000000000000000000000000000000;;		if kubeCfg.ContainerRuntime != kubetypes.RktContainerRuntime {
0000000000000000000000000000000000000000;;			// kubelet defers to the runtime shim to setup networking. Setting
0000000000000000000000000000000000000000;;			// this to nil will prevent it from trying to invoke the plugin.
0000000000000000000000000000000000000000;;			// It's easier to always probe and initialize plugins till cri
0000000000000000000000000000000000000000;;			// becomes the default.
0000000000000000000000000000000000000000;;			klet.networkPlugin = nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			switch kubeCfg.ContainerRuntime {
0000000000000000000000000000000000000000;;			case kubetypes.DockerContainerRuntime:
0000000000000000000000000000000000000000;;				// Create and start the CRI shim running as a grpc server.
0000000000000000000000000000000000000000;;				streamingConfig := getStreamingConfig(kubeCfg, kubeDeps)
0000000000000000000000000000000000000000;;				ds, err := dockershim.NewDockerService(kubeDeps.DockerClient, kubeCfg.SeccompProfileRoot, crOptions.PodSandboxImage,
0000000000000000000000000000000000000000;;					streamingConfig, &pluginSettings, kubeCfg.RuntimeCgroups, kubeCfg.CgroupDriver, crOptions.DockerExecHandlerName,
0000000000000000000000000000000000000000;;					crOptions.DockershimRootDirectory, crOptions.DockerDisableSharedPID)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if err := ds.Start(); err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// For now, the CRI shim redirects the streaming requests to the
0000000000000000000000000000000000000000;;				// kubelet, which handles the requests using DockerService..
0000000000000000000000000000000000000000;;				klet.criHandler = ds
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// The unix socket for kubelet <-> dockershim communication.
0000000000000000000000000000000000000000;;				glog.V(5).Infof("RemoteRuntimeEndpoint: %q, RemoteImageEndpoint: %q",
0000000000000000000000000000000000000000;;					kubeCfg.RemoteRuntimeEndpoint,
0000000000000000000000000000000000000000;;					kubeCfg.RemoteImageEndpoint)
0000000000000000000000000000000000000000;;				glog.V(2).Infof("Starting the GRPC server for the docker CRI shim.")
0000000000000000000000000000000000000000;;				server := dockerremote.NewDockerServer(kubeCfg.RemoteRuntimeEndpoint, ds)
0000000000000000000000000000000000000000;;				if err := server.Start(); err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Create dockerLegacyService when the logging driver is not supported.
0000000000000000000000000000000000000000;;				supported, err := dockershim.IsCRISupportedLogDriver(kubeDeps.DockerClient)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if !supported {
0000000000000000000000000000000000000000;;					klet.dockerLegacyService = dockershim.NewDockerLegacyService(kubeDeps.DockerClient)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case kubetypes.RemoteContainerRuntime:
0000000000000000000000000000000000000000;;				// No-op.
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("unsupported CRI runtime: %q", kubeCfg.ContainerRuntime)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			runtimeService, imageService, err := getRuntimeAndImageServices(kubeCfg)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			runtime, err := kuberuntime.NewKubeGenericRuntimeManager(
0000000000000000000000000000000000000000;;				kubecontainer.FilterEventRecorder(kubeDeps.Recorder),
0000000000000000000000000000000000000000;;				klet.livenessManager,
0000000000000000000000000000000000000000;;				containerRefManager,
0000000000000000000000000000000000000000;;				machineInfo,
0000000000000000000000000000000000000000;;				klet.podManager,
0000000000000000000000000000000000000000;;				kubeDeps.OSInterface,
0000000000000000000000000000000000000000;;				klet,
0000000000000000000000000000000000000000;;				httpClient,
0000000000000000000000000000000000000000;;				imageBackOff,
0000000000000000000000000000000000000000;;				kubeCfg.SerializeImagePulls,
0000000000000000000000000000000000000000;;				float32(kubeCfg.RegistryPullQPS),
0000000000000000000000000000000000000000;;				int(kubeCfg.RegistryBurst),
0000000000000000000000000000000000000000;;				kubeCfg.CPUCFSQuota,
0000000000000000000000000000000000000000;;				runtimeService,
0000000000000000000000000000000000000000;;				imageService,
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			klet.containerRuntime = runtime
0000000000000000000000000000000000000000;;			klet.runner = runtime
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// rkt uses the legacy, non-CRI, integration. Configure it the old way.
0000000000000000000000000000000000000000;;			// TODO: Include hairpin mode settings in rkt?
0000000000000000000000000000000000000000;;			conf := &rkt.Config{
0000000000000000000000000000000000000000;;				Path:            crOptions.RktPath,
0000000000000000000000000000000000000000;;				Stage1Image:     crOptions.RktStage1Image,
0000000000000000000000000000000000000000;;				InsecureOptions: "image,ondisk",
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			runtime, err := rkt.New(
0000000000000000000000000000000000000000;;				crOptions.RktAPIEndpoint,
0000000000000000000000000000000000000000;;				conf,
0000000000000000000000000000000000000000;;				klet,
0000000000000000000000000000000000000000;;				kubeDeps.Recorder,
0000000000000000000000000000000000000000;;				containerRefManager,
0000000000000000000000000000000000000000;;				klet.podManager,
0000000000000000000000000000000000000000;;				klet.livenessManager,
0000000000000000000000000000000000000000;;				httpClient,
0000000000000000000000000000000000000000;;				klet.networkPlugin,
0000000000000000000000000000000000000000;;				hairpinMode == componentconfig.HairpinVeth,
0000000000000000000000000000000000000000;;				utilexec.New(),
0000000000000000000000000000000000000000;;				kubecontainer.RealOS{},
0000000000000000000000000000000000000000;;				imageBackOff,
0000000000000000000000000000000000000000;;				kubeCfg.SerializeImagePulls,
0000000000000000000000000000000000000000;;				float32(kubeCfg.RegistryPullQPS),
0000000000000000000000000000000000000000;;				int(kubeCfg.RegistryBurst),
0000000000000000000000000000000000000000;;				kubeCfg.RuntimeRequestTimeout.Duration,
0000000000000000000000000000000000000000;;			)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			klet.containerRuntime = runtime
0000000000000000000000000000000000000000;;			klet.runner = kubecontainer.DirectStreamingRunner(runtime)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Factor out "StatsProvider" from Kubelet so we don't have a cyclic dependency
0000000000000000000000000000000000000000;;		klet.resourceAnalyzer = stats.NewResourceAnalyzer(klet, kubeCfg.VolumeStatsAggPeriod.Duration, klet.containerRuntime)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock{})
0000000000000000000000000000000000000000;;		klet.runtimeState = newRuntimeState(maxWaitForContainerRuntime)
0000000000000000000000000000000000000000;;		klet.runtimeState.addHealthCheck("PLEG", klet.pleg.Healthy)
0000000000000000000000000000000000000000;;		klet.updatePodCIDR(kubeCfg.PodCIDR)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// setup containerGC
0000000000000000000000000000000000000000;;		containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		klet.containerGC = containerGC
0000000000000000000000000000000000000000;;		klet.containerDeletor = newPodContainerDeletor(klet.containerRuntime, integer.IntMax(containerGCPolicy.MaxPerPodContainer, minDeadContainerInPod))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// setup imageManager
0000000000000000000000000000000000000000;;		imageManager, err := images.NewImageGCManager(klet.containerRuntime, kubeDeps.CAdvisorInterface, kubeDeps.Recorder, nodeRef, imageGCPolicy)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed to initialize image manager: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		klet.imageManager = imageManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if utilfeature.DefaultFeatureGate.Enabled(features.RotateKubeletServerCertificate) && kubeDeps.TLSOptions != nil {
0000000000000000000000000000000000000000;;			var ips []net.IP
0000000000000000000000000000000000000000;;			cfgAddress := net.ParseIP(kubeCfg.Address)
0000000000000000000000000000000000000000;;			if cfgAddress == nil || cfgAddress.IsUnspecified() {
0000000000000000000000000000000000000000;;				if localIPs, err := allLocalIPsWithoutLoopback(); err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					ips = localIPs
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				ips = []net.IP{cfgAddress}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			ips = append(ips, cloudIPs...)
0000000000000000000000000000000000000000;;			names := append([]string{klet.GetHostname(), hostnameOverride}, cloudNames...)
0000000000000000000000000000000000000000;;			klet.serverCertificateManager, err = certificate.NewKubeletServerCertificateManager(klet.kubeClient, kubeCfg, klet.nodeName, ips, names)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("failed to initialize certificate manager: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			kubeDeps.TLSOptions.Config.GetCertificate = func(*tls.ClientHelloInfo) (*tls.Certificate, error) {
0000000000000000000000000000000000000000;;				cert := klet.serverCertificateManager.Current()
0000000000000000000000000000000000000000;;				if cert == nil {
0000000000000000000000000000000000000000;;					return nil, fmt.Errorf("no certificate available")
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return cert, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		klet.probeManager = prober.NewManager(
0000000000000000000000000000000000000000;;			klet.statusManager,
0000000000000000000000000000000000000000;;			klet.livenessManager,
0000000000000000000000000000000000000000;;			klet.runner,
0000000000000000000000000000000000000000;;			containerRefManager,
0000000000000000000000000000000000000000;;			kubeDeps.Recorder)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		klet.volumePluginMgr, err =
0000000000000000000000000000000000000000;;			NewInitializedVolumePluginMgr(klet, secretManager, configMapManager, kubeDeps.VolumePlugins)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If the experimentalMounterPathFlag is set, we do not want to
0000000000000000000000000000000000000000;;		// check node capabilities since the mount path is not the default
0000000000000000000000000000000000000000;;		if len(kubeCfg.ExperimentalMounterPath) != 0 {
0000000000000000000000000000000000000000;;			kubeCfg.ExperimentalCheckNodeCapabilitiesBeforeMount = false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// setup volumeManager
0000000000000000000000000000000000000000;;		klet.volumeManager = volumemanager.NewVolumeManager(
0000000000000000000000000000000000000000;;			kubeCfg.EnableControllerAttachDetach,
0000000000000000000000000000000000000000;;			nodeName,
0000000000000000000000000000000000000000;;			klet.podManager,
0000000000000000000000000000000000000000;;			klet.statusManager,
0000000000000000000000000000000000000000;;			klet.kubeClient,
0000000000000000000000000000000000000000;;			klet.volumePluginMgr,
0000000000000000000000000000000000000000;;			klet.containerRuntime,
0000000000000000000000000000000000000000;;			kubeDeps.Mounter,
0000000000000000000000000000000000000000;;			klet.getPodsDir(),
0000000000000000000000000000000000000000;;			kubeDeps.Recorder,
0000000000000000000000000000000000000000;;			kubeCfg.ExperimentalCheckNodeCapabilitiesBeforeMount,
0000000000000000000000000000000000000000;;			kubeCfg.KeepTerminatedPodVolumes)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		runtimeCache, err := kubecontainer.NewRuntimeCache(klet.containerRuntime)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		klet.runtimeCache = runtimeCache
0000000000000000000000000000000000000000;;		klet.reasonCache = NewReasonCache()
0000000000000000000000000000000000000000;;		klet.workQueue = queue.NewBasicWorkQueue(klet.clock)
0000000000000000000000000000000000000000;;		klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		klet.backOff = flowcontrol.NewBackOff(backOffPeriod, MaxContainerBackOff)
0000000000000000000000000000000000000000;;		klet.podKillingCh = make(chan *kubecontainer.PodPair, podKillingChannelCapacity)
0000000000000000000000000000000000000000;;		klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// setup eviction manager
0000000000000000000000000000000000000000;;		evictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		klet.evictionManager = evictionManager
0000000000000000000000000000000000000000;;		klet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// add sysctl admission
0000000000000000000000000000000000000000;;		runtimeSupport, err := sysctl.NewRuntimeAdmitHandler(klet.containerRuntime)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		safeWhitelist, err := sysctl.NewWhitelist(sysctl.SafeSysctlWhitelist(), v1.SysctlsPodAnnotationKey)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Safe, whitelisted sysctls can always be used as unsafe sysctls in the spec
0000000000000000000000000000000000000000;;		// Hence, we concatenate those two lists.
0000000000000000000000000000000000000000;;		safeAndUnsafeSysctls := append(sysctl.SafeSysctlWhitelist(), kubeCfg.AllowedUnsafeSysctls...)
0000000000000000000000000000000000000000;;		unsafeWhitelist, err := sysctl.NewWhitelist(safeAndUnsafeSysctls, v1.UnsafeSysctlsPodAnnotationKey)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		klet.admitHandlers.AddPodAdmitHandler(runtimeSupport)
0000000000000000000000000000000000000000;;		klet.admitHandlers.AddPodAdmitHandler(safeWhitelist)
0000000000000000000000000000000000000000;;		klet.admitHandlers.AddPodAdmitHandler(unsafeWhitelist)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// enable active deadline handler
0000000000000000000000000000000000000000;;		activeDeadlineHandler, err := newActiveDeadlineHandler(klet.statusManager, kubeDeps.Recorder, klet.clock)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		klet.AddPodSyncLoopHandler(activeDeadlineHandler)
0000000000000000000000000000000000000000;;		klet.AddPodSyncHandler(activeDeadlineHandler)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		criticalPodAdmissionHandler := preemption.NewCriticalPodAdmissionHandler(klet.GetActivePods, killPodNow(klet.podWorkers, kubeDeps.Recorder), kubeDeps.Recorder)
0000000000000000000000000000000000000000;;		klet.admitHandlers.AddPodAdmitHandler(lifecycle.NewPredicateAdmitHandler(klet.getNodeAnyWay, criticalPodAdmissionHandler))
0000000000000000000000000000000000000000;;		// apply functional Option's
0000000000000000000000000000000000000000;;		for _, opt := range kubeDeps.Options {
0000000000000000000000000000000000000000;;			opt(klet)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		klet.appArmorValidator = apparmor.NewValidator(kubeCfg.ContainerRuntime)
0000000000000000000000000000000000000000;;		klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewAppArmorAdmitHandler(klet.appArmorValidator))
0000000000000000000000000000000000000000;;		if utilfeature.DefaultFeatureGate.Enabled(features.Accelerators) {
0000000000000000000000000000000000000000;;			if kubeCfg.ContainerRuntime == kubetypes.DockerContainerRuntime {
0000000000000000000000000000000000000000;;				if klet.gpuManager, err = nvidia.NewNvidiaGPUManager(klet, kubeDeps.DockerClient); err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				glog.Errorf("Accelerators feature is supported with docker runtime only. Disabling this feature internally.")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Set GPU manager to a stub implementation if it is not enabled or cannot be supported.
0000000000000000000000000000000000000000;;		if klet.gpuManager == nil {
0000000000000000000000000000000000000000;;			klet.gpuManager = gpu.NewGPUManagerStub()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Finally, put the most recent version of the config on the Kubelet, so
0000000000000000000000000000000000000000;;		// people can see how it was configured.
0000000000000000000000000000000000000000;;		klet.kubeletConfiguration = *kubeCfg
0000000000000000000000000000000000000000;;		return klet, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type serviceLister interface {
0000000000000000000000000000000000000000;;		List(labels.Selector) ([]*v1.Service, error)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Kubelet is the main kubelet implementation.
0000000000000000000000000000000000000000;;	type Kubelet struct {
0000000000000000000000000000000000000000;;		kubeletConfiguration componentconfig.KubeletConfiguration
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		hostname      string
0000000000000000000000000000000000000000;;		nodeName      types.NodeName
0000000000000000000000000000000000000000;;		runtimeCache  kubecontainer.RuntimeCache
0000000000000000000000000000000000000000;;		kubeClient    clientset.Interface
0000000000000000000000000000000000000000;;		iptClient     utilipt.Interface
0000000000000000000000000000000000000000;;		rootDirectory string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// podWorkers handle syncing Pods in response to events.
0000000000000000000000000000000000000000;;		podWorkers PodWorkers
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// resyncInterval is the interval between periodic full reconciliations of
0000000000000000000000000000000000000000;;		// pods on this node.
0000000000000000000000000000000000000000;;		resyncInterval time.Duration
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// sourcesReady records the sources seen by the kubelet, it is thread-safe.
0000000000000000000000000000000000000000;;		sourcesReady config.SourcesReady
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// podManager is a facade that abstracts away the various sources of pods
0000000000000000000000000000000000000000;;		// this Kubelet services.
0000000000000000000000000000000000000000;;		podManager kubepod.Manager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Needed to observe and respond to situations that could impact node stability
0000000000000000000000000000000000000000;;		evictionManager eviction.Manager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Optional, defaults to /logs/ from /var/log
0000000000000000000000000000000000000000;;		logServer http.Handler
0000000000000000000000000000000000000000;;		// Optional, defaults to simple Docker implementation
0000000000000000000000000000000000000000;;		runner kubecontainer.ContainerCommandRunner
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// cAdvisor used for container information.
0000000000000000000000000000000000000000;;		cadvisor cadvisor.Interface
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Set to true to have the node register itself with the apiserver.
0000000000000000000000000000000000000000;;		registerNode bool
0000000000000000000000000000000000000000;;		// Set to true to have the node register itself as schedulable.
0000000000000000000000000000000000000000;;		registerSchedulable bool
0000000000000000000000000000000000000000;;		// for internal book keeping; access only from within registerWithApiserver
0000000000000000000000000000000000000000;;		registrationCompleted bool
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Set to true if the kubelet is in standalone mode (i.e. setup without an apiserver)
0000000000000000000000000000000000000000;;		standaloneMode bool
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If non-empty, use this for container DNS search.
0000000000000000000000000000000000000000;;		clusterDomain string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If non-nil, use this for container DNS server.
0000000000000000000000000000000000000000;;		clusterDNS []net.IP
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// masterServiceNamespace is the namespace that the master service is exposed in.
0000000000000000000000000000000000000000;;		masterServiceNamespace string
0000000000000000000000000000000000000000;;		// serviceLister knows how to list services
0000000000000000000000000000000000000000;;		serviceLister serviceLister
0000000000000000000000000000000000000000;;		// nodeInfo knows how to get information about the node for this kubelet.
0000000000000000000000000000000000000000;;		nodeInfo predicates.NodeInfo
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// a list of node labels to register
0000000000000000000000000000000000000000;;		nodeLabels map[string]string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Last timestamp when runtime responded on ping.
0000000000000000000000000000000000000000;;		// Mutex is used to protect this value.
0000000000000000000000000000000000000000;;		runtimeState *runtimeState
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Volume plugins.
0000000000000000000000000000000000000000;;		volumePluginMgr *volume.VolumePluginMgr
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Network plugin.
0000000000000000000000000000000000000000;;		networkPlugin network.NetworkPlugin
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Handles container probing.
0000000000000000000000000000000000000000;;		probeManager prober.Manager
0000000000000000000000000000000000000000;;		// Manages container health check results.
0000000000000000000000000000000000000000;;		livenessManager proberesults.Manager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// How long to keep idle streaming command execution/port forwarding
0000000000000000000000000000000000000000;;		// connections open before terminating them
0000000000000000000000000000000000000000;;		streamingConnectionIdleTimeout time.Duration
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The EventRecorder to use
0000000000000000000000000000000000000000;;		recorder record.EventRecorder
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Policy for handling garbage collection of dead containers.
0000000000000000000000000000000000000000;;		containerGC kubecontainer.ContainerGC
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Manager for image garbage collection.
0000000000000000000000000000000000000000;;		imageManager images.ImageGCManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Diskspace manager.
0000000000000000000000000000000000000000;;		diskSpaceManager diskSpaceManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Secret manager.
0000000000000000000000000000000000000000;;		secretManager secret.Manager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// ConfigMap manager.
0000000000000000000000000000000000000000;;		configMapManager configmap.Manager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Cached MachineInfo returned by cadvisor.
0000000000000000000000000000000000000000;;		machineInfo *cadvisorapi.MachineInfo
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		//Cached RootFsInfo returned by cadvisor
0000000000000000000000000000000000000000;;		rootfsInfo *cadvisorapiv2.FsInfo
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Handles certificate rotations.
0000000000000000000000000000000000000000;;		serverCertificateManager certificate.Manager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Syncs pods statuses with apiserver; also used as a cache of statuses.
0000000000000000000000000000000000000000;;		statusManager status.Manager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// VolumeManager runs a set of asynchronous loops that figure out which
0000000000000000000000000000000000000000;;		// volumes need to be attached/mounted/unmounted/detached based on the pods
0000000000000000000000000000000000000000;;		// scheduled on this node and makes it so.
0000000000000000000000000000000000000000;;		volumeManager volumemanager.VolumeManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Cloud provider interface.
0000000000000000000000000000000000000000;;		cloud                   cloudprovider.Interface
0000000000000000000000000000000000000000;;		autoDetectCloudProvider bool
0000000000000000000000000000000000000000;;		// Indicates that the node initialization happens in an external cloud controller
0000000000000000000000000000000000000000;;		externalCloudProvider bool
0000000000000000000000000000000000000000;;		// Reference to this node.
0000000000000000000000000000000000000000;;		nodeRef *clientv1.ObjectReference
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Container runtime.
0000000000000000000000000000000000000000;;		containerRuntime kubecontainer.Runtime
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// reasonCache caches the failure reason of the last creation of all containers, which is
0000000000000000000000000000000000000000;;		// used for generating ContainerStatus.
0000000000000000000000000000000000000000;;		reasonCache *ReasonCache
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// nodeStatusUpdateFrequency specifies how often kubelet posts node status to master.
0000000000000000000000000000000000000000;;		// Note: be cautious when changing the constant, it must work with nodeMonitorGracePeriod
0000000000000000000000000000000000000000;;		// in nodecontroller. There are several constraints:
0000000000000000000000000000000000000000;;		// 1. nodeMonitorGracePeriod must be N times more than nodeStatusUpdateFrequency, where
0000000000000000000000000000000000000000;;		//    N means number of retries allowed for kubelet to post node status. It is pointless
0000000000000000000000000000000000000000;;		//    to make nodeMonitorGracePeriod be less than nodeStatusUpdateFrequency, since there
0000000000000000000000000000000000000000;;		//    will only be fresh values from Kubelet at an interval of nodeStatusUpdateFrequency.
0000000000000000000000000000000000000000;;		//    The constant must be less than podEvictionTimeout.
0000000000000000000000000000000000000000;;		// 2. nodeStatusUpdateFrequency needs to be large enough for kubelet to generate node
0000000000000000000000000000000000000000;;		//    status. Kubelet may fail to update node status reliably if the value is too small,
0000000000000000000000000000000000000000;;		//    as it takes time to gather all necessary node information.
0000000000000000000000000000000000000000;;		nodeStatusUpdateFrequency time.Duration
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Generates pod events.
0000000000000000000000000000000000000000;;		pleg pleg.PodLifecycleEventGenerator
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Store kubecontainer.PodStatus for all pods.
0000000000000000000000000000000000000000;;		podCache kubecontainer.Cache
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// os is a facade for various syscalls that need to be mocked during testing.
0000000000000000000000000000000000000000;;		os kubecontainer.OSInterface
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Watcher of out of memory events.
0000000000000000000000000000000000000000;;		oomWatcher OOMWatcher
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Monitor resource usage
0000000000000000000000000000000000000000;;		resourceAnalyzer stats.ResourceAnalyzer
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Whether or not we should have the QOS cgroup hierarchy for resource management
0000000000000000000000000000000000000000;;		cgroupsPerQOS bool
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If non-empty, pass this to the container runtime as the root cgroup.
0000000000000000000000000000000000000000;;		cgroupRoot string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Mounter to use for volumes.
0000000000000000000000000000000000000000;;		mounter mount.Interface
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Writer interface to use for volumes.
0000000000000000000000000000000000000000;;		writer kubeio.Writer
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Manager of non-Runtime containers.
0000000000000000000000000000000000000000;;		containerManager cm.ContainerManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Maximum Number of Pods which can be run by this Kubelet
0000000000000000000000000000000000000000;;		maxPods int
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Monitor Kubelet's sync loop
0000000000000000000000000000000000000000;;		syncLoopMonitor atomic.Value
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Container restart Backoff
0000000000000000000000000000000000000000;;		backOff *flowcontrol.Backoff
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Channel for sending pods to kill.
0000000000000000000000000000000000000000;;		podKillingCh chan *kubecontainer.PodPair
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The configuration file used as the base to generate the container's
0000000000000000000000000000000000000000;;		// DNS resolver configuration file. This can be used in conjunction with
0000000000000000000000000000000000000000;;		// clusterDomain and clusterDNS.
0000000000000000000000000000000000000000;;		resolverConfig string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Information about the ports which are opened by daemons on Node running this Kubelet server.
0000000000000000000000000000000000000000;;		daemonEndpoints *v1.NodeDaemonEndpoints
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// A queue used to trigger pod workers.
0000000000000000000000000000000000000000;;		workQueue queue.WorkQueue
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// oneTimeInitializer is used to initialize modules that are dependent on the runtime to be up.
0000000000000000000000000000000000000000;;		oneTimeInitializer sync.Once
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If non-nil, use this IP address for the node
0000000000000000000000000000000000000000;;		nodeIP net.IP
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If non-nil, this is a unique identifier for the node in an external database, eg. cloudprovider
0000000000000000000000000000000000000000;;		providerID string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// clock is an interface that provides time related functionality in a way that makes it
0000000000000000000000000000000000000000;;		// easy to test the code.
0000000000000000000000000000000000000000;;		clock clock.Clock
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// outOfDiskTransitionFrequency specifies the amount of time the kubelet has to be actually
0000000000000000000000000000000000000000;;		// not out of disk before it can transition the node condition status from out-of-disk to
0000000000000000000000000000000000000000;;		// not-out-of-disk. This prevents a pod that causes out-of-disk condition from repeatedly
0000000000000000000000000000000000000000;;		// getting rescheduled onto the node.
0000000000000000000000000000000000000000;;		outOfDiskTransitionFrequency time.Duration
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// handlers called during the tryUpdateNodeStatus cycle
0000000000000000000000000000000000000000;;		setNodeStatusFuncs []func(*v1.Node) error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: think about moving this to be centralized in PodWorkers in follow-on.
0000000000000000000000000000000000000000;;		// the list of handlers to call during pod admission.
0000000000000000000000000000000000000000;;		admitHandlers lifecycle.PodAdmitHandlers
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// softAdmithandlers are applied to the pod after it is admitted by the Kubelet, but before it is
0000000000000000000000000000000000000000;;		// run. A pod rejected by a softAdmitHandler will be left in a Pending state indefinitely. If a
0000000000000000000000000000000000000000;;		// rejected pod should not be recreated, or the scheduler is not aware of the rejection rule, the
0000000000000000000000000000000000000000;;		// admission rule should be applied by a softAdmitHandler.
0000000000000000000000000000000000000000;;		softAdmitHandlers lifecycle.PodAdmitHandlers
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// the list of handlers to call during pod sync loop.
0000000000000000000000000000000000000000;;		lifecycle.PodSyncLoopHandlers
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// the list of handlers to call during pod sync.
0000000000000000000000000000000000000000;;		lifecycle.PodSyncHandlers
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// the number of allowed pods per core
0000000000000000000000000000000000000000;;		podsPerCore int
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// enableControllerAttachDetach indicates the Attach/Detach controller
0000000000000000000000000000000000000000;;		// should manage attachment/detachment of volumes scheduled to this node,
0000000000000000000000000000000000000000;;		// and disable kubelet from executing any attach/detach operations
0000000000000000000000000000000000000000;;		enableControllerAttachDetach bool
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// trigger deleting containers in a pod
0000000000000000000000000000000000000000;;		containerDeletor *podContainerDeletor
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// config iptables util rules
0000000000000000000000000000000000000000;;		makeIPTablesUtilChains bool
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The bit of the fwmark space to mark packets for SNAT.
0000000000000000000000000000000000000000;;		iptablesMasqueradeBit int
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The bit of the fwmark space to mark packets for dropping.
0000000000000000000000000000000000000000;;		iptablesDropBit int
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The AppArmor validator for checking whether AppArmor is supported.
0000000000000000000000000000000000000000;;		appArmorValidator apparmor.Validator
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The handler serving CRI streaming calls (exec/attach/port-forward).
0000000000000000000000000000000000000000;;		criHandler http.Handler
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// experimentalHostUserNamespaceDefaulting sets userns=true when users request host namespaces (pid, ipc, net),
0000000000000000000000000000000000000000;;		// are using non-namespaced capabilities (mknod, sys_time, sys_module), the pod contains a privileged container,
0000000000000000000000000000000000000000;;		// or using host path volumes.
0000000000000000000000000000000000000000;;		// This should only be enabled when the container runtime is performing user remapping AND if the
0000000000000000000000000000000000000000;;		// experimental behavior is desired.
0000000000000000000000000000000000000000;;		experimentalHostUserNamespaceDefaulting bool
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// GPU Manager
0000000000000000000000000000000000000000;;		gpuManager gpu.GPUManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// dockerLegacyService contains some legacy methods for backward compatibility.
0000000000000000000000000000000000000000;;		// It should be set only when docker is using non json-file logging driver.
0000000000000000000000000000000000000000;;		dockerLegacyService dockershim.DockerLegacyService
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func allLocalIPsWithoutLoopback() ([]net.IP, error) {
0000000000000000000000000000000000000000;;		interfaces, err := net.Interfaces()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("could not list network interfaces: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var ips []net.IP
0000000000000000000000000000000000000000;;		for _, i := range interfaces {
0000000000000000000000000000000000000000;;			addresses, err := i.Addrs()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("could not list the addresses for network interface %v: %v\n", i, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, address := range addresses {
0000000000000000000000000000000000000000;;				switch v := address.(type) {
0000000000000000000000000000000000000000;;				case *net.IPNet:
0000000000000000000000000000000000000000;;					if !v.IP.IsLoopback() {
0000000000000000000000000000000000000000;;						ips = append(ips, v.IP)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return ips, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// setupDataDirs creates:
0000000000000000000000000000000000000000;;	// 1.  the root directory
0000000000000000000000000000000000000000;;	// 2.  the pods directory
0000000000000000000000000000000000000000;;	// 3.  the plugins directory
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setupDataDirs() error {
0000000000000000000000000000000000000000;;		kl.rootDirectory = path.Clean(kl.rootDirectory)
0000000000000000000000000000000000000000;;		if err := os.MkdirAll(kl.getRootDir(), 0750); err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error creating root directory: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := os.MkdirAll(kl.getPodsDir(), 0750); err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error creating pods directory: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := os.MkdirAll(kl.getPluginsDir(), 0750); err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error creating plugins directory: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Starts garbage collection threads.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) StartGarbageCollection() {
0000000000000000000000000000000000000000;;		loggedContainerGCFailure := false
0000000000000000000000000000000000000000;;		go wait.Until(func() {
0000000000000000000000000000000000000000;;			if err := kl.containerGC.GarbageCollect(); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Container garbage collection failed: %v", err)
0000000000000000000000000000000000000000;;				kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ContainerGCFailed, err.Error())
0000000000000000000000000000000000000000;;				loggedContainerGCFailure = true
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				var vLevel glog.Level = 4
0000000000000000000000000000000000000000;;				if loggedContainerGCFailure {
0000000000000000000000000000000000000000;;					vLevel = 1
0000000000000000000000000000000000000000;;					loggedContainerGCFailure = false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				glog.V(vLevel).Infof("Container garbage collection succeeded")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}, ContainerGCPeriod, wait.NeverStop)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		prevImageGCFailed := false
0000000000000000000000000000000000000000;;		go wait.Until(func() {
0000000000000000000000000000000000000000;;			if err := kl.imageManager.GarbageCollect(); err != nil {
0000000000000000000000000000000000000000;;				if prevImageGCFailed {
0000000000000000000000000000000000000000;;					glog.Errorf("Image garbage collection failed multiple times in a row: %v", err)
0000000000000000000000000000000000000000;;					// Only create an event for repeated failures
0000000000000000000000000000000000000000;;					kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ImageGCFailed, err.Error())
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					glog.Errorf("Image garbage collection failed once. Stats initialization may not have completed yet: %v", err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				prevImageGCFailed = true
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				var vLevel glog.Level = 4
0000000000000000000000000000000000000000;;				if prevImageGCFailed {
0000000000000000000000000000000000000000;;					vLevel = 1
0000000000000000000000000000000000000000;;					prevImageGCFailed = false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				glog.V(vLevel).Infof("Image garbage collection succeeded")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}, ImageGCPeriod, wait.NeverStop)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// initializeModules will initialize internal modules that do not require the container runtime to be up.
0000000000000000000000000000000000000000;;	// Note that the modules here must not depend on modules that are not initialized here.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) initializeModules() error {
0000000000000000000000000000000000000000;;		// Prometheus metrics.
0000000000000000000000000000000000000000;;		metrics.Register(kl.runtimeCache)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Setup filesystem directories.
0000000000000000000000000000000000000000;;		if err := kl.setupDataDirs(); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If the container logs directory does not exist, create it.
0000000000000000000000000000000000000000;;		if _, err := os.Stat(ContainerLogsDir); err != nil {
0000000000000000000000000000000000000000;;			if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Failed to create directory %q: %v", ContainerLogsDir, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start the image manager.
0000000000000000000000000000000000000000;;		kl.imageManager.Start()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start the certificate manager.
0000000000000000000000000000000000000000;;		if utilfeature.DefaultFeatureGate.Enabled(features.RotateKubeletServerCertificate) {
0000000000000000000000000000000000000000;;			kl.serverCertificateManager.Start()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start container manager.
0000000000000000000000000000000000000000;;		node, err := kl.getNodeAnyWay()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Kubelet failed to get node info: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err := kl.containerManager.Start(node, kl.GetActivePods); err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Failed to start ContainerManager %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start out of memory watcher.
0000000000000000000000000000000000000000;;		if err := kl.oomWatcher.Start(kl.nodeRef); err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Failed to start OOM watcher %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Initialize GPUs
0000000000000000000000000000000000000000;;		if err := kl.gpuManager.Start(); err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Failed to start gpuManager %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start resource analyzer
0000000000000000000000000000000000000000;;		kl.resourceAnalyzer.Start()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// initializeRuntimeDependentModules will initialize internal modules that require the container runtime to be up.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) initializeRuntimeDependentModules() {
0000000000000000000000000000000000000000;;		if err := kl.cadvisor.Start(); err != nil {
0000000000000000000000000000000000000000;;			// Fail kubelet and rely on the babysitter to retry starting kubelet.
0000000000000000000000000000000000000000;;			// TODO(random-liu): Add backoff logic in the babysitter
0000000000000000000000000000000000000000;;			glog.Fatalf("Failed to start cAdvisor %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// eviction manager must start after cadvisor because it needs to know if the container runtime has a dedicated imagefs
0000000000000000000000000000000000000000;;		kl.evictionManager.Start(kl.cadvisor, kl.GetActivePods, kl.podResourcesAreReclaimed, kl, evictionMonitoringPeriod)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Run starts the kubelet reacting to config updates
0000000000000000000000000000000000000000;;	func (kl *Kubelet) Run(updates <-chan kubetypes.PodUpdate) {
0000000000000000000000000000000000000000;;		if kl.logServer == nil {
0000000000000000000000000000000000000000;;			kl.logServer = http.StripPrefix("/logs/", http.FileServer(http.Dir("/var/log/")))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kl.kubeClient == nil {
0000000000000000000000000000000000000000;;			glog.Warning("No api server defined - no node status update will be sent.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err := kl.initializeModules(); err != nil {
0000000000000000000000000000000000000000;;			kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())
0000000000000000000000000000000000000000;;			glog.Error(err)
0000000000000000000000000000000000000000;;			kl.runtimeState.setInitError(err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start volume manager
0000000000000000000000000000000000000000;;		go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kl.kubeClient != nil {
0000000000000000000000000000000000000000;;			// Start syncing node status immediately, this may set up things the runtime needs to run.
0000000000000000000000000000000000000000;;			go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		go wait.Until(kl.syncNetworkStatus, 30*time.Second, wait.NeverStop)
0000000000000000000000000000000000000000;;		go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start loop to sync iptables util rules
0000000000000000000000000000000000000000;;		if kl.makeIPTablesUtilChains {
0000000000000000000000000000000000000000;;			go wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start a goroutine responsible for killing pods (that are not properly
0000000000000000000000000000000000000000;;		// handled by pod workers).
0000000000000000000000000000000000000000;;		go wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start gorouting responsible for checking limits in resolv.conf
0000000000000000000000000000000000000000;;		if kl.resolverConfig != "" {
0000000000000000000000000000000000000000;;			go wait.Until(func() { kl.checkLimitsForResolvConf() }, 30*time.Second, wait.NeverStop)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start component sync loops.
0000000000000000000000000000000000000000;;		kl.statusManager.Start()
0000000000000000000000000000000000000000;;		kl.probeManager.Start()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start the pod lifecycle event generator.
0000000000000000000000000000000000000000;;		kl.pleg.Start()
0000000000000000000000000000000000000000;;		kl.syncLoop(updates, kl)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetKubeClient returns the Kubernetes client.
0000000000000000000000000000000000000000;;	// TODO: This is currently only required by network plugins. Replace
0000000000000000000000000000000000000000;;	// with more specific methods.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GetKubeClient() clientset.Interface {
0000000000000000000000000000000000000000;;		return kl.kubeClient
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetClusterDNS returns a list of the DNS servers and a list of the DNS search
0000000000000000000000000000000000000000;;	// domains of the cluster.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GetClusterDNS(pod *v1.Pod) ([]string, []string, bool, error) {
0000000000000000000000000000000000000000;;		var hostDNS, hostSearch []string
0000000000000000000000000000000000000000;;		// Get host DNS settings
0000000000000000000000000000000000000000;;		if kl.resolverConfig != "" {
0000000000000000000000000000000000000000;;			f, err := os.Open(kl.resolverConfig)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, nil, false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			defer f.Close()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			hostDNS, hostSearch, err = kl.parseResolvConf(f)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, nil, false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		useClusterFirstPolicy := ((pod.Spec.DNSPolicy == v1.DNSClusterFirst && !kubecontainer.IsHostNetworkPod(pod)) || pod.Spec.DNSPolicy == v1.DNSClusterFirstWithHostNet)
0000000000000000000000000000000000000000;;		if useClusterFirstPolicy && len(kl.clusterDNS) == 0 {
0000000000000000000000000000000000000000;;			// clusterDNS is not known.
0000000000000000000000000000000000000000;;			// pod with ClusterDNSFirst Policy cannot be created
0000000000000000000000000000000000000000;;			kl.recorder.Eventf(pod, v1.EventTypeWarning, "MissingClusterDNS", "kubelet does not have ClusterDNS IP configured and cannot create Pod using %q policy. Falling back to DNSDefault policy.", pod.Spec.DNSPolicy)
0000000000000000000000000000000000000000;;			log := fmt.Sprintf("kubelet does not have ClusterDNS IP configured and cannot create Pod using %q policy. pod: %q. Falling back to DNSDefault policy.", pod.Spec.DNSPolicy, format.Pod(pod))
0000000000000000000000000000000000000000;;			kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, "MissingClusterDNS", log)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// fallback to DNSDefault
0000000000000000000000000000000000000000;;			useClusterFirstPolicy = false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if !useClusterFirstPolicy {
0000000000000000000000000000000000000000;;			// When the kubelet --resolv-conf flag is set to the empty string, use
0000000000000000000000000000000000000000;;			// DNS settings that override the docker default (which is to use
0000000000000000000000000000000000000000;;			// /etc/resolv.conf) and effectively disable DNS lookups. According to
0000000000000000000000000000000000000000;;			// the bind documentation, the behavior of the DNS client library when
0000000000000000000000000000000000000000;;			// "nameservers" are not specified is to "use the nameserver on the
0000000000000000000000000000000000000000;;			// local machine". A nameserver setting of localhost is equivalent to
0000000000000000000000000000000000000000;;			// this documented behavior.
0000000000000000000000000000000000000000;;			if kl.resolverConfig == "" {
0000000000000000000000000000000000000000;;				hostDNS = []string{"127.0.0.1"}
0000000000000000000000000000000000000000;;				hostSearch = []string{"."}
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				hostSearch = kl.formDNSSearchForDNSDefault(hostSearch, pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return hostDNS, hostSearch, useClusterFirstPolicy, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// for a pod with DNSClusterFirst policy, the cluster DNS server is the only nameserver configured for
0000000000000000000000000000000000000000;;		// the pod. The cluster DNS server itself will forward queries to other nameservers that is configured to use,
0000000000000000000000000000000000000000;;		// in case the cluster DNS server cannot resolve the DNS query itself
0000000000000000000000000000000000000000;;		dns := make([]string, len(kl.clusterDNS))
0000000000000000000000000000000000000000;;		for i, ip := range kl.clusterDNS {
0000000000000000000000000000000000000000;;			dns[i] = ip.String()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		dnsSearch := kl.formDNSSearch(hostSearch, pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return dns, dnsSearch, useClusterFirstPolicy, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncPod is the transaction script for the sync of a single pod.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Arguments:
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// o - the SyncPodOptions for this invocation
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// The workflow is:
0000000000000000000000000000000000000000;;	// * If the pod is being created, record pod worker start latency
0000000000000000000000000000000000000000;;	// * Call generateAPIPodStatus to prepare an v1.PodStatus for the pod
0000000000000000000000000000000000000000;;	// * If the pod is being seen as running for the first time, record pod
0000000000000000000000000000000000000000;;	//   start latency
0000000000000000000000000000000000000000;;	// * Update the status of the pod in the status manager
0000000000000000000000000000000000000000;;	// * Kill the pod if it should not be running
0000000000000000000000000000000000000000;;	// * Create a mirror pod if the pod is a static pod, and does not
0000000000000000000000000000000000000000;;	//   already have a mirror pod
0000000000000000000000000000000000000000;;	// * Create the data directories for the pod if they do not exist
0000000000000000000000000000000000000000;;	// * Wait for volumes to attach/mount
0000000000000000000000000000000000000000;;	// * Fetch the pull secrets for the pod
0000000000000000000000000000000000000000;;	// * Call the container runtime's SyncPod callback
0000000000000000000000000000000000000000;;	// * Update the traffic shaping for the pod's ingress and egress limits
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// If any step of this workflow errors, the error is returned, and is repeated
0000000000000000000000000000000000000000;;	// on the next syncPod call.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) syncPod(o syncPodOptions) error {
0000000000000000000000000000000000000000;;		// pull out the required options
0000000000000000000000000000000000000000;;		pod := o.pod
0000000000000000000000000000000000000000;;		mirrorPod := o.mirrorPod
0000000000000000000000000000000000000000;;		podStatus := o.podStatus
0000000000000000000000000000000000000000;;		updateType := o.updateType
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// if we want to kill a pod, do it now!
0000000000000000000000000000000000000000;;		if updateType == kubetypes.SyncPodKill {
0000000000000000000000000000000000000000;;			killPodOptions := o.killPodOptions
0000000000000000000000000000000000000000;;			if killPodOptions == nil || killPodOptions.PodStatusFunc == nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("kill pod options are required if update type is kill")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			apiPodStatus := killPodOptions.PodStatusFunc(pod, podStatus)
0000000000000000000000000000000000000000;;			kl.statusManager.SetPodStatus(pod, apiPodStatus)
0000000000000000000000000000000000000000;;			// we kill the pod with the specified grace period since this is a termination
0000000000000000000000000000000000000000;;			if err := kl.killPod(pod, nil, podStatus, killPodOptions.PodTerminationGracePeriodSecondsOverride); err != nil {
0000000000000000000000000000000000000000;;				// there was an error killing the pod, so we return that error directly
0000000000000000000000000000000000000000;;				utilruntime.HandleError(err)
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Latency measurements for the main workflow are relative to the
0000000000000000000000000000000000000000;;		// first time the pod was seen by the API server.
0000000000000000000000000000000000000000;;		var firstSeenTime time.Time
0000000000000000000000000000000000000000;;		if firstSeenTimeStr, ok := pod.Annotations[kubetypes.ConfigFirstSeenAnnotationKey]; ok {
0000000000000000000000000000000000000000;;			firstSeenTime = kubetypes.ConvertToTimestamp(firstSeenTimeStr).Get()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Record pod worker start latency if being created
0000000000000000000000000000000000000000;;		// TODO: make pod workers record their own latencies
0000000000000000000000000000000000000000;;		if updateType == kubetypes.SyncPodCreate {
0000000000000000000000000000000000000000;;			if !firstSeenTime.IsZero() {
0000000000000000000000000000000000000000;;				// This is the first time we are syncing the pod. Record the latency
0000000000000000000000000000000000000000;;				// since kubelet first saw the pod if firstSeenTime is set.
0000000000000000000000000000000000000000;;				metrics.PodWorkerStartLatency.Observe(metrics.SinceInMicroseconds(firstSeenTime))
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				glog.V(3).Infof("First seen time not recorded for pod %q", pod.UID)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Generate final API pod status with pod and status manager status
0000000000000000000000000000000000000000;;		apiPodStatus := kl.generateAPIPodStatus(pod, podStatus)
0000000000000000000000000000000000000000;;		// The pod IP may be changed in generateAPIPodStatus if the pod is using host network. (See #24576)
0000000000000000000000000000000000000000;;		// TODO(random-liu): After writing pod spec into container labels, check whether pod is using host network, and
0000000000000000000000000000000000000000;;		// set pod IP to hostIP directly in runtime.GetPodStatus
0000000000000000000000000000000000000000;;		podStatus.IP = apiPodStatus.PodIP
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Record the time it takes for the pod to become running.
0000000000000000000000000000000000000000;;		existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID)
0000000000000000000000000000000000000000;;		if !ok || existingStatus.Phase == v1.PodPending && apiPodStatus.Phase == v1.PodRunning &&
0000000000000000000000000000000000000000;;			!firstSeenTime.IsZero() {
0000000000000000000000000000000000000000;;			metrics.PodStartLatency.Observe(metrics.SinceInMicroseconds(firstSeenTime))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		runnable := kl.canRunPod(pod)
0000000000000000000000000000000000000000;;		if !runnable.Admit {
0000000000000000000000000000000000000000;;			// Pod is not runnable; update the Pod and Container statuses to why.
0000000000000000000000000000000000000000;;			apiPodStatus.Reason = runnable.Reason
0000000000000000000000000000000000000000;;			apiPodStatus.Message = runnable.Message
0000000000000000000000000000000000000000;;			// Waiting containers are not creating.
0000000000000000000000000000000000000000;;			const waitingReason = "Blocked"
0000000000000000000000000000000000000000;;			for _, cs := range apiPodStatus.InitContainerStatuses {
0000000000000000000000000000000000000000;;				if cs.State.Waiting != nil {
0000000000000000000000000000000000000000;;					cs.State.Waiting.Reason = waitingReason
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, cs := range apiPodStatus.ContainerStatuses {
0000000000000000000000000000000000000000;;				if cs.State.Waiting != nil {
0000000000000000000000000000000000000000;;					cs.State.Waiting.Reason = waitingReason
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Update status in the status manager
0000000000000000000000000000000000000000;;		kl.statusManager.SetPodStatus(pod, apiPodStatus)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Kill pod if it should not be running
0000000000000000000000000000000000000000;;		if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed {
0000000000000000000000000000000000000000;;			var syncErr error
0000000000000000000000000000000000000000;;			if err := kl.killPod(pod, nil, podStatus, nil); err != nil {
0000000000000000000000000000000000000000;;				syncErr = fmt.Errorf("error killing pod: %v", err)
0000000000000000000000000000000000000000;;				utilruntime.HandleError(syncErr)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				if !runnable.Admit {
0000000000000000000000000000000000000000;;					// There was no error killing the pod, but the pod cannot be run.
0000000000000000000000000000000000000000;;					// Return an error to signal that the sync loop should back off.
0000000000000000000000000000000000000000;;					syncErr = fmt.Errorf("pod cannot be run: %s", runnable.Message)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return syncErr
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If the network plugin is not ready, only start the pod if it uses the host network
0000000000000000000000000000000000000000;;		if rs := kl.runtimeState.networkErrors(); len(rs) != 0 && !kubecontainer.IsHostNetworkPod(pod) {
0000000000000000000000000000000000000000;;			return fmt.Errorf("network is not ready: %v", rs)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Create Cgroups for the pod and apply resource parameters
0000000000000000000000000000000000000000;;		// to them if cgroups-per-qos flag is enabled.
0000000000000000000000000000000000000000;;		pcm := kl.containerManager.NewPodContainerManager()
0000000000000000000000000000000000000000;;		// If pod has already been terminated then we need not create
0000000000000000000000000000000000000000;;		// or update the pod's cgroup
0000000000000000000000000000000000000000;;		if !kl.podIsTerminated(pod) {
0000000000000000000000000000000000000000;;			// When the kubelet is restarted with the cgroups-per-qos
0000000000000000000000000000000000000000;;			// flag enabled, all the pod's running containers
0000000000000000000000000000000000000000;;			// should be killed intermittently and brought back up
0000000000000000000000000000000000000000;;			// under the qos cgroup hierarchy.
0000000000000000000000000000000000000000;;			// Check if this is the pod's first sync
0000000000000000000000000000000000000000;;			firstSync := true
0000000000000000000000000000000000000000;;			for _, containerStatus := range apiPodStatus.ContainerStatuses {
0000000000000000000000000000000000000000;;				if containerStatus.State.Running != nil {
0000000000000000000000000000000000000000;;					firstSync = false
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Don't kill containers in pod if pod's cgroups already
0000000000000000000000000000000000000000;;			// exists or the pod is running for the first time
0000000000000000000000000000000000000000;;			podKilled := false
0000000000000000000000000000000000000000;;			if !pcm.Exists(pod) && !firstSync {
0000000000000000000000000000000000000000;;				if err := kl.killPod(pod, nil, podStatus, nil); err == nil {
0000000000000000000000000000000000000000;;					podKilled = true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Create and Update pod's Cgroups
0000000000000000000000000000000000000000;;			// Don't create cgroups for run once pod if it was killed above
0000000000000000000000000000000000000000;;			// The current policy is not to restart the run once pods when
0000000000000000000000000000000000000000;;			// the kubelet is restarted with the new flag as run once pods are
0000000000000000000000000000000000000000;;			// expected to run only once and if the kubelet is restarted then
0000000000000000000000000000000000000000;;			// they are not expected to run again.
0000000000000000000000000000000000000000;;			// We don't create and apply updates to cgroup if its a run once pod and was killed above
0000000000000000000000000000000000000000;;			if !(podKilled && pod.Spec.RestartPolicy == v1.RestartPolicyNever) {
0000000000000000000000000000000000000000;;				if !pcm.Exists(pod) {
0000000000000000000000000000000000000000;;					if err := kl.containerManager.UpdateQOSCgroups(); err != nil {
0000000000000000000000000000000000000000;;						glog.V(2).Infof("Failed to update QoS cgroups while syncing pod: %v", err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if err := pcm.EnsureExists(pod); err != nil {
0000000000000000000000000000000000000000;;						return fmt.Errorf("failed to ensure that the pod: %v cgroups exist and are correctly applied: %v", pod.UID, err)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Create Mirror Pod for Static Pod if it doesn't already exist
0000000000000000000000000000000000000000;;		if kubepod.IsStaticPod(pod) {
0000000000000000000000000000000000000000;;			podFullName := kubecontainer.GetPodFullName(pod)
0000000000000000000000000000000000000000;;			deleted := false
0000000000000000000000000000000000000000;;			if mirrorPod != nil {
0000000000000000000000000000000000000000;;				if mirrorPod.DeletionTimestamp != nil || !kl.podManager.IsMirrorPodOf(mirrorPod, pod) {
0000000000000000000000000000000000000000;;					// The mirror pod is semantically different from the static pod. Remove
0000000000000000000000000000000000000000;;					// it. The mirror pod will get recreated later.
0000000000000000000000000000000000000000;;					glog.Warningf("Deleting mirror pod %q because it is outdated", format.Pod(mirrorPod))
0000000000000000000000000000000000000000;;					if err := kl.podManager.DeleteMirrorPod(podFullName); err != nil {
0000000000000000000000000000000000000000;;						glog.Errorf("Failed deleting mirror pod %q: %v", format.Pod(mirrorPod), err)
0000000000000000000000000000000000000000;;					} else {
0000000000000000000000000000000000000000;;						deleted = true
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if mirrorPod == nil || deleted {
0000000000000000000000000000000000000000;;				glog.V(3).Infof("Creating a mirror pod for static pod %q", format.Pod(pod))
0000000000000000000000000000000000000000;;				if err := kl.podManager.CreateMirrorPod(pod); err != nil {
0000000000000000000000000000000000000000;;					glog.Errorf("Failed creating a mirror pod for %q: %v", format.Pod(pod), err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Make data directories for the pod
0000000000000000000000000000000000000000;;		if err := kl.makePodDataDirs(pod); err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to make pod data directories for pod %q: %v", format.Pod(pod), err)
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Wait for volumes to attach/mount
0000000000000000000000000000000000000000;;		if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {
0000000000000000000000000000000000000000;;			kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, "Unable to mount volumes for pod %q: %v", format.Pod(pod), err)
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to mount volumes for pod %q: %v; skipping pod", format.Pod(pod), err)
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Fetch the pull secrets for the pod
0000000000000000000000000000000000000000;;		pullSecrets := kl.getPullSecretsForPod(pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Call the container runtime's SyncPod callback
0000000000000000000000000000000000000000;;		result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)
0000000000000000000000000000000000000000;;		kl.reasonCache.Update(pod.UID, result)
0000000000000000000000000000000000000000;;		if err := result.Error(); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Get pods which should be resynchronized. Currently, the following pod should be resynchronized:
0000000000000000000000000000000000000000;;	//   * pod whose work is ready.
0000000000000000000000000000000000000000;;	//   * internal modules that request sync of a pod.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) getPodsToSync() []*v1.Pod {
0000000000000000000000000000000000000000;;		allPods := kl.podManager.GetPods()
0000000000000000000000000000000000000000;;		podUIDs := kl.workQueue.GetWork()
0000000000000000000000000000000000000000;;		podUIDSet := sets.NewString()
0000000000000000000000000000000000000000;;		for _, podUID := range podUIDs {
0000000000000000000000000000000000000000;;			podUIDSet.Insert(string(podUID))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var podsToSync []*v1.Pod
0000000000000000000000000000000000000000;;		for _, pod := range allPods {
0000000000000000000000000000000000000000;;			if podUIDSet.Has(string(pod.UID)) {
0000000000000000000000000000000000000000;;				// The work of the pod is ready
0000000000000000000000000000000000000000;;				podsToSync = append(podsToSync, pod)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, podSyncLoopHandler := range kl.PodSyncLoopHandlers {
0000000000000000000000000000000000000000;;				if podSyncLoopHandler.ShouldSync(pod) {
0000000000000000000000000000000000000000;;					podsToSync = append(podsToSync, pod)
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return podsToSync
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// deletePod deletes the pod from the internal state of the kubelet by:
0000000000000000000000000000000000000000;;	// 1.  stopping the associated pod worker asynchronously
0000000000000000000000000000000000000000;;	// 2.  signaling to kill the pod by sending on the podKillingCh channel
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// deletePod returns an error if not all sources are ready or the pod is not
0000000000000000000000000000000000000000;;	// found in the runtime cache.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) deletePod(pod *v1.Pod) error {
0000000000000000000000000000000000000000;;		if pod == nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("deletePod does not allow nil pod")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !kl.sourcesReady.AllReady() {
0000000000000000000000000000000000000000;;			// If the sources aren't ready, skip deletion, as we may accidentally delete pods
0000000000000000000000000000000000000000;;			// for sources that haven't reported yet.
0000000000000000000000000000000000000000;;			return fmt.Errorf("skipping delete because sources aren't ready yet")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kl.podWorkers.ForgetWorker(pod.UID)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Runtime cache may not have been updated to with the pod, but it's okay
0000000000000000000000000000000000000000;;		// because the periodic cleanup routine will attempt to delete again later.
0000000000000000000000000000000000000000;;		runningPods, err := kl.runtimeCache.GetPods()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error listing containers: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		runningPod := kubecontainer.Pods(runningPods).FindPod("", pod.UID)
0000000000000000000000000000000000000000;;		if runningPod.IsEmpty() {
0000000000000000000000000000000000000000;;			return fmt.Errorf("pod not found")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podPair := kubecontainer.PodPair{APIPod: pod, RunningPod: &runningPod}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.podKillingCh <- &podPair
0000000000000000000000000000000000000000;;		// TODO: delete the mirror pod here?
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// We leave the volume/directory cleanup to the periodic cleanup routine.
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// isOutOfDisk detects if pods can't fit due to lack of disk space.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) isOutOfDisk() bool {
0000000000000000000000000000000000000000;;		// Check disk space once globally and reject or accept all new pods.
0000000000000000000000000000000000000000;;		withinBounds, err := kl.diskSpaceManager.IsRuntimeDiskSpaceAvailable()
0000000000000000000000000000000000000000;;		// Assume enough space in case of errors.
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Failed to check if disk space is available for the runtime: %v", err)
0000000000000000000000000000000000000000;;		} else if !withinBounds {
0000000000000000000000000000000000000000;;			return true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		withinBounds, err = kl.diskSpaceManager.IsRootDiskSpaceAvailable()
0000000000000000000000000000000000000000;;		// Assume enough space in case of errors.
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Failed to check if disk space is available on the root partition: %v", err)
0000000000000000000000000000000000000000;;		} else if !withinBounds {
0000000000000000000000000000000000000000;;			return true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// rejectPod records an event about the pod with the given reason and message,
0000000000000000000000000000000000000000;;	// and updates the pod to the failed phase in the status manage.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) rejectPod(pod *v1.Pod, reason, message string) {
0000000000000000000000000000000000000000;;		kl.recorder.Eventf(pod, v1.EventTypeWarning, reason, message)
0000000000000000000000000000000000000000;;		kl.statusManager.SetPodStatus(pod, v1.PodStatus{
0000000000000000000000000000000000000000;;			Phase:   v1.PodFailed,
0000000000000000000000000000000000000000;;			Reason:  reason,
0000000000000000000000000000000000000000;;			Message: "Pod " + message})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// canAdmitPod determines if a pod can be admitted, and gives a reason if it
0000000000000000000000000000000000000000;;	// cannot. "pod" is new pod, while "pods" are all admitted pods
0000000000000000000000000000000000000000;;	// The function returns a boolean value indicating whether the pod
0000000000000000000000000000000000000000;;	// can be admitted, a brief single-word reason and a message explaining why
0000000000000000000000000000000000000000;;	// the pod cannot be admitted.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) canAdmitPod(pods []*v1.Pod, pod *v1.Pod) (bool, string, string) {
0000000000000000000000000000000000000000;;		// the kubelet will invoke each pod admit handler in sequence
0000000000000000000000000000000000000000;;		// if any handler rejects, the pod is rejected.
0000000000000000000000000000000000000000;;		// TODO: move out of disk check into a pod admitter
0000000000000000000000000000000000000000;;		// TODO: out of resource eviction should have a pod admitter call-out
0000000000000000000000000000000000000000;;		attrs := &lifecycle.PodAdmitAttributes{Pod: pod, OtherPods: pods}
0000000000000000000000000000000000000000;;		for _, podAdmitHandler := range kl.admitHandlers {
0000000000000000000000000000000000000000;;			if result := podAdmitHandler.Admit(attrs); !result.Admit {
0000000000000000000000000000000000000000;;				return false, result.Reason, result.Message
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// TODO: When disk space scheduling is implemented (#11976), remove the out-of-disk check here and
0000000000000000000000000000000000000000;;		// add the disk space predicate to predicates.GeneralPredicates.
0000000000000000000000000000000000000000;;		if kl.isOutOfDisk() {
0000000000000000000000000000000000000000;;			glog.Warningf("Failed to admit pod %v - %s", format.Pod(pod), "predicate fails due to OutOfDisk")
0000000000000000000000000000000000000000;;			return false, "OutOfDisk", "cannot be started due to lack of disk space."
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return true, "", ""
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (kl *Kubelet) canRunPod(pod *v1.Pod) lifecycle.PodAdmitResult {
0000000000000000000000000000000000000000;;		attrs := &lifecycle.PodAdmitAttributes{Pod: pod}
0000000000000000000000000000000000000000;;		// Get "OtherPods". Rejected pods are failed, so only include admitted pods that are alive.
0000000000000000000000000000000000000000;;		attrs.OtherPods = kl.filterOutTerminatedPods(kl.podManager.GetPods())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, handler := range kl.softAdmitHandlers {
0000000000000000000000000000000000000000;;			if result := handler.Admit(attrs); !result.Admit {
0000000000000000000000000000000000000000;;				return result
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Refactor as a soft admit handler.
0000000000000000000000000000000000000000;;		if err := canRunPod(pod); err != nil {
0000000000000000000000000000000000000000;;			return lifecycle.PodAdmitResult{
0000000000000000000000000000000000000000;;				Admit:   false,
0000000000000000000000000000000000000000;;				Reason:  "Forbidden",
0000000000000000000000000000000000000000;;				Message: err.Error(),
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return lifecycle.PodAdmitResult{Admit: true}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncLoop is the main loop for processing changes. It watches for changes from
0000000000000000000000000000000000000000;;	// three channels (file, apiserver, and http) and creates a union of them. For
0000000000000000000000000000000000000000;;	// any new change seen, will run a sync against desired state and running state. If
0000000000000000000000000000000000000000;;	// no changes are seen to the configuration, will synchronize the last known desired
0000000000000000000000000000000000000000;;	// state every sync-frequency seconds. Never returns.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) syncLoop(updates <-chan kubetypes.PodUpdate, handler SyncHandler) {
0000000000000000000000000000000000000000;;		glog.Info("Starting kubelet main sync loop.")
0000000000000000000000000000000000000000;;		// The resyncTicker wakes up kubelet to checks if there are any pod workers
0000000000000000000000000000000000000000;;		// that need to be sync'd. A one-second period is sufficient because the
0000000000000000000000000000000000000000;;		// sync interval is defaulted to 10s.
0000000000000000000000000000000000000000;;		syncTicker := time.NewTicker(time.Second)
0000000000000000000000000000000000000000;;		defer syncTicker.Stop()
0000000000000000000000000000000000000000;;		housekeepingTicker := time.NewTicker(housekeepingPeriod)
0000000000000000000000000000000000000000;;		defer housekeepingTicker.Stop()
0000000000000000000000000000000000000000;;		plegCh := kl.pleg.Watch()
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 {
0000000000000000000000000000000000000000;;				glog.Infof("skipping pod synchronization - %v", rs)
0000000000000000000000000000000000000000;;				time.Sleep(5 * time.Second)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			kl.syncLoopMonitor.Store(kl.clock.Now())
0000000000000000000000000000000000000000;;			if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			kl.syncLoopMonitor.Store(kl.clock.Now())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncLoopIteration reads from various channels and dispatches pods to the
0000000000000000000000000000000000000000;;	// given handler.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Arguments:
0000000000000000000000000000000000000000;;	// 1.  configCh:       a channel to read config events from
0000000000000000000000000000000000000000;;	// 2.  handler:        the SyncHandler to dispatch pods to
0000000000000000000000000000000000000000;;	// 3.  syncCh:         a channel to read periodic sync events from
0000000000000000000000000000000000000000;;	// 4.  houseKeepingCh: a channel to read housekeeping events from
0000000000000000000000000000000000000000;;	// 5.  plegCh:         a channel to read PLEG updates from
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Events are also read from the kubelet liveness manager's update channel.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// The workflow is to read from one of the channels, handle that event, and
0000000000000000000000000000000000000000;;	// update the timestamp in the sync loop monitor.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Here is an appropriate place to note that despite the syntactical
0000000000000000000000000000000000000000;;	// similarity to the switch statement, the case statements in a select are
0000000000000000000000000000000000000000;;	// evaluated in a pseudorandom order if there are multiple channels ready to
0000000000000000000000000000000000000000;;	// read from when the select is evaluated.  In other words, case statements
0000000000000000000000000000000000000000;;	// are evaluated in random order, and you can not assume that the case
0000000000000000000000000000000000000000;;	// statements evaluate in order if multiple channels have events.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// With that in mind, in truly no particular order, the different channels
0000000000000000000000000000000000000000;;	// are handled as follows:
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// * configCh: dispatch the pods for the config change to the appropriate
0000000000000000000000000000000000000000;;	//             handler callback for the event type
0000000000000000000000000000000000000000;;	// * plegCh: update the runtime cache; sync pod
0000000000000000000000000000000000000000;;	// * syncCh: sync all pods waiting for sync
0000000000000000000000000000000000000000;;	// * houseKeepingCh: trigger cleanup of pods
0000000000000000000000000000000000000000;;	// * liveness manager: sync pods that have failed or in which one or more
0000000000000000000000000000000000000000;;	//                     containers have failed liveness checks
0000000000000000000000000000000000000000;;	func (kl *Kubelet) syncLoopIteration(configCh <-chan kubetypes.PodUpdate, handler SyncHandler,
0000000000000000000000000000000000000000;;		syncCh <-chan time.Time, housekeepingCh <-chan time.Time, plegCh <-chan *pleg.PodLifecycleEvent) bool {
0000000000000000000000000000000000000000;;		select {
0000000000000000000000000000000000000000;;		case u, open := <-configCh:
0000000000000000000000000000000000000000;;			// Update from a config source; dispatch it to the right handler
0000000000000000000000000000000000000000;;			// callback.
0000000000000000000000000000000000000000;;			if !open {
0000000000000000000000000000000000000000;;				glog.Errorf("Update channel is closed. Exiting the sync loop.")
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			switch u.Op {
0000000000000000000000000000000000000000;;			case kubetypes.ADD:
0000000000000000000000000000000000000000;;				glog.V(2).Infof("SyncLoop (ADD, %q): %q", u.Source, format.Pods(u.Pods))
0000000000000000000000000000000000000000;;				// After restarting, kubelet will get all existing pods through
0000000000000000000000000000000000000000;;				// ADD as if they are new pods. These pods will then go through the
0000000000000000000000000000000000000000;;				// admission process and *may* be rejected. This can be resolved
0000000000000000000000000000000000000000;;				// once we have checkpointing.
0000000000000000000000000000000000000000;;				handler.HandlePodAdditions(u.Pods)
0000000000000000000000000000000000000000;;			case kubetypes.UPDATE:
0000000000000000000000000000000000000000;;				glog.V(2).Infof("SyncLoop (UPDATE, %q): %q", u.Source, format.PodsWithDeletiontimestamps(u.Pods))
0000000000000000000000000000000000000000;;				handler.HandlePodUpdates(u.Pods)
0000000000000000000000000000000000000000;;			case kubetypes.REMOVE:
0000000000000000000000000000000000000000;;				glog.V(2).Infof("SyncLoop (REMOVE, %q): %q", u.Source, format.Pods(u.Pods))
0000000000000000000000000000000000000000;;				handler.HandlePodRemoves(u.Pods)
0000000000000000000000000000000000000000;;			case kubetypes.RECONCILE:
0000000000000000000000000000000000000000;;				glog.V(4).Infof("SyncLoop (RECONCILE, %q): %q", u.Source, format.Pods(u.Pods))
0000000000000000000000000000000000000000;;				handler.HandlePodReconcile(u.Pods)
0000000000000000000000000000000000000000;;			case kubetypes.DELETE:
0000000000000000000000000000000000000000;;				glog.V(2).Infof("SyncLoop (DELETE, %q): %q", u.Source, format.Pods(u.Pods))
0000000000000000000000000000000000000000;;				// DELETE is treated as a UPDATE because of graceful deletion.
0000000000000000000000000000000000000000;;				handler.HandlePodUpdates(u.Pods)
0000000000000000000000000000000000000000;;			case kubetypes.SET:
0000000000000000000000000000000000000000;;				// TODO: Do we want to support this?
0000000000000000000000000000000000000000;;				glog.Errorf("Kubelet does not support snapshot update")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Mark the source ready after receiving at least one update from the
0000000000000000000000000000000000000000;;			// source. Once all the sources are marked ready, various cleanup
0000000000000000000000000000000000000000;;			// routines will start reclaiming resources. It is important that this
0000000000000000000000000000000000000000;;			// takes place only after kubelet calls the update handler to process
0000000000000000000000000000000000000000;;			// the update to ensure the internal pod cache is up-to-date.
0000000000000000000000000000000000000000;;			kl.sourcesReady.AddSource(u.Source)
0000000000000000000000000000000000000000;;		case e := <-plegCh:
0000000000000000000000000000000000000000;;			if isSyncPodWorthy(e) {
0000000000000000000000000000000000000000;;				// PLEG event for a pod; sync it.
0000000000000000000000000000000000000000;;				if pod, ok := kl.podManager.GetPodByUID(e.ID); ok {
0000000000000000000000000000000000000000;;					glog.V(2).Infof("SyncLoop (PLEG): %q, event: %#v", format.Pod(pod), e)
0000000000000000000000000000000000000000;;					handler.HandlePodSyncs([]*v1.Pod{pod})
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					// If the pod no longer exists, ignore the event.
0000000000000000000000000000000000000000;;					glog.V(4).Infof("SyncLoop (PLEG): ignore irrelevant event: %#v", e)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if e.Type == pleg.ContainerDied {
0000000000000000000000000000000000000000;;				if containerID, ok := e.Data.(string); ok {
0000000000000000000000000000000000000000;;					kl.cleanUpContainersInPod(e.ID, containerID)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		case <-syncCh:
0000000000000000000000000000000000000000;;			// Sync pods waiting for sync
0000000000000000000000000000000000000000;;			podsToSync := kl.getPodsToSync()
0000000000000000000000000000000000000000;;			if len(podsToSync) == 0 {
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("SyncLoop (SYNC): %d pods; %s", len(podsToSync), format.Pods(podsToSync))
0000000000000000000000000000000000000000;;			kl.HandlePodSyncs(podsToSync)
0000000000000000000000000000000000000000;;		case update := <-kl.livenessManager.Updates():
0000000000000000000000000000000000000000;;			if update.Result == proberesults.Failure {
0000000000000000000000000000000000000000;;				// The liveness manager detected a failure; sync the pod.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// We should not use the pod from livenessManager, because it is never updated after
0000000000000000000000000000000000000000;;				// initialization.
0000000000000000000000000000000000000000;;				pod, ok := kl.podManager.GetPodByUID(update.PodUID)
0000000000000000000000000000000000000000;;				if !ok {
0000000000000000000000000000000000000000;;					// If the pod no longer exists, ignore the update.
0000000000000000000000000000000000000000;;					glog.V(4).Infof("SyncLoop (container unhealthy): ignore irrelevant update: %#v", update)
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				glog.V(1).Infof("SyncLoop (container unhealthy): %q", format.Pod(pod))
0000000000000000000000000000000000000000;;				handler.HandlePodSyncs([]*v1.Pod{pod})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		case <-housekeepingCh:
0000000000000000000000000000000000000000;;			if !kl.sourcesReady.AllReady() {
0000000000000000000000000000000000000000;;				// If the sources aren't ready or volume manager has not yet synced the states,
0000000000000000000000000000000000000000;;				// skip housekeeping, as we may accidentally delete pods from unready sources.
0000000000000000000000000000000000000000;;				glog.V(4).Infof("SyncLoop (housekeeping, skipped): sources aren't ready yet.")
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("SyncLoop (housekeeping)")
0000000000000000000000000000000000000000;;				if err := handler.HandlePodCleanups(); err != nil {
0000000000000000000000000000000000000000;;					glog.Errorf("Failed cleaning pods: %v", err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// dispatchWork starts the asynchronous sync of the pod in a pod worker.
0000000000000000000000000000000000000000;;	// If the pod is terminated, dispatchWork
0000000000000000000000000000000000000000;;	func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) {
0000000000000000000000000000000000000000;;		if kl.podIsTerminated(pod) {
0000000000000000000000000000000000000000;;			if pod.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;				// If the pod is in a terminated state, there is no pod worker to
0000000000000000000000000000000000000000;;				// handle the work item. Check if the DeletionTimestamp has been
0000000000000000000000000000000000000000;;				// set, and force a status update to trigger a pod deletion request
0000000000000000000000000000000000000000;;				// to the apiserver.
0000000000000000000000000000000000000000;;				kl.statusManager.TerminatePod(pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Run the sync in an async worker.
0000000000000000000000000000000000000000;;		kl.podWorkers.UpdatePod(&UpdatePodOptions{
0000000000000000000000000000000000000000;;			Pod:        pod,
0000000000000000000000000000000000000000;;			MirrorPod:  mirrorPod,
0000000000000000000000000000000000000000;;			UpdateType: syncType,
0000000000000000000000000000000000000000;;			OnCompleteFunc: func(err error) {
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		// Note the number of containers for new pods.
0000000000000000000000000000000000000000;;		if syncType == kubetypes.SyncPodCreate {
0000000000000000000000000000000000000000;;			metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO: handle mirror pods in a separate component (issue #17251)
0000000000000000000000000000000000000000;;	func (kl *Kubelet) handleMirrorPod(mirrorPod *v1.Pod, start time.Time) {
0000000000000000000000000000000000000000;;		// Mirror pod ADD/UPDATE/DELETE operations are considered an UPDATE to the
0000000000000000000000000000000000000000;;		// corresponding static pod. Send update to the pod worker if the static
0000000000000000000000000000000000000000;;		// pod exists.
0000000000000000000000000000000000000000;;		if pod, ok := kl.podManager.GetPodByMirrorPod(mirrorPod); ok {
0000000000000000000000000000000000000000;;			kl.dispatchWork(pod, kubetypes.SyncPodUpdate, mirrorPod, start)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// HandlePodAdditions is the callback in SyncHandler for pods being added from
0000000000000000000000000000000000000000;;	// a config source.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) {
0000000000000000000000000000000000000000;;		start := kl.clock.Now()
0000000000000000000000000000000000000000;;		sort.Sort(sliceutils.PodsByCreationTime(pods))
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			existingPods := kl.podManager.GetPods()
0000000000000000000000000000000000000000;;			// Always add the pod to the pod manager. Kubelet relies on the pod
0000000000000000000000000000000000000000;;			// manager as the source of truth for the desired state. If a pod does
0000000000000000000000000000000000000000;;			// not exist in the pod manager, it means that it has been deleted in
0000000000000000000000000000000000000000;;			// the apiserver and no action (other than cleanup) is required.
0000000000000000000000000000000000000000;;			kl.podManager.AddPod(pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if kubepod.IsMirrorPod(pod) {
0000000000000000000000000000000000000000;;				kl.handleMirrorPod(pod, start)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if !kl.podIsTerminated(pod) {
0000000000000000000000000000000000000000;;				// Only go through the admission process if the pod is not
0000000000000000000000000000000000000000;;				// terminated.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// We failed pods that we rejected, so activePods include all admitted
0000000000000000000000000000000000000000;;				// pods that are alive.
0000000000000000000000000000000000000000;;				activePods := kl.filterOutTerminatedPods(existingPods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Check if we can admit the pod; if not, reject it.
0000000000000000000000000000000000000000;;				if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok {
0000000000000000000000000000000000000000;;					kl.rejectPod(pod, reason, message)
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)
0000000000000000000000000000000000000000;;			kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)
0000000000000000000000000000000000000000;;			kl.probeManager.AddPod(pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// HandlePodUpdates is the callback in the SyncHandler interface for pods
0000000000000000000000000000000000000000;;	// being updated from a config source.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) HandlePodUpdates(pods []*v1.Pod) {
0000000000000000000000000000000000000000;;		start := kl.clock.Now()
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			kl.podManager.UpdatePod(pod)
0000000000000000000000000000000000000000;;			if kubepod.IsMirrorPod(pod) {
0000000000000000000000000000000000000000;;				kl.handleMirrorPod(pod, start)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// TODO: Evaluate if we need to validate and reject updates.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)
0000000000000000000000000000000000000000;;			kl.dispatchWork(pod, kubetypes.SyncPodUpdate, mirrorPod, start)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// HandlePodRemoves is the callback in the SyncHandler interface for pods
0000000000000000000000000000000000000000;;	// being removed from a config source.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) {
0000000000000000000000000000000000000000;;		start := kl.clock.Now()
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			kl.podManager.DeletePod(pod)
0000000000000000000000000000000000000000;;			if kubepod.IsMirrorPod(pod) {
0000000000000000000000000000000000000000;;				kl.handleMirrorPod(pod, start)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Deletion is allowed to fail because the periodic cleanup routine
0000000000000000000000000000000000000000;;			// will trigger deletion again.
0000000000000000000000000000000000000000;;			if err := kl.deletePod(pod); err != nil {
0000000000000000000000000000000000000000;;				glog.V(2).Infof("Failed to delete pod %q, err: %v", format.Pod(pod), err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			kl.probeManager.RemovePod(pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// HandlePodReconcile is the callback in the SyncHandler interface for pods
0000000000000000000000000000000000000000;;	// that should be reconciled.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) HandlePodReconcile(pods []*v1.Pod) {
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			// Update the pod in pod manager, status manager will do periodically reconcile according
0000000000000000000000000000000000000000;;			// to the pod manager.
0000000000000000000000000000000000000000;;			kl.podManager.UpdatePod(pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// After an evicted pod is synced, all dead containers in the pod can be removed.
0000000000000000000000000000000000000000;;			if eviction.PodIsEvicted(pod.Status) {
0000000000000000000000000000000000000000;;				if podStatus, err := kl.podCache.Get(pod.UID); err == nil {
0000000000000000000000000000000000000000;;					kl.containerDeletor.deleteContainersInPod("", podStatus, true)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// HandlePodSyncs is the callback in the syncHandler interface for pods
0000000000000000000000000000000000000000;;	// that should be dispatched to pod workers for sync.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) HandlePodSyncs(pods []*v1.Pod) {
0000000000000000000000000000000000000000;;		start := kl.clock.Now()
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)
0000000000000000000000000000000000000000;;			kl.dispatchWork(pod, kubetypes.SyncPodSync, mirrorPod, start)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// LatestLoopEntryTime returns the last time in the sync loop monitor.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) LatestLoopEntryTime() time.Time {
0000000000000000000000000000000000000000;;		val := kl.syncLoopMonitor.Load()
0000000000000000000000000000000000000000;;		if val == nil {
0000000000000000000000000000000000000000;;			return time.Time{}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return val.(time.Time)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// updateRuntimeUp calls the container runtime status callback, initializing
0000000000000000000000000000000000000000;;	// the runtime dependent modules when the container runtime first comes up,
0000000000000000000000000000000000000000;;	// and returns an error if the status check fails.  If the status check is OK,
0000000000000000000000000000000000000000;;	// update the container runtime uptime in the kubelet runtimeState.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) updateRuntimeUp() {
0000000000000000000000000000000000000000;;		s, err := kl.containerRuntime.Status()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Container runtime sanity check failed: %v", err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// rkt uses the legacy, non-CRI integration. Don't check the runtime
0000000000000000000000000000000000000000;;		// conditions for it.
0000000000000000000000000000000000000000;;		if kl.kubeletConfiguration.ContainerRuntime != kubetypes.RktContainerRuntime {
0000000000000000000000000000000000000000;;			if s == nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Container runtime status is nil")
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Periodically log the whole runtime status for debugging.
0000000000000000000000000000000000000000;;			// TODO(random-liu): Consider to send node event when optional
0000000000000000000000000000000000000000;;			// condition is unmet.
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Container runtime status: %v", s)
0000000000000000000000000000000000000000;;			networkReady := s.GetRuntimeCondition(kubecontainer.NetworkReady)
0000000000000000000000000000000000000000;;			if networkReady == nil || !networkReady.Status {
0000000000000000000000000000000000000000;;				glog.Errorf("Container runtime network not ready: %v", networkReady)
0000000000000000000000000000000000000000;;				kl.runtimeState.setNetworkState(fmt.Errorf("runtime network not ready: %v", networkReady))
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				// Set nil if the container runtime network is ready.
0000000000000000000000000000000000000000;;				kl.runtimeState.setNetworkState(nil)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// TODO(random-liu): Add runtime error in runtimeState, and update it
0000000000000000000000000000000000000000;;			// when runtime is not ready, so that the information in RuntimeReady
0000000000000000000000000000000000000000;;			// condition will be propagated to NodeReady condition.
0000000000000000000000000000000000000000;;			runtimeReady := s.GetRuntimeCondition(kubecontainer.RuntimeReady)
0000000000000000000000000000000000000000;;			// If RuntimeReady is not set or is false, report an error.
0000000000000000000000000000000000000000;;			if runtimeReady == nil || !runtimeReady.Status {
0000000000000000000000000000000000000000;;				glog.Errorf("Container runtime not ready: %v", runtimeReady)
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules)
0000000000000000000000000000000000000000;;		kl.runtimeState.setRuntimeSync(kl.clock.Now())
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// updateCloudProviderFromMachineInfo updates the node's provider ID field
0000000000000000000000000000000000000000;;	// from the given cadvisor machine info.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) updateCloudProviderFromMachineInfo(node *v1.Node, info *cadvisorapi.MachineInfo) {
0000000000000000000000000000000000000000;;		if info.CloudProvider != cadvisorapi.UnknownProvider &&
0000000000000000000000000000000000000000;;			info.CloudProvider != cadvisorapi.Baremetal {
0000000000000000000000000000000000000000;;			// The cloud providers from pkg/cloudprovider/providers/* that update ProviderID
0000000000000000000000000000000000000000;;			// will use the format of cloudprovider://project/availability_zone/instance_name
0000000000000000000000000000000000000000;;			// here we only have the cloudprovider and the instance name so we leave project
0000000000000000000000000000000000000000;;			// and availability zone empty for compatibility.
0000000000000000000000000000000000000000;;			node.Spec.ProviderID = strings.ToLower(string(info.CloudProvider)) +
0000000000000000000000000000000000000000;;				":////" + string(info.InstanceID)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetConfiguration returns the KubeletConfiguration used to configure the kubelet.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GetConfiguration() componentconfig.KubeletConfiguration {
0000000000000000000000000000000000000000;;		return kl.kubeletConfiguration
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// BirthCry sends an event that the kubelet has started up.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) BirthCry() {
0000000000000000000000000000000000000000;;		// Make an event that kubelet restarted.
0000000000000000000000000000000000000000;;		kl.recorder.Eventf(kl.nodeRef, v1.EventTypeNormal, events.StartingKubelet, "Starting kubelet.")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// StreamingConnectionIdleTimeout returns the timeout for streaming connections to the HTTP server.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) StreamingConnectionIdleTimeout() time.Duration {
0000000000000000000000000000000000000000;;		return kl.streamingConnectionIdleTimeout
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ResyncInterval returns the interval used for periodic syncs.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) ResyncInterval() time.Duration {
0000000000000000000000000000000000000000;;		return kl.resyncInterval
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ListenAndServe runs the kubelet HTTP server.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) ListenAndServe(address net.IP, port uint, tlsOptions *server.TLSOptions, auth server.AuthInterface, enableDebuggingHandlers, enableContentionProfiling bool) {
0000000000000000000000000000000000000000;;		server.ListenAndServeKubeletServer(kl, kl.resourceAnalyzer, address, port, tlsOptions, auth, enableDebuggingHandlers, enableContentionProfiling, kl.containerRuntime, kl.criHandler)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ListenAndServeReadOnly runs the kubelet HTTP server in read-only mode.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) ListenAndServeReadOnly(address net.IP, port uint) {
0000000000000000000000000000000000000000;;		server.ListenAndServeKubeletReadOnlyServer(kl, kl.resourceAnalyzer, address, port, kl.containerRuntime)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Delete the eligible dead container instances in a pod. Depending on the configuration, the latest dead containers may be kept around.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) cleanUpContainersInPod(podId types.UID, exitedContainerID string) {
0000000000000000000000000000000000000000;;		if podStatus, err := kl.podCache.Get(podId); err == nil {
0000000000000000000000000000000000000000;;			removeAll := false
0000000000000000000000000000000000000000;;			if syncedPod, ok := kl.podManager.GetPodByUID(podId); ok {
0000000000000000000000000000000000000000;;				// When an evicted pod has already synced, all containers can be removed.
0000000000000000000000000000000000000000;;				removeAll = eviction.PodIsEvicted(syncedPod.Status)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			kl.containerDeletor.deleteContainersInPod(exitedContainerID, podStatus, removeAll)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// isSyncPodWorthy filters out events that are not worthy of pod syncing
0000000000000000000000000000000000000000;;	func isSyncPodWorthy(event *pleg.PodLifecycleEvent) bool {
0000000000000000000000000000000000000000;;		// ContatnerRemoved doesn't affect pod state
0000000000000000000000000000000000000000;;		return event.Type != pleg.ContainerRemoved
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Gets the streaming server configuration to use with in-process CRI shims.
0000000000000000000000000000000000000000;;	func getStreamingConfig(kubeCfg *componentconfig.KubeletConfiguration, kubeDeps *KubeletDeps) *streaming.Config {
0000000000000000000000000000000000000000;;		config := &streaming.Config{
0000000000000000000000000000000000000000;;			// Use a relative redirect (no scheme or host).
0000000000000000000000000000000000000000;;			BaseURL: &url.URL{
0000000000000000000000000000000000000000;;				Path: "/cri/",
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			StreamIdleTimeout:               kubeCfg.StreamingConnectionIdleTimeout.Duration,
0000000000000000000000000000000000000000;;			StreamCreationTimeout:           streaming.DefaultConfig.StreamCreationTimeout,
0000000000000000000000000000000000000000;;			SupportedRemoteCommandProtocols: streaming.DefaultConfig.SupportedRemoteCommandProtocols,
0000000000000000000000000000000000000000;;			SupportedPortForwardProtocols:   streaming.DefaultConfig.SupportedPortForwardProtocols,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kubeDeps.TLSOptions != nil {
0000000000000000000000000000000000000000;;			config.TLSConfig = kubeDeps.TLSOptions.Config
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return config
0000000000000000000000000000000000000000;;	}
