0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
aab5f6785b93caaf7c0cb91f56f21ffed6107f3d;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"math"
0000000000000000000000000000000000000000;;		"net"
0000000000000000000000000000000000000000;;		goruntime "runtime"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		apierrors "k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/conversion"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		utilnet "k8s.io/apimachinery/pkg/util/net"
0000000000000000000000000000000000000000;;		utilfeature "k8s.io/apiserver/pkg/util/feature"
0000000000000000000000000000000000000000;;		k8s_api_v1 "k8s.io/kubernetes/pkg/api/v1"
0000000000000000000000000000000000000000;;		v1helper "k8s.io/kubernetes/pkg/api/v1/helper"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/cloudprovider"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/features"
0000000000000000000000000000000000000000;;		kubeletapis "k8s.io/kubernetes/pkg/kubelet/apis"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/cadvisor"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/events"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/sliceutils"
0000000000000000000000000000000000000000;;		nodeutil "k8s.io/kubernetes/pkg/util/node"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/version"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/volume/util/volumehelper"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/plugin/pkg/scheduler/algorithm"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// maxImagesInNodeStatus is the number of max images we store in image status.
0000000000000000000000000000000000000000;;		maxImagesInNodeStatus = 50
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// maxNamesPerImageInNodeStatus is max number of names per image stored in
0000000000000000000000000000000000000000;;		// the node status.
0000000000000000000000000000000000000000;;		maxNamesPerImageInNodeStatus = 5
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// registerWithApiServer registers the node with the cluster master. It is safe
0000000000000000000000000000000000000000;;	// to call multiple times, but not concurrently (kl.registrationCompleted is
0000000000000000000000000000000000000000;;	// not locked).
0000000000000000000000000000000000000000;;	func (kl *Kubelet) registerWithApiServer() {
0000000000000000000000000000000000000000;;		if kl.registrationCompleted {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		step := 100 * time.Millisecond
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			time.Sleep(step)
0000000000000000000000000000000000000000;;			step = step * 2
0000000000000000000000000000000000000000;;			if step >= 7*time.Second {
0000000000000000000000000000000000000000;;				step = 7 * time.Second
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			node, err := kl.initialNode()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Unable to construct v1.Node object for kubelet: %v", err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			glog.Infof("Attempting to register node %s", node.Name)
0000000000000000000000000000000000000000;;			registered := kl.tryRegisterWithApiServer(node)
0000000000000000000000000000000000000000;;			if registered {
0000000000000000000000000000000000000000;;				glog.Infof("Successfully registered node %s", node.Name)
0000000000000000000000000000000000000000;;				kl.registrationCompleted = true
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// tryRegisterWithApiServer makes an attempt to register the given node with
0000000000000000000000000000000000000000;;	// the API server, returning a boolean indicating whether the attempt was
0000000000000000000000000000000000000000;;	// successful.  If a node with the same name already exists, it reconciles the
0000000000000000000000000000000000000000;;	// value of the annotation for controller-managed attach-detach of attachable
0000000000000000000000000000000000000000;;	// persistent volumes for the node.  If a node of the same name exists but has
0000000000000000000000000000000000000000;;	// a different externalID value, it attempts to delete that node so that a
0000000000000000000000000000000000000000;;	// later attempt can recreate it.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) tryRegisterWithApiServer(node *v1.Node) bool {
0000000000000000000000000000000000000000;;		_, err := kl.kubeClient.Core().Nodes().Create(node)
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			return true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if !apierrors.IsAlreadyExists(err) {
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to register node %q with API server: %v", kl.nodeName, err)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		existingNode, err := kl.kubeClient.Core().Nodes().Get(string(kl.nodeName), metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to register node %q with API server: error getting existing node: %v", kl.nodeName, err)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if existingNode == nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to register node %q with API server: no node instance returned", kl.nodeName)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		clonedNode, err := conversion.NewCloner().DeepCopy(existingNode)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to clone %q node object %#v: %v", kl.nodeName, existingNode, err)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		originalNode, ok := clonedNode.(*v1.Node)
0000000000000000000000000000000000000000;;		if !ok || originalNode == nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to cast %q node object %#v to v1.Node", kl.nodeName, clonedNode)
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if existingNode.Spec.ExternalID == node.Spec.ExternalID {
0000000000000000000000000000000000000000;;			glog.Infof("Node %s was previously registered", kl.nodeName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Edge case: the node was previously registered; reconcile
0000000000000000000000000000000000000000;;			// the value of the controller-managed attach-detach
0000000000000000000000000000000000000000;;			// annotation.
0000000000000000000000000000000000000000;;			requiresUpdate := kl.reconcileCMADAnnotationWithExistingNode(node, existingNode)
0000000000000000000000000000000000000000;;			if requiresUpdate {
0000000000000000000000000000000000000000;;				if _, err := nodeutil.PatchNodeStatus(kl.kubeClient, types.NodeName(kl.nodeName),
0000000000000000000000000000000000000000;;					originalNode, existingNode); err != nil {
0000000000000000000000000000000000000000;;					glog.Errorf("Unable to reconcile node %q with API server: error updating node: %v", kl.nodeName, err)
0000000000000000000000000000000000000000;;					return false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			return true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.Errorf(
0000000000000000000000000000000000000000;;			"Previously node %q had externalID %q; now it is %q; will delete and recreate.",
0000000000000000000000000000000000000000;;			kl.nodeName, node.Spec.ExternalID, existingNode.Spec.ExternalID,
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		if err := kl.kubeClient.Core().Nodes().Delete(node.Name, nil); err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to register node %q with API server: error deleting old node: %v", kl.nodeName, err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.Infof("Deleted old node object %q", kl.nodeName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// reconcileCMADAnnotationWithExistingNode reconciles the controller-managed
0000000000000000000000000000000000000000;;	// attach-detach annotation on a new node and the existing node, returning
0000000000000000000000000000000000000000;;	// whether the existing node must be updated.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) reconcileCMADAnnotationWithExistingNode(node, existingNode *v1.Node) bool {
0000000000000000000000000000000000000000;;		var (
0000000000000000000000000000000000000000;;			existingCMAAnnotation    = existingNode.Annotations[volumehelper.ControllerManagedAttachAnnotation]
0000000000000000000000000000000000000000;;			newCMAAnnotation, newSet = node.Annotations[volumehelper.ControllerManagedAttachAnnotation]
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if newCMAAnnotation == existingCMAAnnotation {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If the just-constructed node and the existing node do
0000000000000000000000000000000000000000;;		// not have the same value, update the existing node with
0000000000000000000000000000000000000000;;		// the correct value of the annotation.
0000000000000000000000000000000000000000;;		if !newSet {
0000000000000000000000000000000000000000;;			glog.Info("Controller attach-detach setting changed to false; updating existing Node")
0000000000000000000000000000000000000000;;			delete(existingNode.Annotations, volumehelper.ControllerManagedAttachAnnotation)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.Info("Controller attach-detach setting changed to true; updating existing Node")
0000000000000000000000000000000000000000;;			if existingNode.Annotations == nil {
0000000000000000000000000000000000000000;;				existingNode.Annotations = make(map[string]string)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			existingNode.Annotations[volumehelper.ControllerManagedAttachAnnotation] = newCMAAnnotation
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// initialNode constructs the initial v1.Node for this Kubelet, incorporating node
0000000000000000000000000000000000000000;;	// labels, information from the cloud provider, and Kubelet configuration.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) initialNode() (*v1.Node, error) {
0000000000000000000000000000000000000000;;		node := &v1.Node{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name: string(kl.nodeName),
0000000000000000000000000000000000000000;;				Labels: map[string]string{
0000000000000000000000000000000000000000;;					kubeletapis.LabelHostname: kl.hostname,
0000000000000000000000000000000000000000;;					kubeletapis.LabelOS:       goruntime.GOOS,
0000000000000000000000000000000000000000;;					kubeletapis.LabelArch:     goruntime.GOARCH,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.NodeSpec{
0000000000000000000000000000000000000000;;				Unschedulable: !kl.registerSchedulable,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		nodeTaints := make([]v1.Taint, 0)
0000000000000000000000000000000000000000;;		if len(kl.kubeletConfiguration.RegisterWithTaints) > 0 {
0000000000000000000000000000000000000000;;			taints := make([]v1.Taint, len(kl.kubeletConfiguration.RegisterWithTaints))
0000000000000000000000000000000000000000;;			for i := range kl.kubeletConfiguration.RegisterWithTaints {
0000000000000000000000000000000000000000;;				if err := k8s_api_v1.Convert_api_Taint_To_v1_Taint(&kl.kubeletConfiguration.RegisterWithTaints[i], &taints[i], nil); err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			nodeTaints = append(nodeTaints, taints...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kl.externalCloudProvider {
0000000000000000000000000000000000000000;;			taint := v1.Taint{
0000000000000000000000000000000000000000;;				Key:    algorithm.TaintExternalCloudProvider,
0000000000000000000000000000000000000000;;				Value:  "true",
0000000000000000000000000000000000000000;;				Effect: v1.TaintEffectNoSchedule,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			nodeTaints = append(nodeTaints, taint)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(nodeTaints) > 0 {
0000000000000000000000000000000000000000;;			node.Spec.Taints = nodeTaints
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Initially, set NodeNetworkUnavailable to true.
0000000000000000000000000000000000000000;;		if kl.providerRequiresNetworkingConfiguration() {
0000000000000000000000000000000000000000;;			node.Status.Conditions = append(node.Status.Conditions, v1.NodeCondition{
0000000000000000000000000000000000000000;;				Type:               v1.NodeNetworkUnavailable,
0000000000000000000000000000000000000000;;				Status:             v1.ConditionTrue,
0000000000000000000000000000000000000000;;				Reason:             "NoRouteCreated",
0000000000000000000000000000000000000000;;				Message:            "Node created without a route",
0000000000000000000000000000000000000000;;				LastTransitionTime: metav1.NewTime(kl.clock.Now()),
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kl.enableControllerAttachDetach {
0000000000000000000000000000000000000000;;			if node.Annotations == nil {
0000000000000000000000000000000000000000;;				node.Annotations = make(map[string]string)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			glog.Infof("Setting node annotation to enable volume controller attach/detach")
0000000000000000000000000000000000000000;;			node.Annotations[volumehelper.ControllerManagedAttachAnnotation] = "true"
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.Infof("Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kl.kubeletConfiguration.KeepTerminatedPodVolumes {
0000000000000000000000000000000000000000;;			if node.Annotations == nil {
0000000000000000000000000000000000000000;;				node.Annotations = make(map[string]string)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("Setting node annotation to keep pod volumes of terminated pods attached to the node")
0000000000000000000000000000000000000000;;			node.Annotations[volumehelper.KeepTerminatedPodVolumesAnnotation] = "true"
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// @question: should this be place after the call to the cloud provider? which also applies labels
0000000000000000000000000000000000000000;;		for k, v := range kl.nodeLabels {
0000000000000000000000000000000000000000;;			if cv, found := node.ObjectMeta.Labels[k]; found {
0000000000000000000000000000000000000000;;				glog.Warningf("the node label %s=%s will overwrite default setting %s", k, v, cv)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			node.ObjectMeta.Labels[k] = v
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kl.providerID != "" {
0000000000000000000000000000000000000000;;			node.Spec.ProviderID = kl.providerID
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kl.cloud != nil {
0000000000000000000000000000000000000000;;			instances, ok := kl.cloud.Instances()
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("failed to get instances from cloud provider")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// TODO(roberthbailey): Can we do this without having credentials to talk
0000000000000000000000000000000000000000;;			// to the cloud provider?
0000000000000000000000000000000000000000;;			// TODO: ExternalID is deprecated, we'll have to drop this code
0000000000000000000000000000000000000000;;			externalID, err := instances.ExternalID(kl.nodeName)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("failed to get external ID from cloud provider: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			node.Spec.ExternalID = externalID
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// TODO: We can't assume that the node has credentials to talk to the
0000000000000000000000000000000000000000;;			// cloudprovider from arbitrary nodes. At most, we should talk to a
0000000000000000000000000000000000000000;;			// local metadata server here.
0000000000000000000000000000000000000000;;			if node.Spec.ProviderID == "" {
0000000000000000000000000000000000000000;;				node.Spec.ProviderID, err = cloudprovider.GetInstanceProviderID(kl.cloud, kl.nodeName)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			instanceType, err := instances.InstanceType(kl.nodeName)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if instanceType != "" {
0000000000000000000000000000000000000000;;				glog.Infof("Adding node label from cloud provider: %s=%s", kubeletapis.LabelInstanceType, instanceType)
0000000000000000000000000000000000000000;;				node.ObjectMeta.Labels[kubeletapis.LabelInstanceType] = instanceType
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// If the cloud has zone information, label the node with the zone information
0000000000000000000000000000000000000000;;			zones, ok := kl.cloud.Zones()
0000000000000000000000000000000000000000;;			if ok {
0000000000000000000000000000000000000000;;				zone, err := zones.GetZone()
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return nil, fmt.Errorf("failed to get zone from cloud provider: %v", err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if zone.FailureDomain != "" {
0000000000000000000000000000000000000000;;					glog.Infof("Adding node label from cloud provider: %s=%s", kubeletapis.LabelZoneFailureDomain, zone.FailureDomain)
0000000000000000000000000000000000000000;;					node.ObjectMeta.Labels[kubeletapis.LabelZoneFailureDomain] = zone.FailureDomain
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if zone.Region != "" {
0000000000000000000000000000000000000000;;					glog.Infof("Adding node label from cloud provider: %s=%s", kubeletapis.LabelZoneRegion, zone.Region)
0000000000000000000000000000000000000000;;					node.ObjectMeta.Labels[kubeletapis.LabelZoneRegion] = zone.Region
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			node.Spec.ExternalID = kl.hostname
0000000000000000000000000000000000000000;;			if kl.autoDetectCloudProvider {
0000000000000000000000000000000000000000;;				// If no cloud provider is defined - use the one detected by cadvisor
0000000000000000000000000000000000000000;;				info, err := kl.GetCachedMachineInfo()
0000000000000000000000000000000000000000;;				if err == nil {
0000000000000000000000000000000000000000;;					kl.updateCloudProviderFromMachineInfo(node, info)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kl.setNodeStatus(node)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return node, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncNodeStatus should be called periodically from a goroutine.
0000000000000000000000000000000000000000;;	// It synchronizes node status to master, registering the kubelet first if
0000000000000000000000000000000000000000;;	// necessary.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) syncNodeStatus() {
0000000000000000000000000000000000000000;;		if kl.kubeClient == nil {
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kl.registerNode {
0000000000000000000000000000000000000000;;			// This will exit immediately if it doesn't need to do anything.
0000000000000000000000000000000000000000;;			kl.registerWithApiServer()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := kl.updateNodeStatus(); err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Unable to update node status: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// updateNodeStatus updates node status to master with retries.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) updateNodeStatus() error {
0000000000000000000000000000000000000000;;		for i := 0; i < nodeStatusUpdateRetry; i++ {
0000000000000000000000000000000000000000;;			if err := kl.tryUpdateNodeStatus(i); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Error updating node status, will retry: %v", err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("update node status exceeds retry count")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// tryUpdateNodeStatus tries to update node status to master. If ReconcileCBR0
0000000000000000000000000000000000000000;;	// is set, this function will also confirm that cbr0 is configured correctly.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) tryUpdateNodeStatus(tryNumber int) error {
0000000000000000000000000000000000000000;;		// In large clusters, GET and PUT operations on Node objects coming
0000000000000000000000000000000000000000;;		// from here are the majority of load on apiserver and etcd.
0000000000000000000000000000000000000000;;		// To reduce the load on etcd, we are serving GET operations from
0000000000000000000000000000000000000000;;		// apiserver cache (the data might be slightly delayed but it doesn't
0000000000000000000000000000000000000000;;		// seem to cause more conflict - the delays are pretty small).
0000000000000000000000000000000000000000;;		// If it result in a conflict, all retries are served directly from etcd.
0000000000000000000000000000000000000000;;		opts := metav1.GetOptions{}
0000000000000000000000000000000000000000;;		if tryNumber == 0 {
0000000000000000000000000000000000000000;;			util.FromApiserverCache(&opts)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		node, err := kl.kubeClient.Core().Nodes().Get(string(kl.nodeName), opts)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error getting node %q: %v", kl.nodeName, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		clonedNode, err := conversion.NewCloner().DeepCopy(node)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error clone node %q: %v", kl.nodeName, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		originalNode, ok := clonedNode.(*v1.Node)
0000000000000000000000000000000000000000;;		if !ok || originalNode == nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to cast %q node object %#v to v1.Node", kl.nodeName, clonedNode)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.updatePodCIDR(node.Spec.PodCIDR)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.setNodeStatus(node)
0000000000000000000000000000000000000000;;		// Patch the current status on the API server
0000000000000000000000000000000000000000;;		updatedNode, err := nodeutil.PatchNodeStatus(kl.kubeClient, types.NodeName(kl.nodeName), originalNode, node)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// If update finishes successfully, mark the volumeInUse as reportedInUse to indicate
0000000000000000000000000000000000000000;;		// those volumes are already updated in the node's status
0000000000000000000000000000000000000000;;		kl.volumeManager.MarkVolumesAsReportedInUse(updatedNode.Status.VolumesInUse)
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// recordNodeStatusEvent records an event of the given type with the given
0000000000000000000000000000000000000000;;	// message for the node.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) recordNodeStatusEvent(eventType, event string) {
0000000000000000000000000000000000000000;;		glog.V(2).Infof("Recording %s event message for node %s", event, kl.nodeName)
0000000000000000000000000000000000000000;;		// TODO: This requires a transaction, either both node status is updated
0000000000000000000000000000000000000000;;		// and event is recorded or neither should happen, see issue #6055.
0000000000000000000000000000000000000000;;		kl.recorder.Eventf(kl.nodeRef, eventType, event, "Node %s status is now: %s", kl.nodeName, event)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Set IP and hostname addresses for the node.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeAddress(node *v1.Node) error {
0000000000000000000000000000000000000000;;		if kl.nodeIP != nil {
0000000000000000000000000000000000000000;;			if err := kl.validateNodeIP(); err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("failed to validate nodeIP: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(2).Infof("Using node IP: %q", kl.nodeIP.String())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kl.externalCloudProvider {
0000000000000000000000000000000000000000;;			// We rely on the external cloud provider to supply the addresses.
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		} else if kl.cloud != nil {
0000000000000000000000000000000000000000;;			instances, ok := kl.cloud.Instances()
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				return fmt.Errorf("failed to get instances from cloud provider")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// TODO(roberthbailey): Can we do this without having credentials to talk
0000000000000000000000000000000000000000;;			// to the cloud provider?
0000000000000000000000000000000000000000;;			// TODO(justinsb): We can if CurrentNodeName() was actually CurrentNode() and returned an interface
0000000000000000000000000000000000000000;;			// TODO: If IP addresses couldn't be fetched from the cloud provider, should kubelet fallback on the other methods for getting the IP below?
0000000000000000000000000000000000000000;;			nodeAddresses, err := instances.NodeAddresses(kl.nodeName)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("failed to get node address from cloud provider: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if kl.nodeIP != nil {
0000000000000000000000000000000000000000;;				for _, nodeAddress := range nodeAddresses {
0000000000000000000000000000000000000000;;					if nodeAddress.Address == kl.nodeIP.String() {
0000000000000000000000000000000000000000;;						node.Status.Addresses = []v1.NodeAddress{
0000000000000000000000000000000000000000;;							{Type: nodeAddress.Type, Address: nodeAddress.Address},
0000000000000000000000000000000000000000;;							{Type: v1.NodeHostName, Address: kl.GetHostname()},
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						return nil
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return fmt.Errorf("failed to get node address from cloud provider that matches ip: %v", kl.nodeIP)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Only add a NodeHostName address if the cloudprovider did not specify one
0000000000000000000000000000000000000000;;			// (we assume the cloudprovider knows best)
0000000000000000000000000000000000000000;;			var addressNodeHostName *v1.NodeAddress
0000000000000000000000000000000000000000;;			for i := range nodeAddresses {
0000000000000000000000000000000000000000;;				if nodeAddresses[i].Type == v1.NodeHostName {
0000000000000000000000000000000000000000;;					addressNodeHostName = &nodeAddresses[i]
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if addressNodeHostName == nil {
0000000000000000000000000000000000000000;;				hostnameAddress := v1.NodeAddress{Type: v1.NodeHostName, Address: kl.GetHostname()}
0000000000000000000000000000000000000000;;				nodeAddresses = append(nodeAddresses, hostnameAddress)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				glog.V(2).Infof("Using Node Hostname from cloudprovider: %q", addressNodeHostName.Address)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			node.Status.Addresses = nodeAddresses
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			var ipAddr net.IP
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// 1) Use nodeIP if set
0000000000000000000000000000000000000000;;			// 2) If the user has specified an IP to HostnameOverride, use it
0000000000000000000000000000000000000000;;			// 3) Lookup the IP from node name by DNS and use the first non-loopback ipv4 address
0000000000000000000000000000000000000000;;			// 4) Try to get the IP from the network interface used as default gateway
0000000000000000000000000000000000000000;;			if kl.nodeIP != nil {
0000000000000000000000000000000000000000;;				ipAddr = kl.nodeIP
0000000000000000000000000000000000000000;;				node.ObjectMeta.Annotations[kubeletapis.AnnotationProvidedIPAddr] = kl.nodeIP.String()
0000000000000000000000000000000000000000;;			} else if addr := net.ParseIP(kl.hostname); addr != nil {
0000000000000000000000000000000000000000;;				ipAddr = addr
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				var addrs []net.IP
0000000000000000000000000000000000000000;;				addrs, err = net.LookupIP(node.Name)
0000000000000000000000000000000000000000;;				for _, addr := range addrs {
0000000000000000000000000000000000000000;;					if !addr.IsLoopback() && addr.To4() != nil {
0000000000000000000000000000000000000000;;						ipAddr = addr
0000000000000000000000000000000000000000;;						break
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if ipAddr == nil {
0000000000000000000000000000000000000000;;					ipAddr, err = utilnet.ChooseHostInterface()
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if ipAddr == nil {
0000000000000000000000000000000000000000;;				// We tried everything we could, but the IP address wasn't fetchable; error out
0000000000000000000000000000000000000000;;				return fmt.Errorf("can't get ip address of node %s. error: %v", node.Name, err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				node.Status.Addresses = []v1.NodeAddress{
0000000000000000000000000000000000000000;;					{Type: v1.NodeInternalIP, Address: ipAddr.String()},
0000000000000000000000000000000000000000;;					{Type: v1.NodeHostName, Address: kl.GetHostname()},
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeStatusMachineInfo(node *v1.Node) {
0000000000000000000000000000000000000000;;		// Note: avoid blindly overwriting the capacity in case opaque
0000000000000000000000000000000000000000;;		//       resources are being advertised.
0000000000000000000000000000000000000000;;		if node.Status.Capacity == nil {
0000000000000000000000000000000000000000;;			node.Status.Capacity = v1.ResourceList{}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// populate GPU capacity.
0000000000000000000000000000000000000000;;		gpuCapacity := kl.gpuManager.Capacity()
0000000000000000000000000000000000000000;;		if gpuCapacity != nil {
0000000000000000000000000000000000000000;;			for k, v := range gpuCapacity {
0000000000000000000000000000000000000000;;				node.Status.Capacity[k] = v
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Post NotReady if we cannot get MachineInfo from cAdvisor. This needs to start
0000000000000000000000000000000000000000;;		// cAdvisor locally, e.g. for test-cmd.sh, and in integration test.
0000000000000000000000000000000000000000;;		info, err := kl.GetCachedMachineInfo()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			// TODO(roberthbailey): This is required for test-cmd.sh to pass.
0000000000000000000000000000000000000000;;			// See if the test should be updated instead.
0000000000000000000000000000000000000000;;			node.Status.Capacity[v1.ResourceCPU] = *resource.NewMilliQuantity(0, resource.DecimalSI)
0000000000000000000000000000000000000000;;			node.Status.Capacity[v1.ResourceMemory] = resource.MustParse("0Gi")
0000000000000000000000000000000000000000;;			node.Status.Capacity[v1.ResourcePods] = *resource.NewQuantity(int64(kl.maxPods), resource.DecimalSI)
0000000000000000000000000000000000000000;;			glog.Errorf("Error getting machine info: %v", err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			node.Status.NodeInfo.MachineID = info.MachineID
0000000000000000000000000000000000000000;;			node.Status.NodeInfo.SystemUUID = info.SystemUUID
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for rName, rCap := range cadvisor.CapacityFromMachineInfo(info) {
0000000000000000000000000000000000000000;;				node.Status.Capacity[rName] = rCap
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if kl.podsPerCore > 0 {
0000000000000000000000000000000000000000;;				node.Status.Capacity[v1.ResourcePods] = *resource.NewQuantity(
0000000000000000000000000000000000000000;;					int64(math.Min(float64(info.NumCores*kl.podsPerCore), float64(kl.maxPods))), resource.DecimalSI)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				node.Status.Capacity[v1.ResourcePods] = *resource.NewQuantity(
0000000000000000000000000000000000000000;;					int64(kl.maxPods), resource.DecimalSI)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if node.Status.NodeInfo.BootID != "" &&
0000000000000000000000000000000000000000;;				node.Status.NodeInfo.BootID != info.BootID {
0000000000000000000000000000000000000000;;				// TODO: This requires a transaction, either both node status is updated
0000000000000000000000000000000000000000;;				// and event is recorded or neither should happen, see issue #6055.
0000000000000000000000000000000000000000;;				kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.NodeRebooted,
0000000000000000000000000000000000000000;;					"Node %s has been rebooted, boot id: %s", kl.nodeName, info.BootID)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			node.Status.NodeInfo.BootID = info.BootID
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if utilfeature.DefaultFeatureGate.Enabled(features.LocalStorageCapacityIsolation) {
0000000000000000000000000000000000000000;;				// TODO: all the node resources should use GetCapacity instead of deriving the
0000000000000000000000000000000000000000;;				// capacity for every node status request
0000000000000000000000000000000000000000;;				initialCapacity := kl.containerManager.GetCapacity()
0000000000000000000000000000000000000000;;				if initialCapacity != nil {
0000000000000000000000000000000000000000;;					node.Status.Capacity[v1.ResourceStorageScratch] = initialCapacity[v1.ResourceStorageScratch]
0000000000000000000000000000000000000000;;					imageCapacity, ok := initialCapacity[v1.ResourceStorageOverlay]
0000000000000000000000000000000000000000;;					if ok {
0000000000000000000000000000000000000000;;						node.Status.Capacity[v1.ResourceStorageOverlay] = imageCapacity
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Set Allocatable.
0000000000000000000000000000000000000000;;		if node.Status.Allocatable == nil {
0000000000000000000000000000000000000000;;			node.Status.Allocatable = make(v1.ResourceList)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Remove opaque integer resources from allocatable that are no longer
0000000000000000000000000000000000000000;;		// present in capacity.
0000000000000000000000000000000000000000;;		for k := range node.Status.Allocatable {
0000000000000000000000000000000000000000;;			_, found := node.Status.Capacity[k]
0000000000000000000000000000000000000000;;			if !found && v1helper.IsOpaqueIntResourceName(k) {
0000000000000000000000000000000000000000;;				delete(node.Status.Allocatable, k)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		allocatableReservation := kl.containerManager.GetNodeAllocatableReservation()
0000000000000000000000000000000000000000;;		for k, v := range node.Status.Capacity {
0000000000000000000000000000000000000000;;			value := *(v.Copy())
0000000000000000000000000000000000000000;;			if res, exists := allocatableReservation[k]; exists {
0000000000000000000000000000000000000000;;				value.Sub(res)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if value.Sign() < 0 {
0000000000000000000000000000000000000000;;				// Negative Allocatable resources don't make sense.
0000000000000000000000000000000000000000;;				value.Set(0)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			node.Status.Allocatable[k] = value
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Set versioninfo for the node.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeStatusVersionInfo(node *v1.Node) {
0000000000000000000000000000000000000000;;		verinfo, err := kl.cadvisor.VersionInfo()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Error getting version info: %v", err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			node.Status.NodeInfo.KernelVersion = verinfo.KernelVersion
0000000000000000000000000000000000000000;;			node.Status.NodeInfo.OSImage = verinfo.ContainerOsVersion
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			runtimeVersion := "Unknown"
0000000000000000000000000000000000000000;;			if runtimeVer, err := kl.containerRuntime.Version(); err == nil {
0000000000000000000000000000000000000000;;				runtimeVersion = runtimeVer.String()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			node.Status.NodeInfo.ContainerRuntimeVersion = fmt.Sprintf("%s://%s", kl.containerRuntime.Type(), runtimeVersion)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			node.Status.NodeInfo.KubeletVersion = version.Get().String()
0000000000000000000000000000000000000000;;			// TODO: kube-proxy might be different version from kubelet in the future
0000000000000000000000000000000000000000;;			node.Status.NodeInfo.KubeProxyVersion = version.Get().String()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Set daemonEndpoints for the node.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeStatusDaemonEndpoints(node *v1.Node) {
0000000000000000000000000000000000000000;;		node.Status.DaemonEndpoints = *kl.daemonEndpoints
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Set images list for the node
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeStatusImages(node *v1.Node) {
0000000000000000000000000000000000000000;;		// Update image list of this node
0000000000000000000000000000000000000000;;		var imagesOnNode []v1.ContainerImage
0000000000000000000000000000000000000000;;		containerImages, err := kl.imageManager.GetImageList()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Error getting image list: %v", err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// sort the images from max to min, and only set top N images into the node status.
0000000000000000000000000000000000000000;;			sort.Sort(sliceutils.ByImageSize(containerImages))
0000000000000000000000000000000000000000;;			if maxImagesInNodeStatus < len(containerImages) {
0000000000000000000000000000000000000000;;				containerImages = containerImages[0:maxImagesInNodeStatus]
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, image := range containerImages {
0000000000000000000000000000000000000000;;				names := append(image.RepoDigests, image.RepoTags...)
0000000000000000000000000000000000000000;;				// Report up to maxNamesPerImageInNodeStatus names per image.
0000000000000000000000000000000000000000;;				if len(names) > maxNamesPerImageInNodeStatus {
0000000000000000000000000000000000000000;;					names = names[0:maxNamesPerImageInNodeStatus]
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				imagesOnNode = append(imagesOnNode, v1.ContainerImage{
0000000000000000000000000000000000000000;;					Names:     names,
0000000000000000000000000000000000000000;;					SizeBytes: image.Size,
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		node.Status.Images = imagesOnNode
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Set the GOOS and GOARCH for this node
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeStatusGoRuntime(node *v1.Node) {
0000000000000000000000000000000000000000;;		node.Status.NodeInfo.OperatingSystem = goruntime.GOOS
0000000000000000000000000000000000000000;;		node.Status.NodeInfo.Architecture = goruntime.GOARCH
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Set status for the node.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeStatusInfo(node *v1.Node) {
0000000000000000000000000000000000000000;;		kl.setNodeStatusMachineInfo(node)
0000000000000000000000000000000000000000;;		kl.setNodeStatusVersionInfo(node)
0000000000000000000000000000000000000000;;		kl.setNodeStatusDaemonEndpoints(node)
0000000000000000000000000000000000000000;;		kl.setNodeStatusImages(node)
0000000000000000000000000000000000000000;;		kl.setNodeStatusGoRuntime(node)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Set Ready condition for the node.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeReadyCondition(node *v1.Node) {
0000000000000000000000000000000000000000;;		// NOTE(aaronlevy): NodeReady condition needs to be the last in the list of node conditions.
0000000000000000000000000000000000000000;;		// This is due to an issue with version skewed kubelet and master components.
0000000000000000000000000000000000000000;;		// ref: https://github.com/kubernetes/kubernetes/issues/16961
0000000000000000000000000000000000000000;;		currentTime := metav1.NewTime(kl.clock.Now())
0000000000000000000000000000000000000000;;		var newNodeReadyCondition v1.NodeCondition
0000000000000000000000000000000000000000;;		rs := append(kl.runtimeState.runtimeErrors(), kl.runtimeState.networkErrors()...)
0000000000000000000000000000000000000000;;		if len(rs) == 0 {
0000000000000000000000000000000000000000;;			newNodeReadyCondition = v1.NodeCondition{
0000000000000000000000000000000000000000;;				Type:              v1.NodeReady,
0000000000000000000000000000000000000000;;				Status:            v1.ConditionTrue,
0000000000000000000000000000000000000000;;				Reason:            "KubeletReady",
0000000000000000000000000000000000000000;;				Message:           "kubelet is posting ready status",
0000000000000000000000000000000000000000;;				LastHeartbeatTime: currentTime,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			newNodeReadyCondition = v1.NodeCondition{
0000000000000000000000000000000000000000;;				Type:              v1.NodeReady,
0000000000000000000000000000000000000000;;				Status:            v1.ConditionFalse,
0000000000000000000000000000000000000000;;				Reason:            "KubeletNotReady",
0000000000000000000000000000000000000000;;				Message:           strings.Join(rs, ","),
0000000000000000000000000000000000000000;;				LastHeartbeatTime: currentTime,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Append AppArmor status if it's enabled.
0000000000000000000000000000000000000000;;		// TODO(tallclair): This is a temporary message until node feature reporting is added.
0000000000000000000000000000000000000000;;		if newNodeReadyCondition.Status == v1.ConditionTrue &&
0000000000000000000000000000000000000000;;			kl.appArmorValidator != nil && kl.appArmorValidator.ValidateHost() == nil {
0000000000000000000000000000000000000000;;			newNodeReadyCondition.Message = fmt.Sprintf("%s. AppArmor enabled", newNodeReadyCondition.Message)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Record any soft requirements that were not met in the container manager.
0000000000000000000000000000000000000000;;		status := kl.containerManager.Status()
0000000000000000000000000000000000000000;;		if status.SoftRequirements != nil {
0000000000000000000000000000000000000000;;			newNodeReadyCondition.Message = fmt.Sprintf("%s. WARNING: %s", newNodeReadyCondition.Message, status.SoftRequirements.Error())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		readyConditionUpdated := false
0000000000000000000000000000000000000000;;		needToRecordEvent := false
0000000000000000000000000000000000000000;;		for i := range node.Status.Conditions {
0000000000000000000000000000000000000000;;			if node.Status.Conditions[i].Type == v1.NodeReady {
0000000000000000000000000000000000000000;;				if node.Status.Conditions[i].Status == newNodeReadyCondition.Status {
0000000000000000000000000000000000000000;;					newNodeReadyCondition.LastTransitionTime = node.Status.Conditions[i].LastTransitionTime
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					newNodeReadyCondition.LastTransitionTime = currentTime
0000000000000000000000000000000000000000;;					needToRecordEvent = true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				node.Status.Conditions[i] = newNodeReadyCondition
0000000000000000000000000000000000000000;;				readyConditionUpdated = true
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !readyConditionUpdated {
0000000000000000000000000000000000000000;;			newNodeReadyCondition.LastTransitionTime = currentTime
0000000000000000000000000000000000000000;;			node.Status.Conditions = append(node.Status.Conditions, newNodeReadyCondition)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if needToRecordEvent {
0000000000000000000000000000000000000000;;			if newNodeReadyCondition.Status == v1.ConditionTrue {
0000000000000000000000000000000000000000;;				kl.recordNodeStatusEvent(v1.EventTypeNormal, events.NodeReady)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				kl.recordNodeStatusEvent(v1.EventTypeNormal, events.NodeNotReady)
0000000000000000000000000000000000000000;;				glog.Infof("Node became not ready: %+v", newNodeReadyCondition)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// setNodeMemoryPressureCondition for the node.
0000000000000000000000000000000000000000;;	// TODO: this needs to move somewhere centralized...
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeMemoryPressureCondition(node *v1.Node) {
0000000000000000000000000000000000000000;;		currentTime := metav1.NewTime(kl.clock.Now())
0000000000000000000000000000000000000000;;		var condition *v1.NodeCondition
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check if NodeMemoryPressure condition already exists and if it does, just pick it up for update.
0000000000000000000000000000000000000000;;		for i := range node.Status.Conditions {
0000000000000000000000000000000000000000;;			if node.Status.Conditions[i].Type == v1.NodeMemoryPressure {
0000000000000000000000000000000000000000;;				condition = &node.Status.Conditions[i]
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newCondition := false
0000000000000000000000000000000000000000;;		// If the NodeMemoryPressure condition doesn't exist, create one
0000000000000000000000000000000000000000;;		if condition == nil {
0000000000000000000000000000000000000000;;			condition = &v1.NodeCondition{
0000000000000000000000000000000000000000;;				Type:   v1.NodeMemoryPressure,
0000000000000000000000000000000000000000;;				Status: v1.ConditionUnknown,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// cannot be appended to node.Status.Conditions here because it gets
0000000000000000000000000000000000000000;;			// copied to the slice. So if we append to the slice here none of the
0000000000000000000000000000000000000000;;			// updates we make below are reflected in the slice.
0000000000000000000000000000000000000000;;			newCondition = true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Update the heartbeat time
0000000000000000000000000000000000000000;;		condition.LastHeartbeatTime = currentTime
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Note: The conditions below take care of the case when a new NodeMemoryPressure condition is
0000000000000000000000000000000000000000;;		// created and as well as the case when the condition already exists. When a new condition
0000000000000000000000000000000000000000;;		// is created its status is set to v1.ConditionUnknown which matches either
0000000000000000000000000000000000000000;;		// condition.Status != v1.ConditionTrue or
0000000000000000000000000000000000000000;;		// condition.Status != v1.ConditionFalse in the conditions below depending on whether
0000000000000000000000000000000000000000;;		// the kubelet is under memory pressure or not.
0000000000000000000000000000000000000000;;		if kl.evictionManager.IsUnderMemoryPressure() {
0000000000000000000000000000000000000000;;			if condition.Status != v1.ConditionTrue {
0000000000000000000000000000000000000000;;				condition.Status = v1.ConditionTrue
0000000000000000000000000000000000000000;;				condition.Reason = "KubeletHasInsufficientMemory"
0000000000000000000000000000000000000000;;				condition.Message = "kubelet has insufficient memory available"
0000000000000000000000000000000000000000;;				condition.LastTransitionTime = currentTime
0000000000000000000000000000000000000000;;				kl.recordNodeStatusEvent(v1.EventTypeNormal, "NodeHasInsufficientMemory")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			if condition.Status != v1.ConditionFalse {
0000000000000000000000000000000000000000;;				condition.Status = v1.ConditionFalse
0000000000000000000000000000000000000000;;				condition.Reason = "KubeletHasSufficientMemory"
0000000000000000000000000000000000000000;;				condition.Message = "kubelet has sufficient memory available"
0000000000000000000000000000000000000000;;				condition.LastTransitionTime = currentTime
0000000000000000000000000000000000000000;;				kl.recordNodeStatusEvent(v1.EventTypeNormal, "NodeHasSufficientMemory")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if newCondition {
0000000000000000000000000000000000000000;;			node.Status.Conditions = append(node.Status.Conditions, *condition)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// setNodeDiskPressureCondition for the node.
0000000000000000000000000000000000000000;;	// TODO: this needs to move somewhere centralized...
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeDiskPressureCondition(node *v1.Node) {
0000000000000000000000000000000000000000;;		currentTime := metav1.NewTime(kl.clock.Now())
0000000000000000000000000000000000000000;;		var condition *v1.NodeCondition
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check if NodeDiskPressure condition already exists and if it does, just pick it up for update.
0000000000000000000000000000000000000000;;		for i := range node.Status.Conditions {
0000000000000000000000000000000000000000;;			if node.Status.Conditions[i].Type == v1.NodeDiskPressure {
0000000000000000000000000000000000000000;;				condition = &node.Status.Conditions[i]
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newCondition := false
0000000000000000000000000000000000000000;;		// If the NodeDiskPressure condition doesn't exist, create one
0000000000000000000000000000000000000000;;		if condition == nil {
0000000000000000000000000000000000000000;;			condition = &v1.NodeCondition{
0000000000000000000000000000000000000000;;				Type:   v1.NodeDiskPressure,
0000000000000000000000000000000000000000;;				Status: v1.ConditionUnknown,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// cannot be appended to node.Status.Conditions here because it gets
0000000000000000000000000000000000000000;;			// copied to the slice. So if we append to the slice here none of the
0000000000000000000000000000000000000000;;			// updates we make below are reflected in the slice.
0000000000000000000000000000000000000000;;			newCondition = true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Update the heartbeat time
0000000000000000000000000000000000000000;;		condition.LastHeartbeatTime = currentTime
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Note: The conditions below take care of the case when a new NodeDiskPressure condition is
0000000000000000000000000000000000000000;;		// created and as well as the case when the condition already exists. When a new condition
0000000000000000000000000000000000000000;;		// is created its status is set to v1.ConditionUnknown which matches either
0000000000000000000000000000000000000000;;		// condition.Status != v1.ConditionTrue or
0000000000000000000000000000000000000000;;		// condition.Status != v1.ConditionFalse in the conditions below depending on whether
0000000000000000000000000000000000000000;;		// the kubelet is under disk pressure or not.
0000000000000000000000000000000000000000;;		if kl.evictionManager.IsUnderDiskPressure() {
0000000000000000000000000000000000000000;;			if condition.Status != v1.ConditionTrue {
0000000000000000000000000000000000000000;;				condition.Status = v1.ConditionTrue
0000000000000000000000000000000000000000;;				condition.Reason = "KubeletHasDiskPressure"
0000000000000000000000000000000000000000;;				condition.Message = "kubelet has disk pressure"
0000000000000000000000000000000000000000;;				condition.LastTransitionTime = currentTime
0000000000000000000000000000000000000000;;				kl.recordNodeStatusEvent(v1.EventTypeNormal, "NodeHasDiskPressure")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			if condition.Status != v1.ConditionFalse {
0000000000000000000000000000000000000000;;				condition.Status = v1.ConditionFalse
0000000000000000000000000000000000000000;;				condition.Reason = "KubeletHasNoDiskPressure"
0000000000000000000000000000000000000000;;				condition.Message = "kubelet has no disk pressure"
0000000000000000000000000000000000000000;;				condition.LastTransitionTime = currentTime
0000000000000000000000000000000000000000;;				kl.recordNodeStatusEvent(v1.EventTypeNormal, "NodeHasNoDiskPressure")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if newCondition {
0000000000000000000000000000000000000000;;			node.Status.Conditions = append(node.Status.Conditions, *condition)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Set OODCondition for the node.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeOODCondition(node *v1.Node) {
0000000000000000000000000000000000000000;;		currentTime := metav1.NewTime(kl.clock.Now())
0000000000000000000000000000000000000000;;		var nodeOODCondition *v1.NodeCondition
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check if NodeOutOfDisk condition already exists and if it does, just pick it up for update.
0000000000000000000000000000000000000000;;		for i := range node.Status.Conditions {
0000000000000000000000000000000000000000;;			if node.Status.Conditions[i].Type == v1.NodeOutOfDisk {
0000000000000000000000000000000000000000;;				nodeOODCondition = &node.Status.Conditions[i]
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newOODCondition := false
0000000000000000000000000000000000000000;;		// If the NodeOutOfDisk condition doesn't exist, create one.
0000000000000000000000000000000000000000;;		if nodeOODCondition == nil {
0000000000000000000000000000000000000000;;			nodeOODCondition = &v1.NodeCondition{
0000000000000000000000000000000000000000;;				Type:   v1.NodeOutOfDisk,
0000000000000000000000000000000000000000;;				Status: v1.ConditionUnknown,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// nodeOODCondition cannot be appended to node.Status.Conditions here because it gets
0000000000000000000000000000000000000000;;			// copied to the slice. So if we append nodeOODCondition to the slice here none of the
0000000000000000000000000000000000000000;;			// updates we make to nodeOODCondition below are reflected in the slice.
0000000000000000000000000000000000000000;;			newOODCondition = true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Update the heartbeat time irrespective of all the conditions.
0000000000000000000000000000000000000000;;		nodeOODCondition.LastHeartbeatTime = currentTime
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Note: The conditions below take care of the case when a new NodeOutOfDisk condition is
0000000000000000000000000000000000000000;;		// created and as well as the case when the condition already exists. When a new condition
0000000000000000000000000000000000000000;;		// is created its status is set to v1.ConditionUnknown which matches either
0000000000000000000000000000000000000000;;		// nodeOODCondition.Status != v1.ConditionTrue or
0000000000000000000000000000000000000000;;		// nodeOODCondition.Status != v1.ConditionFalse in the conditions below depending on whether
0000000000000000000000000000000000000000;;		// the kubelet is out of disk or not.
0000000000000000000000000000000000000000;;		if kl.isOutOfDisk() {
0000000000000000000000000000000000000000;;			if nodeOODCondition.Status != v1.ConditionTrue {
0000000000000000000000000000000000000000;;				nodeOODCondition.Status = v1.ConditionTrue
0000000000000000000000000000000000000000;;				nodeOODCondition.Reason = "KubeletOutOfDisk"
0000000000000000000000000000000000000000;;				nodeOODCondition.Message = "out of disk space"
0000000000000000000000000000000000000000;;				nodeOODCondition.LastTransitionTime = currentTime
0000000000000000000000000000000000000000;;				kl.recordNodeStatusEvent(v1.EventTypeNormal, "NodeOutOfDisk")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			if nodeOODCondition.Status != v1.ConditionFalse {
0000000000000000000000000000000000000000;;				// Update the out of disk condition when the condition status is unknown even if we
0000000000000000000000000000000000000000;;				// are within the outOfDiskTransitionFrequency duration. We do this to set the
0000000000000000000000000000000000000000;;				// condition status correctly at kubelet startup.
0000000000000000000000000000000000000000;;				if nodeOODCondition.Status == v1.ConditionUnknown || kl.clock.Since(nodeOODCondition.LastTransitionTime.Time) >= kl.outOfDiskTransitionFrequency {
0000000000000000000000000000000000000000;;					nodeOODCondition.Status = v1.ConditionFalse
0000000000000000000000000000000000000000;;					nodeOODCondition.Reason = "KubeletHasSufficientDisk"
0000000000000000000000000000000000000000;;					nodeOODCondition.Message = "kubelet has sufficient disk space available"
0000000000000000000000000000000000000000;;					nodeOODCondition.LastTransitionTime = currentTime
0000000000000000000000000000000000000000;;					kl.recordNodeStatusEvent(v1.EventTypeNormal, "NodeHasSufficientDisk")
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					glog.Infof("Node condition status for OutOfDisk is false, but last transition time is less than %s", kl.outOfDiskTransitionFrequency)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if newOODCondition {
0000000000000000000000000000000000000000;;			node.Status.Conditions = append(node.Status.Conditions, *nodeOODCondition)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Maintains Node.Spec.Unschedulable value from previous run of tryUpdateNodeStatus()
0000000000000000000000000000000000000000;;	// TODO: why is this a package var?
0000000000000000000000000000000000000000;;	var oldNodeUnschedulable bool
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// record if node schedulable change.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) recordNodeSchedulableEvent(node *v1.Node) {
0000000000000000000000000000000000000000;;		if oldNodeUnschedulable != node.Spec.Unschedulable {
0000000000000000000000000000000000000000;;			if node.Spec.Unschedulable {
0000000000000000000000000000000000000000;;				kl.recordNodeStatusEvent(v1.EventTypeNormal, events.NodeNotSchedulable)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				kl.recordNodeStatusEvent(v1.EventTypeNormal, events.NodeSchedulable)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			oldNodeUnschedulable = node.Spec.Unschedulable
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Update VolumesInUse field in Node Status only after states are synced up at least once
0000000000000000000000000000000000000000;;	// in volume reconciler.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeVolumesInUseStatus(node *v1.Node) {
0000000000000000000000000000000000000000;;		// Make sure to only update node status after reconciler starts syncing up states
0000000000000000000000000000000000000000;;		if kl.volumeManager.ReconcilerStatesHasBeenSynced() {
0000000000000000000000000000000000000000;;			node.Status.VolumesInUse = kl.volumeManager.GetVolumesInUse()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// setNodeStatus fills in the Status fields of the given Node, overwriting
0000000000000000000000000000000000000000;;	// any fields that are currently set.
0000000000000000000000000000000000000000;;	// TODO(madhusudancs): Simplify the logic for setting node conditions and
0000000000000000000000000000000000000000;;	// refactor the node status condition code out to a different file.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) setNodeStatus(node *v1.Node) {
0000000000000000000000000000000000000000;;		for _, f := range kl.setNodeStatusFuncs {
0000000000000000000000000000000000000000;;			if err := f(node); err != nil {
0000000000000000000000000000000000000000;;				glog.Warningf("Failed to set some node status fields: %s", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// defaultNodeStatusFuncs is a factory that generates the default set of
0000000000000000000000000000000000000000;;	// setNodeStatus funcs
0000000000000000000000000000000000000000;;	func (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error {
0000000000000000000000000000000000000000;;		// initial set of node status update handlers, can be modified by Option's
0000000000000000000000000000000000000000;;		withoutError := func(f func(*v1.Node)) func(*v1.Node) error {
0000000000000000000000000000000000000000;;			return func(n *v1.Node) error {
0000000000000000000000000000000000000000;;				f(n)
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return []func(*v1.Node) error{
0000000000000000000000000000000000000000;;			kl.setNodeAddress,
0000000000000000000000000000000000000000;;			withoutError(kl.setNodeStatusInfo),
0000000000000000000000000000000000000000;;			withoutError(kl.setNodeOODCondition),
0000000000000000000000000000000000000000;;			withoutError(kl.setNodeMemoryPressureCondition),
0000000000000000000000000000000000000000;;			withoutError(kl.setNodeDiskPressureCondition),
0000000000000000000000000000000000000000;;			withoutError(kl.setNodeReadyCondition),
0000000000000000000000000000000000000000;;			withoutError(kl.setNodeVolumesInUseStatus),
0000000000000000000000000000000000000000;;			withoutError(kl.recordNodeSchedulableEvent),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Validate given node IP belongs to the current host
0000000000000000000000000000000000000000;;	func (kl *Kubelet) validateNodeIP() error {
0000000000000000000000000000000000000000;;		if kl.nodeIP == nil {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Honor IP limitations set in setNodeStatus()
0000000000000000000000000000000000000000;;		if kl.nodeIP.IsLoopback() {
0000000000000000000000000000000000000000;;			return fmt.Errorf("nodeIP can't be loopback address")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kl.nodeIP.To4() == nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("nodeIP must be IPv4 address")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		addrs, err := net.InterfaceAddrs()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, addr := range addrs {
0000000000000000000000000000000000000000;;			var ip net.IP
0000000000000000000000000000000000000000;;			switch v := addr.(type) {
0000000000000000000000000000000000000000;;			case *net.IPNet:
0000000000000000000000000000000000000000;;				ip = v.IP
0000000000000000000000000000000000000000;;			case *net.IPAddr:
0000000000000000000000000000000000000000;;				ip = v.IP
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if ip != nil && ip.Equal(kl.nodeIP) {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fmt.Errorf("Node IP: %q not found in the host's network interfaces", kl.nodeIP.String())
0000000000000000000000000000000000000000;;	}
