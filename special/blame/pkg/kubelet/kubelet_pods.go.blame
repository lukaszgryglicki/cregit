0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
266eb08abb15154da4c0d887a3224856c04ae856;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"bytes"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"io"
0000000000000000000000000000000000000000;;		"io/ioutil"
0000000000000000000000000000000000000000;;		"net/http"
0000000000000000000000000000000000000000;;		"net/url"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"path"
0000000000000000000000000000000000000000;;		"path/filepath"
0000000000000000000000000000000000000000;;		"runtime"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		utilvalidation "k8s.io/apimachinery/pkg/util/validation"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/validation/field"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/remotecommand"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		v1helper "k8s.io/kubernetes/pkg/api/v1/helper"
0000000000000000000000000000000000000000;;		v1qos "k8s.io/kubernetes/pkg/api/v1/helper/qos"
0000000000000000000000000000000000000000;;		podutil "k8s.io/kubernetes/pkg/api/v1/pod"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api/v1/resource"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api/v1/validation"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/fieldpath"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/cm"
0000000000000000000000000000000000000000;;		kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/envvars"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/images"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/server/portforward"
0000000000000000000000000000000000000000;;		remotecommandserver "k8s.io/kubernetes/pkg/kubelet/server/remotecommand"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/status"
0000000000000000000000000000000000000000;;		kubetypes "k8s.io/kubernetes/pkg/kubelet/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/format"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/volume"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/volume/util/volumehelper"
0000000000000000000000000000000000000000;;		volumevalidation "k8s.io/kubernetes/pkg/volume/validation"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/third_party/forked/golang/expansion"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Get a list of pods that have data directories.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) listPodsFromDisk() ([]types.UID, error) {
0000000000000000000000000000000000000000;;		podInfos, err := ioutil.ReadDir(kl.getPodsDir())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pods := []types.UID{}
0000000000000000000000000000000000000000;;		for i := range podInfos {
0000000000000000000000000000000000000000;;			if podInfos[i].IsDir() {
0000000000000000000000000000000000000000;;				pods = append(pods, types.UID(podInfos[i].Name()))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pods, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetActivePods returns non-terminal pods
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GetActivePods() []*v1.Pod {
0000000000000000000000000000000000000000;;		allPods := kl.podManager.GetPods()
0000000000000000000000000000000000000000;;		activePods := kl.filterOutTerminatedPods(allPods)
0000000000000000000000000000000000000000;;		return activePods
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// makeDevices determines the devices for the given container.
0000000000000000000000000000000000000000;;	// Experimental.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) makeDevices(pod *v1.Pod, container *v1.Container) ([]kubecontainer.DeviceInfo, error) {
0000000000000000000000000000000000000000;;		if container.Resources.Limits.NvidiaGPU().IsZero() {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		nvidiaGPUPaths, err := kl.gpuManager.AllocateGPU(pod, container)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var devices []kubecontainer.DeviceInfo
0000000000000000000000000000000000000000;;		for _, path := range nvidiaGPUPaths {
0000000000000000000000000000000000000000;;			// Devices have to be mapped one to one because of nvidia CUDA library requirements.
0000000000000000000000000000000000000000;;			devices = append(devices, kubecontainer.DeviceInfo{PathOnHost: path, PathInContainer: path, Permissions: "mrw"})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return devices, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// makeMounts determines the mount points for the given container.
0000000000000000000000000000000000000000;;	func makeMounts(pod *v1.Pod, podDir string, container *v1.Container, hostName, hostDomain, podIP string, podVolumes kubecontainer.VolumeMap) ([]kubecontainer.Mount, error) {
0000000000000000000000000000000000000000;;		// Kubernetes only mounts on /etc/hosts if :
0000000000000000000000000000000000000000;;		// - container does not use hostNetwork and
0000000000000000000000000000000000000000;;		// - container is not an infrastructure(pause) container
0000000000000000000000000000000000000000;;		// - container is not already mounting on /etc/hosts
0000000000000000000000000000000000000000;;		// When the pause container is being created, its IP is still unknown. Hence, PodIP will not have been set.
0000000000000000000000000000000000000000;;		// OS is not Windows
0000000000000000000000000000000000000000;;		mountEtcHostsFile := !pod.Spec.HostNetwork && len(podIP) > 0 && runtime.GOOS != "windows"
0000000000000000000000000000000000000000;;		glog.V(3).Infof("container: %v/%v/%v podIP: %q creating hosts mount: %v", pod.Namespace, pod.Name, container.Name, podIP, mountEtcHostsFile)
0000000000000000000000000000000000000000;;		mounts := []kubecontainer.Mount{}
0000000000000000000000000000000000000000;;		for _, mount := range container.VolumeMounts {
0000000000000000000000000000000000000000;;			mountEtcHostsFile = mountEtcHostsFile && (mount.MountPath != etcHostsPath)
0000000000000000000000000000000000000000;;			vol, ok := podVolumes[mount.Name]
0000000000000000000000000000000000000000;;			if !ok || vol.Mounter == nil {
0000000000000000000000000000000000000000;;				glog.Warningf("Mount cannot be satisfied for container %q, because the volume is missing or the volume mounter is nil: %q", container.Name, mount)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			relabelVolume := false
0000000000000000000000000000000000000000;;			// If the volume supports SELinux and it has not been
0000000000000000000000000000000000000000;;			// relabeled already and it is not a read-only volume,
0000000000000000000000000000000000000000;;			// relabel it and mark it as labeled
0000000000000000000000000000000000000000;;			if vol.Mounter.GetAttributes().Managed && vol.Mounter.GetAttributes().SupportsSELinux && !vol.SELinuxLabeled {
0000000000000000000000000000000000000000;;				vol.SELinuxLabeled = true
0000000000000000000000000000000000000000;;				relabelVolume = true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			hostPath, err := volume.GetPath(vol.Mounter)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if mount.SubPath != "" {
0000000000000000000000000000000000000000;;				if filepath.IsAbs(mount.SubPath) {
0000000000000000000000000000000000000000;;					return nil, fmt.Errorf("error SubPath `%s` must not be an absolute path", mount.SubPath)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				err = volumevalidation.ValidatePathNoBacksteps(mount.SubPath)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return nil, fmt.Errorf("unable to provision SubPath `%s`: %v", mount.SubPath, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				fileinfo, err := os.Lstat(hostPath)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				perm := fileinfo.Mode()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				hostPath = filepath.Join(hostPath, mount.SubPath)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if subPathExists, err := util.FileOrSymlinkExists(hostPath); err != nil {
0000000000000000000000000000000000000000;;					glog.Errorf("Could not determine if subPath %s exists; will not attempt to change its permissions", hostPath)
0000000000000000000000000000000000000000;;				} else if !subPathExists {
0000000000000000000000000000000000000000;;					// Create the sub path now because if it's auto-created later when referenced, it may have an
0000000000000000000000000000000000000000;;					// incorrect ownership and mode. For example, the sub path directory must have at least g+rwx
0000000000000000000000000000000000000000;;					// when the pod specifies an fsGroup, and if the directory is not created here, Docker will
0000000000000000000000000000000000000000;;					// later auto-create it with the incorrect mode 0750
0000000000000000000000000000000000000000;;					if err := os.MkdirAll(hostPath, perm); err != nil {
0000000000000000000000000000000000000000;;						glog.Errorf("failed to mkdir:%s", hostPath)
0000000000000000000000000000000000000000;;						return nil, err
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// chmod the sub path because umask may have prevented us from making the sub path with the same
0000000000000000000000000000000000000000;;					// permissions as the mounter path
0000000000000000000000000000000000000000;;					if err := os.Chmod(hostPath, perm); err != nil {
0000000000000000000000000000000000000000;;						return nil, err
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Docker Volume Mounts fail on Windows if it is not of the form C:/
0000000000000000000000000000000000000000;;			containerPath := mount.MountPath
0000000000000000000000000000000000000000;;			if runtime.GOOS == "windows" {
0000000000000000000000000000000000000000;;				if (strings.HasPrefix(hostPath, "/") || strings.HasPrefix(hostPath, "\\")) && !strings.Contains(hostPath, ":") {
0000000000000000000000000000000000000000;;					hostPath = "c:" + hostPath
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if (strings.HasPrefix(containerPath, "/") || strings.HasPrefix(containerPath, "\\")) && !strings.Contains(containerPath, ":") {
0000000000000000000000000000000000000000;;					containerPath = "c:" + containerPath
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			mounts = append(mounts, kubecontainer.Mount{
0000000000000000000000000000000000000000;;				Name:           mount.Name,
0000000000000000000000000000000000000000;;				ContainerPath:  containerPath,
0000000000000000000000000000000000000000;;				HostPath:       hostPath,
0000000000000000000000000000000000000000;;				ReadOnly:       mount.ReadOnly,
0000000000000000000000000000000000000000;;				SELinuxRelabel: relabelVolume,
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if mountEtcHostsFile {
0000000000000000000000000000000000000000;;			hostAliases := pod.Spec.HostAliases
0000000000000000000000000000000000000000;;			hostsMount, err := makeHostsMount(podDir, podIP, hostName, hostDomain, hostAliases)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			mounts = append(mounts, *hostsMount)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return mounts, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// makeHostsMount makes the mountpoint for the hosts file that the containers
0000000000000000000000000000000000000000;;	// in a pod are injected with.
0000000000000000000000000000000000000000;;	func makeHostsMount(podDir, podIP, hostName, hostDomainName string, hostAliases []v1.HostAlias) (*kubecontainer.Mount, error) {
0000000000000000000000000000000000000000;;		hostsFilePath := path.Join(podDir, "etc-hosts")
0000000000000000000000000000000000000000;;		if err := ensureHostsFile(hostsFilePath, podIP, hostName, hostDomainName, hostAliases); err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return &kubecontainer.Mount{
0000000000000000000000000000000000000000;;			Name:           "k8s-managed-etc-hosts",
0000000000000000000000000000000000000000;;			ContainerPath:  etcHostsPath,
0000000000000000000000000000000000000000;;			HostPath:       hostsFilePath,
0000000000000000000000000000000000000000;;			ReadOnly:       false,
0000000000000000000000000000000000000000;;			SELinuxRelabel: true,
0000000000000000000000000000000000000000;;		}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ensureHostsFile ensures that the given host file has an up-to-date ip, host
0000000000000000000000000000000000000000;;	// name, and domain name.
0000000000000000000000000000000000000000;;	func ensureHostsFile(fileName, hostIP, hostName, hostDomainName string, hostAliases []v1.HostAlias) error {
0000000000000000000000000000000000000000;;		content := hostsFileContent(hostIP, hostName, hostDomainName, hostAliases)
0000000000000000000000000000000000000000;;		return ioutil.WriteFile(fileName, content, 0644)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// hostsFileContent is the content of the managed etc hosts
0000000000000000000000000000000000000000;;	func hostsFileContent(hostIP, hostName, hostDomainName string, hostAliases []v1.HostAlias) []byte {
0000000000000000000000000000000000000000;;		var buffer bytes.Buffer
0000000000000000000000000000000000000000;;		buffer.WriteString("# Kubernetes-managed hosts file.\n")
0000000000000000000000000000000000000000;;		buffer.WriteString("127.0.0.1\tlocalhost\n")                      // ipv4 localhost
0000000000000000000000000000000000000000;;		buffer.WriteString("::1\tlocalhost ip6-localhost ip6-loopback\n") // ipv6 localhost
0000000000000000000000000000000000000000;;		buffer.WriteString("fe00::0\tip6-localnet\n")
0000000000000000000000000000000000000000;;		buffer.WriteString("fe00::0\tip6-mcastprefix\n")
0000000000000000000000000000000000000000;;		buffer.WriteString("fe00::1\tip6-allnodes\n")
0000000000000000000000000000000000000000;;		buffer.WriteString("fe00::2\tip6-allrouters\n")
0000000000000000000000000000000000000000;;		if len(hostDomainName) > 0 {
0000000000000000000000000000000000000000;;			buffer.WriteString(fmt.Sprintf("%s\t%s.%s\t%s\n", hostIP, hostName, hostDomainName, hostName))
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			buffer.WriteString(fmt.Sprintf("%s\t%s\n", hostIP, hostName))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// write each IP/hostname pair as an entry into hosts file
0000000000000000000000000000000000000000;;		for _, hostAlias := range hostAliases {
0000000000000000000000000000000000000000;;			for _, hostname := range hostAlias.Hostnames {
0000000000000000000000000000000000000000;;				buffer.WriteString(fmt.Sprintf("%s\t%s\n", hostAlias.IP, hostname))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return buffer.Bytes()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// truncatePodHostnameIfNeeded truncates the pod hostname if it's longer than 63 chars.
0000000000000000000000000000000000000000;;	func truncatePodHostnameIfNeeded(podName, hostname string) (string, error) {
0000000000000000000000000000000000000000;;		// Cap hostname at 63 chars (specification is 64bytes which is 63 chars and the null terminating char).
0000000000000000000000000000000000000000;;		const hostnameMaxLen = 63
0000000000000000000000000000000000000000;;		if len(hostname) <= hostnameMaxLen {
0000000000000000000000000000000000000000;;			return hostname, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		truncated := hostname[:hostnameMaxLen]
0000000000000000000000000000000000000000;;		glog.Errorf("hostname for pod:%q was longer than %d. Truncated hostname to :%q", podName, hostnameMaxLen, truncated)
0000000000000000000000000000000000000000;;		// hostname should not end with '-' or '.'
0000000000000000000000000000000000000000;;		truncated = strings.TrimRight(truncated, "-.")
0000000000000000000000000000000000000000;;		if len(truncated) == 0 {
0000000000000000000000000000000000000000;;			// This should never happen.
0000000000000000000000000000000000000000;;			return "", fmt.Errorf("hostname for pod %q was invalid: %q", podName, hostname)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return truncated, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GeneratePodHostNameAndDomain creates a hostname and domain name for a pod,
0000000000000000000000000000000000000000;;	// given that pod's spec and annotations or returns an error.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GeneratePodHostNameAndDomain(pod *v1.Pod) (string, string, error) {
0000000000000000000000000000000000000000;;		// TODO(vmarmol): Handle better.
0000000000000000000000000000000000000000;;		clusterDomain := kl.clusterDomain
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		hostname := pod.Name
0000000000000000000000000000000000000000;;		if len(pod.Spec.Hostname) > 0 {
0000000000000000000000000000000000000000;;			if msgs := utilvalidation.IsDNS1123Label(pod.Spec.Hostname); len(msgs) != 0 {
0000000000000000000000000000000000000000;;				return "", "", fmt.Errorf("Pod Hostname %q is not a valid DNS label: %s", pod.Spec.Hostname, strings.Join(msgs, ";"))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			hostname = pod.Spec.Hostname
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		hostname, err := truncatePodHostnameIfNeeded(pod.Name, hostname)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		hostDomain := ""
0000000000000000000000000000000000000000;;		if len(pod.Spec.Subdomain) > 0 {
0000000000000000000000000000000000000000;;			if msgs := utilvalidation.IsDNS1123Label(pod.Spec.Subdomain); len(msgs) != 0 {
0000000000000000000000000000000000000000;;				return "", "", fmt.Errorf("Pod Subdomain %q is not a valid DNS label: %s", pod.Spec.Subdomain, strings.Join(msgs, ";"))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			hostDomain = fmt.Sprintf("%s.%s.svc.%s", pod.Spec.Subdomain, pod.Namespace, clusterDomain)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return hostname, hostDomain, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetPodCgroupParent gets pod cgroup parent from container manager.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GetPodCgroupParent(pod *v1.Pod) string {
0000000000000000000000000000000000000000;;		pcm := kl.containerManager.NewPodContainerManager()
0000000000000000000000000000000000000000;;		_, cgroupParent := pcm.GetPodContainerName(pod)
0000000000000000000000000000000000000000;;		return cgroupParent
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GenerateRunContainerOptions generates the RunContainerOptions, which can be used by
0000000000000000000000000000000000000000;;	// the container runtime to set parameters for launching a container.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GenerateRunContainerOptions(pod *v1.Pod, container *v1.Container, podIP string) (*kubecontainer.RunContainerOptions, bool, error) {
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		useClusterFirstPolicy := false
0000000000000000000000000000000000000000;;		cgroupParent := kl.GetPodCgroupParent(pod)
0000000000000000000000000000000000000000;;		opts := &kubecontainer.RunContainerOptions{CgroupParent: cgroupParent}
0000000000000000000000000000000000000000;;		hostname, hostDomainName, err := kl.GeneratePodHostNameAndDomain(pod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		opts.Hostname = hostname
0000000000000000000000000000000000000000;;		podName := volumehelper.GetUniquePodName(pod)
0000000000000000000000000000000000000000;;		volumes := kl.volumeManager.GetMountedVolumesForPod(podName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		opts.PortMappings = kubecontainer.MakePortMappings(container)
0000000000000000000000000000000000000000;;		// TODO(random-liu): Move following convert functions into pkg/kubelet/container
0000000000000000000000000000000000000000;;		opts.Devices, err = kl.makeDevices(pod, container)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		opts.Mounts, err = makeMounts(pod, kl.getPodDir(pod.UID), container, hostname, hostDomainName, podIP, volumes)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		opts.Envs, err = kl.makeEnvironmentVariables(pod, container, podIP)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Disabling adding TerminationMessagePath on Windows as these files would be mounted as docker volume and
0000000000000000000000000000000000000000;;		// Docker for Windows has a bug where only directories can be mounted
0000000000000000000000000000000000000000;;		if len(container.TerminationMessagePath) != 0 && runtime.GOOS != "windows" {
0000000000000000000000000000000000000000;;			p := kl.getPodContainerDir(pod.UID, container.Name)
0000000000000000000000000000000000000000;;			if err := os.MkdirAll(p, 0750); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Error on creating %q: %v", p, err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				opts.PodContainerDir = p
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		opts.DNS, opts.DNSSearch, useClusterFirstPolicy, err = kl.GetClusterDNS(pod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// only do this check if the experimental behavior is enabled, otherwise allow it to default to false
0000000000000000000000000000000000000000;;		if kl.experimentalHostUserNamespaceDefaulting {
0000000000000000000000000000000000000000;;			opts.EnableHostUserNamespace = kl.enableHostUserNamespace(pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return opts, useClusterFirstPolicy, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var masterServices = sets.NewString("kubernetes")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getServiceEnvVarMap makes a map[string]string of env vars for services a
0000000000000000000000000000000000000000;;	// pod in namespace ns should see.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) getServiceEnvVarMap(ns string) (map[string]string, error) {
0000000000000000000000000000000000000000;;		var (
0000000000000000000000000000000000000000;;			serviceMap = make(map[string]*v1.Service)
0000000000000000000000000000000000000000;;			m          = make(map[string]string)
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Get all service resources from the master (via a cache),
0000000000000000000000000000000000000000;;		// and populate them into service environment variables.
0000000000000000000000000000000000000000;;		if kl.serviceLister == nil {
0000000000000000000000000000000000000000;;			// Kubelets without masters (e.g. plain GCE ContainerVM) don't set env vars.
0000000000000000000000000000000000000000;;			return m, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		services, err := kl.serviceLister.List(labels.Everything())
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return m, fmt.Errorf("failed to list services when setting up env vars.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// project the services in namespace ns onto the master services
0000000000000000000000000000000000000000;;		for i := range services {
0000000000000000000000000000000000000000;;			service := services[i]
0000000000000000000000000000000000000000;;			// ignore services where ClusterIP is "None" or empty
0000000000000000000000000000000000000000;;			if !v1helper.IsServiceIPSet(service) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			serviceName := service.Name
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			switch service.Namespace {
0000000000000000000000000000000000000000;;			// for the case whether the master service namespace is the namespace the pod
0000000000000000000000000000000000000000;;			// is in, the pod should receive all the services in the namespace.
0000000000000000000000000000000000000000;;			//
0000000000000000000000000000000000000000;;			// ordering of the case clauses below enforces this
0000000000000000000000000000000000000000;;			case ns:
0000000000000000000000000000000000000000;;				serviceMap[serviceName] = service
0000000000000000000000000000000000000000;;			case kl.masterServiceNamespace:
0000000000000000000000000000000000000000;;				if masterServices.Has(serviceName) {
0000000000000000000000000000000000000000;;					if _, exists := serviceMap[serviceName]; !exists {
0000000000000000000000000000000000000000;;						serviceMap[serviceName] = service
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		mappedServices := []*v1.Service{}
0000000000000000000000000000000000000000;;		for key := range serviceMap {
0000000000000000000000000000000000000000;;			mappedServices = append(mappedServices, serviceMap[key])
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, e := range envvars.FromServices(mappedServices) {
0000000000000000000000000000000000000000;;			m[e.Name] = e.Value
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return m, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Make the environment variables for a pod in the given namespace.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) makeEnvironmentVariables(pod *v1.Pod, container *v1.Container, podIP string) ([]kubecontainer.EnvVar, error) {
0000000000000000000000000000000000000000;;		var result []kubecontainer.EnvVar
0000000000000000000000000000000000000000;;		// Note:  These are added to the docker Config, but are not included in the checksum computed
0000000000000000000000000000000000000000;;		// by kubecontainer.HashContainer(...).  That way, we can still determine whether an
0000000000000000000000000000000000000000;;		// v1.Container is already running by its hash. (We don't want to restart a container just
0000000000000000000000000000000000000000;;		// because some service changed.)
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// Note that there is a race between Kubelet seeing the pod and kubelet seeing the service.
0000000000000000000000000000000000000000;;		// To avoid this users can: (1) wait between starting a service and starting; or (2) detect
0000000000000000000000000000000000000000;;		// missing service env var and exit and be restarted; or (3) use DNS instead of env vars
0000000000000000000000000000000000000000;;		// and keep trying to resolve the DNS name of the service (recommended).
0000000000000000000000000000000000000000;;		serviceEnv, err := kl.getServiceEnvVarMap(pod.Namespace)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return result, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var (
0000000000000000000000000000000000000000;;			configMaps = make(map[string]*v1.ConfigMap)
0000000000000000000000000000000000000000;;			secrets    = make(map[string]*v1.Secret)
0000000000000000000000000000000000000000;;			tmpEnv     = make(map[string]string)
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Env will override EnvFrom variables.
0000000000000000000000000000000000000000;;		// Process EnvFrom first then allow Env to replace existing values.
0000000000000000000000000000000000000000;;		for _, envFrom := range container.EnvFrom {
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case envFrom.ConfigMapRef != nil:
0000000000000000000000000000000000000000;;				cm := envFrom.ConfigMapRef
0000000000000000000000000000000000000000;;				name := cm.Name
0000000000000000000000000000000000000000;;				configMap, ok := configMaps[name]
0000000000000000000000000000000000000000;;				if !ok {
0000000000000000000000000000000000000000;;					if kl.kubeClient == nil {
0000000000000000000000000000000000000000;;						return result, fmt.Errorf("Couldn't get configMap %v/%v, no kubeClient defined", pod.Namespace, name)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					optional := cm.Optional != nil && *cm.Optional
0000000000000000000000000000000000000000;;					configMap, err = kl.configMapManager.GetConfigMap(pod.Namespace, name)
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						if errors.IsNotFound(err) && optional {
0000000000000000000000000000000000000000;;							// ignore error when marked optional
0000000000000000000000000000000000000000;;							continue
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						return result, err
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					configMaps[name] = configMap
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				invalidKeys := []string{}
0000000000000000000000000000000000000000;;				for k, v := range configMap.Data {
0000000000000000000000000000000000000000;;					if len(envFrom.Prefix) > 0 {
0000000000000000000000000000000000000000;;						k = envFrom.Prefix + k
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if errMsgs := utilvalidation.IsCIdentifier(k); len(errMsgs) != 0 {
0000000000000000000000000000000000000000;;						invalidKeys = append(invalidKeys, k)
0000000000000000000000000000000000000000;;						continue
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					tmpEnv[k] = v
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if len(invalidKeys) > 0 {
0000000000000000000000000000000000000000;;					sort.Strings(invalidKeys)
0000000000000000000000000000000000000000;;					kl.recorder.Eventf(pod, v1.EventTypeWarning, "InvalidEnvironmentVariableNames", "Keys [%s] from the EnvFrom configMap %s/%s were skipped since they are considered invalid environment variable names.", strings.Join(invalidKeys, ", "), pod.Namespace, name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case envFrom.SecretRef != nil:
0000000000000000000000000000000000000000;;				s := envFrom.SecretRef
0000000000000000000000000000000000000000;;				name := s.Name
0000000000000000000000000000000000000000;;				secret, ok := secrets[name]
0000000000000000000000000000000000000000;;				if !ok {
0000000000000000000000000000000000000000;;					if kl.kubeClient == nil {
0000000000000000000000000000000000000000;;						return result, fmt.Errorf("Couldn't get secret %v/%v, no kubeClient defined", pod.Namespace, name)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					optional := s.Optional != nil && *s.Optional
0000000000000000000000000000000000000000;;					secret, err = kl.secretManager.GetSecret(pod.Namespace, name)
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						if errors.IsNotFound(err) && optional {
0000000000000000000000000000000000000000;;							// ignore error when marked optional
0000000000000000000000000000000000000000;;							continue
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						return result, err
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					secrets[name] = secret
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				invalidKeys := []string{}
0000000000000000000000000000000000000000;;				for k, v := range secret.Data {
0000000000000000000000000000000000000000;;					if len(envFrom.Prefix) > 0 {
0000000000000000000000000000000000000000;;						k = envFrom.Prefix + k
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if errMsgs := utilvalidation.IsCIdentifier(k); len(errMsgs) != 0 {
0000000000000000000000000000000000000000;;						invalidKeys = append(invalidKeys, k)
0000000000000000000000000000000000000000;;						continue
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					tmpEnv[k] = string(v)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if len(invalidKeys) > 0 {
0000000000000000000000000000000000000000;;					sort.Strings(invalidKeys)
0000000000000000000000000000000000000000;;					kl.recorder.Eventf(pod, v1.EventTypeWarning, "InvalidEnvironmentVariableNames", "Keys [%s] from the EnvFrom secret %s/%s were skipped since they are considered invalid environment variable names.", strings.Join(invalidKeys, ", "), pod.Namespace, name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Determine the final values of variables:
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// 1.  Determine the final value of each variable:
0000000000000000000000000000000000000000;;		//     a.  If the variable's Value is set, expand the `$(var)` references to other
0000000000000000000000000000000000000000;;		//         variables in the .Value field; the sources of variables are the declared
0000000000000000000000000000000000000000;;		//         variables of the container and the service environment variables
0000000000000000000000000000000000000000;;		//     b.  If a source is defined for an environment variable, resolve the source
0000000000000000000000000000000000000000;;		// 2.  Create the container's environment in the order variables are declared
0000000000000000000000000000000000000000;;		// 3.  Add remaining service environment vars
0000000000000000000000000000000000000000;;		var (
0000000000000000000000000000000000000000;;			mappingFunc = expansion.MappingFuncFor(tmpEnv, serviceEnv)
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		for _, envVar := range container.Env {
0000000000000000000000000000000000000000;;			runtimeVal := envVar.Value
0000000000000000000000000000000000000000;;			if runtimeVal != "" {
0000000000000000000000000000000000000000;;				// Step 1a: expand variable references
0000000000000000000000000000000000000000;;				runtimeVal = expansion.Expand(runtimeVal, mappingFunc)
0000000000000000000000000000000000000000;;			} else if envVar.ValueFrom != nil {
0000000000000000000000000000000000000000;;				// Step 1b: resolve alternate env var sources
0000000000000000000000000000000000000000;;				switch {
0000000000000000000000000000000000000000;;				case envVar.ValueFrom.FieldRef != nil:
0000000000000000000000000000000000000000;;					runtimeVal, err = kl.podFieldSelectorRuntimeValue(envVar.ValueFrom.FieldRef, pod, podIP)
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						return result, err
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				case envVar.ValueFrom.ResourceFieldRef != nil:
0000000000000000000000000000000000000000;;					defaultedPod, defaultedContainer, err := kl.defaultPodLimitsForDownwardApi(pod, container)
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						return result, err
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					runtimeVal, err = containerResourceRuntimeValue(envVar.ValueFrom.ResourceFieldRef, defaultedPod, defaultedContainer)
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						return result, err
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				case envVar.ValueFrom.ConfigMapKeyRef != nil:
0000000000000000000000000000000000000000;;					cm := envVar.ValueFrom.ConfigMapKeyRef
0000000000000000000000000000000000000000;;					name := cm.Name
0000000000000000000000000000000000000000;;					key := cm.Key
0000000000000000000000000000000000000000;;					optional := cm.Optional != nil && *cm.Optional
0000000000000000000000000000000000000000;;					configMap, ok := configMaps[name]
0000000000000000000000000000000000000000;;					if !ok {
0000000000000000000000000000000000000000;;						if kl.kubeClient == nil {
0000000000000000000000000000000000000000;;							return result, fmt.Errorf("Couldn't get configMap %v/%v, no kubeClient defined", pod.Namespace, name)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						configMap, err = kl.configMapManager.GetConfigMap(pod.Namespace, name)
0000000000000000000000000000000000000000;;						if err != nil {
0000000000000000000000000000000000000000;;							if errors.IsNotFound(err) && optional {
0000000000000000000000000000000000000000;;								// ignore error when marked optional
0000000000000000000000000000000000000000;;								continue
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							return result, err
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						configMaps[name] = configMap
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					runtimeVal, ok = configMap.Data[key]
0000000000000000000000000000000000000000;;					if !ok {
0000000000000000000000000000000000000000;;						if optional {
0000000000000000000000000000000000000000;;							continue
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						return result, fmt.Errorf("Couldn't find key %v in ConfigMap %v/%v", key, pod.Namespace, name)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				case envVar.ValueFrom.SecretKeyRef != nil:
0000000000000000000000000000000000000000;;					s := envVar.ValueFrom.SecretKeyRef
0000000000000000000000000000000000000000;;					name := s.Name
0000000000000000000000000000000000000000;;					key := s.Key
0000000000000000000000000000000000000000;;					optional := s.Optional != nil && *s.Optional
0000000000000000000000000000000000000000;;					secret, ok := secrets[name]
0000000000000000000000000000000000000000;;					if !ok {
0000000000000000000000000000000000000000;;						if kl.kubeClient == nil {
0000000000000000000000000000000000000000;;							return result, fmt.Errorf("Couldn't get secret %v/%v, no kubeClient defined", pod.Namespace, name)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						secret, err = kl.secretManager.GetSecret(pod.Namespace, name)
0000000000000000000000000000000000000000;;						if err != nil {
0000000000000000000000000000000000000000;;							if errors.IsNotFound(err) && optional {
0000000000000000000000000000000000000000;;								// ignore error when marked optional
0000000000000000000000000000000000000000;;								continue
0000000000000000000000000000000000000000;;							}
0000000000000000000000000000000000000000;;							return result, err
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						secrets[name] = secret
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					runtimeValBytes, ok := secret.Data[key]
0000000000000000000000000000000000000000;;					if !ok {
0000000000000000000000000000000000000000;;						if optional {
0000000000000000000000000000000000000000;;							continue
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						return result, fmt.Errorf("Couldn't find key %v in Secret %v/%v", key, pod.Namespace, name)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					runtimeVal = string(runtimeValBytes)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Accesses apiserver+Pods.
0000000000000000000000000000000000000000;;			// So, the master may set service env vars, or kubelet may.  In case both are doing
0000000000000000000000000000000000000000;;			// it, we delete the key from the kubelet-generated ones so we don't have duplicate
0000000000000000000000000000000000000000;;			// env vars.
0000000000000000000000000000000000000000;;			// TODO: remove this next line once all platforms use apiserver+Pods.
0000000000000000000000000000000000000000;;			delete(serviceEnv, envVar.Name)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			tmpEnv[envVar.Name] = runtimeVal
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Append the env vars
0000000000000000000000000000000000000000;;		for k, v := range tmpEnv {
0000000000000000000000000000000000000000;;			result = append(result, kubecontainer.EnvVar{Name: k, Value: v})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Append remaining service env vars.
0000000000000000000000000000000000000000;;		for k, v := range serviceEnv {
0000000000000000000000000000000000000000;;			// Accesses apiserver+Pods.
0000000000000000000000000000000000000000;;			// So, the master may set service env vars, or kubelet may.  In case both are doing
0000000000000000000000000000000000000000;;			// it, we skip the key from the kubelet-generated ones so we don't have duplicate
0000000000000000000000000000000000000000;;			// env vars.
0000000000000000000000000000000000000000;;			// TODO: remove this next line once all platforms use apiserver+Pods.
0000000000000000000000000000000000000000;;			if _, present := tmpEnv[k]; !present {
0000000000000000000000000000000000000000;;				result = append(result, kubecontainer.EnvVar{Name: k, Value: v})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// podFieldSelectorRuntimeValue returns the runtime value of the given
0000000000000000000000000000000000000000;;	// selector for a pod.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) podFieldSelectorRuntimeValue(fs *v1.ObjectFieldSelector, pod *v1.Pod, podIP string) (string, error) {
0000000000000000000000000000000000000000;;		internalFieldPath, _, err := api.Scheme.ConvertFieldLabel(fs.APIVersion, "Pod", fs.FieldPath, "")
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		switch internalFieldPath {
0000000000000000000000000000000000000000;;		case "spec.nodeName":
0000000000000000000000000000000000000000;;			return pod.Spec.NodeName, nil
0000000000000000000000000000000000000000;;		case "spec.serviceAccountName":
0000000000000000000000000000000000000000;;			return pod.Spec.ServiceAccountName, nil
0000000000000000000000000000000000000000;;		case "status.hostIP":
0000000000000000000000000000000000000000;;			hostIP, err := kl.getHostIPAnyWay()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return "", err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return hostIP.String(), nil
0000000000000000000000000000000000000000;;		case "status.podIP":
0000000000000000000000000000000000000000;;			return podIP, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return fieldpath.ExtractFieldPathAsString(pod, internalFieldPath)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// containerResourceRuntimeValue returns the value of the provided container resource
0000000000000000000000000000000000000000;;	func containerResourceRuntimeValue(fs *v1.ResourceFieldSelector, pod *v1.Pod, container *v1.Container) (string, error) {
0000000000000000000000000000000000000000;;		containerName := fs.ContainerName
0000000000000000000000000000000000000000;;		if len(containerName) == 0 {
0000000000000000000000000000000000000000;;			return resource.ExtractContainerResourceValue(fs, container)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			return resource.ExtractResourceValueByContainerName(fs, pod, containerName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// One of the following arguments must be non-nil: runningPod, status.
0000000000000000000000000000000000000000;;	// TODO: Modify containerRuntime.KillPod() to accept the right arguments.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) killPod(pod *v1.Pod, runningPod *kubecontainer.Pod, status *kubecontainer.PodStatus, gracePeriodOverride *int64) error {
0000000000000000000000000000000000000000;;		var p kubecontainer.Pod
0000000000000000000000000000000000000000;;		if runningPod != nil {
0000000000000000000000000000000000000000;;			p = *runningPod
0000000000000000000000000000000000000000;;		} else if status != nil {
0000000000000000000000000000000000000000;;			p = kubecontainer.ConvertPodStatusToRunningPod(kl.GetRuntime().Type(), status)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			return fmt.Errorf("one of the two arguments must be non-nil: runningPod, status")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Call the container runtime KillPod method which stops all running containers of the pod
0000000000000000000000000000000000000000;;		if err := kl.containerRuntime.KillPod(pod, p, gracePeriodOverride); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := kl.containerManager.UpdateQOSCgroups(); err != nil {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("Failed to update QoS cgroups while killing pod: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// makePodDataDirs creates the dirs for the pod datas.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) makePodDataDirs(pod *v1.Pod) error {
0000000000000000000000000000000000000000;;		uid := pod.UID
0000000000000000000000000000000000000000;;		if err := os.MkdirAll(kl.getPodDir(uid), 0750); err != nil && !os.IsExist(err) {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := os.MkdirAll(kl.getPodVolumesDir(uid), 0750); err != nil && !os.IsExist(err) {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := os.MkdirAll(kl.getPodPluginsDir(uid), 0750); err != nil && !os.IsExist(err) {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getPullSecretsForPod inspects the Pod and retrieves the referenced pull
0000000000000000000000000000000000000000;;	// secrets.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) getPullSecretsForPod(pod *v1.Pod) []v1.Secret {
0000000000000000000000000000000000000000;;		pullSecrets := []v1.Secret{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, secretRef := range pod.Spec.ImagePullSecrets {
0000000000000000000000000000000000000000;;			secret, err := kl.secretManager.GetSecret(pod.Namespace, secretRef.Name)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Warningf("Unable to retrieve pull secret %s/%s for %s/%s due to %v.  The image pull may not succeed.", pod.Namespace, secretRef.Name, pod.Namespace, pod.Name, err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			pullSecrets = append(pullSecrets, *secret)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return pullSecrets
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// podIsTerminated returns true if pod is in the terminated state ("Failed" or "Succeeded").
0000000000000000000000000000000000000000;;	func (kl *Kubelet) podIsTerminated(pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		// Check the cached pod status which was set after the last sync.
0000000000000000000000000000000000000000;;		status, ok := kl.statusManager.GetPodStatus(pod.UID)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			// If there is no cached status, use the status from the
0000000000000000000000000000000000000000;;			// apiserver. This is useful if kubelet has recently been
0000000000000000000000000000000000000000;;			// restarted.
0000000000000000000000000000000000000000;;			status = pod.Status
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return status.Phase == v1.PodFailed || status.Phase == v1.PodSucceeded || (pod.DeletionTimestamp != nil && notRunning(status.ContainerStatuses))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodResourcesAreReclaimed returns true if all required node-level resources that a pod was consuming have
0000000000000000000000000000000000000000;;	// been reclaimed by the kubelet.  Reclaiming resources is a prerequisite to deleting a pod from the API server.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) PodResourcesAreReclaimed(pod *v1.Pod, status v1.PodStatus) bool {
0000000000000000000000000000000000000000;;		if !notRunning(status.ContainerStatuses) {
0000000000000000000000000000000000000000;;			// We shouldnt delete pods that still have running containers
0000000000000000000000000000000000000000;;			glog.V(3).Infof("Pod %q is terminated, but some containers are still running", format.Pod(pod))
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kl.podVolumesExist(pod.UID) && !kl.kubeletConfiguration.KeepTerminatedPodVolumes {
0000000000000000000000000000000000000000;;			// We shouldnt delete pods whose volumes have not been cleaned up if we are not keeping terminated pod volumes
0000000000000000000000000000000000000000;;			glog.V(3).Infof("Pod %q is terminated, but some volumes have not been cleaned up", format.Pod(pod))
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if kl.kubeletConfiguration.CgroupsPerQOS {
0000000000000000000000000000000000000000;;			pcm := kl.containerManager.NewPodContainerManager()
0000000000000000000000000000000000000000;;			if pcm.Exists(pod) {
0000000000000000000000000000000000000000;;				glog.V(3).Infof("Pod %q is terminated, but pod cgroup sandbox has not been cleaned up", format.Pod(pod))
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// podResourcesAreReclaimed simply calls PodResourcesAreReclaimed with the most up-to-date status.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) podResourcesAreReclaimed(pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		status, ok := kl.statusManager.GetPodStatus(pod.UID)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			status = pod.Status
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return kl.PodResourcesAreReclaimed(pod, status)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// notRunning returns true if every status is terminated or waiting, or the status list
0000000000000000000000000000000000000000;;	// is empty.
0000000000000000000000000000000000000000;;	func notRunning(statuses []v1.ContainerStatus) bool {
0000000000000000000000000000000000000000;;		for _, status := range statuses {
0000000000000000000000000000000000000000;;			if status.State.Terminated == nil && status.State.Waiting == nil {
0000000000000000000000000000000000000000;;				return false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// filterOutTerminatedPods returns the given pods which the status manager
0000000000000000000000000000000000000000;;	// does not consider failed or succeeded.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) filterOutTerminatedPods(pods []*v1.Pod) []*v1.Pod {
0000000000000000000000000000000000000000;;		var filteredPods []*v1.Pod
0000000000000000000000000000000000000000;;		for _, p := range pods {
0000000000000000000000000000000000000000;;			if kl.podIsTerminated(p) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			filteredPods = append(filteredPods, p)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return filteredPods
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// removeOrphanedPodStatuses removes obsolete entries in podStatus where
0000000000000000000000000000000000000000;;	// the pod is no longer considered bound to this node.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) removeOrphanedPodStatuses(pods []*v1.Pod, mirrorPods []*v1.Pod) {
0000000000000000000000000000000000000000;;		podUIDs := make(map[types.UID]bool)
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			podUIDs[pod.UID] = true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pod := range mirrorPods {
0000000000000000000000000000000000000000;;			podUIDs[pod.UID] = true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kl.statusManager.RemoveOrphanedStatuses(podUIDs)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// HandlePodCleanups performs a series of cleanup work, including terminating
0000000000000000000000000000000000000000;;	// pod workers, killing unwanted pods, and removing orphaned volumes/pod
0000000000000000000000000000000000000000;;	// directories.
0000000000000000000000000000000000000000;;	// NOTE: This function is executed by the main sync loop, so it
0000000000000000000000000000000000000000;;	// should not contain any blocking calls.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) HandlePodCleanups() error {
0000000000000000000000000000000000000000;;		// The kubelet lacks checkpointing, so we need to introspect the set of pods
0000000000000000000000000000000000000000;;		// in the cgroup tree prior to inspecting the set of pods in our pod manager.
0000000000000000000000000000000000000000;;		// this ensures our view of the cgroup tree does not mistakenly observe pods
0000000000000000000000000000000000000000;;		// that are added after the fact...
0000000000000000000000000000000000000000;;		var (
0000000000000000000000000000000000000000;;			cgroupPods map[types.UID]cm.CgroupName
0000000000000000000000000000000000000000;;			err        error
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		if kl.cgroupsPerQOS {
0000000000000000000000000000000000000000;;			pcm := kl.containerManager.NewPodContainerManager()
0000000000000000000000000000000000000000;;			cgroupPods, err = pcm.GetAllPodsFromCgroups()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("failed to get list of pods that still exist on cgroup mounts: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		allPods, mirrorPods := kl.podManager.GetPodsAndMirrorPods()
0000000000000000000000000000000000000000;;		// Pod phase progresses monotonically. Once a pod has reached a final state,
0000000000000000000000000000000000000000;;		// it should never leave regardless of the restart policy. The statuses
0000000000000000000000000000000000000000;;		// of such pods should not be changed, and there is no need to sync them.
0000000000000000000000000000000000000000;;		// TODO: the logic here does not handle two cases:
0000000000000000000000000000000000000000;;		//   1. If the containers were removed immediately after they died, kubelet
0000000000000000000000000000000000000000;;		//      may fail to generate correct statuses, let alone filtering correctly.
0000000000000000000000000000000000000000;;		//   2. If kubelet restarted before writing the terminated status for a pod
0000000000000000000000000000000000000000;;		//      to the apiserver, it could still restart the terminated pod (even
0000000000000000000000000000000000000000;;		//      though the pod was not considered terminated by the apiserver).
0000000000000000000000000000000000000000;;		// These two conditions could be alleviated by checkpointing kubelet.
0000000000000000000000000000000000000000;;		activePods := kl.filterOutTerminatedPods(allPods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		desiredPods := make(map[types.UID]empty)
0000000000000000000000000000000000000000;;		for _, pod := range activePods {
0000000000000000000000000000000000000000;;			desiredPods[pod.UID] = empty{}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Stop the workers for no-longer existing pods.
0000000000000000000000000000000000000000;;		// TODO: is here the best place to forget pod workers?
0000000000000000000000000000000000000000;;		kl.podWorkers.ForgetNonExistingPodWorkers(desiredPods)
0000000000000000000000000000000000000000;;		kl.probeManager.CleanupPods(activePods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		runningPods, err := kl.runtimeCache.GetPods()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Error listing containers: %#v", err)
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, pod := range runningPods {
0000000000000000000000000000000000000000;;			if _, found := desiredPods[pod.ID]; !found {
0000000000000000000000000000000000000000;;				kl.podKillingCh <- &kubecontainer.PodPair{APIPod: nil, RunningPod: pod}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.removeOrphanedPodStatuses(allPods, mirrorPods)
0000000000000000000000000000000000000000;;		// Note that we just killed the unwanted pods. This may not have reflected
0000000000000000000000000000000000000000;;		// in the cache. We need to bypass the cache to get the latest set of
0000000000000000000000000000000000000000;;		// running pods to clean up the volumes.
0000000000000000000000000000000000000000;;		// TODO: Evaluate the performance impact of bypassing the runtime cache.
0000000000000000000000000000000000000000;;		runningPods, err = kl.containerRuntime.GetPods(false)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Error listing containers: %#v", err)
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Remove any orphaned volumes.
0000000000000000000000000000000000000000;;		// Note that we pass all pods (including terminated pods) to the function,
0000000000000000000000000000000000000000;;		// so that we don't remove volumes associated with terminated but not yet
0000000000000000000000000000000000000000;;		// deleted pods.
0000000000000000000000000000000000000000;;		err = kl.cleanupOrphanedPodDirs(allPods, runningPods)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			// We want all cleanup tasks to be run even if one of them failed. So
0000000000000000000000000000000000000000;;			// we just log an error here and continue other cleanup tasks.
0000000000000000000000000000000000000000;;			// This also applies to the other clean up tasks.
0000000000000000000000000000000000000000;;			glog.Errorf("Failed cleaning up orphaned pod directories: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Remove any orphaned mirror pods.
0000000000000000000000000000000000000000;;		kl.podManager.DeleteOrphanedMirrorPods()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Remove any cgroups in the hierarchy for pods that are no longer running.
0000000000000000000000000000000000000000;;		if kl.cgroupsPerQOS {
0000000000000000000000000000000000000000;;			kl.cleanupOrphanedPodCgroups(cgroupPods, activePods)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.backOff.GC()
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// podKiller launches a goroutine to kill a pod received from the channel if
0000000000000000000000000000000000000000;;	// another goroutine isn't already in action.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) podKiller() {
0000000000000000000000000000000000000000;;		killing := sets.NewString()
0000000000000000000000000000000000000000;;		// guard for the killing set
0000000000000000000000000000000000000000;;		lock := sync.Mutex{}
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case podPair, ok := <-kl.podKillingCh:
0000000000000000000000000000000000000000;;				if !ok {
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				runningPod := podPair.RunningPod
0000000000000000000000000000000000000000;;				apiPod := podPair.APIPod
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				lock.Lock()
0000000000000000000000000000000000000000;;				exists := killing.Has(string(runningPod.ID))
0000000000000000000000000000000000000000;;				if !exists {
0000000000000000000000000000000000000000;;					killing.Insert(string(runningPod.ID))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				lock.Unlock()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if !exists {
0000000000000000000000000000000000000000;;					go func(apiPod *v1.Pod, runningPod *kubecontainer.Pod) {
0000000000000000000000000000000000000000;;						glog.V(2).Infof("Killing unwanted pod %q", runningPod.Name)
0000000000000000000000000000000000000000;;						err := kl.killPod(apiPod, runningPod, nil, nil)
0000000000000000000000000000000000000000;;						if err != nil {
0000000000000000000000000000000000000000;;							glog.Errorf("Failed killing the pod %q: %v", runningPod.Name, err)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;						lock.Lock()
0000000000000000000000000000000000000000;;						killing.Delete(string(runningPod.ID))
0000000000000000000000000000000000000000;;						lock.Unlock()
0000000000000000000000000000000000000000;;					}(apiPod, runningPod)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// checkHostPortConflicts detects pods with conflicted host ports.
0000000000000000000000000000000000000000;;	func hasHostPortConflicts(pods []*v1.Pod) bool {
0000000000000000000000000000000000000000;;		ports := sets.String{}
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			if errs := validation.AccumulateUniqueHostPorts(pod.Spec.Containers, &ports, field.NewPath("spec", "containers")); len(errs) > 0 {
0000000000000000000000000000000000000000;;				glog.Errorf("Pod %q: HostPort is already allocated, ignoring: %v", format.Pod(pod), errs)
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if errs := validation.AccumulateUniqueHostPorts(pod.Spec.InitContainers, &ports, field.NewPath("spec", "initContainers")); len(errs) > 0 {
0000000000000000000000000000000000000000;;				glog.Errorf("Pod %q: HostPort is already allocated, ignoring: %v", format.Pod(pod), errs)
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// validateContainerLogStatus returns the container ID for the desired container to retrieve logs for, based on the state
0000000000000000000000000000000000000000;;	// of the container. The previous flag will only return the logs for the last terminated container, otherwise, the current
0000000000000000000000000000000000000000;;	// running container is preferred over a previous termination. If info about the container is not available then a specific
0000000000000000000000000000000000000000;;	// error is returned to the end user.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) validateContainerLogStatus(podName string, podStatus *v1.PodStatus, containerName string, previous bool) (containerID kubecontainer.ContainerID, err error) {
0000000000000000000000000000000000000000;;		var cID string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cStatus, found := podutil.GetContainerStatus(podStatus.ContainerStatuses, containerName)
0000000000000000000000000000000000000000;;		// if not found, check the init containers
0000000000000000000000000000000000000000;;		if !found {
0000000000000000000000000000000000000000;;			cStatus, found = podutil.GetContainerStatus(podStatus.InitContainerStatuses, containerName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !found {
0000000000000000000000000000000000000000;;			return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is not available", containerName, podName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		lastState := cStatus.LastTerminationState
0000000000000000000000000000000000000000;;		waiting, running, terminated := cStatus.State.Waiting, cStatus.State.Running, cStatus.State.Terminated
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		switch {
0000000000000000000000000000000000000000;;		case previous:
0000000000000000000000000000000000000000;;			if lastState.Terminated == nil {
0000000000000000000000000000000000000000;;				return kubecontainer.ContainerID{}, fmt.Errorf("previous terminated container %q in pod %q not found", containerName, podName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			cID = lastState.Terminated.ContainerID
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case running != nil:
0000000000000000000000000000000000000000;;			cID = cStatus.ContainerID
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case terminated != nil:
0000000000000000000000000000000000000000;;			cID = terminated.ContainerID
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case lastState.Terminated != nil:
0000000000000000000000000000000000000000;;			cID = lastState.Terminated.ContainerID
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case waiting != nil:
0000000000000000000000000000000000000000;;			// output some info for the most common pending failures
0000000000000000000000000000000000000000;;			switch reason := waiting.Reason; reason {
0000000000000000000000000000000000000000;;			case images.ErrImagePull.Error():
0000000000000000000000000000000000000000;;				return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is waiting to start: image can't be pulled", containerName, podName)
0000000000000000000000000000000000000000;;			case images.ErrImagePullBackOff.Error():
0000000000000000000000000000000000000000;;				return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is waiting to start: trying and failing to pull image", containerName, podName)
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is waiting to start: %v", containerName, podName, reason)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			// unrecognized state
0000000000000000000000000000000000000000;;			return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is waiting to start - no logs yet", containerName, podName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return kubecontainer.ParseContainerID(cID), nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetKubeletContainerLogs returns logs from the container
0000000000000000000000000000000000000000;;	// TODO: this method is returning logs of random container attempts, when it should be returning the most recent attempt
0000000000000000000000000000000000000000;;	// or all of them.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GetKubeletContainerLogs(podFullName, containerName string, logOptions *v1.PodLogOptions, stdout, stderr io.Writer) error {
0000000000000000000000000000000000000000;;		// Pod workers periodically write status to statusManager. If status is not
0000000000000000000000000000000000000000;;		// cached there, something is wrong (or kubelet just restarted and hasn't
0000000000000000000000000000000000000000;;		// caught up yet). Just assume the pod is not ready yet.
0000000000000000000000000000000000000000;;		name, namespace, err := kubecontainer.ParsePodFullName(podFullName)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("unable to parse pod full name %q: %v", podFullName, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pod, ok := kl.GetPodByName(namespace, name)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			return fmt.Errorf("pod %q cannot be found - no logs available", name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podUID := pod.UID
0000000000000000000000000000000000000000;;		if mirrorPod, ok := kl.podManager.GetMirrorPodByPod(pod); ok {
0000000000000000000000000000000000000000;;			podUID = mirrorPod.UID
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podStatus, found := kl.statusManager.GetPodStatus(podUID)
0000000000000000000000000000000000000000;;		if !found {
0000000000000000000000000000000000000000;;			// If there is no cached status, use the status from the
0000000000000000000000000000000000000000;;			// apiserver. This is useful if kubelet has recently been
0000000000000000000000000000000000000000;;			// restarted.
0000000000000000000000000000000000000000;;			podStatus = pod.Status
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Consolidate the logic here with kuberuntime.GetContainerLogs, here we convert container name to containerID,
0000000000000000000000000000000000000000;;		// but inside kuberuntime we convert container id back to container name and restart count.
0000000000000000000000000000000000000000;;		// TODO: After separate container log lifecycle management, we should get log based on the existing log files
0000000000000000000000000000000000000000;;		// instead of container status.
0000000000000000000000000000000000000000;;		containerID, err := kl.validateContainerLogStatus(pod.Name, &podStatus, containerName, logOptions.Previous)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Do a zero-byte write to stdout before handing off to the container runtime.
0000000000000000000000000000000000000000;;		// This ensures at least one Write call is made to the writer when copying starts,
0000000000000000000000000000000000000000;;		// even if we then block waiting for log output from the container.
0000000000000000000000000000000000000000;;		if _, err := stdout.Write([]byte{}); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if kl.dockerLegacyService != nil {
0000000000000000000000000000000000000000;;			// dockerLegacyService should only be non-nil when we actually need it, so
0000000000000000000000000000000000000000;;			// inject it into the runtimeService.
0000000000000000000000000000000000000000;;			// TODO(random-liu): Remove this hack after deprecating unsupported log driver.
0000000000000000000000000000000000000000;;			return kl.dockerLegacyService.GetContainerLogs(pod, containerID, logOptions, stdout, stderr)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return kl.containerRuntime.GetContainerLogs(pod, containerID, logOptions, stdout, stderr)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetPhase returns the phase of a pod given its container info.
0000000000000000000000000000000000000000;;	// This func is exported to simplify integration with 3rd party kubelet
0000000000000000000000000000000000000000;;	// integrations like kubernetes-mesos.
0000000000000000000000000000000000000000;;	func GetPhase(spec *v1.PodSpec, info []v1.ContainerStatus) v1.PodPhase {
0000000000000000000000000000000000000000;;		initialized := 0
0000000000000000000000000000000000000000;;		pendingInitialization := 0
0000000000000000000000000000000000000000;;		failedInitialization := 0
0000000000000000000000000000000000000000;;		for _, container := range spec.InitContainers {
0000000000000000000000000000000000000000;;			containerStatus, ok := podutil.GetContainerStatus(info, container.Name)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				pendingInitialization++
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case containerStatus.State.Running != nil:
0000000000000000000000000000000000000000;;				pendingInitialization++
0000000000000000000000000000000000000000;;			case containerStatus.State.Terminated != nil:
0000000000000000000000000000000000000000;;				if containerStatus.State.Terminated.ExitCode == 0 {
0000000000000000000000000000000000000000;;					initialized++
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					failedInitialization++
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case containerStatus.State.Waiting != nil:
0000000000000000000000000000000000000000;;				if containerStatus.LastTerminationState.Terminated != nil {
0000000000000000000000000000000000000000;;					if containerStatus.LastTerminationState.Terminated.ExitCode == 0 {
0000000000000000000000000000000000000000;;						initialized++
0000000000000000000000000000000000000000;;					} else {
0000000000000000000000000000000000000000;;						failedInitialization++
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					pendingInitialization++
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				pendingInitialization++
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		unknown := 0
0000000000000000000000000000000000000000;;		running := 0
0000000000000000000000000000000000000000;;		waiting := 0
0000000000000000000000000000000000000000;;		stopped := 0
0000000000000000000000000000000000000000;;		failed := 0
0000000000000000000000000000000000000000;;		succeeded := 0
0000000000000000000000000000000000000000;;		for _, container := range spec.Containers {
0000000000000000000000000000000000000000;;			containerStatus, ok := podutil.GetContainerStatus(info, container.Name)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				unknown++
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case containerStatus.State.Running != nil:
0000000000000000000000000000000000000000;;				running++
0000000000000000000000000000000000000000;;			case containerStatus.State.Terminated != nil:
0000000000000000000000000000000000000000;;				stopped++
0000000000000000000000000000000000000000;;				if containerStatus.State.Terminated.ExitCode == 0 {
0000000000000000000000000000000000000000;;					succeeded++
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					failed++
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			case containerStatus.State.Waiting != nil:
0000000000000000000000000000000000000000;;				if containerStatus.LastTerminationState.Terminated != nil {
0000000000000000000000000000000000000000;;					stopped++
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					waiting++
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				unknown++
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if failedInitialization > 0 && spec.RestartPolicy == v1.RestartPolicyNever {
0000000000000000000000000000000000000000;;			return v1.PodFailed
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		switch {
0000000000000000000000000000000000000000;;		case pendingInitialization > 0:
0000000000000000000000000000000000000000;;			fallthrough
0000000000000000000000000000000000000000;;		case waiting > 0:
0000000000000000000000000000000000000000;;			glog.V(5).Infof("pod waiting > 0, pending")
0000000000000000000000000000000000000000;;			// One or more containers has not been started
0000000000000000000000000000000000000000;;			return v1.PodPending
0000000000000000000000000000000000000000;;		case running > 0 && unknown == 0:
0000000000000000000000000000000000000000;;			// All containers have been started, and at least
0000000000000000000000000000000000000000;;			// one container is running
0000000000000000000000000000000000000000;;			return v1.PodRunning
0000000000000000000000000000000000000000;;		case running == 0 && stopped > 0 && unknown == 0:
0000000000000000000000000000000000000000;;			// All containers are terminated
0000000000000000000000000000000000000000;;			if spec.RestartPolicy == v1.RestartPolicyAlways {
0000000000000000000000000000000000000000;;				// All containers are in the process of restarting
0000000000000000000000000000000000000000;;				return v1.PodRunning
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if stopped == succeeded {
0000000000000000000000000000000000000000;;				// RestartPolicy is not Always, and all
0000000000000000000000000000000000000000;;				// containers are terminated in success
0000000000000000000000000000000000000000;;				return v1.PodSucceeded
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if spec.RestartPolicy == v1.RestartPolicyNever {
0000000000000000000000000000000000000000;;				// RestartPolicy is Never, and all containers are
0000000000000000000000000000000000000000;;				// terminated with at least one in failure
0000000000000000000000000000000000000000;;				return v1.PodFailed
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// RestartPolicy is OnFailure, and at least one in failure
0000000000000000000000000000000000000000;;			// and in the process of restarting
0000000000000000000000000000000000000000;;			return v1.PodRunning
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			glog.V(5).Infof("pod default case, pending")
0000000000000000000000000000000000000000;;			return v1.PodPending
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// generateAPIPodStatus creates the final API pod status for a pod, given the
0000000000000000000000000000000000000000;;	// internal pod status.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) generateAPIPodStatus(pod *v1.Pod, podStatus *kubecontainer.PodStatus) v1.PodStatus {
0000000000000000000000000000000000000000;;		glog.V(3).Infof("Generating status for %q", format.Pod(pod))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// check if an internal module has requested the pod is evicted.
0000000000000000000000000000000000000000;;		for _, podSyncHandler := range kl.PodSyncHandlers {
0000000000000000000000000000000000000000;;			if result := podSyncHandler.ShouldEvict(pod); result.Evict {
0000000000000000000000000000000000000000;;				return v1.PodStatus{
0000000000000000000000000000000000000000;;					Phase:   v1.PodFailed,
0000000000000000000000000000000000000000;;					Reason:  result.Reason,
0000000000000000000000000000000000000000;;					Message: result.Message,
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		s := kl.convertStatusToAPIStatus(pod, podStatus)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Assume info is ready to process
0000000000000000000000000000000000000000;;		spec := &pod.Spec
0000000000000000000000000000000000000000;;		allStatus := append(append([]v1.ContainerStatus{}, s.ContainerStatuses...), s.InitContainerStatuses...)
0000000000000000000000000000000000000000;;		s.Phase = GetPhase(spec, allStatus)
0000000000000000000000000000000000000000;;		kl.probeManager.UpdatePodStatus(pod.UID, s)
0000000000000000000000000000000000000000;;		s.Conditions = append(s.Conditions, status.GeneratePodInitializedCondition(spec, s.InitContainerStatuses, s.Phase))
0000000000000000000000000000000000000000;;		s.Conditions = append(s.Conditions, status.GeneratePodReadyCondition(spec, s.ContainerStatuses, s.Phase))
0000000000000000000000000000000000000000;;		// s (the PodStatus we are creating) will not have a PodScheduled condition yet, because converStatusToAPIStatus()
0000000000000000000000000000000000000000;;		// does not create one. If the existing PodStatus has a PodScheduled condition, then copy it into s and make sure
0000000000000000000000000000000000000000;;		// it is set to true. If the existing PodStatus does not have a PodScheduled condition, then create one that is set to true.
0000000000000000000000000000000000000000;;		if _, oldPodScheduled := podutil.GetPodCondition(&pod.Status, v1.PodScheduled); oldPodScheduled != nil {
0000000000000000000000000000000000000000;;			s.Conditions = append(s.Conditions, *oldPodScheduled)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podutil.UpdatePodCondition(&pod.Status, &v1.PodCondition{
0000000000000000000000000000000000000000;;			Type:   v1.PodScheduled,
0000000000000000000000000000000000000000;;			Status: v1.ConditionTrue,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if !kl.standaloneMode {
0000000000000000000000000000000000000000;;			hostIP, err := kl.getHostIPAnyWay()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Cannot get host IP: %v", err)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				s.HostIP = hostIP.String()
0000000000000000000000000000000000000000;;				if kubecontainer.IsHostNetworkPod(pod) && s.PodIP == "" {
0000000000000000000000000000000000000000;;					s.PodIP = hostIP.String()
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return *s
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// convertStatusToAPIStatus creates an api PodStatus for the given pod from
0000000000000000000000000000000000000000;;	// the given internal pod status.  It is purely transformative and does not
0000000000000000000000000000000000000000;;	// alter the kubelet state at all.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) convertStatusToAPIStatus(pod *v1.Pod, podStatus *kubecontainer.PodStatus) *v1.PodStatus {
0000000000000000000000000000000000000000;;		var apiPodStatus v1.PodStatus
0000000000000000000000000000000000000000;;		apiPodStatus.PodIP = podStatus.IP
0000000000000000000000000000000000000000;;		// set status for Pods created on versions of kube older than 1.6
0000000000000000000000000000000000000000;;		apiPodStatus.QOSClass = v1qos.GetPodQOS(pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		apiPodStatus.ContainerStatuses = kl.convertToAPIContainerStatuses(
0000000000000000000000000000000000000000;;			pod, podStatus,
0000000000000000000000000000000000000000;;			pod.Status.ContainerStatuses,
0000000000000000000000000000000000000000;;			pod.Spec.Containers,
0000000000000000000000000000000000000000;;			len(pod.Spec.InitContainers) > 0,
0000000000000000000000000000000000000000;;			false,
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		apiPodStatus.InitContainerStatuses = kl.convertToAPIContainerStatuses(
0000000000000000000000000000000000000000;;			pod, podStatus,
0000000000000000000000000000000000000000;;			pod.Status.InitContainerStatuses,
0000000000000000000000000000000000000000;;			pod.Spec.InitContainers,
0000000000000000000000000000000000000000;;			len(pod.Spec.InitContainers) > 0,
0000000000000000000000000000000000000000;;			true,
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return &apiPodStatus
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// convertToAPIContainerStatuses converts the given internal container
0000000000000000000000000000000000000000;;	// statuses into API container statuses.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) convertToAPIContainerStatuses(pod *v1.Pod, podStatus *kubecontainer.PodStatus, previousStatus []v1.ContainerStatus, containers []v1.Container, hasInitContainers, isInitContainer bool) []v1.ContainerStatus {
0000000000000000000000000000000000000000;;		convertContainerStatus := func(cs *kubecontainer.ContainerStatus) *v1.ContainerStatus {
0000000000000000000000000000000000000000;;			cid := cs.ID.String()
0000000000000000000000000000000000000000;;			status := &v1.ContainerStatus{
0000000000000000000000000000000000000000;;				Name:         cs.Name,
0000000000000000000000000000000000000000;;				RestartCount: int32(cs.RestartCount),
0000000000000000000000000000000000000000;;				Image:        cs.Image,
0000000000000000000000000000000000000000;;				ImageID:      cs.ImageID,
0000000000000000000000000000000000000000;;				ContainerID:  cid,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			switch cs.State {
0000000000000000000000000000000000000000;;			case kubecontainer.ContainerStateRunning:
0000000000000000000000000000000000000000;;				status.State.Running = &v1.ContainerStateRunning{StartedAt: metav1.NewTime(cs.StartedAt)}
0000000000000000000000000000000000000000;;			case kubecontainer.ContainerStateCreated:
0000000000000000000000000000000000000000;;				// Treat containers in the "created" state as if they are exited.
0000000000000000000000000000000000000000;;				// The pod workers are supposed start all containers it creates in
0000000000000000000000000000000000000000;;				// one sync (syncPod) iteration. There should not be any normal
0000000000000000000000000000000000000000;;				// "created" containers when the pod worker generates the status at
0000000000000000000000000000000000000000;;				// the beginning of a sync iteration.
0000000000000000000000000000000000000000;;				fallthrough
0000000000000000000000000000000000000000;;			case kubecontainer.ContainerStateExited:
0000000000000000000000000000000000000000;;				status.State.Terminated = &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;					ExitCode:    int32(cs.ExitCode),
0000000000000000000000000000000000000000;;					Reason:      cs.Reason,
0000000000000000000000000000000000000000;;					Message:     cs.Message,
0000000000000000000000000000000000000000;;					StartedAt:   metav1.NewTime(cs.StartedAt),
0000000000000000000000000000000000000000;;					FinishedAt:  metav1.NewTime(cs.FinishedAt),
0000000000000000000000000000000000000000;;					ContainerID: cid,
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				status.State.Waiting = &v1.ContainerStateWaiting{}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return status
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Fetch old containers statuses from old pod status.
0000000000000000000000000000000000000000;;		oldStatuses := make(map[string]v1.ContainerStatus, len(containers))
0000000000000000000000000000000000000000;;		for _, status := range previousStatus {
0000000000000000000000000000000000000000;;			oldStatuses[status.Name] = status
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Set all container statuses to default waiting state
0000000000000000000000000000000000000000;;		statuses := make(map[string]*v1.ContainerStatus, len(containers))
0000000000000000000000000000000000000000;;		defaultWaitingState := v1.ContainerState{Waiting: &v1.ContainerStateWaiting{Reason: "ContainerCreating"}}
0000000000000000000000000000000000000000;;		if hasInitContainers {
0000000000000000000000000000000000000000;;			defaultWaitingState = v1.ContainerState{Waiting: &v1.ContainerStateWaiting{Reason: "PodInitializing"}}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, container := range containers {
0000000000000000000000000000000000000000;;			status := &v1.ContainerStatus{
0000000000000000000000000000000000000000;;				Name:  container.Name,
0000000000000000000000000000000000000000;;				Image: container.Image,
0000000000000000000000000000000000000000;;				State: defaultWaitingState,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Apply some values from the old statuses as the default values.
0000000000000000000000000000000000000000;;			if oldStatus, found := oldStatuses[container.Name]; found {
0000000000000000000000000000000000000000;;				status.RestartCount = oldStatus.RestartCount
0000000000000000000000000000000000000000;;				status.LastTerminationState = oldStatus.LastTerminationState
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			statuses[container.Name] = status
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Make the latest container status comes first.
0000000000000000000000000000000000000000;;		sort.Sort(sort.Reverse(kubecontainer.SortContainerStatusesByCreationTime(podStatus.ContainerStatuses)))
0000000000000000000000000000000000000000;;		// Set container statuses according to the statuses seen in pod status
0000000000000000000000000000000000000000;;		containerSeen := map[string]int{}
0000000000000000000000000000000000000000;;		for _, cStatus := range podStatus.ContainerStatuses {
0000000000000000000000000000000000000000;;			cName := cStatus.Name
0000000000000000000000000000000000000000;;			if _, ok := statuses[cName]; !ok {
0000000000000000000000000000000000000000;;				// This would also ignore the infra container.
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if containerSeen[cName] >= 2 {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			status := convertContainerStatus(cStatus)
0000000000000000000000000000000000000000;;			if containerSeen[cName] == 0 {
0000000000000000000000000000000000000000;;				statuses[cName] = status
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				statuses[cName].LastTerminationState = status.State
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			containerSeen[cName] = containerSeen[cName] + 1
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Handle the containers failed to be started, which should be in Waiting state.
0000000000000000000000000000000000000000;;		for _, container := range containers {
0000000000000000000000000000000000000000;;			if isInitContainer {
0000000000000000000000000000000000000000;;				// If the init container is terminated with exit code 0, it won't be restarted.
0000000000000000000000000000000000000000;;				// TODO(random-liu): Handle this in a cleaner way.
0000000000000000000000000000000000000000;;				s := podStatus.FindContainerStatusByName(container.Name)
0000000000000000000000000000000000000000;;				if s != nil && s.State == kubecontainer.ContainerStateExited && s.ExitCode == 0 {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// If a container should be restarted in next syncpod, it is *Waiting*.
0000000000000000000000000000000000000000;;			if !kubecontainer.ShouldContainerBeRestarted(&container, pod, podStatus) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			status := statuses[container.Name]
0000000000000000000000000000000000000000;;			reason, message, ok := kl.reasonCache.Get(pod.UID, container.Name)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				// In fact, we could also apply Waiting state here, but it is less informative,
0000000000000000000000000000000000000000;;				// and the container will be restarted soon, so we prefer the original state here.
0000000000000000000000000000000000000000;;				// Note that with the current implementation of ShouldContainerBeRestarted the original state here
0000000000000000000000000000000000000000;;				// could be:
0000000000000000000000000000000000000000;;				//   * Waiting: There is no associated historical container and start failure reason record.
0000000000000000000000000000000000000000;;				//   * Terminated: The container is terminated.
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if status.State.Terminated != nil {
0000000000000000000000000000000000000000;;				status.LastTerminationState = status.State
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			status.State = v1.ContainerState{
0000000000000000000000000000000000000000;;				Waiting: &v1.ContainerStateWaiting{
0000000000000000000000000000000000000000;;					Reason:  reason.Error(),
0000000000000000000000000000000000000000;;					Message: message,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			statuses[container.Name] = status
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var containerStatuses []v1.ContainerStatus
0000000000000000000000000000000000000000;;		for _, status := range statuses {
0000000000000000000000000000000000000000;;			containerStatuses = append(containerStatuses, *status)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Sort the container statuses since clients of this interface expect the list
0000000000000000000000000000000000000000;;		// of containers in a pod has a deterministic order.
0000000000000000000000000000000000000000;;		if isInitContainer {
0000000000000000000000000000000000000000;;			kubetypes.SortInitContainerStatuses(pod, containerStatuses)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			sort.Sort(kubetypes.SortedContainerStatuses(containerStatuses))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return containerStatuses
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Returns logs of current machine.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) ServeLogs(w http.ResponseWriter, req *http.Request) {
0000000000000000000000000000000000000000;;		// TODO: whitelist logs we are willing to serve
0000000000000000000000000000000000000000;;		kl.logServer.ServeHTTP(w, req)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// findContainer finds and returns the container with the given pod ID, full name, and container name.
0000000000000000000000000000000000000000;;	// It returns nil if not found.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) findContainer(podFullName string, podUID types.UID, containerName string) (*kubecontainer.Container, error) {
0000000000000000000000000000000000000000;;		pods, err := kl.containerRuntime.GetPods(false)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podUID = kl.podManager.TranslatePodUID(podUID)
0000000000000000000000000000000000000000;;		pod := kubecontainer.Pods(pods).FindPod(podFullName, podUID)
0000000000000000000000000000000000000000;;		return pod.FindContainerByName(containerName), nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Run a command in a container, returns the combined stdout, stderr as an array of bytes
0000000000000000000000000000000000000000;;	func (kl *Kubelet) RunInContainer(podFullName string, podUID types.UID, containerName string, cmd []string) ([]byte, error) {
0000000000000000000000000000000000000000;;		container, err := kl.findContainer(podFullName, podUID, containerName)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if container == nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("container not found (%q)", containerName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// TODO(tallclair): Pass a proper timeout value.
0000000000000000000000000000000000000000;;		return kl.runner.RunInContainer(container.ID, cmd, 0)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ExecInContainer executes a command in a container, connecting the supplied
0000000000000000000000000000000000000000;;	// stdin/stdout/stderr to the command's IO streams.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) ExecInContainer(podFullName string, podUID types.UID, containerName string, cmd []string, stdin io.Reader, stdout, stderr io.WriteCloser, tty bool, resize <-chan remotecommand.TerminalSize, timeout time.Duration) error {
0000000000000000000000000000000000000000;;		streamingRuntime, ok := kl.containerRuntime.(kubecontainer.DirectStreamingRuntime)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			return fmt.Errorf("streaming methods not supported by runtime")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		container, err := kl.findContainer(podFullName, podUID, containerName)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if container == nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("container not found (%q)", containerName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return streamingRuntime.ExecInContainer(container.ID, cmd, stdin, stdout, stderr, tty, resize, timeout)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// AttachContainer uses the container runtime to attach the given streams to
0000000000000000000000000000000000000000;;	// the given container.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) AttachContainer(podFullName string, podUID types.UID, containerName string, stdin io.Reader, stdout, stderr io.WriteCloser, tty bool, resize <-chan remotecommand.TerminalSize) error {
0000000000000000000000000000000000000000;;		streamingRuntime, ok := kl.containerRuntime.(kubecontainer.DirectStreamingRuntime)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			return fmt.Errorf("streaming methods not supported by runtime")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		container, err := kl.findContainer(podFullName, podUID, containerName)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if container == nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("container not found (%q)", containerName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return streamingRuntime.AttachContainer(container.ID, stdin, stdout, stderr, tty, resize)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PortForward connects to the pod's port and copies data between the port
0000000000000000000000000000000000000000;;	// and the stream.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) PortForward(podFullName string, podUID types.UID, port int32, stream io.ReadWriteCloser) error {
0000000000000000000000000000000000000000;;		streamingRuntime, ok := kl.containerRuntime.(kubecontainer.DirectStreamingRuntime)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			return fmt.Errorf("streaming methods not supported by runtime")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pods, err := kl.containerRuntime.GetPods(false)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podUID = kl.podManager.TranslatePodUID(podUID)
0000000000000000000000000000000000000000;;		pod := kubecontainer.Pods(pods).FindPod(podFullName, podUID)
0000000000000000000000000000000000000000;;		if pod.IsEmpty() {
0000000000000000000000000000000000000000;;			return fmt.Errorf("pod not found (%q)", podFullName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return streamingRuntime.PortForward(&pod, port, stream)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetExec gets the URL the exec will be served from, or nil if the Kubelet will serve it.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GetExec(podFullName string, podUID types.UID, containerName string, cmd []string, streamOpts remotecommandserver.Options) (*url.URL, error) {
0000000000000000000000000000000000000000;;		switch streamingRuntime := kl.containerRuntime.(type) {
0000000000000000000000000000000000000000;;		case kubecontainer.DirectStreamingRuntime:
0000000000000000000000000000000000000000;;			// Kubelet will serve the exec directly.
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		case kubecontainer.IndirectStreamingRuntime:
0000000000000000000000000000000000000000;;			container, err := kl.findContainer(podFullName, podUID, containerName)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if container == nil {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("container not found (%q)", containerName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return streamingRuntime.GetExec(container.ID, cmd, streamOpts.Stdin, streamOpts.Stdout, streamOpts.Stderr, streamOpts.TTY)
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("container runtime does not support exec")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetAttach gets the URL the attach will be served from, or nil if the Kubelet will serve it.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GetAttach(podFullName string, podUID types.UID, containerName string, streamOpts remotecommandserver.Options) (*url.URL, error) {
0000000000000000000000000000000000000000;;		switch streamingRuntime := kl.containerRuntime.(type) {
0000000000000000000000000000000000000000;;		case kubecontainer.DirectStreamingRuntime:
0000000000000000000000000000000000000000;;			// Kubelet will serve the attach directly.
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		case kubecontainer.IndirectStreamingRuntime:
0000000000000000000000000000000000000000;;			container, err := kl.findContainer(podFullName, podUID, containerName)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if container == nil {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("container %s not found in pod %s", containerName, podFullName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// The TTY setting for attach must match the TTY setting in the initial container configuration,
0000000000000000000000000000000000000000;;			// since whether the process is running in a TTY cannot be changed after it has started.  We
0000000000000000000000000000000000000000;;			// need the api.Pod to get the TTY status.
0000000000000000000000000000000000000000;;			pod, found := kl.GetPodByFullName(podFullName)
0000000000000000000000000000000000000000;;			if !found || (string(podUID) != "" && pod.UID != podUID) {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("pod %s not found", podFullName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			containerSpec := kubecontainer.GetContainerSpec(pod, containerName)
0000000000000000000000000000000000000000;;			if containerSpec == nil {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("container %s not found in pod %s", containerName, podFullName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			tty := containerSpec.TTY
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			return streamingRuntime.GetAttach(container.ID, streamOpts.Stdin, streamOpts.Stdout, streamOpts.Stderr, tty)
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("container runtime does not support attach")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetPortForward gets the URL the port-forward will be served from, or nil if the Kubelet will serve it.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) GetPortForward(podName, podNamespace string, podUID types.UID, portForwardOpts portforward.V4Options) (*url.URL, error) {
0000000000000000000000000000000000000000;;		switch streamingRuntime := kl.containerRuntime.(type) {
0000000000000000000000000000000000000000;;		case kubecontainer.DirectStreamingRuntime:
0000000000000000000000000000000000000000;;			// Kubelet will serve the attach directly.
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		case kubecontainer.IndirectStreamingRuntime:
0000000000000000000000000000000000000000;;			pods, err := kl.containerRuntime.GetPods(false)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			podUID = kl.podManager.TranslatePodUID(podUID)
0000000000000000000000000000000000000000;;			podFullName := kubecontainer.BuildPodFullName(podName, podNamespace)
0000000000000000000000000000000000000000;;			pod := kubecontainer.Pods(pods).FindPod(podFullName, podUID)
0000000000000000000000000000000000000000;;			if pod.IsEmpty() {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("pod not found (%q)", podFullName)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			return streamingRuntime.GetPortForward(podName, podNamespace, podUID, portForwardOpts.Ports)
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("container runtime does not support port-forward")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// cleanupOrphanedPodCgroups removes cgroups that should no longer exist.
0000000000000000000000000000000000000000;;	// it reconciles the cached state of cgroupPods with the specified list of runningPods
0000000000000000000000000000000000000000;;	func (kl *Kubelet) cleanupOrphanedPodCgroups(cgroupPods map[types.UID]cm.CgroupName, activePods []*v1.Pod) {
0000000000000000000000000000000000000000;;		// Add all running pods to the set that we want to preserve
0000000000000000000000000000000000000000;;		podSet := sets.NewString()
0000000000000000000000000000000000000000;;		for _, pod := range activePods {
0000000000000000000000000000000000000000;;			podSet.Insert(string(pod.UID))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pcm := kl.containerManager.NewPodContainerManager()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Iterate over all the found pods to verify if they should be running
0000000000000000000000000000000000000000;;		for uid, val := range cgroupPods {
0000000000000000000000000000000000000000;;			// if the pod is in the running set, its not a candidate for cleanup
0000000000000000000000000000000000000000;;			if podSet.Has(string(uid)) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// If volumes have not been unmounted/detached, do not delete the cgroup
0000000000000000000000000000000000000000;;			// so any memory backed volumes don't have their charges propagated to the
0000000000000000000000000000000000000000;;			// parent croup.  If the volumes still exist, reduce the cpu shares for any
0000000000000000000000000000000000000000;;			// process in the cgroup to the minimum value while we wait.  if the kubelet
0000000000000000000000000000000000000000;;			// is configured to keep terminated volumes, we will delete the cgroup and not block.
0000000000000000000000000000000000000000;;			if podVolumesExist := kl.podVolumesExist(uid); podVolumesExist && !kl.kubeletConfiguration.KeepTerminatedPodVolumes {
0000000000000000000000000000000000000000;;				glog.V(3).Infof("Orphaned pod %q found, but volumes not yet removed.  Reducing cpu to minimum", uid)
0000000000000000000000000000000000000000;;				if err := pcm.ReduceCPULimits(val); err != nil {
0000000000000000000000000000000000000000;;					glog.Warningf("Failed to reduce cpu time for pod %q pending volume cleanup due to %v", uid, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(3).Infof("Orphaned pod %q found, removing pod cgroups", uid)
0000000000000000000000000000000000000000;;			// Destroy all cgroups of pod that should not be running,
0000000000000000000000000000000000000000;;			// by first killing all the attached processes to these cgroups.
0000000000000000000000000000000000000000;;			// We ignore errors thrown by the method, as the housekeeping loop would
0000000000000000000000000000000000000000;;			// again try to delete these unwanted pod cgroups
0000000000000000000000000000000000000000;;			go pcm.Destroy(val)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// enableHostUserNamespace determines if the host user namespace should be used by the container runtime.
0000000000000000000000000000000000000000;;	// Returns true if the pod is using a host pid, pic, or network namespace, the pod is using a non-namespaced
0000000000000000000000000000000000000000;;	// capability, the pod contains a privileged container, or the pod has a host path volume.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// NOTE: when if a container shares any namespace with another container it must also share the user namespace
0000000000000000000000000000000000000000;;	// or it will not have the correct capabilities in the namespace.  This means that host user namespace
0000000000000000000000000000000000000000;;	// is enabled per pod, not per container.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) enableHostUserNamespace(pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		if kubecontainer.HasPrivilegedContainer(pod) || hasHostNamespace(pod) ||
0000000000000000000000000000000000000000;;			hasHostVolume(pod) || hasNonNamespacedCapability(pod) || kl.hasHostMountPVC(pod) {
0000000000000000000000000000000000000000;;			return true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// hasNonNamespacedCapability returns true if MKNOD, SYS_TIME, or SYS_MODULE is requested for any container.
0000000000000000000000000000000000000000;;	func hasNonNamespacedCapability(pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		for _, c := range pod.Spec.Containers {
0000000000000000000000000000000000000000;;			if c.SecurityContext != nil && c.SecurityContext.Capabilities != nil {
0000000000000000000000000000000000000000;;				for _, cap := range c.SecurityContext.Capabilities.Add {
0000000000000000000000000000000000000000;;					if cap == "MKNOD" || cap == "SYS_TIME" || cap == "SYS_MODULE" {
0000000000000000000000000000000000000000;;						return true
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// hasHostVolume returns true if the pod spec has a HostPath volume.
0000000000000000000000000000000000000000;;	func hasHostVolume(pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		for _, v := range pod.Spec.Volumes {
0000000000000000000000000000000000000000;;			if v.HostPath != nil {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// hasHostNamespace returns true if hostIPC, hostNetwork, or hostPID are set to true.
0000000000000000000000000000000000000000;;	func hasHostNamespace(pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		if pod.Spec.SecurityContext == nil {
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pod.Spec.HostIPC || pod.Spec.HostNetwork || pod.Spec.HostPID
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// hasHostMountPVC returns true if a PVC is referencing a HostPath volume.
0000000000000000000000000000000000000000;;	func (kl *Kubelet) hasHostMountPVC(pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		for _, volume := range pod.Spec.Volumes {
0000000000000000000000000000000000000000;;			if volume.PersistentVolumeClaim != nil {
0000000000000000000000000000000000000000;;				pvc, err := kl.kubeClient.Core().PersistentVolumeClaims(pod.Namespace).Get(volume.PersistentVolumeClaim.ClaimName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					glog.Warningf("unable to retrieve pvc %s:%s - %v", pod.Namespace, volume.PersistentVolumeClaim.ClaimName, err)
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if pvc != nil {
0000000000000000000000000000000000000000;;					referencedVolume, err := kl.kubeClient.Core().PersistentVolumes().Get(pvc.Spec.VolumeName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;					if err != nil {
0000000000000000000000000000000000000000;;						glog.Warningf("unable to retrieve pvc %s - %v", pvc.Spec.VolumeName, err)
0000000000000000000000000000000000000000;;						continue
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if referencedVolume != nil && referencedVolume.Spec.HostPath != nil {
0000000000000000000000000000000000000000;;						return true
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
