0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
b1f20585de7783b76bea2e50de61b43be0a9abb1;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package prober
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"testing"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset/fake"
0000000000000000000000000000000000000000;;		kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
0000000000000000000000000000000000000000;;		kubepod "k8s.io/kubernetes/pkg/kubelet/pod"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/prober/results"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/status"
0000000000000000000000000000000000000000;;		statustest "k8s.io/kubernetes/pkg/kubelet/status/testing"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/probe"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/exec"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func init() {
0000000000000000000000000000000000000000;;		runtime.ReallyCrash = true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDoProbe(t *testing.T) {
0000000000000000000000000000000000000000;;		m := newTestManager()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Test statuses.
0000000000000000000000000000000000000000;;		runningStatus := getTestRunningStatus()
0000000000000000000000000000000000000000;;		pendingStatus := getTestRunningStatus()
0000000000000000000000000000000000000000;;		pendingStatus.ContainerStatuses[0].State.Running = nil
0000000000000000000000000000000000000000;;		terminatedStatus := getTestRunningStatus()
0000000000000000000000000000000000000000;;		terminatedStatus.ContainerStatuses[0].State.Running = nil
0000000000000000000000000000000000000000;;		terminatedStatus.ContainerStatuses[0].State.Terminated = &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;			StartedAt: metav1.Now(),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		otherStatus := getTestRunningStatus()
0000000000000000000000000000000000000000;;		otherStatus.ContainerStatuses[0].Name = "otherContainer"
0000000000000000000000000000000000000000;;		failedStatus := getTestRunningStatus()
0000000000000000000000000000000000000000;;		failedStatus.Phase = v1.PodFailed
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		tests := []struct {
0000000000000000000000000000000000000000;;			probe          v1.Probe
0000000000000000000000000000000000000000;;			podStatus      *v1.PodStatus
0000000000000000000000000000000000000000;;			expectContinue bool
0000000000000000000000000000000000000000;;			expectSet      bool
0000000000000000000000000000000000000000;;			expectedResult results.Result
0000000000000000000000000000000000000000;;		}{
0000000000000000000000000000000000000000;;			{ // No status.
0000000000000000000000000000000000000000;;				expectContinue: true,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{ // Pod failed
0000000000000000000000000000000000000000;;				podStatus: &failedStatus,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{ // No container status
0000000000000000000000000000000000000000;;				podStatus:      &otherStatus,
0000000000000000000000000000000000000000;;				expectContinue: true,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{ // Container waiting
0000000000000000000000000000000000000000;;				podStatus:      &pendingStatus,
0000000000000000000000000000000000000000;;				expectContinue: true,
0000000000000000000000000000000000000000;;				expectSet:      true,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{ // Container terminated
0000000000000000000000000000000000000000;;				podStatus: &terminatedStatus,
0000000000000000000000000000000000000000;;				expectSet: true,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{ // Probe successful.
0000000000000000000000000000000000000000;;				podStatus:      &runningStatus,
0000000000000000000000000000000000000000;;				expectContinue: true,
0000000000000000000000000000000000000000;;				expectSet:      true,
0000000000000000000000000000000000000000;;				expectedResult: results.Success,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{ // Initial delay passed
0000000000000000000000000000000000000000;;				podStatus: &runningStatus,
0000000000000000000000000000000000000000;;				probe: v1.Probe{
0000000000000000000000000000000000000000;;					InitialDelaySeconds: -100,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				expectContinue: true,
0000000000000000000000000000000000000000;;				expectSet:      true,
0000000000000000000000000000000000000000;;				expectedResult: results.Success,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, probeType := range [...]probeType{liveness, readiness} {
0000000000000000000000000000000000000000;;			for i, test := range tests {
0000000000000000000000000000000000000000;;				w := newTestWorker(m, probeType, test.probe)
0000000000000000000000000000000000000000;;				if test.podStatus != nil {
0000000000000000000000000000000000000000;;					m.statusManager.SetPodStatus(w.pod, *test.podStatus)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if c := w.doProbe(); c != test.expectContinue {
0000000000000000000000000000000000000000;;					t.Errorf("[%s-%d] Expected continue to be %v but got %v", probeType, i, test.expectContinue, c)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				result, ok := resultsManager(m, probeType).Get(testContainerID)
0000000000000000000000000000000000000000;;				if ok != test.expectSet {
0000000000000000000000000000000000000000;;					t.Errorf("[%s-%d] Expected to have result: %v but got %v", probeType, i, test.expectSet, ok)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if result != test.expectedResult {
0000000000000000000000000000000000000000;;					t.Errorf("[%s-%d] Expected result: %v but got %v", probeType, i, test.expectedResult, result)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Clean up.
0000000000000000000000000000000000000000;;				m.statusManager = status.NewManager(&fake.Clientset{}, kubepod.NewBasicPodManager(nil, nil, nil), &statustest.FakePodDeletionSafetyProvider{})
0000000000000000000000000000000000000000;;				resultsManager(m, probeType).Remove(testContainerID)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestInitialDelay(t *testing.T) {
0000000000000000000000000000000000000000;;		m := newTestManager()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, probeType := range [...]probeType{liveness, readiness} {
0000000000000000000000000000000000000000;;			w := newTestWorker(m, probeType, v1.Probe{
0000000000000000000000000000000000000000;;				InitialDelaySeconds: 10,
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			m.statusManager.SetPodStatus(w.pod, getTestRunningStatus())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			expectContinue(t, w, w.doProbe(), "during initial delay")
0000000000000000000000000000000000000000;;			expectResult(t, w, results.Result(probeType == liveness), "during initial delay")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// 100 seconds later...
0000000000000000000000000000000000000000;;			laterStatus := getTestRunningStatus()
0000000000000000000000000000000000000000;;			laterStatus.ContainerStatuses[0].State.Running.StartedAt.Time =
0000000000000000000000000000000000000000;;				time.Now().Add(-100 * time.Second)
0000000000000000000000000000000000000000;;			m.statusManager.SetPodStatus(w.pod, laterStatus)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Second call should succeed (already waited).
0000000000000000000000000000000000000000;;			expectContinue(t, w, w.doProbe(), "after initial delay")
0000000000000000000000000000000000000000;;			expectResult(t, w, results.Success, "after initial delay")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestFailureThreshold(t *testing.T) {
0000000000000000000000000000000000000000;;		m := newTestManager()
0000000000000000000000000000000000000000;;		w := newTestWorker(m, readiness, v1.Probe{SuccessThreshold: 1, FailureThreshold: 3})
0000000000000000000000000000000000000000;;		m.statusManager.SetPodStatus(w.pod, getTestRunningStatus())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i := 0; i < 2; i++ {
0000000000000000000000000000000000000000;;			// First probe should succeed.
0000000000000000000000000000000000000000;;			m.prober.exec = fakeExecProber{probe.Success, nil}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for j := 0; j < 3; j++ {
0000000000000000000000000000000000000000;;				msg := fmt.Sprintf("%d success (%d)", j+1, i)
0000000000000000000000000000000000000000;;				expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;				expectResult(t, w, results.Success, msg)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Prober starts failing :(
0000000000000000000000000000000000000000;;			m.prober.exec = fakeExecProber{probe.Failure, nil}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Next 2 probes should still be "success".
0000000000000000000000000000000000000000;;			for j := 0; j < 2; j++ {
0000000000000000000000000000000000000000;;				msg := fmt.Sprintf("%d failing (%d)", j+1, i)
0000000000000000000000000000000000000000;;				expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;				expectResult(t, w, results.Success, msg)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Third & following fail.
0000000000000000000000000000000000000000;;			for j := 0; j < 3; j++ {
0000000000000000000000000000000000000000;;				msg := fmt.Sprintf("%d failure (%d)", j+3, i)
0000000000000000000000000000000000000000;;				expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;				expectResult(t, w, results.Failure, msg)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSuccessThreshold(t *testing.T) {
0000000000000000000000000000000000000000;;		m := newTestManager()
0000000000000000000000000000000000000000;;		w := newTestWorker(m, readiness, v1.Probe{SuccessThreshold: 3, FailureThreshold: 1})
0000000000000000000000000000000000000000;;		m.statusManager.SetPodStatus(w.pod, getTestRunningStatus())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Start out failure.
0000000000000000000000000000000000000000;;		w.resultsManager.Set(testContainerID, results.Failure, &v1.Pod{})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i := 0; i < 2; i++ {
0000000000000000000000000000000000000000;;			// Probe defaults to Failure.
0000000000000000000000000000000000000000;;			for j := 0; j < 2; j++ {
0000000000000000000000000000000000000000;;				msg := fmt.Sprintf("%d success (%d)", j+1, i)
0000000000000000000000000000000000000000;;				expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;				expectResult(t, w, results.Failure, msg)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Continuing success!
0000000000000000000000000000000000000000;;			for j := 0; j < 3; j++ {
0000000000000000000000000000000000000000;;				msg := fmt.Sprintf("%d success (%d)", j+3, i)
0000000000000000000000000000000000000000;;				expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;				expectResult(t, w, results.Success, msg)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Prober flakes :(
0000000000000000000000000000000000000000;;			m.prober.exec = fakeExecProber{probe.Failure, nil}
0000000000000000000000000000000000000000;;			msg := fmt.Sprintf("1 failure (%d)", i)
0000000000000000000000000000000000000000;;			expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;			expectResult(t, w, results.Failure, msg)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Back to success.
0000000000000000000000000000000000000000;;			m.prober.exec = fakeExecProber{probe.Success, nil}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestCleanUp(t *testing.T) {
0000000000000000000000000000000000000000;;		m := newTestManager()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, probeType := range [...]probeType{liveness, readiness} {
0000000000000000000000000000000000000000;;			key := probeKey{testPodUID, testContainerName, probeType}
0000000000000000000000000000000000000000;;			w := newTestWorker(m, probeType, v1.Probe{})
0000000000000000000000000000000000000000;;			m.statusManager.SetPodStatus(w.pod, getTestRunningStatus())
0000000000000000000000000000000000000000;;			go w.run()
0000000000000000000000000000000000000000;;			m.workers[key] = w
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Wait for worker to run.
0000000000000000000000000000000000000000;;			condition := func() (bool, error) {
0000000000000000000000000000000000000000;;				ready, _ := resultsManager(m, probeType).Get(testContainerID)
0000000000000000000000000000000000000000;;				return ready == results.Success, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if ready, _ := condition(); !ready {
0000000000000000000000000000000000000000;;				if err := wait.Poll(100*time.Millisecond, wait.ForeverTestTimeout, condition); err != nil {
0000000000000000000000000000000000000000;;					t.Fatalf("[%s] Error waiting for worker ready: %v", probeType, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for i := 0; i < 10; i++ {
0000000000000000000000000000000000000000;;				w.stop() // Stop should be callable multiple times without consequence.
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if err := waitForWorkerExit(m, []probeKey{key}); err != nil {
0000000000000000000000000000000000000000;;				t.Fatalf("[%s] error waiting for worker exit: %v", probeType, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if _, ok := resultsManager(m, probeType).Get(testContainerID); ok {
0000000000000000000000000000000000000000;;				t.Errorf("[%s] Expected result to be cleared.", probeType)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if _, ok := m.workers[key]; ok {
0000000000000000000000000000000000000000;;				t.Errorf("[%s] Expected worker to be cleared.", probeType)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestHandleCrash(t *testing.T) {
0000000000000000000000000000000000000000;;		runtime.ReallyCrash = false // Test that we *don't* really crash.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		m := newTestManager()
0000000000000000000000000000000000000000;;		w := newTestWorker(m, readiness, v1.Probe{})
0000000000000000000000000000000000000000;;		m.statusManager.SetPodStatus(w.pod, getTestRunningStatus())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		expectContinue(t, w, w.doProbe(), "Initial successful probe.")
0000000000000000000000000000000000000000;;		expectResult(t, w, results.Success, "Initial successful probe.")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Prober starts crashing.
0000000000000000000000000000000000000000;;		m.prober = &prober{
0000000000000000000000000000000000000000;;			refManager: kubecontainer.NewRefManager(),
0000000000000000000000000000000000000000;;			recorder:   &record.FakeRecorder{},
0000000000000000000000000000000000000000;;			exec:       crashingExecProber{},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// doProbe should recover from the crash, and keep going.
0000000000000000000000000000000000000000;;		expectContinue(t, w, w.doProbe(), "Crashing probe.")
0000000000000000000000000000000000000000;;		expectResult(t, w, results.Success, "Crashing probe unchanged.")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func expectResult(t *testing.T, w *worker, expectedResult results.Result, msg string) {
0000000000000000000000000000000000000000;;		result, ok := resultsManager(w.probeManager, w.probeType).Get(w.containerID)
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			t.Errorf("[%s - %s] Expected result to be set, but was not set", w.probeType, msg)
0000000000000000000000000000000000000000;;		} else if result != expectedResult {
0000000000000000000000000000000000000000;;			t.Errorf("[%s - %s] Expected result to be %v, but was %v",
0000000000000000000000000000000000000000;;				w.probeType, msg, expectedResult, result)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func expectContinue(t *testing.T, w *worker, c bool, msg string) {
0000000000000000000000000000000000000000;;		if !c {
0000000000000000000000000000000000000000;;			t.Errorf("[%s - %s] Expected to continue, but did not", w.probeType, msg)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func resultsManager(m *manager, probeType probeType) results.Manager {
0000000000000000000000000000000000000000;;		switch probeType {
0000000000000000000000000000000000000000;;		case readiness:
0000000000000000000000000000000000000000;;			return m.readinessManager
0000000000000000000000000000000000000000;;		case liveness:
0000000000000000000000000000000000000000;;			return m.livenessManager
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		panic(fmt.Errorf("Unhandled case: %v", probeType))
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type crashingExecProber struct{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p crashingExecProber) Probe(_ exec.Cmd) (probe.Result, string, error) {
0000000000000000000000000000000000000000;;		panic("Intentional Probe crash.")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestOnHoldOnLivenessCheckFailure(t *testing.T) {
0000000000000000000000000000000000000000;;		m := newTestManager()
0000000000000000000000000000000000000000;;		w := newTestWorker(m, liveness, v1.Probe{SuccessThreshold: 1, FailureThreshold: 1})
0000000000000000000000000000000000000000;;		status := getTestRunningStatus()
0000000000000000000000000000000000000000;;		m.statusManager.SetPodStatus(w.pod, getTestRunningStatus())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// First probe should fail.
0000000000000000000000000000000000000000;;		m.prober.exec = fakeExecProber{probe.Failure, nil}
0000000000000000000000000000000000000000;;		msg := "first probe"
0000000000000000000000000000000000000000;;		expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;		expectResult(t, w, results.Failure, msg)
0000000000000000000000000000000000000000;;		if !w.onHold {
0000000000000000000000000000000000000000;;			t.Errorf("Prober should be on hold due to liveness check failure")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Set fakeExecProber to return success. However, the result will remain
0000000000000000000000000000000000000000;;		// failure because the worker is on hold and won't probe.
0000000000000000000000000000000000000000;;		m.prober.exec = fakeExecProber{probe.Success, nil}
0000000000000000000000000000000000000000;;		msg = "while on hold"
0000000000000000000000000000000000000000;;		expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;		expectResult(t, w, results.Failure, msg)
0000000000000000000000000000000000000000;;		if !w.onHold {
0000000000000000000000000000000000000000;;			t.Errorf("Prober should be on hold due to liveness check failure")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Set a new container ID to lift the hold. The next probe will succeed.
0000000000000000000000000000000000000000;;		status.ContainerStatuses[0].ContainerID = "test://newCont_ID"
0000000000000000000000000000000000000000;;		m.statusManager.SetPodStatus(w.pod, status)
0000000000000000000000000000000000000000;;		msg = "hold lifted"
0000000000000000000000000000000000000000;;		expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;		expectResult(t, w, results.Success, msg)
0000000000000000000000000000000000000000;;		if w.onHold {
0000000000000000000000000000000000000000;;			t.Errorf("Prober should not be on hold anymore")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestResultRunOnLivenessCheckFailure(t *testing.T) {
0000000000000000000000000000000000000000;;		m := newTestManager()
0000000000000000000000000000000000000000;;		w := newTestWorker(m, liveness, v1.Probe{SuccessThreshold: 1, FailureThreshold: 3})
0000000000000000000000000000000000000000;;		m.statusManager.SetPodStatus(w.pod, getTestRunningStatus())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		m.prober.exec = fakeExecProber{probe.Success, nil}
0000000000000000000000000000000000000000;;		msg := "inital probe success"
0000000000000000000000000000000000000000;;		expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;		expectResult(t, w, results.Success, msg)
0000000000000000000000000000000000000000;;		if w.resultRun != 1 {
0000000000000000000000000000000000000000;;			t.Errorf("Prober resultRun should 1")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		m.prober.exec = fakeExecProber{probe.Failure, nil}
0000000000000000000000000000000000000000;;		msg = "probe failure, result success"
0000000000000000000000000000000000000000;;		expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;		expectResult(t, w, results.Success, msg)
0000000000000000000000000000000000000000;;		if w.resultRun != 1 {
0000000000000000000000000000000000000000;;			t.Errorf("Prober resultRun should 1")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		m.prober.exec = fakeExecProber{probe.Failure, nil}
0000000000000000000000000000000000000000;;		msg = "2nd probe failure, result success"
0000000000000000000000000000000000000000;;		expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;		expectResult(t, w, results.Success, msg)
0000000000000000000000000000000000000000;;		if w.resultRun != 2 {
0000000000000000000000000000000000000000;;			t.Errorf("Prober resultRun should be 2")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Exceeding FailureThreshold should cause resultRun to
0000000000000000000000000000000000000000;;		// reset to 1 so that the probe on the restarted pod
0000000000000000000000000000000000000000;;		// also gets FailureThreshold attempts to succeed.
0000000000000000000000000000000000000000;;		m.prober.exec = fakeExecProber{probe.Failure, nil}
0000000000000000000000000000000000000000;;		msg = "3rd probe failure, result failure"
0000000000000000000000000000000000000000;;		expectContinue(t, w, w.doProbe(), msg)
0000000000000000000000000000000000000000;;		expectResult(t, w, results.Failure, msg)
0000000000000000000000000000000000000000;;		if w.resultRun != 1 {
0000000000000000000000000000000000000000;;			t.Errorf("Prober resultRun should be reset to 1")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
