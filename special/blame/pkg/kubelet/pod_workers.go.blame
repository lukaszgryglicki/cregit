0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2014 The Kubernetes Authors.
aa97f5ab3fe336cb502b8274a713768d831f4da8;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/events"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/eviction"
0000000000000000000000000000000000000000;;		kubetypes "k8s.io/kubernetes/pkg/kubelet/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/format"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/queue"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// OnCompleteFunc is a function that is invoked when an operation completes.
0000000000000000000000000000000000000000;;	// If err is non-nil, the operation did not complete successfully.
0000000000000000000000000000000000000000;;	type OnCompleteFunc func(err error)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodStatusFunc is a function that is invoked to generate a pod status.
0000000000000000000000000000000000000000;;	type PodStatusFunc func(pod *v1.Pod, podStatus *kubecontainer.PodStatus) v1.PodStatus
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// KillPodOptions are options when performing a pod update whose update type is kill.
0000000000000000000000000000000000000000;;	type KillPodOptions struct {
0000000000000000000000000000000000000000;;		// PodStatusFunc is the function to invoke to set pod status in response to a kill request.
0000000000000000000000000000000000000000;;		PodStatusFunc PodStatusFunc
0000000000000000000000000000000000000000;;		// PodTerminationGracePeriodSecondsOverride is optional override to use if a pod is being killed as part of kill operation.
0000000000000000000000000000000000000000;;		PodTerminationGracePeriodSecondsOverride *int64
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// UpdatePodOptions is an options struct to pass to a UpdatePod operation.
0000000000000000000000000000000000000000;;	type UpdatePodOptions struct {
0000000000000000000000000000000000000000;;		// pod to update
0000000000000000000000000000000000000000;;		Pod *v1.Pod
0000000000000000000000000000000000000000;;		// the mirror pod for the pod to update, if it is a static pod
0000000000000000000000000000000000000000;;		MirrorPod *v1.Pod
0000000000000000000000000000000000000000;;		// the type of update (create, update, sync, kill)
0000000000000000000000000000000000000000;;		UpdateType kubetypes.SyncPodType
0000000000000000000000000000000000000000;;		// optional callback function when operation completes
0000000000000000000000000000000000000000;;		// this callback is not guaranteed to be completed since a pod worker may
0000000000000000000000000000000000000000;;		// drop update requests if it was fulfilling a previous request.  this is
0000000000000000000000000000000000000000;;		// only guaranteed to be invoked in response to a kill pod request which is
0000000000000000000000000000000000000000;;		// always delivered.
0000000000000000000000000000000000000000;;		OnCompleteFunc OnCompleteFunc
0000000000000000000000000000000000000000;;		// if update type is kill, use the specified options to kill the pod.
0000000000000000000000000000000000000000;;		KillPodOptions *KillPodOptions
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// PodWorkers is an abstract interface for testability.
0000000000000000000000000000000000000000;;	type PodWorkers interface {
0000000000000000000000000000000000000000;;		UpdatePod(options *UpdatePodOptions)
0000000000000000000000000000000000000000;;		ForgetNonExistingPodWorkers(desiredPods map[types.UID]empty)
0000000000000000000000000000000000000000;;		ForgetWorker(uid types.UID)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// syncPodOptions provides the arguments to a SyncPod operation.
0000000000000000000000000000000000000000;;	type syncPodOptions struct {
0000000000000000000000000000000000000000;;		// the mirror pod for the pod to sync, if it is a static pod
0000000000000000000000000000000000000000;;		mirrorPod *v1.Pod
0000000000000000000000000000000000000000;;		// pod to sync
0000000000000000000000000000000000000000;;		pod *v1.Pod
0000000000000000000000000000000000000000;;		// the type of update (create, update, sync)
0000000000000000000000000000000000000000;;		updateType kubetypes.SyncPodType
0000000000000000000000000000000000000000;;		// the current status
0000000000000000000000000000000000000000;;		podStatus *kubecontainer.PodStatus
0000000000000000000000000000000000000000;;		// if update type is kill, use the specified options to kill the pod.
0000000000000000000000000000000000000000;;		killPodOptions *KillPodOptions
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// the function to invoke to perform a sync.
0000000000000000000000000000000000000000;;	type syncPodFnType func(options syncPodOptions) error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// jitter factor for resyncInterval
0000000000000000000000000000000000000000;;		workerResyncIntervalJitterFactor = 0.5
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// jitter factor for backOffPeriod
0000000000000000000000000000000000000000;;		workerBackOffPeriodJitterFactor = 0.5
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type podWorkers struct {
0000000000000000000000000000000000000000;;		// Protects all per worker fields.
0000000000000000000000000000000000000000;;		podLock sync.Mutex
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Tracks all running per-pod goroutines - per-pod goroutine will be
0000000000000000000000000000000000000000;;		// processing updates received through its corresponding channel.
0000000000000000000000000000000000000000;;		podUpdates map[types.UID]chan UpdatePodOptions
0000000000000000000000000000000000000000;;		// Track the current state of per-pod goroutines.
0000000000000000000000000000000000000000;;		// Currently all update request for a given pod coming when another
0000000000000000000000000000000000000000;;		// update of this pod is being processed are ignored.
0000000000000000000000000000000000000000;;		isWorking map[types.UID]bool
0000000000000000000000000000000000000000;;		// Tracks the last undelivered work item for this pod - a work item is
0000000000000000000000000000000000000000;;		// undelivered if it comes in while the worker is working.
0000000000000000000000000000000000000000;;		lastUndeliveredWorkUpdate map[types.UID]UpdatePodOptions
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		workQueue queue.WorkQueue
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This function is run to sync the desired stated of pod.
0000000000000000000000000000000000000000;;		// NOTE: This function has to be thread-safe - it can be called for
0000000000000000000000000000000000000000;;		// different pods at the same time.
0000000000000000000000000000000000000000;;		syncPodFn syncPodFnType
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The EventRecorder to use
0000000000000000000000000000000000000000;;		recorder record.EventRecorder
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// backOffPeriod is the duration to back off when there is a sync error.
0000000000000000000000000000000000000000;;		backOffPeriod time.Duration
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// resyncInterval is the duration to wait until the next sync.
0000000000000000000000000000000000000000;;		resyncInterval time.Duration
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// podCache stores kubecontainer.PodStatus for all pods.
0000000000000000000000000000000000000000;;		podCache kubecontainer.Cache
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newPodWorkers(syncPodFn syncPodFnType, recorder record.EventRecorder, workQueue queue.WorkQueue,
0000000000000000000000000000000000000000;;		resyncInterval, backOffPeriod time.Duration, podCache kubecontainer.Cache) *podWorkers {
0000000000000000000000000000000000000000;;		return &podWorkers{
0000000000000000000000000000000000000000;;			podUpdates:                map[types.UID]chan UpdatePodOptions{},
0000000000000000000000000000000000000000;;			isWorking:                 map[types.UID]bool{},
0000000000000000000000000000000000000000;;			lastUndeliveredWorkUpdate: map[types.UID]UpdatePodOptions{},
0000000000000000000000000000000000000000;;			syncPodFn:                 syncPodFn,
0000000000000000000000000000000000000000;;			recorder:                  recorder,
0000000000000000000000000000000000000000;;			workQueue:                 workQueue,
0000000000000000000000000000000000000000;;			resyncInterval:            resyncInterval,
0000000000000000000000000000000000000000;;			backOffPeriod:             backOffPeriod,
0000000000000000000000000000000000000000;;			podCache:                  podCache,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *podWorkers) managePodLoop(podUpdates <-chan UpdatePodOptions) {
0000000000000000000000000000000000000000;;		var lastSyncTime time.Time
0000000000000000000000000000000000000000;;		for update := range podUpdates {
0000000000000000000000000000000000000000;;			err := func() error {
0000000000000000000000000000000000000000;;				podUID := update.Pod.UID
0000000000000000000000000000000000000000;;				// This is a blocking call that would return only if the cache
0000000000000000000000000000000000000000;;				// has an entry for the pod that is newer than minRuntimeCache
0000000000000000000000000000000000000000;;				// Time. This ensures the worker doesn't start syncing until
0000000000000000000000000000000000000000;;				// after the cache is at least newer than the finished time of
0000000000000000000000000000000000000000;;				// the previous sync.
0000000000000000000000000000000000000000;;				status, err := p.podCache.GetNewerThan(podUID, lastSyncTime)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				err = p.syncPodFn(syncPodOptions{
0000000000000000000000000000000000000000;;					mirrorPod:      update.MirrorPod,
0000000000000000000000000000000000000000;;					pod:            update.Pod,
0000000000000000000000000000000000000000;;					podStatus:      status,
0000000000000000000000000000000000000000;;					killPodOptions: update.KillPodOptions,
0000000000000000000000000000000000000000;;					updateType:     update.UpdateType,
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;				lastSyncTime = time.Now()
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;			// notify the call-back function if the operation succeeded or not
0000000000000000000000000000000000000000;;			if update.OnCompleteFunc != nil {
0000000000000000000000000000000000000000;;				update.OnCompleteFunc(err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Error syncing pod %s (%q), skipping: %v", update.Pod.UID, format.Pod(update.Pod), err)
0000000000000000000000000000000000000000;;				// if we failed sync, we throw more specific events for why it happened.
0000000000000000000000000000000000000000;;				// as a result, i question the value of this event.
0000000000000000000000000000000000000000;;				// TODO: determine if we can remove this in a future release.
0000000000000000000000000000000000000000;;				// do not include descriptive text that can vary on why it failed so in a pathological
0000000000000000000000000000000000000000;;				// scenario, kubelet does not create enough discrete events that miss default aggregation
0000000000000000000000000000000000000000;;				// window.
0000000000000000000000000000000000000000;;				p.recorder.Eventf(update.Pod, v1.EventTypeWarning, events.FailedSync, "Error syncing pod")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			p.wrapUp(update.Pod.UID, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Apply the new setting to the specified pod.
0000000000000000000000000000000000000000;;	// If the options provide an OnCompleteFunc, the function is invoked if the update is accepted.
0000000000000000000000000000000000000000;;	// Update requests are ignored if a kill pod request is pending.
0000000000000000000000000000000000000000;;	func (p *podWorkers) UpdatePod(options *UpdatePodOptions) {
0000000000000000000000000000000000000000;;		pod := options.Pod
0000000000000000000000000000000000000000;;		uid := pod.UID
0000000000000000000000000000000000000000;;		var podUpdates chan UpdatePodOptions
0000000000000000000000000000000000000000;;		var exists bool
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		p.podLock.Lock()
0000000000000000000000000000000000000000;;		defer p.podLock.Unlock()
0000000000000000000000000000000000000000;;		if podUpdates, exists = p.podUpdates[uid]; !exists {
0000000000000000000000000000000000000000;;			// We need to have a buffer here, because checkForUpdates() method that
0000000000000000000000000000000000000000;;			// puts an update into channel is called from the same goroutine where
0000000000000000000000000000000000000000;;			// the channel is consumed. However, it is guaranteed that in such case
0000000000000000000000000000000000000000;;			// the channel is empty, so buffer of size 1 is enough.
0000000000000000000000000000000000000000;;			podUpdates = make(chan UpdatePodOptions, 1)
0000000000000000000000000000000000000000;;			p.podUpdates[uid] = podUpdates
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Creating a new pod worker either means this is a new pod, or that the
0000000000000000000000000000000000000000;;			// kubelet just restarted. In either case the kubelet is willing to believe
0000000000000000000000000000000000000000;;			// the status of the pod for the first pod worker sync. See corresponding
0000000000000000000000000000000000000000;;			// comment in syncPod.
0000000000000000000000000000000000000000;;			go func() {
0000000000000000000000000000000000000000;;				defer runtime.HandleCrash()
0000000000000000000000000000000000000000;;				p.managePodLoop(podUpdates)
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !p.isWorking[pod.UID] {
0000000000000000000000000000000000000000;;			p.isWorking[pod.UID] = true
0000000000000000000000000000000000000000;;			podUpdates <- *options
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// if a request to kill a pod is pending, we do not let anything overwrite that request.
0000000000000000000000000000000000000000;;			update, found := p.lastUndeliveredWorkUpdate[pod.UID]
0000000000000000000000000000000000000000;;			if !found || update.UpdateType != kubetypes.SyncPodKill {
0000000000000000000000000000000000000000;;				p.lastUndeliveredWorkUpdate[pod.UID] = *options
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *podWorkers) removeWorker(uid types.UID) {
0000000000000000000000000000000000000000;;		if ch, ok := p.podUpdates[uid]; ok {
0000000000000000000000000000000000000000;;			close(ch)
0000000000000000000000000000000000000000;;			delete(p.podUpdates, uid)
0000000000000000000000000000000000000000;;			// If there is an undelivered work update for this pod we need to remove it
0000000000000000000000000000000000000000;;			// since per-pod goroutine won't be able to put it to the already closed
0000000000000000000000000000000000000000;;			// channel when it finish processing the current work update.
0000000000000000000000000000000000000000;;			if _, cached := p.lastUndeliveredWorkUpdate[uid]; cached {
0000000000000000000000000000000000000000;;				delete(p.lastUndeliveredWorkUpdate, uid)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	func (p *podWorkers) ForgetWorker(uid types.UID) {
0000000000000000000000000000000000000000;;		p.podLock.Lock()
0000000000000000000000000000000000000000;;		defer p.podLock.Unlock()
0000000000000000000000000000000000000000;;		p.removeWorker(uid)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *podWorkers) ForgetNonExistingPodWorkers(desiredPods map[types.UID]empty) {
0000000000000000000000000000000000000000;;		p.podLock.Lock()
0000000000000000000000000000000000000000;;		defer p.podLock.Unlock()
0000000000000000000000000000000000000000;;		for key := range p.podUpdates {
0000000000000000000000000000000000000000;;			if _, exists := desiredPods[key]; !exists {
0000000000000000000000000000000000000000;;				p.removeWorker(key)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *podWorkers) wrapUp(uid types.UID, syncErr error) {
0000000000000000000000000000000000000000;;		// Requeue the last update if the last sync returned error.
0000000000000000000000000000000000000000;;		switch {
0000000000000000000000000000000000000000;;		case syncErr == nil:
0000000000000000000000000000000000000000;;			// No error; requeue at the regular resync interval.
0000000000000000000000000000000000000000;;			p.workQueue.Enqueue(uid, wait.Jitter(p.resyncInterval, workerResyncIntervalJitterFactor))
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			// Error occurred during the sync; back off and then retry.
0000000000000000000000000000000000000000;;			p.workQueue.Enqueue(uid, wait.Jitter(p.backOffPeriod, workerBackOffPeriodJitterFactor))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		p.checkForUpdates(uid)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p *podWorkers) checkForUpdates(uid types.UID) {
0000000000000000000000000000000000000000;;		p.podLock.Lock()
0000000000000000000000000000000000000000;;		defer p.podLock.Unlock()
0000000000000000000000000000000000000000;;		if workUpdate, exists := p.lastUndeliveredWorkUpdate[uid]; exists {
0000000000000000000000000000000000000000;;			p.podUpdates[uid] <- workUpdate
0000000000000000000000000000000000000000;;			delete(p.lastUndeliveredWorkUpdate, uid)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			p.isWorking[uid] = false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// killPodNow returns a KillPodFunc that can be used to kill a pod.
0000000000000000000000000000000000000000;;	// It is intended to be injected into other modules that need to kill a pod.
0000000000000000000000000000000000000000;;	func killPodNow(podWorkers PodWorkers, recorder record.EventRecorder) eviction.KillPodFunc {
0000000000000000000000000000000000000000;;		return func(pod *v1.Pod, status v1.PodStatus, gracePeriodOverride *int64) error {
0000000000000000000000000000000000000000;;			// determine the grace period to use when killing the pod
0000000000000000000000000000000000000000;;			gracePeriod := int64(0)
0000000000000000000000000000000000000000;;			if gracePeriodOverride != nil {
0000000000000000000000000000000000000000;;				gracePeriod = *gracePeriodOverride
0000000000000000000000000000000000000000;;			} else if pod.Spec.TerminationGracePeriodSeconds != nil {
0000000000000000000000000000000000000000;;				gracePeriod = *pod.Spec.TerminationGracePeriodSeconds
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// we timeout and return an error if we don't get a callback within a reasonable time.
0000000000000000000000000000000000000000;;			// the default timeout is relative to the grace period (we settle on 10s to wait for kubelet->runtime traffic to complete in sigkill)
0000000000000000000000000000000000000000;;			timeout := int64(gracePeriod + (gracePeriod / 2))
0000000000000000000000000000000000000000;;			minTimeout := int64(10)
0000000000000000000000000000000000000000;;			if timeout < minTimeout {
0000000000000000000000000000000000000000;;				timeout = minTimeout
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			timeoutDuration := time.Duration(timeout) * time.Second
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// open a channel we block against until we get a result
0000000000000000000000000000000000000000;;			type response struct {
0000000000000000000000000000000000000000;;				err error
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			ch := make(chan response)
0000000000000000000000000000000000000000;;			podWorkers.UpdatePod(&UpdatePodOptions{
0000000000000000000000000000000000000000;;				Pod:        pod,
0000000000000000000000000000000000000000;;				UpdateType: kubetypes.SyncPodKill,
0000000000000000000000000000000000000000;;				OnCompleteFunc: func(err error) {
0000000000000000000000000000000000000000;;					ch <- response{err: err}
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				KillPodOptions: &KillPodOptions{
0000000000000000000000000000000000000000;;					PodStatusFunc: func(p *v1.Pod, podStatus *kubecontainer.PodStatus) v1.PodStatus {
0000000000000000000000000000000000000000;;						return status
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					PodTerminationGracePeriodSecondsOverride: gracePeriodOverride,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// wait for either a response, or a timeout
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case r := <-ch:
0000000000000000000000000000000000000000;;				return r.err
0000000000000000000000000000000000000000;;			case <-time.After(timeoutDuration):
0000000000000000000000000000000000000000;;				recorder.Eventf(pod, v1.EventTypeWarning, events.ExceededGracePeriod, "Container runtime did not kill the pod within specified grace period.")
0000000000000000000000000000000000000000;;				return fmt.Errorf("timeout waiting to kill pod")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
