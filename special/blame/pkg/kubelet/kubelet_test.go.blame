0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2014 The Kubernetes Authors.
9fbdd758c00a160e902805405146a779d3acf5d8;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"io/ioutil"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"testing"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cadvisorapi "github.com/google/cadvisor/info/v1"
0000000000000000000000000000000000000000;;		cadvisorapiv2 "github.com/google/cadvisor/info/v2"
0000000000000000000000000000000000000000;;		"github.com/stretchr/testify/assert"
0000000000000000000000000000000000000000;;		"github.com/stretchr/testify/require"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		clientv1 "k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/clock"
0000000000000000000000000000000000000000;;		utilruntime "k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/flowcontrol"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/componentconfig"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/capabilities"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/clientset/fake"
0000000000000000000000000000000000000000;;		cadvisortest "k8s.io/kubernetes/pkg/kubelet/cadvisor/testing"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/cm"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/config"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/configmap"
0000000000000000000000000000000000000000;;		kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
0000000000000000000000000000000000000000;;		containertest "k8s.io/kubernetes/pkg/kubelet/container/testing"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/eviction"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/gpu"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/images"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/lifecycle"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/network"
0000000000000000000000000000000000000000;;		nettest "k8s.io/kubernetes/pkg/kubelet/network/testing"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/pleg"
0000000000000000000000000000000000000000;;		kubepod "k8s.io/kubernetes/pkg/kubelet/pod"
0000000000000000000000000000000000000000;;		podtest "k8s.io/kubernetes/pkg/kubelet/pod/testing"
0000000000000000000000000000000000000000;;		proberesults "k8s.io/kubernetes/pkg/kubelet/prober/results"
0000000000000000000000000000000000000000;;		probetest "k8s.io/kubernetes/pkg/kubelet/prober/testing"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/secret"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/server/stats"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/status"
0000000000000000000000000000000000000000;;		statustest "k8s.io/kubernetes/pkg/kubelet/status/testing"
0000000000000000000000000000000000000000;;		kubetypes "k8s.io/kubernetes/pkg/kubelet/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/queue"
0000000000000000000000000000000000000000;;		kubeletvolume "k8s.io/kubernetes/pkg/kubelet/volumemanager"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/mount"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/volume"
0000000000000000000000000000000000000000;;		_ "k8s.io/kubernetes/pkg/volume/host_path"
0000000000000000000000000000000000000000;;		volumetest "k8s.io/kubernetes/pkg/volume/testing"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/volume/util/volumehelper"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func init() {
0000000000000000000000000000000000000000;;		utilruntime.ReallyCrash = true
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		testKubeletHostname = "127.0.0.1"
0000000000000000000000000000000000000000;;		testKubeletHostIP   = "127.0.0.1"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO(harry) any global place for these two?
0000000000000000000000000000000000000000;;		// Reasonable size range of all container images. 90%ile of images on dockerhub drops into this range.
0000000000000000000000000000000000000000;;		minImgSize int64 = 23 * 1024 * 1024
0000000000000000000000000000000000000000;;		maxImgSize int64 = 1000 * 1024 * 1024
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// fakeImageGCManager is a fake image gc manager for testing. It will return image
0000000000000000000000000000000000000000;;	// list from fake runtime directly instead of caching it.
0000000000000000000000000000000000000000;;	type fakeImageGCManager struct {
0000000000000000000000000000000000000000;;		fakeImageService kubecontainer.ImageService
0000000000000000000000000000000000000000;;		images.ImageGCManager
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (f *fakeImageGCManager) GetImageList() ([]kubecontainer.Image, error) {
0000000000000000000000000000000000000000;;		return f.fakeImageService.ListImages()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type TestKubelet struct {
0000000000000000000000000000000000000000;;		kubelet          *Kubelet
0000000000000000000000000000000000000000;;		fakeRuntime      *containertest.FakeRuntime
0000000000000000000000000000000000000000;;		fakeCadvisor     *cadvisortest.Mock
0000000000000000000000000000000000000000;;		fakeKubeClient   *fake.Clientset
0000000000000000000000000000000000000000;;		fakeMirrorClient *podtest.FakeMirrorClient
0000000000000000000000000000000000000000;;		fakeClock        *clock.FakeClock
0000000000000000000000000000000000000000;;		mounter          mount.Interface
0000000000000000000000000000000000000000;;		volumePlugin     *volumetest.FakeVolumePlugin
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (tk *TestKubelet) Cleanup() {
0000000000000000000000000000000000000000;;		if tk.kubelet != nil {
0000000000000000000000000000000000000000;;			os.RemoveAll(tk.kubelet.rootDirectory)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// newTestKubelet returns test kubelet with two images.
0000000000000000000000000000000000000000;;	func newTestKubelet(t *testing.T, controllerAttachDetachEnabled bool) *TestKubelet {
0000000000000000000000000000000000000000;;		imageList := []kubecontainer.Image{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ID:       "abc",
0000000000000000000000000000000000000000;;				RepoTags: []string{"gcr.io/google_containers:v1", "gcr.io/google_containers:v2"},
0000000000000000000000000000000000000000;;				Size:     123,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ID:       "efg",
0000000000000000000000000000000000000000;;				RepoTags: []string{"gcr.io/google_containers:v3", "gcr.io/google_containers:v4"},
0000000000000000000000000000000000000000;;				Size:     456,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return newTestKubeletWithImageList(t, imageList, controllerAttachDetachEnabled)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newTestKubeletWithImageList(
0000000000000000000000000000000000000000;;		t *testing.T,
0000000000000000000000000000000000000000;;		imageList []kubecontainer.Image,
0000000000000000000000000000000000000000;;		controllerAttachDetachEnabled bool) *TestKubelet {
0000000000000000000000000000000000000000;;		fakeRuntime := &containertest.FakeRuntime{}
0000000000000000000000000000000000000000;;		fakeRuntime.RuntimeType = "test"
0000000000000000000000000000000000000000;;		fakeRuntime.VersionInfo = "1.5.0"
0000000000000000000000000000000000000000;;		fakeRuntime.ImageList = imageList
0000000000000000000000000000000000000000;;		// Set ready conditions by default.
0000000000000000000000000000000000000000;;		fakeRuntime.RuntimeStatus = &kubecontainer.RuntimeStatus{
0000000000000000000000000000000000000000;;			Conditions: []kubecontainer.RuntimeCondition{
0000000000000000000000000000000000000000;;				{Type: "RuntimeReady", Status: true},
0000000000000000000000000000000000000000;;				{Type: "NetworkReady", Status: true},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakeRecorder := &record.FakeRecorder{}
0000000000000000000000000000000000000000;;		fakeKubeClient := &fake.Clientset{}
0000000000000000000000000000000000000000;;		kubelet := &Kubelet{}
0000000000000000000000000000000000000000;;		kubelet.recorder = fakeRecorder
0000000000000000000000000000000000000000;;		kubelet.kubeClient = fakeKubeClient
0000000000000000000000000000000000000000;;		kubelet.os = &containertest.FakeOS{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.hostname = testKubeletHostname
0000000000000000000000000000000000000000;;		kubelet.nodeName = types.NodeName(testKubeletHostname)
0000000000000000000000000000000000000000;;		kubelet.runtimeState = newRuntimeState(maxWaitForContainerRuntime)
0000000000000000000000000000000000000000;;		kubelet.runtimeState.setNetworkState(nil)
0000000000000000000000000000000000000000;;		kubelet.networkPlugin, _ = network.InitNetworkPlugin([]network.NetworkPlugin{}, "", nettest.NewFakeHost(nil), componentconfig.HairpinNone, "", 1440)
0000000000000000000000000000000000000000;;		if tempDir, err := ioutil.TempDir("/tmp", "kubelet_test."); err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("can't make a temp rootdir: %v", err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			kubelet.rootDirectory = tempDir
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := os.MkdirAll(kubelet.rootDirectory, 0750); err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("can't mkdir(%q): %v", kubelet.rootDirectory, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kubelet.sourcesReady = config.NewSourcesReady(func(_ sets.String) bool { return true })
0000000000000000000000000000000000000000;;		kubelet.masterServiceNamespace = metav1.NamespaceDefault
0000000000000000000000000000000000000000;;		kubelet.serviceLister = testServiceLister{}
0000000000000000000000000000000000000000;;		kubelet.nodeInfo = testNodeInfo{
0000000000000000000000000000000000000000;;			nodes: []*v1.Node{
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;						Name: string(kubelet.nodeName),
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					Status: v1.NodeStatus{
0000000000000000000000000000000000000000;;						Conditions: []v1.NodeCondition{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Type:    v1.NodeReady,
0000000000000000000000000000000000000000;;								Status:  v1.ConditionTrue,
0000000000000000000000000000000000000000;;								Reason:  "Ready",
0000000000000000000000000000000000000000;;								Message: "Node ready",
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						Addresses: []v1.NodeAddress{
0000000000000000000000000000000000000000;;							{
0000000000000000000000000000000000000000;;								Type:    v1.NodeInternalIP,
0000000000000000000000000000000000000000;;								Address: testKubeletHostIP,
0000000000000000000000000000000000000000;;							},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kubelet.recorder = fakeRecorder
0000000000000000000000000000000000000000;;		if err := kubelet.setupDataDirs(); err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("can't initialize kubelet data dirs: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kubelet.daemonEndpoints = &v1.NodeDaemonEndpoints{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		mockCadvisor := &cadvisortest.Mock{}
0000000000000000000000000000000000000000;;		kubelet.cadvisor = mockCadvisor
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakeMirrorClient := podtest.NewFakeMirrorClient()
0000000000000000000000000000000000000000;;		secretManager := secret.NewSimpleSecretManager(kubelet.kubeClient)
0000000000000000000000000000000000000000;;		kubelet.secretManager = secretManager
0000000000000000000000000000000000000000;;		configMapManager := configmap.NewSimpleConfigMapManager(kubelet.kubeClient)
0000000000000000000000000000000000000000;;		kubelet.configMapManager = configMapManager
0000000000000000000000000000000000000000;;		kubelet.podManager = kubepod.NewBasicPodManager(fakeMirrorClient, kubelet.secretManager, kubelet.configMapManager)
0000000000000000000000000000000000000000;;		kubelet.statusManager = status.NewManager(fakeKubeClient, kubelet.podManager, &statustest.FakePodDeletionSafetyProvider{})
0000000000000000000000000000000000000000;;		diskSpaceManager, err := newDiskSpaceManager(mockCadvisor, DiskSpacePolicy{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			t.Fatalf("can't initialize disk space manager: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kubelet.diskSpaceManager = diskSpaceManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.containerRuntime = fakeRuntime
0000000000000000000000000000000000000000;;		kubelet.runtimeCache = containertest.NewFakeRuntimeCache(kubelet.containerRuntime)
0000000000000000000000000000000000000000;;		kubelet.reasonCache = NewReasonCache()
0000000000000000000000000000000000000000;;		kubelet.podCache = containertest.NewFakeCache(kubelet.containerRuntime)
0000000000000000000000000000000000000000;;		kubelet.podWorkers = &fakePodWorkers{
0000000000000000000000000000000000000000;;			syncPodFn: kubelet.syncPod,
0000000000000000000000000000000000000000;;			cache:     kubelet.podCache,
0000000000000000000000000000000000000000;;			t:         t,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.probeManager = probetest.FakeManager{}
0000000000000000000000000000000000000000;;		kubelet.livenessManager = proberesults.NewManager()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.containerManager = cm.NewStubContainerManager()
0000000000000000000000000000000000000000;;		fakeNodeRef := &clientv1.ObjectReference{
0000000000000000000000000000000000000000;;			Kind:      "Node",
0000000000000000000000000000000000000000;;			Name:      testKubeletHostname,
0000000000000000000000000000000000000000;;			UID:       types.UID(testKubeletHostname),
0000000000000000000000000000000000000000;;			Namespace: "",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		fakeImageGCPolicy := images.ImageGCPolicy{
0000000000000000000000000000000000000000;;			HighThresholdPercent: 90,
0000000000000000000000000000000000000000;;			LowThresholdPercent:  80,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		imageGCManager, err := images.NewImageGCManager(fakeRuntime, mockCadvisor, fakeRecorder, fakeNodeRef, fakeImageGCPolicy)
0000000000000000000000000000000000000000;;		assert.NoError(t, err)
0000000000000000000000000000000000000000;;		kubelet.imageManager = &fakeImageGCManager{
0000000000000000000000000000000000000000;;			fakeImageService: fakeRuntime,
0000000000000000000000000000000000000000;;			ImageGCManager:   imageGCManager,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		fakeClock := clock.NewFakeClock(time.Now())
0000000000000000000000000000000000000000;;		kubelet.backOff = flowcontrol.NewBackOff(time.Second, time.Minute)
0000000000000000000000000000000000000000;;		kubelet.backOff.Clock = fakeClock
0000000000000000000000000000000000000000;;		kubelet.podKillingCh = make(chan *kubecontainer.PodPair, 20)
0000000000000000000000000000000000000000;;		kubelet.resyncInterval = 10 * time.Second
0000000000000000000000000000000000000000;;		kubelet.workQueue = queue.NewBasicWorkQueue(fakeClock)
0000000000000000000000000000000000000000;;		// Relist period does not affect the tests.
0000000000000000000000000000000000000000;;		kubelet.pleg = pleg.NewGenericPLEG(fakeRuntime, 100, time.Hour, nil, clock.RealClock{})
0000000000000000000000000000000000000000;;		kubelet.clock = fakeClock
0000000000000000000000000000000000000000;;		kubelet.setNodeStatusFuncs = kubelet.defaultNodeStatusFuncs()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Factor out "StatsProvider" from Kubelet so we don't have a cyclic dependency
0000000000000000000000000000000000000000;;		volumeStatsAggPeriod := time.Second * 10
0000000000000000000000000000000000000000;;		kubelet.resourceAnalyzer = stats.NewResourceAnalyzer(kubelet, volumeStatsAggPeriod, kubelet.containerRuntime)
0000000000000000000000000000000000000000;;		nodeRef := &clientv1.ObjectReference{
0000000000000000000000000000000000000000;;			Kind:      "Node",
0000000000000000000000000000000000000000;;			Name:      string(kubelet.nodeName),
0000000000000000000000000000000000000000;;			UID:       types.UID(kubelet.nodeName),
0000000000000000000000000000000000000000;;			Namespace: "",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// setup eviction manager
0000000000000000000000000000000000000000;;		evictionManager, evictionAdmitHandler := eviction.NewManager(kubelet.resourceAnalyzer, eviction.Config{}, killPodNow(kubelet.podWorkers, fakeRecorder), kubelet.imageManager, kubelet.containerGC, fakeRecorder, nodeRef, kubelet.clock)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.evictionManager = evictionManager
0000000000000000000000000000000000000000;;		kubelet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler)
0000000000000000000000000000000000000000;;		// Add this as cleanup predicate pod admitter
0000000000000000000000000000000000000000;;		kubelet.admitHandlers.AddPodAdmitHandler(lifecycle.NewPredicateAdmitHandler(kubelet.getNodeAnyWay, lifecycle.NewAdmissionFailureHandlerStub()))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		plug := &volumetest.FakeVolumePlugin{PluginName: "fake", Host: nil}
0000000000000000000000000000000000000000;;		kubelet.volumePluginMgr, err =
0000000000000000000000000000000000000000;;			NewInitializedVolumePluginMgr(kubelet, kubelet.secretManager, kubelet.configMapManager, []volume.VolumePlugin{plug})
0000000000000000000000000000000000000000;;		require.NoError(t, err, "Failed to initialize VolumePluginMgr")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.mounter = &mount.FakeMounter{}
0000000000000000000000000000000000000000;;		kubelet.volumeManager = kubeletvolume.NewVolumeManager(
0000000000000000000000000000000000000000;;			controllerAttachDetachEnabled,
0000000000000000000000000000000000000000;;			kubelet.nodeName,
0000000000000000000000000000000000000000;;			kubelet.podManager,
0000000000000000000000000000000000000000;;			kubelet.statusManager,
0000000000000000000000000000000000000000;;			fakeKubeClient,
0000000000000000000000000000000000000000;;			kubelet.volumePluginMgr,
0000000000000000000000000000000000000000;;			fakeRuntime,
0000000000000000000000000000000000000000;;			kubelet.mounter,
0000000000000000000000000000000000000000;;			kubelet.getPodsDir(),
0000000000000000000000000000000000000000;;			kubelet.recorder,
0000000000000000000000000000000000000000;;			false, /* experimentalCheckNodeCapabilitiesBeforeMount*/
0000000000000000000000000000000000000000;;			false /* keepTerminatedPodVolumes */)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// enable active deadline handler
0000000000000000000000000000000000000000;;		activeDeadlineHandler, err := newActiveDeadlineHandler(kubelet.statusManager, kubelet.recorder, kubelet.clock)
0000000000000000000000000000000000000000;;		require.NoError(t, err, "Can't initialize active deadline handler")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.AddPodSyncLoopHandler(activeDeadlineHandler)
0000000000000000000000000000000000000000;;		kubelet.AddPodSyncHandler(activeDeadlineHandler)
0000000000000000000000000000000000000000;;		kubelet.gpuManager = gpu.NewGPUManagerStub()
0000000000000000000000000000000000000000;;		return &TestKubelet{kubelet, fakeRuntime, mockCadvisor, fakeKubeClient, fakeMirrorClient, fakeClock, nil, plug}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newTestPods(count int) []*v1.Pod {
0000000000000000000000000000000000000000;;		pods := make([]*v1.Pod, count)
0000000000000000000000000000000000000000;;		for i := 0; i < count; i++ {
0000000000000000000000000000000000000000;;			pods[i] = &v1.Pod{
0000000000000000000000000000000000000000;;				Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;					HostNetwork: true,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					UID:  types.UID(10000 + i),
0000000000000000000000000000000000000000;;					Name: fmt.Sprintf("pod%d", i),
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pods
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var emptyPodUIDs map[types.UID]kubetypes.SyncPodType
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSyncLoopAbort(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		kubelet.runtimeState.setRuntimeSync(time.Now())
0000000000000000000000000000000000000000;;		// The syncLoop waits on time.After(resyncInterval), set it really big so that we don't race for
0000000000000000000000000000000000000000;;		// the channel close
0000000000000000000000000000000000000000;;		kubelet.resyncInterval = time.Second * 30
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ch := make(chan kubetypes.PodUpdate)
0000000000000000000000000000000000000000;;		close(ch)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// sanity check (also prevent this test from hanging in the next step)
0000000000000000000000000000000000000000;;		ok := kubelet.syncLoopIteration(ch, kubelet, make(chan time.Time), make(chan time.Time), make(chan *pleg.PodLifecycleEvent, 1))
0000000000000000000000000000000000000000;;		require.False(t, ok, "Expected syncLoopIteration to return !ok since update chan was closed")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// this should terminate immediately; if it hangs then the syncLoopIteration isn't aborting properly
0000000000000000000000000000000000000000;;		kubelet.syncLoop(ch, kubelet)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSyncPodsStartPod(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		fakeRuntime := testKubelet.fakeRuntime
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			podWithUidNameNsSpec("12345678", "foo", "new", v1.PodSpec{
0000000000000000000000000000000000000000;;				Containers: []v1.Container{
0000000000000000000000000000000000000000;;					{Name: "bar"},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;		kubelet.HandlePodSyncs(pods)
0000000000000000000000000000000000000000;;		fakeRuntime.AssertStartedPods([]string{string(pods[0].UID)})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSyncPodsDeletesWhenSourcesAreReady(t *testing.T) {
0000000000000000000000000000000000000000;;		ready := false
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		fakeRuntime := testKubelet.fakeRuntime
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		kubelet.sourcesReady = config.NewSourcesReady(func(_ sets.String) bool { return ready })
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakeRuntime.PodList = []*containertest.FakePod{
0000000000000000000000000000000000000000;;			{Pod: &kubecontainer.Pod{
0000000000000000000000000000000000000000;;				ID:        "12345678",
0000000000000000000000000000000000000000;;				Name:      "foo",
0000000000000000000000000000000000000000;;				Namespace: "new",
0000000000000000000000000000000000000000;;				Containers: []*kubecontainer.Container{
0000000000000000000000000000000000000000;;					{Name: "bar"},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kubelet.HandlePodCleanups()
0000000000000000000000000000000000000000;;		// Sources are not ready yet. Don't remove any pods.
0000000000000000000000000000000000000000;;		fakeRuntime.AssertKilledPods([]string{})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		ready = true
0000000000000000000000000000000000000000;;		kubelet.HandlePodCleanups()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Sources are ready. Remove unwanted pods.
0000000000000000000000000000000000000000;;		fakeRuntime.AssertKilledPods([]string{"12345678"})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type testNodeLister struct {
0000000000000000000000000000000000000000;;		nodes []*v1.Node
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type testNodeInfo struct {
0000000000000000000000000000000000000000;;		nodes []*v1.Node
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (ls testNodeInfo) GetNodeInfo(id string) (*v1.Node, error) {
0000000000000000000000000000000000000000;;		for _, node := range ls.nodes {
0000000000000000000000000000000000000000;;			if node.Name == id {
0000000000000000000000000000000000000000;;				return node, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil, fmt.Errorf("Node with name: %s does not exist", id)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (ls testNodeLister) List(selector labels.Selector) ([]*v1.Node, error) {
0000000000000000000000000000000000000000;;		return ls.nodes, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Tests that we handle port conflicts correctly by setting the failed status in status map.
0000000000000000000000000000000000000000;;	func TestHandlePortConflicts(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.nodeInfo = testNodeInfo{nodes: []*v1.Node{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{Name: string(kl.nodeName)},
0000000000000000000000000000000000000000;;				Status: v1.NodeStatus{
0000000000000000000000000000000000000000;;					Allocatable: v1.ResourceList{
0000000000000000000000000000000000000000;;						v1.ResourcePods: *resource.NewQuantity(110, resource.DecimalSI),
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		spec := v1.PodSpec{NodeName: string(kl.nodeName), Containers: []v1.Container{{Ports: []v1.ContainerPort{{HostPort: 80}}}}}
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			podWithUidNameNsSpec("123456789", "newpod", "foo", spec),
0000000000000000000000000000000000000000;;			podWithUidNameNsSpec("987654321", "oldpod", "foo", spec),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Make sure the Pods are in the reverse order of creation time.
0000000000000000000000000000000000000000;;		pods[1].CreationTimestamp = metav1.NewTime(time.Now())
0000000000000000000000000000000000000000;;		pods[0].CreationTimestamp = metav1.NewTime(time.Now().Add(1 * time.Second))
0000000000000000000000000000000000000000;;		// The newer pod should be rejected.
0000000000000000000000000000000000000000;;		notfittingPod := pods[0]
0000000000000000000000000000000000000000;;		fittingPod := pods[1]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.HandlePodAdditions(pods)
0000000000000000000000000000000000000000;;		// Check pod status stored in the status map.
0000000000000000000000000000000000000000;;		// notfittingPod should be Failed
0000000000000000000000000000000000000000;;		status, found := kl.statusManager.GetPodStatus(notfittingPod.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", notfittingPod.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodFailed, status.Phase)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// fittingPod should be Pending
0000000000000000000000000000000000000000;;		status, found = kl.statusManager.GetPodStatus(fittingPod.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", fittingPod.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodPending, status.Phase)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Tests that we handle host name conflicts correctly by setting the failed status in status map.
0000000000000000000000000000000000000000;;	func TestHandleHostNameConflicts(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.nodeInfo = testNodeInfo{nodes: []*v1.Node{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{Name: "127.0.0.1"},
0000000000000000000000000000000000000000;;				Status: v1.NodeStatus{
0000000000000000000000000000000000000000;;					Allocatable: v1.ResourceList{
0000000000000000000000000000000000000000;;						v1.ResourcePods: *resource.NewQuantity(110, resource.DecimalSI),
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// default NodeName in test is 127.0.0.1
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			podWithUidNameNsSpec("123456789", "notfittingpod", "foo", v1.PodSpec{NodeName: "127.0.0.2"}),
0000000000000000000000000000000000000000;;			podWithUidNameNsSpec("987654321", "fittingpod", "foo", v1.PodSpec{NodeName: "127.0.0.1"}),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		notfittingPod := pods[0]
0000000000000000000000000000000000000000;;		fittingPod := pods[1]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.HandlePodAdditions(pods)
0000000000000000000000000000000000000000;;		// Check pod status stored in the status map.
0000000000000000000000000000000000000000;;		// notfittingPod should be Failed
0000000000000000000000000000000000000000;;		status, found := kl.statusManager.GetPodStatus(notfittingPod.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", notfittingPod.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodFailed, status.Phase)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// fittingPod should be Pending
0000000000000000000000000000000000000000;;		status, found = kl.statusManager.GetPodStatus(fittingPod.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", fittingPod.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodPending, status.Phase)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Tests that we handle not matching labels selector correctly by setting the failed status in status map.
0000000000000000000000000000000000000000;;	func TestHandleNodeSelector(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		nodes := []*v1.Node{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{Name: testKubeletHostname, Labels: map[string]string{"key": "B"}},
0000000000000000000000000000000000000000;;				Status: v1.NodeStatus{
0000000000000000000000000000000000000000;;					Allocatable: v1.ResourceList{
0000000000000000000000000000000000000000;;						v1.ResourcePods: *resource.NewQuantity(110, resource.DecimalSI),
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kl.nodeInfo = testNodeInfo{nodes: nodes}
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			podWithUidNameNsSpec("123456789", "podA", "foo", v1.PodSpec{NodeSelector: map[string]string{"key": "A"}}),
0000000000000000000000000000000000000000;;			podWithUidNameNsSpec("987654321", "podB", "foo", v1.PodSpec{NodeSelector: map[string]string{"key": "B"}}),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// The first pod should be rejected.
0000000000000000000000000000000000000000;;		notfittingPod := pods[0]
0000000000000000000000000000000000000000;;		fittingPod := pods[1]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.HandlePodAdditions(pods)
0000000000000000000000000000000000000000;;		// Check pod status stored in the status map.
0000000000000000000000000000000000000000;;		// notfittingPod should be Failed
0000000000000000000000000000000000000000;;		status, found := kl.statusManager.GetPodStatus(notfittingPod.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", notfittingPod.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodFailed, status.Phase)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// fittingPod should be Pending
0000000000000000000000000000000000000000;;		status, found = kl.statusManager.GetPodStatus(fittingPod.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", fittingPod.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodPending, status.Phase)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Tests that we handle exceeded resources correctly by setting the failed status in status map.
0000000000000000000000000000000000000000;;	func TestHandleMemExceeded(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		nodes := []*v1.Node{
0000000000000000000000000000000000000000;;			{ObjectMeta: metav1.ObjectMeta{Name: testKubeletHostname},
0000000000000000000000000000000000000000;;				Status: v1.NodeStatus{Capacity: v1.ResourceList{}, Allocatable: v1.ResourceList{
0000000000000000000000000000000000000000;;					v1.ResourceCPU:    *resource.NewMilliQuantity(10, resource.DecimalSI),
0000000000000000000000000000000000000000;;					v1.ResourceMemory: *resource.NewQuantity(100, resource.BinarySI),
0000000000000000000000000000000000000000;;					v1.ResourcePods:   *resource.NewQuantity(40, resource.DecimalSI),
0000000000000000000000000000000000000000;;				}}},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kl.nodeInfo = testNodeInfo{nodes: nodes}
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		spec := v1.PodSpec{NodeName: string(kl.nodeName),
0000000000000000000000000000000000000000;;			Containers: []v1.Container{{Resources: v1.ResourceRequirements{
0000000000000000000000000000000000000000;;				Requests: v1.ResourceList{
0000000000000000000000000000000000000000;;					"memory": resource.MustParse("90"),
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}}},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			podWithUidNameNsSpec("123456789", "newpod", "foo", spec),
0000000000000000000000000000000000000000;;			podWithUidNameNsSpec("987654321", "oldpod", "foo", spec),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Make sure the Pods are in the reverse order of creation time.
0000000000000000000000000000000000000000;;		pods[1].CreationTimestamp = metav1.NewTime(time.Now())
0000000000000000000000000000000000000000;;		pods[0].CreationTimestamp = metav1.NewTime(time.Now().Add(1 * time.Second))
0000000000000000000000000000000000000000;;		// The newer pod should be rejected.
0000000000000000000000000000000000000000;;		notfittingPod := pods[0]
0000000000000000000000000000000000000000;;		fittingPod := pods[1]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.HandlePodAdditions(pods)
0000000000000000000000000000000000000000;;		// Check pod status stored in the status map.
0000000000000000000000000000000000000000;;		// notfittingPod should be Failed
0000000000000000000000000000000000000000;;		status, found := kl.statusManager.GetPodStatus(notfittingPod.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", notfittingPod.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodFailed, status.Phase)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// fittingPod should be Pending
0000000000000000000000000000000000000000;;		status, found = kl.statusManager.GetPodStatus(fittingPod.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", fittingPod.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodPending, status.Phase)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO(filipg): This test should be removed once StatusSyncer can do garbage collection without external signal.
0000000000000000000000000000000000000000;;	func TestPurgingObsoleteStatusMapEntries(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		versionInfo := &cadvisorapi.VersionInfo{
0000000000000000000000000000000000000000;;			KernelVersion:      "3.16.0-0.bpo.4-amd64",
0000000000000000000000000000000000000000;;			ContainerOsVersion: "Debian GNU/Linux 7 (wheezy)",
0000000000000000000000000000000000000000;;			DockerVersion:      "1.5.0",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(versionInfo, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			{ObjectMeta: metav1.ObjectMeta{Name: "pod1", UID: "1234"}, Spec: v1.PodSpec{Containers: []v1.Container{{Ports: []v1.ContainerPort{{HostPort: 80}}}}}},
0000000000000000000000000000000000000000;;			{ObjectMeta: metav1.ObjectMeta{Name: "pod2", UID: "4567"}, Spec: v1.PodSpec{Containers: []v1.Container{{Ports: []v1.ContainerPort{{HostPort: 80}}}}}},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podToTest := pods[1]
0000000000000000000000000000000000000000;;		// Run once to populate the status map.
0000000000000000000000000000000000000000;;		kl.HandlePodAdditions(pods)
0000000000000000000000000000000000000000;;		if _, found := kl.statusManager.GetPodStatus(podToTest.UID); !found {
0000000000000000000000000000000000000000;;			t.Fatalf("expected to have status cached for pod2")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Sync with empty pods so that the entry in status map will be removed.
0000000000000000000000000000000000000000;;		kl.podManager.SetPods([]*v1.Pod{})
0000000000000000000000000000000000000000;;		kl.HandlePodCleanups()
0000000000000000000000000000000000000000;;		if _, found := kl.statusManager.GetPodStatus(podToTest.UID); found {
0000000000000000000000000000000000000000;;			t.Fatalf("expected to not have status cached for pod2")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestValidateContainerLogStatus(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		containerName := "x"
0000000000000000000000000000000000000000;;		testCases := []struct {
0000000000000000000000000000000000000000;;			statuses []v1.ContainerStatus
0000000000000000000000000000000000000000;;			success  bool // whether getting logs for the container should succeed.
0000000000000000000000000000000000000000;;			pSuccess bool // whether getting logs for the previous container should succeed.
0000000000000000000000000000000000000000;;		}{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				statuses: []v1.ContainerStatus{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name: containerName,
0000000000000000000000000000000000000000;;						State: v1.ContainerState{
0000000000000000000000000000000000000000;;							Running: &v1.ContainerStateRunning{},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;						LastTerminationState: v1.ContainerState{
0000000000000000000000000000000000000000;;							Terminated: &v1.ContainerStateTerminated{},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				success:  true,
0000000000000000000000000000000000000000;;				pSuccess: true,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				statuses: []v1.ContainerStatus{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name: containerName,
0000000000000000000000000000000000000000;;						State: v1.ContainerState{
0000000000000000000000000000000000000000;;							Running: &v1.ContainerStateRunning{},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				success:  true,
0000000000000000000000000000000000000000;;				pSuccess: false,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				statuses: []v1.ContainerStatus{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name: containerName,
0000000000000000000000000000000000000000;;						State: v1.ContainerState{
0000000000000000000000000000000000000000;;							Terminated: &v1.ContainerStateTerminated{},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				success:  true,
0000000000000000000000000000000000000000;;				pSuccess: false,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				statuses: []v1.ContainerStatus{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name: containerName,
0000000000000000000000000000000000000000;;						State: v1.ContainerState{
0000000000000000000000000000000000000000;;							Waiting: &v1.ContainerStateWaiting{},
0000000000000000000000000000000000000000;;						},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				success:  false,
0000000000000000000000000000000000000000;;				pSuccess: false,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				statuses: []v1.ContainerStatus{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  containerName,
0000000000000000000000000000000000000000;;						State: v1.ContainerState{Waiting: &v1.ContainerStateWaiting{Reason: "ErrImagePull"}},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				success:  false,
0000000000000000000000000000000000000000;;				pSuccess: false,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				statuses: []v1.ContainerStatus{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:  containerName,
0000000000000000000000000000000000000000;;						State: v1.ContainerState{Waiting: &v1.ContainerStateWaiting{Reason: "ErrImagePullBackOff"}},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				success:  false,
0000000000000000000000000000000000000000;;				pSuccess: false,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i, tc := range testCases {
0000000000000000000000000000000000000000;;			// Access the log of the most recent container
0000000000000000000000000000000000000000;;			previous := false
0000000000000000000000000000000000000000;;			podStatus := &v1.PodStatus{ContainerStatuses: tc.statuses}
0000000000000000000000000000000000000000;;			_, err := kubelet.validateContainerLogStatus("podName", podStatus, containerName, previous)
0000000000000000000000000000000000000000;;			if !tc.success {
0000000000000000000000000000000000000000;;				assert.Error(t, err, fmt.Sprintf("[case %d] error", i))
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				assert.NoError(t, err, "[case %d] error", i)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Access the log of the previous, terminated container
0000000000000000000000000000000000000000;;			previous = true
0000000000000000000000000000000000000000;;			_, err = kubelet.validateContainerLogStatus("podName", podStatus, containerName, previous)
0000000000000000000000000000000000000000;;			if !tc.pSuccess {
0000000000000000000000000000000000000000;;				assert.Error(t, err, fmt.Sprintf("[case %d] error", i))
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				assert.NoError(t, err, "[case %d] error", i)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Access the log of a container that's not in the pod
0000000000000000000000000000000000000000;;			_, err = kubelet.validateContainerLogStatus("podName", podStatus, "blah", false)
0000000000000000000000000000000000000000;;			assert.Error(t, err, fmt.Sprintf("[case %d] invalid container name should cause an error", i))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// updateDiskSpacePolicy creates a new DiskSpaceManager with a new policy. This new manager along
0000000000000000000000000000000000000000;;	// with the mock FsInfo values added to Cadvisor should make the kubelet report that it has
0000000000000000000000000000000000000000;;	// sufficient disk space or it is out of disk, depending on the capacity, availability and
0000000000000000000000000000000000000000;;	// threshold values.
0000000000000000000000000000000000000000;;	func updateDiskSpacePolicy(kubelet *Kubelet, mockCadvisor *cadvisortest.Mock, rootCap, dockerCap, rootAvail, dockerAvail uint64, rootThreshold, dockerThreshold int) error {
0000000000000000000000000000000000000000;;		dockerimagesFsInfo := cadvisorapiv2.FsInfo{Capacity: rootCap * mb, Available: rootAvail * mb}
0000000000000000000000000000000000000000;;		rootFsInfo := cadvisorapiv2.FsInfo{Capacity: dockerCap * mb, Available: dockerAvail * mb}
0000000000000000000000000000000000000000;;		mockCadvisor.On("ImagesFsInfo").Return(dockerimagesFsInfo, nil)
0000000000000000000000000000000000000000;;		mockCadvisor.On("RootFsInfo").Return(rootFsInfo, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dsp := DiskSpacePolicy{DockerFreeDiskMB: rootThreshold, RootFreeDiskMB: dockerThreshold}
0000000000000000000000000000000000000000;;		diskSpaceManager, err := newDiskSpaceManager(mockCadvisor, dsp)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kubelet.diskSpaceManager = diskSpaceManager
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestCreateMirrorPod(t *testing.T) {
0000000000000000000000000000000000000000;;		for _, updateType := range []kubetypes.SyncPodType{kubetypes.SyncPodCreate, kubetypes.SyncPodUpdate} {
0000000000000000000000000000000000000000;;			testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;			defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;			testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;			testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;			testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;			testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;			testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;			manager := testKubelet.fakeMirrorClient
0000000000000000000000000000000000000000;;			pod := podWithUidNameNs("12345678", "bar", "foo")
0000000000000000000000000000000000000000;;			pod.Annotations[kubetypes.ConfigSourceAnnotationKey] = "file"
0000000000000000000000000000000000000000;;			pods := []*v1.Pod{pod}
0000000000000000000000000000000000000000;;			kl.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;			err := kl.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;				pod:        pod,
0000000000000000000000000000000000000000;;				podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;				updateType: updateType,
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			assert.NoError(t, err)
0000000000000000000000000000000000000000;;			podFullName := kubecontainer.GetPodFullName(pod)
0000000000000000000000000000000000000000;;			assert.True(t, manager.HasPod(podFullName), "Expected mirror pod %q to be created", podFullName)
0000000000000000000000000000000000000000;;			assert.Equal(t, 1, manager.NumOfPods(), "Expected only 1 mirror pod %q, got %+v", podFullName, manager.GetPods())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDeleteOutdatedMirrorPod(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		manager := testKubelet.fakeMirrorClient
0000000000000000000000000000000000000000;;		pod := podWithUidNameNsSpec("12345678", "foo", "ns", v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "1234", Image: "foo"},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod.Annotations[kubetypes.ConfigSourceAnnotationKey] = "file"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Mirror pod has an outdated spec.
0000000000000000000000000000000000000000;;		mirrorPod := podWithUidNameNsSpec("11111111", "foo", "ns", v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "1234", Image: "bar"},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		mirrorPod.Annotations[kubetypes.ConfigSourceAnnotationKey] = "api"
0000000000000000000000000000000000000000;;		mirrorPod.Annotations[kubetypes.ConfigMirrorAnnotationKey] = "mirror"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{pod, mirrorPod}
0000000000000000000000000000000000000000;;		kl.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;		err := kl.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			mirrorPod:  mirrorPod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.NoError(t, err)
0000000000000000000000000000000000000000;;		name := kubecontainer.GetPodFullName(pod)
0000000000000000000000000000000000000000;;		creates, deletes := manager.GetCounts(name)
0000000000000000000000000000000000000000;;		if creates != 1 || deletes != 1 {
0000000000000000000000000000000000000000;;			t.Errorf("expected 1 creation and 1 deletion of %q, got %d, %d", name, creates, deletes)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDeleteOrphanedMirrorPods(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		manager := testKubelet.fakeMirrorClient
0000000000000000000000000000000000000000;;		orphanPods := []*v1.Pod{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					UID:       "12345678",
0000000000000000000000000000000000000000;;					Name:      "pod1",
0000000000000000000000000000000000000000;;					Namespace: "ns",
0000000000000000000000000000000000000000;;					Annotations: map[string]string{
0000000000000000000000000000000000000000;;						kubetypes.ConfigSourceAnnotationKey: "api",
0000000000000000000000000000000000000000;;						kubetypes.ConfigMirrorAnnotationKey: "mirror",
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					UID:       "12345679",
0000000000000000000000000000000000000000;;					Name:      "pod2",
0000000000000000000000000000000000000000;;					Namespace: "ns",
0000000000000000000000000000000000000000;;					Annotations: map[string]string{
0000000000000000000000000000000000000000;;						kubetypes.ConfigSourceAnnotationKey: "api",
0000000000000000000000000000000000000000;;						kubetypes.ConfigMirrorAnnotationKey: "mirror",
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.podManager.SetPods(orphanPods)
0000000000000000000000000000000000000000;;		// Sync with an empty pod list to delete all mirror pods.
0000000000000000000000000000000000000000;;		kl.HandlePodCleanups()
0000000000000000000000000000000000000000;;		assert.Len(t, manager.GetPods(), 0, "Expected 0 mirror pods")
0000000000000000000000000000000000000000;;		for _, pod := range orphanPods {
0000000000000000000000000000000000000000;;			name := kubecontainer.GetPodFullName(pod)
0000000000000000000000000000000000000000;;			creates, deletes := manager.GetCounts(name)
0000000000000000000000000000000000000000;;			if creates != 0 || deletes != 1 {
0000000000000000000000000000000000000000;;				t.Errorf("expected 0 creation and one deletion of %q, got %d, %d", name, creates, deletes)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestGetContainerInfoForMirrorPods(t *testing.T) {
0000000000000000000000000000000000000000;;		// pods contain one static and one mirror pod with the same name but
0000000000000000000000000000000000000000;;		// different UIDs.
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					UID:       "1234",
0000000000000000000000000000000000000000;;					Name:      "qux",
0000000000000000000000000000000000000000;;					Namespace: "ns",
0000000000000000000000000000000000000000;;					Annotations: map[string]string{
0000000000000000000000000000000000000000;;						kubetypes.ConfigSourceAnnotationKey: "file",
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;					Containers: []v1.Container{
0000000000000000000000000000000000000000;;						{Name: "foo"},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					UID:       "5678",
0000000000000000000000000000000000000000;;					Name:      "qux",
0000000000000000000000000000000000000000;;					Namespace: "ns",
0000000000000000000000000000000000000000;;					Annotations: map[string]string{
0000000000000000000000000000000000000000;;						kubetypes.ConfigSourceAnnotationKey: "api",
0000000000000000000000000000000000000000;;						kubetypes.ConfigMirrorAnnotationKey: "mirror",
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;					Containers: []v1.Container{
0000000000000000000000000000000000000000;;						{Name: "foo"},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		containerID := "ab2cdf"
0000000000000000000000000000000000000000;;		containerPath := fmt.Sprintf("/docker/%v", containerID)
0000000000000000000000000000000000000000;;		containerInfo := cadvisorapi.ContainerInfo{
0000000000000000000000000000000000000000;;			ContainerReference: cadvisorapi.ContainerReference{
0000000000000000000000000000000000000000;;				Name: containerPath,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		fakeRuntime := testKubelet.fakeRuntime
0000000000000000000000000000000000000000;;		mockCadvisor := testKubelet.fakeCadvisor
0000000000000000000000000000000000000000;;		cadvisorReq := &cadvisorapi.ContainerInfoRequest{}
0000000000000000000000000000000000000000;;		mockCadvisor.On("DockerContainer", containerID, cadvisorReq).Return(containerInfo, nil)
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakeRuntime.PodList = []*containertest.FakePod{
0000000000000000000000000000000000000000;;			{Pod: &kubecontainer.Pod{
0000000000000000000000000000000000000000;;				ID:        "1234",
0000000000000000000000000000000000000000;;				Name:      "qux",
0000000000000000000000000000000000000000;;				Namespace: "ns",
0000000000000000000000000000000000000000;;				Containers: []*kubecontainer.Container{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name: "foo",
0000000000000000000000000000000000000000;;						ID:   kubecontainer.ContainerID{Type: "test", ID: containerID},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;		// Use the mirror pod UID to retrieve the stats.
0000000000000000000000000000000000000000;;		stats, err := kubelet.GetContainerInfo("qux_ns", "5678", "foo", cadvisorReq)
0000000000000000000000000000000000000000;;		assert.NoError(t, err)
0000000000000000000000000000000000000000;;		require.NotNil(t, stats)
0000000000000000000000000000000000000000;;		mockCadvisor.AssertExpectations(t)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestHostNetworkAllowed(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		capabilities.SetForTests(capabilities.Capabilities{
0000000000000000000000000000000000000000;;			PrivilegedSources: capabilities.PrivilegedSources{
0000000000000000000000000000000000000000;;				HostNetworkSources: []string{kubetypes.ApiserverSource, kubetypes.FileSource},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod := podWithUidNameNsSpec("12345678", "foo", "new", v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "foo"},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			HostNetwork: true,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod.Annotations[kubetypes.ConfigSourceAnnotationKey] = kubetypes.FileSource
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods([]*v1.Pod{pod})
0000000000000000000000000000000000000000;;		err := kubelet.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.NoError(t, err, "expected pod infra creation to succeed")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestHostNetworkDisallowed(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		capabilities.SetForTests(capabilities.Capabilities{
0000000000000000000000000000000000000000;;			PrivilegedSources: capabilities.PrivilegedSources{
0000000000000000000000000000000000000000;;				HostNetworkSources: []string{},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod := podWithUidNameNsSpec("12345678", "foo", "new", v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "foo"},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			HostNetwork: true,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod.Annotations[kubetypes.ConfigSourceAnnotationKey] = kubetypes.FileSource
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := kubelet.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.Error(t, err, "expected pod infra creation to fail")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestHostPIDAllowed(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		capabilities.SetForTests(capabilities.Capabilities{
0000000000000000000000000000000000000000;;			PrivilegedSources: capabilities.PrivilegedSources{
0000000000000000000000000000000000000000;;				HostPIDSources: []string{kubetypes.ApiserverSource, kubetypes.FileSource},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod := podWithUidNameNsSpec("12345678", "foo", "new", v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "foo"},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			HostPID: true,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod.Annotations[kubetypes.ConfigSourceAnnotationKey] = kubetypes.FileSource
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods([]*v1.Pod{pod})
0000000000000000000000000000000000000000;;		err := kubelet.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.NoError(t, err, "expected pod infra creation to succeed")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestHostPIDDisallowed(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		capabilities.SetForTests(capabilities.Capabilities{
0000000000000000000000000000000000000000;;			PrivilegedSources: capabilities.PrivilegedSources{
0000000000000000000000000000000000000000;;				HostPIDSources: []string{},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod := podWithUidNameNsSpec("12345678", "foo", "new", v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "foo"},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			HostPID: true,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod.Annotations[kubetypes.ConfigSourceAnnotationKey] = kubetypes.FileSource
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := kubelet.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.Error(t, err, "expected pod infra creation to fail")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestHostIPCAllowed(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		capabilities.SetForTests(capabilities.Capabilities{
0000000000000000000000000000000000000000;;			PrivilegedSources: capabilities.PrivilegedSources{
0000000000000000000000000000000000000000;;				HostIPCSources: []string{kubetypes.ApiserverSource, kubetypes.FileSource},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod := podWithUidNameNsSpec("12345678", "foo", "new", v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "foo"},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			HostIPC: true,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod.Annotations[kubetypes.ConfigSourceAnnotationKey] = kubetypes.FileSource
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods([]*v1.Pod{pod})
0000000000000000000000000000000000000000;;		err := kubelet.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.NoError(t, err, "expected pod infra creation to succeed")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestHostIPCDisallowed(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		capabilities.SetForTests(capabilities.Capabilities{
0000000000000000000000000000000000000000;;			PrivilegedSources: capabilities.PrivilegedSources{
0000000000000000000000000000000000000000;;				HostIPCSources: []string{},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod := podWithUidNameNsSpec("12345678", "foo", "new", v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "foo"},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			HostIPC: true,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		pod.Annotations[kubetypes.ConfigSourceAnnotationKey] = kubetypes.FileSource
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := kubelet.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.Error(t, err, "expected pod infra creation to fail")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestPrivilegeContainerAllowed(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		capabilities.SetForTests(capabilities.Capabilities{
0000000000000000000000000000000000000000;;			AllowPrivileged: true,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		privileged := true
0000000000000000000000000000000000000000;;		pod := podWithUidNameNsSpec("12345678", "foo", "new", v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "foo", SecurityContext: &v1.SecurityContext{Privileged: &privileged}},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods([]*v1.Pod{pod})
0000000000000000000000000000000000000000;;		err := kubelet.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.NoError(t, err, "expected pod infra creation to succeed")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestPrivilegedContainerDisallowed(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		capabilities.SetForTests(capabilities.Capabilities{
0000000000000000000000000000000000000000;;			AllowPrivileged: false,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		privileged := true
0000000000000000000000000000000000000000;;		pod := podWithUidNameNsSpec("12345678", "foo", "new", v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "foo", SecurityContext: &v1.SecurityContext{Privileged: &privileged}},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := kubelet.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.Error(t, err, "expected pod infra creation to fail")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestNetworkErrorsWithoutHostNetwork(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.runtimeState.setNetworkState(fmt.Errorf("simulated network error"))
0000000000000000000000000000000000000000;;		capabilities.SetForTests(capabilities.Capabilities{
0000000000000000000000000000000000000000;;			PrivilegedSources: capabilities.PrivilegedSources{
0000000000000000000000000000000000000000;;				HostNetworkSources: []string{kubetypes.ApiserverSource, kubetypes.FileSource},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pod := podWithUidNameNsSpec("12345678", "hostnetwork", "new", v1.PodSpec{
0000000000000000000000000000000000000000;;			HostNetwork: false,
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			Containers: []v1.Container{
0000000000000000000000000000000000000000;;				{Name: "foo"},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods([]*v1.Pod{pod})
0000000000000000000000000000000000000000;;		err := kubelet.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.Error(t, err, "expected pod with hostNetwork=false to fail when network in error")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pod.Annotations[kubetypes.ConfigSourceAnnotationKey] = kubetypes.FileSource
0000000000000000000000000000000000000000;;		pod.Spec.HostNetwork = true
0000000000000000000000000000000000000000;;		err = kubelet.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodUpdate,
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		assert.NoError(t, err, "expected pod with hostNetwork=true to succeed when network in error")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestFilterOutTerminatedPods(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		pods := newTestPods(5)
0000000000000000000000000000000000000000;;		now := metav1.NewTime(time.Now())
0000000000000000000000000000000000000000;;		pods[0].Status.Phase = v1.PodFailed
0000000000000000000000000000000000000000;;		pods[1].Status.Phase = v1.PodSucceeded
0000000000000000000000000000000000000000;;		// The pod is terminating, should not filter out.
0000000000000000000000000000000000000000;;		pods[2].Status.Phase = v1.PodRunning
0000000000000000000000000000000000000000;;		pods[2].DeletionTimestamp = &now
0000000000000000000000000000000000000000;;		pods[2].Status.ContainerStatuses = []v1.ContainerStatus{
0000000000000000000000000000000000000000;;			{State: v1.ContainerState{
0000000000000000000000000000000000000000;;				Running: &v1.ContainerStateRunning{
0000000000000000000000000000000000000000;;					StartedAt: now,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pods[3].Status.Phase = v1.PodPending
0000000000000000000000000000000000000000;;		pods[4].Status.Phase = v1.PodRunning
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		expected := []*v1.Pod{pods[2], pods[3], pods[4]}
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;		actual := kubelet.filterOutTerminatedPods(pods)
0000000000000000000000000000000000000000;;		assert.Equal(t, expected, actual)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSyncPodsSetStatusToFailedForPodsThatRunTooLong(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		fakeRuntime := testKubelet.fakeRuntime
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		now := metav1.Now()
0000000000000000000000000000000000000000;;		startTime := metav1.NewTime(now.Time.Add(-1 * time.Minute))
0000000000000000000000000000000000000000;;		exceededActiveDeadlineSeconds := int64(30)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					UID:       "12345678",
0000000000000000000000000000000000000000;;					Name:      "bar",
0000000000000000000000000000000000000000;;					Namespace: "new",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;					Containers: []v1.Container{
0000000000000000000000000000000000000000;;						{Name: "foo"},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					ActiveDeadlineSeconds: &exceededActiveDeadlineSeconds,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Status: v1.PodStatus{
0000000000000000000000000000000000000000;;					StartTime: &startTime,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakeRuntime.PodList = []*containertest.FakePod{
0000000000000000000000000000000000000000;;			{Pod: &kubecontainer.Pod{
0000000000000000000000000000000000000000;;				ID:        "12345678",
0000000000000000000000000000000000000000;;				Name:      "bar",
0000000000000000000000000000000000000000;;				Namespace: "new",
0000000000000000000000000000000000000000;;				Containers: []*kubecontainer.Container{
0000000000000000000000000000000000000000;;					{Name: "foo"},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Let the pod worker sets the status to fail after this sync.
0000000000000000000000000000000000000000;;		kubelet.HandlePodUpdates(pods)
0000000000000000000000000000000000000000;;		status, found := kubelet.statusManager.GetPodStatus(pods[0].UID)
0000000000000000000000000000000000000000;;		assert.True(t, found, "expected to found status for pod %q", pods[0].UID)
0000000000000000000000000000000000000000;;		assert.Equal(t, v1.PodFailed, status.Phase)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSyncPodsDoesNotSetPodsThatDidNotRunTooLongToFailed(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		fakeRuntime := testKubelet.fakeRuntime
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		now := metav1.Now()
0000000000000000000000000000000000000000;;		startTime := metav1.NewTime(now.Time.Add(-1 * time.Minute))
0000000000000000000000000000000000000000;;		exceededActiveDeadlineSeconds := int64(300)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					UID:       "12345678",
0000000000000000000000000000000000000000;;					Name:      "bar",
0000000000000000000000000000000000000000;;					Namespace: "new",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;					Containers: []v1.Container{
0000000000000000000000000000000000000000;;						{Name: "foo"},
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					ActiveDeadlineSeconds: &exceededActiveDeadlineSeconds,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				Status: v1.PodStatus{
0000000000000000000000000000000000000000;;					StartTime: &startTime,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fakeRuntime.PodList = []*containertest.FakePod{
0000000000000000000000000000000000000000;;			{Pod: &kubecontainer.Pod{
0000000000000000000000000000000000000000;;				ID:        "12345678",
0000000000000000000000000000000000000000;;				Name:      "bar",
0000000000000000000000000000000000000000;;				Namespace: "new",
0000000000000000000000000000000000000000;;				Containers: []*kubecontainer.Container{
0000000000000000000000000000000000000000;;					{Name: "foo"},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;		kubelet.HandlePodUpdates(pods)
0000000000000000000000000000000000000000;;		status, found := kubelet.statusManager.GetPodStatus(pods[0].UID)
0000000000000000000000000000000000000000;;		assert.True(t, found, "expected to found status for pod %q", pods[0].UID)
0000000000000000000000000000000000000000;;		assert.NotEqual(t, v1.PodFailed, status.Phase)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podWithUidNameNs(uid types.UID, name, namespace string) *v1.Pod {
0000000000000000000000000000000000000000;;		return &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				UID:         uid,
0000000000000000000000000000000000000000;;				Name:        name,
0000000000000000000000000000000000000000;;				Namespace:   namespace,
0000000000000000000000000000000000000000;;				Annotations: map[string]string{},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func podWithUidNameNsSpec(uid types.UID, name, namespace string, spec v1.PodSpec) *v1.Pod {
0000000000000000000000000000000000000000;;		pod := podWithUidNameNs(uid, name, namespace)
0000000000000000000000000000000000000000;;		pod.Spec = spec
0000000000000000000000000000000000000000;;		return pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDeletePodDirsForDeletedPods(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			podWithUidNameNs("12345678", "pod1", "ns"),
0000000000000000000000000000000000000000;;			podWithUidNameNs("12345679", "pod2", "ns"),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;		// Sync to create pod directories.
0000000000000000000000000000000000000000;;		kl.HandlePodSyncs(kl.podManager.GetPods())
0000000000000000000000000000000000000000;;		for i := range pods {
0000000000000000000000000000000000000000;;			assert.True(t, dirExists(kl.getPodDir(pods[i].UID)), "Expected directory to exist for pod %d", i)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Pod 1 has been deleted and no longer exists.
0000000000000000000000000000000000000000;;		kl.podManager.SetPods([]*v1.Pod{pods[0]})
0000000000000000000000000000000000000000;;		kl.HandlePodCleanups()
0000000000000000000000000000000000000000;;		assert.True(t, dirExists(kl.getPodDir(pods[0].UID)), "Expected directory to exist for pod 0")
0000000000000000000000000000000000000000;;		assert.False(t, dirExists(kl.getPodDir(pods[1].UID)), "Expected directory to be deleted for pod 1")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func syncAndVerifyPodDir(t *testing.T, testKubelet *TestKubelet, pods []*v1.Pod, podsToCheck []*v1.Pod, shouldExist bool) {
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;		kl.HandlePodSyncs(pods)
0000000000000000000000000000000000000000;;		kl.HandlePodCleanups()
0000000000000000000000000000000000000000;;		for i, pod := range podsToCheck {
0000000000000000000000000000000000000000;;			exist := dirExists(kl.getPodDir(pod.UID))
0000000000000000000000000000000000000000;;			assert.Equal(t, shouldExist, exist, "directory of pod %d", i)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDoesNotDeletePodDirsForTerminatedPods(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			podWithUidNameNs("12345678", "pod1", "ns"),
0000000000000000000000000000000000000000;;			podWithUidNameNs("12345679", "pod2", "ns"),
0000000000000000000000000000000000000000;;			podWithUidNameNs("12345680", "pod3", "ns"),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		syncAndVerifyPodDir(t, testKubelet, pods, pods, true)
0000000000000000000000000000000000000000;;		// Pod 1 failed, and pod 2 succeeded. None of the pod directories should be
0000000000000000000000000000000000000000;;		// deleted.
0000000000000000000000000000000000000000;;		kl.statusManager.SetPodStatus(pods[1], v1.PodStatus{Phase: v1.PodFailed})
0000000000000000000000000000000000000000;;		kl.statusManager.SetPodStatus(pods[2], v1.PodStatus{Phase: v1.PodSucceeded})
0000000000000000000000000000000000000000;;		syncAndVerifyPodDir(t, testKubelet, pods, pods, true)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestDoesNotDeletePodDirsIfContainerIsRunning(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("Start").Return(nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		runningPod := &kubecontainer.Pod{
0000000000000000000000000000000000000000;;			ID:        "12345678",
0000000000000000000000000000000000000000;;			Name:      "pod1",
0000000000000000000000000000000000000000;;			Namespace: "ns",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		apiPod := podWithUidNameNs(runningPod.ID, runningPod.Name, runningPod.Namespace)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Sync once to create pod directory; confirm that the pod directory has
0000000000000000000000000000000000000000;;		// already been created.
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{apiPod}
0000000000000000000000000000000000000000;;		syncAndVerifyPodDir(t, testKubelet, pods, []*v1.Pod{apiPod}, true)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Pretend the pod is deleted from apiserver, but is still active on the node.
0000000000000000000000000000000000000000;;		// The pod directory should not be removed.
0000000000000000000000000000000000000000;;		pods = []*v1.Pod{}
0000000000000000000000000000000000000000;;		testKubelet.fakeRuntime.PodList = []*containertest.FakePod{{runningPod, ""}}
0000000000000000000000000000000000000000;;		syncAndVerifyPodDir(t, testKubelet, pods, []*v1.Pod{apiPod}, true)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The pod is deleted and also not active on the node. The pod directory
0000000000000000000000000000000000000000;;		// should be removed.
0000000000000000000000000000000000000000;;		pods = []*v1.Pod{}
0000000000000000000000000000000000000000;;		testKubelet.fakeRuntime.PodList = []*containertest.FakePod{}
0000000000000000000000000000000000000000;;		syncAndVerifyPodDir(t, testKubelet, pods, []*v1.Pod{apiPod}, false)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestGetPodsToSync(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		clock := testKubelet.fakeClock
0000000000000000000000000000000000000000;;		pods := newTestPods(5)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		exceededActiveDeadlineSeconds := int64(30)
0000000000000000000000000000000000000000;;		notYetActiveDeadlineSeconds := int64(120)
0000000000000000000000000000000000000000;;		startTime := metav1.NewTime(clock.Now())
0000000000000000000000000000000000000000;;		pods[0].Status.StartTime = &startTime
0000000000000000000000000000000000000000;;		pods[0].Spec.ActiveDeadlineSeconds = &exceededActiveDeadlineSeconds
0000000000000000000000000000000000000000;;		pods[1].Status.StartTime = &startTime
0000000000000000000000000000000000000000;;		pods[1].Spec.ActiveDeadlineSeconds = &notYetActiveDeadlineSeconds
0000000000000000000000000000000000000000;;		pods[2].Status.StartTime = &startTime
0000000000000000000000000000000000000000;;		pods[2].Spec.ActiveDeadlineSeconds = &exceededActiveDeadlineSeconds
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;		kubelet.workQueue.Enqueue(pods[2].UID, 0)
0000000000000000000000000000000000000000;;		kubelet.workQueue.Enqueue(pods[3].UID, 30*time.Second)
0000000000000000000000000000000000000000;;		kubelet.workQueue.Enqueue(pods[4].UID, 2*time.Minute)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		clock.Step(1 * time.Minute)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		expected := []*v1.Pod{pods[2], pods[3], pods[0]}
0000000000000000000000000000000000000000;;		podsToSync := kubelet.getPodsToSync()
0000000000000000000000000000000000000000;;		sort.Sort(podsByUID(expected))
0000000000000000000000000000000000000000;;		sort.Sort(podsByUID(podsToSync))
0000000000000000000000000000000000000000;;		assert.Equal(t, expected, podsToSync)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestGenerateAPIPodStatusWithSortedContainers(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		numContainers := 10
0000000000000000000000000000000000000000;;		expectedOrder := []string{}
0000000000000000000000000000000000000000;;		cStatuses := []*kubecontainer.ContainerStatus{}
0000000000000000000000000000000000000000;;		specContainerList := []v1.Container{}
0000000000000000000000000000000000000000;;		for i := 0; i < numContainers; i++ {
0000000000000000000000000000000000000000;;			id := fmt.Sprintf("%v", i)
0000000000000000000000000000000000000000;;			containerName := fmt.Sprintf("%vcontainer", id)
0000000000000000000000000000000000000000;;			expectedOrder = append(expectedOrder, containerName)
0000000000000000000000000000000000000000;;			cStatus := &kubecontainer.ContainerStatus{
0000000000000000000000000000000000000000;;				ID:   kubecontainer.BuildContainerID("test", id),
0000000000000000000000000000000000000000;;				Name: containerName,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Rearrange container statuses
0000000000000000000000000000000000000000;;			if i%2 == 0 {
0000000000000000000000000000000000000000;;				cStatuses = append(cStatuses, cStatus)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				cStatuses = append([]*kubecontainer.ContainerStatus{cStatus}, cStatuses...)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			specContainerList = append(specContainerList, v1.Container{Name: containerName})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pod := podWithUidNameNs("uid1", "foo", "test")
0000000000000000000000000000000000000000;;		pod.Spec = v1.PodSpec{
0000000000000000000000000000000000000000;;			Containers: specContainerList,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		status := &kubecontainer.PodStatus{
0000000000000000000000000000000000000000;;			ID:                pod.UID,
0000000000000000000000000000000000000000;;			Name:              pod.Name,
0000000000000000000000000000000000000000;;			Namespace:         pod.Namespace,
0000000000000000000000000000000000000000;;			ContainerStatuses: cStatuses,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := 0; i < 5; i++ {
0000000000000000000000000000000000000000;;			apiStatus := kubelet.generateAPIPodStatus(pod, status)
0000000000000000000000000000000000000000;;			for i, c := range apiStatus.ContainerStatuses {
0000000000000000000000000000000000000000;;				if expectedOrder[i] != c.Name {
0000000000000000000000000000000000000000;;					t.Fatalf("Container status not sorted, expected %v at index %d, but found %v", expectedOrder[i], i, c.Name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func verifyContainerStatuses(t *testing.T, statuses []v1.ContainerStatus, state, lastTerminationState map[string]v1.ContainerState, message string) {
0000000000000000000000000000000000000000;;		for _, s := range statuses {
0000000000000000000000000000000000000000;;			assert.Equal(t, s.State, state[s.Name], "%s: state", message)
0000000000000000000000000000000000000000;;			assert.Equal(t, s.LastTerminationState, lastTerminationState[s.Name], "%s: last terminated state", message)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Test generateAPIPodStatus with different reason cache and old api pod status.
0000000000000000000000000000000000000000;;	func TestGenerateAPIPodStatusWithReasonCache(t *testing.T) {
0000000000000000000000000000000000000000;;		// The following waiting reason and message  are generated in convertStatusToAPIStatus()
0000000000000000000000000000000000000000;;		startWaitingReason := "ContainerCreating"
0000000000000000000000000000000000000000;;		initWaitingReason := "PodInitializing"
0000000000000000000000000000000000000000;;		testTimestamp := time.Unix(123456789, 987654321)
0000000000000000000000000000000000000000;;		testErrorReason := fmt.Errorf("test-error")
0000000000000000000000000000000000000000;;		emptyContainerID := (&kubecontainer.ContainerID{}).String()
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		pod := podWithUidNameNs("12345678", "foo", "new")
0000000000000000000000000000000000000000;;		pod.Spec = v1.PodSpec{RestartPolicy: v1.RestartPolicyOnFailure}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podStatus := &kubecontainer.PodStatus{
0000000000000000000000000000000000000000;;			ID:        pod.UID,
0000000000000000000000000000000000000000;;			Name:      pod.Name,
0000000000000000000000000000000000000000;;			Namespace: pod.Namespace,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		tests := []struct {
0000000000000000000000000000000000000000;;			containers    []v1.Container
0000000000000000000000000000000000000000;;			statuses      []*kubecontainer.ContainerStatus
0000000000000000000000000000000000000000;;			reasons       map[string]error
0000000000000000000000000000000000000000;;			oldStatuses   []v1.ContainerStatus
0000000000000000000000000000000000000000;;			expectedState map[string]v1.ContainerState
0000000000000000000000000000000000000000;;			// Only set expectedInitState when it is different from expectedState
0000000000000000000000000000000000000000;;			expectedInitState            map[string]v1.ContainerState
0000000000000000000000000000000000000000;;			expectedLastTerminationState map[string]v1.ContainerState
0000000000000000000000000000000000000000;;		}{
0000000000000000000000000000000000000000;;			// For container with no historical record, State should be Waiting, LastTerminationState should be retrieved from
0000000000000000000000000000000000000000;;			// old status from apiserver.
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				containers: []v1.Container{{Name: "without-old-record"}, {Name: "with-old-record"}},
0000000000000000000000000000000000000000;;				statuses:   []*kubecontainer.ContainerStatus{},
0000000000000000000000000000000000000000;;				reasons:    map[string]error{},
0000000000000000000000000000000000000000;;				oldStatuses: []v1.ContainerStatus{{
0000000000000000000000000000000000000000;;					Name:                 "with-old-record",
0000000000000000000000000000000000000000;;					LastTerminationState: v1.ContainerState{Terminated: &v1.ContainerStateTerminated{}},
0000000000000000000000000000000000000000;;				}},
0000000000000000000000000000000000000000;;				expectedState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"without-old-record": {Waiting: &v1.ContainerStateWaiting{
0000000000000000000000000000000000000000;;						Reason: startWaitingReason,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"with-old-record": {Waiting: &v1.ContainerStateWaiting{
0000000000000000000000000000000000000000;;						Reason: startWaitingReason,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				expectedInitState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"without-old-record": {Waiting: &v1.ContainerStateWaiting{
0000000000000000000000000000000000000000;;						Reason: initWaitingReason,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"with-old-record": {Waiting: &v1.ContainerStateWaiting{
0000000000000000000000000000000000000000;;						Reason: initWaitingReason,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				expectedLastTerminationState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"with-old-record": {Terminated: &v1.ContainerStateTerminated{}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			// For running container, State should be Running, LastTerminationState should be retrieved from latest terminated status.
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				containers: []v1.Container{{Name: "running"}},
0000000000000000000000000000000000000000;;				statuses: []*kubecontainer.ContainerStatus{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:      "running",
0000000000000000000000000000000000000000;;						State:     kubecontainer.ContainerStateRunning,
0000000000000000000000000000000000000000;;						StartedAt: testTimestamp,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:     "running",
0000000000000000000000000000000000000000;;						State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;						ExitCode: 1,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				reasons:     map[string]error{},
0000000000000000000000000000000000000000;;				oldStatuses: []v1.ContainerStatus{},
0000000000000000000000000000000000000000;;				expectedState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"running": {Running: &v1.ContainerStateRunning{
0000000000000000000000000000000000000000;;						StartedAt: metav1.NewTime(testTimestamp),
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				expectedLastTerminationState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"running": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    1,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			// For terminated container:
0000000000000000000000000000000000000000;;			// * If there is no recent start error record, State should be Terminated, LastTerminationState should be retrieved from
0000000000000000000000000000000000000000;;			// second latest terminated status;
0000000000000000000000000000000000000000;;			// * If there is recent start error record, State should be Waiting, LastTerminationState should be retrieved from latest
0000000000000000000000000000000000000000;;			// terminated status;
0000000000000000000000000000000000000000;;			// * If ExitCode = 0, restart policy is RestartPolicyOnFailure, the container shouldn't be restarted. No matter there is
0000000000000000000000000000000000000000;;			// recent start error or not, State should be Terminated, LastTerminationState should be retrieved from second latest
0000000000000000000000000000000000000000;;			// terminated status.
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				containers: []v1.Container{{Name: "without-reason"}, {Name: "with-reason"}},
0000000000000000000000000000000000000000;;				statuses: []*kubecontainer.ContainerStatus{
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:     "without-reason",
0000000000000000000000000000000000000000;;						State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;						ExitCode: 1,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:     "with-reason",
0000000000000000000000000000000000000000;;						State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;						ExitCode: 2,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:     "without-reason",
0000000000000000000000000000000000000000;;						State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;						ExitCode: 3,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:     "with-reason",
0000000000000000000000000000000000000000;;						State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;						ExitCode: 4,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:     "succeed",
0000000000000000000000000000000000000000;;						State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;						ExitCode: 0,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;					{
0000000000000000000000000000000000000000;;						Name:     "succeed",
0000000000000000000000000000000000000000;;						State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;						ExitCode: 5,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				reasons:     map[string]error{"with-reason": testErrorReason, "succeed": testErrorReason},
0000000000000000000000000000000000000000;;				oldStatuses: []v1.ContainerStatus{},
0000000000000000000000000000000000000000;;				expectedState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"without-reason": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    1,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"with-reason": {Waiting: &v1.ContainerStateWaiting{Reason: testErrorReason.Error()}},
0000000000000000000000000000000000000000;;					"succeed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    0,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				expectedLastTerminationState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"without-reason": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    3,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"with-reason": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    2,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"succeed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    5,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i, test := range tests {
0000000000000000000000000000000000000000;;			kubelet.reasonCache = NewReasonCache()
0000000000000000000000000000000000000000;;			for n, e := range test.reasons {
0000000000000000000000000000000000000000;;				kubelet.reasonCache.add(pod.UID, n, e, "")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pod.Spec.Containers = test.containers
0000000000000000000000000000000000000000;;			pod.Status.ContainerStatuses = test.oldStatuses
0000000000000000000000000000000000000000;;			podStatus.ContainerStatuses = test.statuses
0000000000000000000000000000000000000000;;			apiStatus := kubelet.generateAPIPodStatus(pod, podStatus)
0000000000000000000000000000000000000000;;			verifyContainerStatuses(t, apiStatus.ContainerStatuses, test.expectedState, test.expectedLastTerminationState, fmt.Sprintf("case %d", i))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Everything should be the same for init containers
0000000000000000000000000000000000000000;;		for i, test := range tests {
0000000000000000000000000000000000000000;;			kubelet.reasonCache = NewReasonCache()
0000000000000000000000000000000000000000;;			for n, e := range test.reasons {
0000000000000000000000000000000000000000;;				kubelet.reasonCache.add(pod.UID, n, e, "")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pod.Spec.InitContainers = test.containers
0000000000000000000000000000000000000000;;			pod.Status.InitContainerStatuses = test.oldStatuses
0000000000000000000000000000000000000000;;			podStatus.ContainerStatuses = test.statuses
0000000000000000000000000000000000000000;;			apiStatus := kubelet.generateAPIPodStatus(pod, podStatus)
0000000000000000000000000000000000000000;;			expectedState := test.expectedState
0000000000000000000000000000000000000000;;			if test.expectedInitState != nil {
0000000000000000000000000000000000000000;;				expectedState = test.expectedInitState
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			verifyContainerStatuses(t, apiStatus.InitContainerStatuses, expectedState, test.expectedLastTerminationState, fmt.Sprintf("case %d", i))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Test generateAPIPodStatus with different restart policies.
0000000000000000000000000000000000000000;;	func TestGenerateAPIPodStatusWithDifferentRestartPolicies(t *testing.T) {
0000000000000000000000000000000000000000;;		testErrorReason := fmt.Errorf("test-error")
0000000000000000000000000000000000000000;;		emptyContainerID := (&kubecontainer.ContainerID{}).String()
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("VersionInfo").Return(&cadvisorapi.VersionInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		pod := podWithUidNameNs("12345678", "foo", "new")
0000000000000000000000000000000000000000;;		containers := []v1.Container{{Name: "succeed"}, {Name: "failed"}}
0000000000000000000000000000000000000000;;		podStatus := &kubecontainer.PodStatus{
0000000000000000000000000000000000000000;;			ID:        pod.UID,
0000000000000000000000000000000000000000;;			Name:      pod.Name,
0000000000000000000000000000000000000000;;			Namespace: pod.Namespace,
0000000000000000000000000000000000000000;;			ContainerStatuses: []*kubecontainer.ContainerStatus{
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					Name:     "succeed",
0000000000000000000000000000000000000000;;					State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;					ExitCode: 0,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					Name:     "failed",
0000000000000000000000000000000000000000;;					State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;					ExitCode: 1,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					Name:     "succeed",
0000000000000000000000000000000000000000;;					State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;					ExitCode: 2,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				{
0000000000000000000000000000000000000000;;					Name:     "failed",
0000000000000000000000000000000000000000;;					State:    kubecontainer.ContainerStateExited,
0000000000000000000000000000000000000000;;					ExitCode: 3,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		kubelet.reasonCache.add(pod.UID, "succeed", testErrorReason, "")
0000000000000000000000000000000000000000;;		kubelet.reasonCache.add(pod.UID, "failed", testErrorReason, "")
0000000000000000000000000000000000000000;;		for c, test := range []struct {
0000000000000000000000000000000000000000;;			restartPolicy                v1.RestartPolicy
0000000000000000000000000000000000000000;;			expectedState                map[string]v1.ContainerState
0000000000000000000000000000000000000000;;			expectedLastTerminationState map[string]v1.ContainerState
0000000000000000000000000000000000000000;;			// Only set expectedInitState when it is different from expectedState
0000000000000000000000000000000000000000;;			expectedInitState map[string]v1.ContainerState
0000000000000000000000000000000000000000;;			// Only set expectedInitLastTerminationState when it is different from expectedLastTerminationState
0000000000000000000000000000000000000000;;			expectedInitLastTerminationState map[string]v1.ContainerState
0000000000000000000000000000000000000000;;		}{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				restartPolicy: v1.RestartPolicyNever,
0000000000000000000000000000000000000000;;				expectedState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"succeed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    0,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"failed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    1,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				expectedLastTerminationState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"succeed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    2,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"failed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    3,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				restartPolicy: v1.RestartPolicyOnFailure,
0000000000000000000000000000000000000000;;				expectedState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"succeed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    0,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"failed": {Waiting: &v1.ContainerStateWaiting{Reason: testErrorReason.Error()}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				expectedLastTerminationState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"succeed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    2,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"failed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    1,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				restartPolicy: v1.RestartPolicyAlways,
0000000000000000000000000000000000000000;;				expectedState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"succeed": {Waiting: &v1.ContainerStateWaiting{Reason: testErrorReason.Error()}},
0000000000000000000000000000000000000000;;					"failed":  {Waiting: &v1.ContainerStateWaiting{Reason: testErrorReason.Error()}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				expectedLastTerminationState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"succeed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    0,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"failed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    1,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				// If the init container is terminated with exit code 0, it won't be restarted even when the
0000000000000000000000000000000000000000;;				// restart policy is RestartAlways.
0000000000000000000000000000000000000000;;				expectedInitState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"succeed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    0,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"failed": {Waiting: &v1.ContainerStateWaiting{Reason: testErrorReason.Error()}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				expectedInitLastTerminationState: map[string]v1.ContainerState{
0000000000000000000000000000000000000000;;					"succeed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    2,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;					"failed": {Terminated: &v1.ContainerStateTerminated{
0000000000000000000000000000000000000000;;						ExitCode:    1,
0000000000000000000000000000000000000000;;						ContainerID: emptyContainerID,
0000000000000000000000000000000000000000;;					}},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		} {
0000000000000000000000000000000000000000;;			pod.Spec.RestartPolicy = test.restartPolicy
0000000000000000000000000000000000000000;;			// Test normal containers
0000000000000000000000000000000000000000;;			pod.Spec.Containers = containers
0000000000000000000000000000000000000000;;			apiStatus := kubelet.generateAPIPodStatus(pod, podStatus)
0000000000000000000000000000000000000000;;			expectedState, expectedLastTerminationState := test.expectedState, test.expectedLastTerminationState
0000000000000000000000000000000000000000;;			verifyContainerStatuses(t, apiStatus.ContainerStatuses, expectedState, expectedLastTerminationState, fmt.Sprintf("case %d", c))
0000000000000000000000000000000000000000;;			pod.Spec.Containers = nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Test init containers
0000000000000000000000000000000000000000;;			pod.Spec.InitContainers = containers
0000000000000000000000000000000000000000;;			apiStatus = kubelet.generateAPIPodStatus(pod, podStatus)
0000000000000000000000000000000000000000;;			if test.expectedInitState != nil {
0000000000000000000000000000000000000000;;				expectedState = test.expectedInitState
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if test.expectedInitLastTerminationState != nil {
0000000000000000000000000000000000000000;;				expectedLastTerminationState = test.expectedInitLastTerminationState
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			verifyContainerStatuses(t, apiStatus.InitContainerStatuses, expectedState, expectedLastTerminationState, fmt.Sprintf("case %d", c))
0000000000000000000000000000000000000000;;			pod.Spec.InitContainers = nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// testPodAdmitHandler is a lifecycle.PodAdmitHandler for testing.
0000000000000000000000000000000000000000;;	type testPodAdmitHandler struct {
0000000000000000000000000000000000000000;;		// list of pods to reject.
0000000000000000000000000000000000000000;;		podsToReject []*v1.Pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Admit rejects all pods in the podsToReject list with a matching UID.
0000000000000000000000000000000000000000;;	func (a *testPodAdmitHandler) Admit(attrs *lifecycle.PodAdmitAttributes) lifecycle.PodAdmitResult {
0000000000000000000000000000000000000000;;		for _, podToReject := range a.podsToReject {
0000000000000000000000000000000000000000;;			if podToReject.UID == attrs.Pod.UID {
0000000000000000000000000000000000000000;;				return lifecycle.PodAdmitResult{Admit: false, Reason: "Rejected", Message: "Pod is rejected"}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return lifecycle.PodAdmitResult{Admit: true}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Test verifies that the kubelet invokes an admission handler during HandlePodAdditions.
0000000000000000000000000000000000000000;;	func TestHandlePodAdditionsInvokesPodAdmitHandlers(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		kl.nodeInfo = testNodeInfo{nodes: []*v1.Node{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{Name: string(kl.nodeName)},
0000000000000000000000000000000000000000;;				Status: v1.NodeStatus{
0000000000000000000000000000000000000000;;					Allocatable: v1.ResourceList{
0000000000000000000000000000000000000000;;						v1.ResourcePods: *resource.NewQuantity(110, resource.DecimalSI),
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}}
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("MachineInfo").Return(&cadvisorapi.MachineInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("ImagesFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;		testKubelet.fakeCadvisor.On("RootFsInfo").Return(cadvisorapiv2.FsInfo{}, nil)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					UID:       "123456789",
0000000000000000000000000000000000000000;;					Name:      "podA",
0000000000000000000000000000000000000000;;					Namespace: "foo",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			{
0000000000000000000000000000000000000000;;				ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;					UID:       "987654321",
0000000000000000000000000000000000000000;;					Name:      "podB",
0000000000000000000000000000000000000000;;					Namespace: "foo",
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		podToReject := pods[0]
0000000000000000000000000000000000000000;;		podToAdmit := pods[1]
0000000000000000000000000000000000000000;;		podsToReject := []*v1.Pod{podToReject}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.admitHandlers.AddPodAdmitHandler(&testPodAdmitHandler{podsToReject: podsToReject})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kl.HandlePodAdditions(pods)
0000000000000000000000000000000000000000;;		// Check pod status stored in the status map.
0000000000000000000000000000000000000000;;		// podToReject should be Failed
0000000000000000000000000000000000000000;;		status, found := kl.statusManager.GetPodStatus(podToReject.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", podToAdmit.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodFailed, status.Phase)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// podToAdmit should be Pending
0000000000000000000000000000000000000000;;		status, found = kl.statusManager.GetPodStatus(podToAdmit.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", podToAdmit.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodPending, status.Phase)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// testPodSyncLoopHandler is a lifecycle.PodSyncLoopHandler that is used for testing.
0000000000000000000000000000000000000000;;	type testPodSyncLoopHandler struct {
0000000000000000000000000000000000000000;;		// list of pods to sync
0000000000000000000000000000000000000000;;		podsToSync []*v1.Pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ShouldSync evaluates if the pod should be synced from the kubelet.
0000000000000000000000000000000000000000;;	func (a *testPodSyncLoopHandler) ShouldSync(pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		for _, podToSync := range a.podsToSync {
0000000000000000000000000000000000000000;;			if podToSync.UID == pod.UID {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestGetPodsToSyncInvokesPodSyncLoopHandlers ensures that the get pods to sync routine invokes the handler.
0000000000000000000000000000000000000000;;	func TestGetPodsToSyncInvokesPodSyncLoopHandlers(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		pods := newTestPods(5)
0000000000000000000000000000000000000000;;		expected := []*v1.Pod{pods[0]}
0000000000000000000000000000000000000000;;		kubelet.AddPodSyncLoopHandler(&testPodSyncLoopHandler{expected})
0000000000000000000000000000000000000000;;		kubelet.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podsToSync := kubelet.getPodsToSync()
0000000000000000000000000000000000000000;;		sort.Sort(podsByUID(expected))
0000000000000000000000000000000000000000;;		sort.Sort(podsByUID(podsToSync))
0000000000000000000000000000000000000000;;		assert.Equal(t, expected, podsToSync)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// testPodSyncHandler is a lifecycle.PodSyncHandler that is used for testing.
0000000000000000000000000000000000000000;;	type testPodSyncHandler struct {
0000000000000000000000000000000000000000;;		// list of pods to evict.
0000000000000000000000000000000000000000;;		podsToEvict []*v1.Pod
0000000000000000000000000000000000000000;;		// the reason for the eviction
0000000000000000000000000000000000000000;;		reason string
0000000000000000000000000000000000000000;;		// the message for the eviction
0000000000000000000000000000000000000000;;		message string
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ShouldEvict evaluates if the pod should be evicted from the kubelet.
0000000000000000000000000000000000000000;;	func (a *testPodSyncHandler) ShouldEvict(pod *v1.Pod) lifecycle.ShouldEvictResponse {
0000000000000000000000000000000000000000;;		for _, podToEvict := range a.podsToEvict {
0000000000000000000000000000000000000000;;			if podToEvict.UID == pod.UID {
0000000000000000000000000000000000000000;;				return lifecycle.ShouldEvictResponse{Evict: true, Reason: a.reason, Message: a.message}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return lifecycle.ShouldEvictResponse{Evict: false}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TestGenerateAPIPodStatusInvokesPodSyncHandlers invokes the handlers and reports the proper status
0000000000000000000000000000000000000000;;	func TestGenerateAPIPodStatusInvokesPodSyncHandlers(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kubelet := testKubelet.kubelet
0000000000000000000000000000000000000000;;		pod := newTestPods(1)[0]
0000000000000000000000000000000000000000;;		podsToEvict := []*v1.Pod{pod}
0000000000000000000000000000000000000000;;		kubelet.AddPodSyncHandler(&testPodSyncHandler{podsToEvict, "Evicted", "because"})
0000000000000000000000000000000000000000;;		status := &kubecontainer.PodStatus{
0000000000000000000000000000000000000000;;			ID:        pod.UID,
0000000000000000000000000000000000000000;;			Name:      pod.Name,
0000000000000000000000000000000000000000;;			Namespace: pod.Namespace,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		apiStatus := kubelet.generateAPIPodStatus(pod, status)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodFailed, apiStatus.Phase)
0000000000000000000000000000000000000000;;		require.Equal(t, "Evicted", apiStatus.Reason)
0000000000000000000000000000000000000000;;		require.Equal(t, "because", apiStatus.Message)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func TestSyncPodKillPod(t *testing.T) {
0000000000000000000000000000000000000000;;		testKubelet := newTestKubelet(t, false /* controllerAttachDetachEnabled */)
0000000000000000000000000000000000000000;;		defer testKubelet.Cleanup()
0000000000000000000000000000000000000000;;		kl := testKubelet.kubelet
0000000000000000000000000000000000000000;;		pod := &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				UID:       "12345678",
0000000000000000000000000000000000000000;;				Name:      "bar",
0000000000000000000000000000000000000000;;				Namespace: "foo",
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		pods := []*v1.Pod{pod}
0000000000000000000000000000000000000000;;		kl.podManager.SetPods(pods)
0000000000000000000000000000000000000000;;		gracePeriodOverride := int64(0)
0000000000000000000000000000000000000000;;		err := kl.syncPod(syncPodOptions{
0000000000000000000000000000000000000000;;			pod:        pod,
0000000000000000000000000000000000000000;;			podStatus:  &kubecontainer.PodStatus{},
0000000000000000000000000000000000000000;;			updateType: kubetypes.SyncPodKill,
0000000000000000000000000000000000000000;;			killPodOptions: &KillPodOptions{
0000000000000000000000000000000000000000;;				PodStatusFunc: func(p *v1.Pod, podStatus *kubecontainer.PodStatus) v1.PodStatus {
0000000000000000000000000000000000000000;;					return v1.PodStatus{
0000000000000000000000000000000000000000;;						Phase:   v1.PodFailed,
0000000000000000000000000000000000000000;;						Reason:  "reason",
0000000000000000000000000000000000000000;;						Message: "message",
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;				PodTerminationGracePeriodSecondsOverride: &gracePeriodOverride,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		require.NoError(t, err)
0000000000000000000000000000000000000000;;		// Check pod status stored in the status map.
0000000000000000000000000000000000000000;;		status, found := kl.statusManager.GetPodStatus(pod.UID)
0000000000000000000000000000000000000000;;		require.True(t, found, "Status of pod %q is not found in the status map", pod.UID)
0000000000000000000000000000000000000000;;		require.Equal(t, v1.PodFailed, status.Phase)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func waitForVolumeUnmount(
0000000000000000000000000000000000000000;;		volumeManager kubeletvolume.VolumeManager,
0000000000000000000000000000000000000000;;		pod *v1.Pod) error {
0000000000000000000000000000000000000000;;		var podVolumes kubecontainer.VolumeMap
0000000000000000000000000000000000000000;;		err := retryWithExponentialBackOff(
0000000000000000000000000000000000000000;;			time.Duration(50*time.Millisecond),
0000000000000000000000000000000000000000;;			func() (bool, error) {
0000000000000000000000000000000000000000;;				// Verify volumes detached
0000000000000000000000000000000000000000;;				podVolumes = volumeManager.GetMountedVolumesForPod(
0000000000000000000000000000000000000000;;					volumehelper.GetUniquePodName(pod))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				if len(podVolumes) != 0 {
0000000000000000000000000000000000000000;;					return false, nil
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf(
0000000000000000000000000000000000000000;;				"Expected volumes to be unmounted. But some volumes are still mounted: %#v", podVolumes)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func waitForVolumeDetach(
0000000000000000000000000000000000000000;;		volumeName v1.UniqueVolumeName,
0000000000000000000000000000000000000000;;		volumeManager kubeletvolume.VolumeManager) error {
0000000000000000000000000000000000000000;;		attachedVolumes := []v1.UniqueVolumeName{}
0000000000000000000000000000000000000000;;		err := retryWithExponentialBackOff(
0000000000000000000000000000000000000000;;			time.Duration(50*time.Millisecond),
0000000000000000000000000000000000000000;;			func() (bool, error) {
0000000000000000000000000000000000000000;;				// Verify volumes detached
0000000000000000000000000000000000000000;;				volumeAttached := volumeManager.VolumeIsAttached(volumeName)
0000000000000000000000000000000000000000;;				return !volumeAttached, nil
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf(
0000000000000000000000000000000000000000;;				"Expected volumes to be detached. But some volumes are still attached: %#v", attachedVolumes)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func retryWithExponentialBackOff(initialDuration time.Duration, fn wait.ConditionFunc) error {
0000000000000000000000000000000000000000;;		backoff := wait.Backoff{
0000000000000000000000000000000000000000;;			Duration: initialDuration,
0000000000000000000000000000000000000000;;			Factor:   3,
0000000000000000000000000000000000000000;;			Jitter:   0,
0000000000000000000000000000000000000000;;			Steps:    6,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return wait.ExponentialBackoff(backoff, fn)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func simulateVolumeInUseUpdate(
0000000000000000000000000000000000000000;;		volumeName v1.UniqueVolumeName,
0000000000000000000000000000000000000000;;		stopCh <-chan struct{},
0000000000000000000000000000000000000000;;		volumeManager kubeletvolume.VolumeManager) {
0000000000000000000000000000000000000000;;		ticker := time.NewTicker(100 * time.Millisecond)
0000000000000000000000000000000000000000;;		defer ticker.Stop()
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case <-ticker.C:
0000000000000000000000000000000000000000;;				volumeManager.MarkVolumesAsReportedInUse(
0000000000000000000000000000000000000000;;					[]v1.UniqueVolumeName{volumeName})
0000000000000000000000000000000000000000;;			case <-stopCh:
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func runVolumeManager(kubelet *Kubelet) chan struct{} {
0000000000000000000000000000000000000000;;		stopCh := make(chan struct{})
0000000000000000000000000000000000000000;;		go kubelet.volumeManager.Run(kubelet.sourcesReady, stopCh)
0000000000000000000000000000000000000000;;		return stopCh
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Sort pods by UID.
0000000000000000000000000000000000000000;;	type podsByUID []*v1.Pod
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (p podsByUID) Len() int           { return len(p) }
0000000000000000000000000000000000000000;;	func (p podsByUID) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }
0000000000000000000000000000000000000000;;	func (p podsByUID) Less(i, j int) bool { return p[i].UID < p[j].UID }
