0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2017 The Kubernetes Authors.
250bf1924309b22e0287c59dfc7ed32d6dd07ceb;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package cm
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"path"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		v1qos "k8s.io/kubernetes/pkg/api/v1/helper/qos"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api/v1/resource"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// how often the qos cgroup manager will perform periodic update
0000000000000000000000000000000000000000;;		// of the qos level cgroup resource constraints
0000000000000000000000000000000000000000;;		periodicQOSCgroupUpdateInterval = 1 * time.Minute
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type QOSContainerManager interface {
0000000000000000000000000000000000000000;;		Start(func() v1.ResourceList, ActivePodsFunc) error
0000000000000000000000000000000000000000;;		GetQOSContainersInfo() QOSContainersInfo
0000000000000000000000000000000000000000;;		UpdateCgroups() error
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type qosContainerManagerImpl struct {
0000000000000000000000000000000000000000;;		sync.Mutex
0000000000000000000000000000000000000000;;		nodeInfo           *v1.Node
0000000000000000000000000000000000000000;;		qosContainersInfo  QOSContainersInfo
0000000000000000000000000000000000000000;;		subsystems         *CgroupSubsystems
0000000000000000000000000000000000000000;;		cgroupManager      CgroupManager
0000000000000000000000000000000000000000;;		activePods         ActivePodsFunc
0000000000000000000000000000000000000000;;		getNodeAllocatable func() v1.ResourceList
0000000000000000000000000000000000000000;;		cgroupRoot         string
0000000000000000000000000000000000000000;;		qosReserved        map[v1.ResourceName]int64
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewQOSContainerManager(subsystems *CgroupSubsystems, cgroupRoot string, nodeConfig NodeConfig) (QOSContainerManager, error) {
0000000000000000000000000000000000000000;;		if !nodeConfig.CgroupsPerQOS {
0000000000000000000000000000000000000000;;			return &qosContainerManagerNoop{
0000000000000000000000000000000000000000;;				cgroupRoot: CgroupName(nodeConfig.CgroupRoot),
0000000000000000000000000000000000000000;;			}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return &qosContainerManagerImpl{
0000000000000000000000000000000000000000;;			subsystems:    subsystems,
0000000000000000000000000000000000000000;;			cgroupManager: NewCgroupManager(subsystems, nodeConfig.CgroupDriver),
0000000000000000000000000000000000000000;;			cgroupRoot:    cgroupRoot,
0000000000000000000000000000000000000000;;			qosReserved:   nodeConfig.ExperimentalQOSReserved,
0000000000000000000000000000000000000000;;		}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *qosContainerManagerImpl) GetQOSContainersInfo() QOSContainersInfo {
0000000000000000000000000000000000000000;;		return m.qosContainersInfo
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *qosContainerManagerImpl) Start(getNodeAllocatable func() v1.ResourceList, activePods ActivePodsFunc) error {
0000000000000000000000000000000000000000;;		cm := m.cgroupManager
0000000000000000000000000000000000000000;;		rootContainer := m.cgroupRoot
0000000000000000000000000000000000000000;;		if !cm.Exists(CgroupName(rootContainer)) {
0000000000000000000000000000000000000000;;			return fmt.Errorf("root container %s doesn't exist", rootContainer)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Top level for Qos containers are created only for Burstable
0000000000000000000000000000000000000000;;		// and Best Effort classes
0000000000000000000000000000000000000000;;		qosClasses := map[v1.PodQOSClass]string{
0000000000000000000000000000000000000000;;			v1.PodQOSBurstable:  path.Join(rootContainer, strings.ToLower(string(v1.PodQOSBurstable))),
0000000000000000000000000000000000000000;;			v1.PodQOSBestEffort: path.Join(rootContainer, strings.ToLower(string(v1.PodQOSBestEffort))),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Create containers for both qos classes
0000000000000000000000000000000000000000;;		for qosClass, containerName := range qosClasses {
0000000000000000000000000000000000000000;;			// get the container's absolute name
0000000000000000000000000000000000000000;;			absoluteContainerName := CgroupName(containerName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			resourceParameters := &ResourceConfig{}
0000000000000000000000000000000000000000;;			// the BestEffort QoS class has a statically configured minShares value
0000000000000000000000000000000000000000;;			if qosClass == v1.PodQOSBestEffort {
0000000000000000000000000000000000000000;;				minShares := int64(MinShares)
0000000000000000000000000000000000000000;;				resourceParameters.CpuShares = &minShares
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// containerConfig object stores the cgroup specifications
0000000000000000000000000000000000000000;;			containerConfig := &CgroupConfig{
0000000000000000000000000000000000000000;;				Name:               absoluteContainerName,
0000000000000000000000000000000000000000;;				ResourceParameters: resourceParameters,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// check if it exists
0000000000000000000000000000000000000000;;			if !cm.Exists(absoluteContainerName) {
0000000000000000000000000000000000000000;;				if err := cm.Create(containerConfig); err != nil {
0000000000000000000000000000000000000000;;					return fmt.Errorf("failed to create top level %v QOS cgroup : %v", qosClass, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				// to ensure we actually have the right state, we update the config on startup
0000000000000000000000000000000000000000;;				if err := cm.Update(containerConfig); err != nil {
0000000000000000000000000000000000000000;;					return fmt.Errorf("failed to update top level %v QOS cgroup : %v", qosClass, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Store the top level qos container names
0000000000000000000000000000000000000000;;		m.qosContainersInfo = QOSContainersInfo{
0000000000000000000000000000000000000000;;			Guaranteed: rootContainer,
0000000000000000000000000000000000000000;;			Burstable:  qosClasses[v1.PodQOSBurstable],
0000000000000000000000000000000000000000;;			BestEffort: qosClasses[v1.PodQOSBestEffort],
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		m.getNodeAllocatable = getNodeAllocatable
0000000000000000000000000000000000000000;;		m.activePods = activePods
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// update qos cgroup tiers on startup and in periodic intervals
0000000000000000000000000000000000000000;;		// to ensure desired state is in synch with actual state.
0000000000000000000000000000000000000000;;		go wait.Until(func() {
0000000000000000000000000000000000000000;;			err := m.UpdateCgroups()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Warningf("[ContainerManager] Failed to reserve QoS requests: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}, periodicQOSCgroupUpdateInterval, wait.NeverStop)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *qosContainerManagerImpl) setCPUCgroupConfig(configs map[v1.PodQOSClass]*CgroupConfig) error {
0000000000000000000000000000000000000000;;		pods := m.activePods()
0000000000000000000000000000000000000000;;		burstablePodCPURequest := int64(0)
0000000000000000000000000000000000000000;;		for i := range pods {
0000000000000000000000000000000000000000;;			pod := pods[i]
0000000000000000000000000000000000000000;;			qosClass := v1qos.GetPodQOS(pod)
0000000000000000000000000000000000000000;;			if qosClass != v1.PodQOSBurstable {
0000000000000000000000000000000000000000;;				// we only care about the burstable qos tier
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			req, _ := resource.PodRequestsAndLimits(pod)
0000000000000000000000000000000000000000;;			if request, found := req[v1.ResourceCPU]; found {
0000000000000000000000000000000000000000;;				burstablePodCPURequest += request.MilliValue()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// make sure best effort is always 2 shares
0000000000000000000000000000000000000000;;		bestEffortCPUShares := int64(MinShares)
0000000000000000000000000000000000000000;;		configs[v1.PodQOSBestEffort].ResourceParameters.CpuShares = &bestEffortCPUShares
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// set burstable shares based on current observe state
0000000000000000000000000000000000000000;;		burstableCPUShares := MilliCPUToShares(burstablePodCPURequest)
0000000000000000000000000000000000000000;;		if burstableCPUShares < int64(MinShares) {
0000000000000000000000000000000000000000;;			burstableCPUShares = int64(MinShares)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		configs[v1.PodQOSBurstable].ResourceParameters.CpuShares = &burstableCPUShares
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// setMemoryReserve sums the memory limits of all pods in a QOS class,
0000000000000000000000000000000000000000;;	// calculates QOS class memory limits, and set those limits in the
0000000000000000000000000000000000000000;;	// CgroupConfig for each QOS class.
0000000000000000000000000000000000000000;;	func (m *qosContainerManagerImpl) setMemoryReserve(configs map[v1.PodQOSClass]*CgroupConfig, percentReserve int64) {
0000000000000000000000000000000000000000;;		qosMemoryRequests := map[v1.PodQOSClass]int64{
0000000000000000000000000000000000000000;;			v1.PodQOSGuaranteed: 0,
0000000000000000000000000000000000000000;;			v1.PodQOSBurstable:  0,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Sum the pod limits for pods in each QOS class
0000000000000000000000000000000000000000;;		pods := m.activePods()
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			podMemoryRequest := int64(0)
0000000000000000000000000000000000000000;;			qosClass := v1qos.GetPodQOS(pod)
0000000000000000000000000000000000000000;;			if qosClass == v1.PodQOSBestEffort {
0000000000000000000000000000000000000000;;				// limits are not set for Best Effort pods
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			req, _ := resource.PodRequestsAndLimits(pod)
0000000000000000000000000000000000000000;;			if request, found := req[v1.ResourceMemory]; found {
0000000000000000000000000000000000000000;;				podMemoryRequest += request.Value()
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			qosMemoryRequests[qosClass] += podMemoryRequest
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		resources := m.getNodeAllocatable()
0000000000000000000000000000000000000000;;		allocatableResource, ok := resources[v1.ResourceMemory]
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("[Container Manager] Allocatable memory value could not be determined.  Not setting QOS memory limts.")
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		allocatable := allocatableResource.Value()
0000000000000000000000000000000000000000;;		if allocatable == 0 {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("[Container Manager] Memory allocatable reported as 0, might be in standalone mode.  Not setting QOS memory limts.")
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for qos, limits := range qosMemoryRequests {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("[Container Manager] %s pod requests total %d bytes (reserve %d%%)", qos, limits, percentReserve)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Calculate QOS memory limits
0000000000000000000000000000000000000000;;		burstableLimit := allocatable - (qosMemoryRequests[v1.PodQOSGuaranteed] * percentReserve / 100)
0000000000000000000000000000000000000000;;		bestEffortLimit := burstableLimit - (qosMemoryRequests[v1.PodQOSBurstable] * percentReserve / 100)
0000000000000000000000000000000000000000;;		configs[v1.PodQOSBurstable].ResourceParameters.Memory = &burstableLimit
0000000000000000000000000000000000000000;;		configs[v1.PodQOSBestEffort].ResourceParameters.Memory = &bestEffortLimit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// retrySetMemoryReserve checks for any QoS cgroups over the limit
0000000000000000000000000000000000000000;;	// that was attempted to be set in the first Update() and adjusts
0000000000000000000000000000000000000000;;	// their memory limit to the usage to prevent further growth.
0000000000000000000000000000000000000000;;	func (m *qosContainerManagerImpl) retrySetMemoryReserve(configs map[v1.PodQOSClass]*CgroupConfig, percentReserve int64) {
0000000000000000000000000000000000000000;;		// Unreclaimable memory usage may already exceeded the desired limit
0000000000000000000000000000000000000000;;		// Attempt to set the limit near the current usage to put pressure
0000000000000000000000000000000000000000;;		// on the cgroup and prevent further growth.
0000000000000000000000000000000000000000;;		for qos, config := range configs {
0000000000000000000000000000000000000000;;			stats, err := m.cgroupManager.GetResourceStats(config.Name)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.V(2).Infof("[Container Manager] %v", err)
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			usage := stats.MemoryStats.Usage
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Because there is no good way to determine of the original Update()
0000000000000000000000000000000000000000;;			// on the memory resource was successful, we determine failure of the
0000000000000000000000000000000000000000;;			// first attempt by checking if the usage is above the limit we attempt
0000000000000000000000000000000000000000;;			// to set.  If it is, we assume the first attempt to set the limit failed
0000000000000000000000000000000000000000;;			// and try again setting the limit to the usage.  Otherwise we leave
0000000000000000000000000000000000000000;;			// the CgroupConfig as is.
0000000000000000000000000000000000000000;;			if configs[qos].ResourceParameters.Memory != nil && usage > *configs[qos].ResourceParameters.Memory {
0000000000000000000000000000000000000000;;				configs[qos].ResourceParameters.Memory = &usage
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *qosContainerManagerImpl) UpdateCgroups() error {
0000000000000000000000000000000000000000;;		m.Lock()
0000000000000000000000000000000000000000;;		defer m.Unlock()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		qosConfigs := map[v1.PodQOSClass]*CgroupConfig{
0000000000000000000000000000000000000000;;			v1.PodQOSBurstable: {
0000000000000000000000000000000000000000;;				Name:               CgroupName(m.qosContainersInfo.Burstable),
0000000000000000000000000000000000000000;;				ResourceParameters: &ResourceConfig{},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			v1.PodQOSBestEffort: {
0000000000000000000000000000000000000000;;				Name:               CgroupName(m.qosContainersInfo.BestEffort),
0000000000000000000000000000000000000000;;				ResourceParameters: &ResourceConfig{},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// update the qos level cgroup settings for cpu shares
0000000000000000000000000000000000000000;;		if err := m.setCPUCgroupConfig(qosConfigs); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for resource, percentReserve := range m.qosReserved {
0000000000000000000000000000000000000000;;			switch resource {
0000000000000000000000000000000000000000;;			case v1.ResourceMemory:
0000000000000000000000000000000000000000;;				m.setMemoryReserve(qosConfigs, percentReserve)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		updateSuccess := true
0000000000000000000000000000000000000000;;		for _, config := range qosConfigs {
0000000000000000000000000000000000000000;;			err := m.cgroupManager.Update(config)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				updateSuccess = false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if updateSuccess {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("[ContainerManager]: Updated QoS cgroup configuration")
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If the resource can adjust the ResourceConfig to increase likelihood of
0000000000000000000000000000000000000000;;		// success, call the adjustment function here.  Otherwise, the Update() will
0000000000000000000000000000000000000000;;		// be called again with the same values.
0000000000000000000000000000000000000000;;		for resource, percentReserve := range m.qosReserved {
0000000000000000000000000000000000000000;;			switch resource {
0000000000000000000000000000000000000000;;			case v1.ResourceMemory:
0000000000000000000000000000000000000000;;				m.retrySetMemoryReserve(qosConfigs, percentReserve)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, config := range qosConfigs {
0000000000000000000000000000000000000000;;			err := m.cgroupManager.Update(config)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.V(2).Infof("[ContainerManager]: Failed to update QoS cgroup configuration")
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(2).Infof("[ContainerManager]: Updated QoS cgroup configuration on retry")
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type qosContainerManagerNoop struct {
0000000000000000000000000000000000000000;;		cgroupRoot CgroupName
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ QOSContainerManager = &qosContainerManagerNoop{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *qosContainerManagerNoop) GetQOSContainersInfo() QOSContainersInfo {
0000000000000000000000000000000000000000;;		return QOSContainersInfo{}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *qosContainerManagerNoop) Start(_ func() v1.ResourceList, _ ActivePodsFunc) error {
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *qosContainerManagerNoop) UpdateCgroups() error {
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
