0000000000000000000000000000000000000000;;	// +build linux
9eada883bc684ef2f9ffbc58f6c5fe04f070be99;pkg/kubelet/container_manager_linux.go[pkg/kubelet/container_manager_linux.go][pkg/kubelet/cm/container_manager_linux.go];	
0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package cm
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"bufio"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"io/ioutil"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"os/exec"
0000000000000000000000000000000000000000;;		"path"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		cadvisorapiv2 "github.com/google/cadvisor/info/v2"
0000000000000000000000000000000000000000;;		"github.com/opencontainers/runc/libcontainer/cgroups"
0000000000000000000000000000000000000000;;		"github.com/opencontainers/runc/libcontainer/cgroups/fs"
0000000000000000000000000000000000000000;;		"github.com/opencontainers/runc/libcontainer/configs"
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		utilerrors "k8s.io/apimachinery/pkg/util/errors"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/cadvisor"
0000000000000000000000000000000000000000;;		cmutil "k8s.io/kubernetes/pkg/kubelet/cm/util"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/qos"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/mount"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/oom"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/procfs"
0000000000000000000000000000000000000000;;		utilsysctl "k8s.io/kubernetes/pkg/util/sysctl"
0000000000000000000000000000000000000000;;		utilversion "k8s.io/kubernetes/pkg/util/version"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// The percent of the machine memory capacity. The value is used to calculate
0000000000000000000000000000000000000000;;		// docker memory resource container's hardlimit to workaround docker memory
0000000000000000000000000000000000000000;;		// leakage issue. Please see kubernetes/issues/9881 for more detail.
0000000000000000000000000000000000000000;;		DockerMemoryLimitThresholdPercent = 70
0000000000000000000000000000000000000000;;		// The minimum memory limit allocated to docker container: 150Mi
0000000000000000000000000000000000000000;;		MinDockerMemoryLimit = 150 * 1024 * 1024
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		dockerProcessName     = "docker"
0000000000000000000000000000000000000000;;		dockerPidFile         = "/var/run/docker.pid"
0000000000000000000000000000000000000000;;		containerdProcessName = "docker-containerd"
0000000000000000000000000000000000000000;;		containerdPidFile     = "/run/docker/libcontainerd/docker-containerd.pid"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var (
0000000000000000000000000000000000000000;;		// The docker version in which containerd was introduced.
0000000000000000000000000000000000000000;;		containerdAPIVersion = utilversion.MustParseGeneric("1.23")
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// A non-user container tracked by the Kubelet.
0000000000000000000000000000000000000000;;	type systemContainer struct {
0000000000000000000000000000000000000000;;		// Absolute name of the container.
0000000000000000000000000000000000000000;;		name string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// CPU limit in millicores.
0000000000000000000000000000000000000000;;		cpuMillicores int64
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Function that ensures the state of the container.
0000000000000000000000000000000000000000;;		// m is the cgroup manager for the specified container.
0000000000000000000000000000000000000000;;		ensureStateFunc func(m *fs.Manager) error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Manager for the cgroups of the external container.
0000000000000000000000000000000000000000;;		manager *fs.Manager
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newSystemCgroups(containerName string) *systemContainer {
0000000000000000000000000000000000000000;;		return &systemContainer{
0000000000000000000000000000000000000000;;			name:    containerName,
0000000000000000000000000000000000000000;;			manager: createManager(containerName),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type containerManagerImpl struct {
0000000000000000000000000000000000000000;;		sync.RWMutex
0000000000000000000000000000000000000000;;		cadvisorInterface cadvisor.Interface
0000000000000000000000000000000000000000;;		mountUtil         mount.Interface
0000000000000000000000000000000000000000;;		NodeConfig
0000000000000000000000000000000000000000;;		status Status
0000000000000000000000000000000000000000;;		// External containers being managed.
0000000000000000000000000000000000000000;;		systemContainers []*systemContainer
0000000000000000000000000000000000000000;;		qosContainers    QOSContainersInfo
0000000000000000000000000000000000000000;;		// Tasks that are run periodically
0000000000000000000000000000000000000000;;		periodicTasks []func()
0000000000000000000000000000000000000000;;		// holds all the mounted cgroup subsystems
0000000000000000000000000000000000000000;;		subsystems *CgroupSubsystems
0000000000000000000000000000000000000000;;		nodeInfo   *v1.Node
0000000000000000000000000000000000000000;;		// Interface for cgroup management
0000000000000000000000000000000000000000;;		cgroupManager CgroupManager
0000000000000000000000000000000000000000;;		// Capacity of this node.
0000000000000000000000000000000000000000;;		capacity v1.ResourceList
0000000000000000000000000000000000000000;;		// Absolute cgroupfs path to a cgroup that Kubelet needs to place all pods under.
0000000000000000000000000000000000000000;;		// This path include a top level container for enforcing Node Allocatable.
0000000000000000000000000000000000000000;;		cgroupRoot string
0000000000000000000000000000000000000000;;		// Event recorder interface.
0000000000000000000000000000000000000000;;		recorder record.EventRecorder
0000000000000000000000000000000000000000;;		// Interface for QoS cgroup management
0000000000000000000000000000000000000000;;		qosContainerManager QOSContainerManager
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type features struct {
0000000000000000000000000000000000000000;;		cpuHardcapping bool
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var _ ContainerManager = &containerManagerImpl{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// checks if the required cgroups subsystems are mounted.
0000000000000000000000000000000000000000;;	// As of now, only 'cpu' and 'memory' are required.
0000000000000000000000000000000000000000;;	// cpu quota is a soft requirement.
0000000000000000000000000000000000000000;;	func validateSystemRequirements(mountUtil mount.Interface) (features, error) {
0000000000000000000000000000000000000000;;		const (
0000000000000000000000000000000000000000;;			cgroupMountType = "cgroup"
0000000000000000000000000000000000000000;;			localErr        = "system validation failed"
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		var (
0000000000000000000000000000000000000000;;			cpuMountPoint string
0000000000000000000000000000000000000000;;			f             features
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;		mountPoints, err := mountUtil.List()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return f, fmt.Errorf("%s - %v", localErr, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		expectedCgroups := sets.NewString("cpu", "cpuacct", "cpuset", "memory")
0000000000000000000000000000000000000000;;		for _, mountPoint := range mountPoints {
0000000000000000000000000000000000000000;;			if mountPoint.Type == cgroupMountType {
0000000000000000000000000000000000000000;;				for _, opt := range mountPoint.Opts {
0000000000000000000000000000000000000000;;					if expectedCgroups.Has(opt) {
0000000000000000000000000000000000000000;;						expectedCgroups.Delete(opt)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if opt == "cpu" {
0000000000000000000000000000000000000000;;						cpuMountPoint = mountPoint.Path
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if expectedCgroups.Len() > 0 {
0000000000000000000000000000000000000000;;			return f, fmt.Errorf("%s - Following Cgroup subsystem not mounted: %v", localErr, expectedCgroups.List())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check if cpu quota is available.
0000000000000000000000000000000000000000;;		// CPU cgroup is required and so it expected to be mounted at this point.
0000000000000000000000000000000000000000;;		periodExists, err := util.FileExists(path.Join(cpuMountPoint, "cpu.cfs_period_us"))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("failed to detect if CPU cgroup cpu.cfs_period_us is available - %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		quotaExists, err := util.FileExists(path.Join(cpuMountPoint, "cpu.cfs_quota_us"))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("failed to detect if CPU cgroup cpu.cfs_quota_us is available - %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if quotaExists && periodExists {
0000000000000000000000000000000000000000;;			f.cpuHardcapping = true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return f, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO(vmarmol): Add limits to the system containers.
0000000000000000000000000000000000000000;;	// Takes the absolute name of the specified containers.
0000000000000000000000000000000000000000;;	// Empty container name disables use of the specified container.
0000000000000000000000000000000000000000;;	func NewContainerManager(mountUtil mount.Interface, cadvisorInterface cadvisor.Interface, nodeConfig NodeConfig, failSwapOn bool, recorder record.EventRecorder) (ContainerManager, error) {
0000000000000000000000000000000000000000;;		subsystems, err := GetCgroupSubsystems()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed to get mounted cgroup subsystems: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check whether swap is enabled. The Kubelet does not support running with swap enabled.
0000000000000000000000000000000000000000;;		cmd := exec.Command("cat", "/proc/swaps")
0000000000000000000000000000000000000000;;		stdout, err := cmd.StdoutPipe()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := cmd.Start(); err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var buf []string
0000000000000000000000000000000000000000;;		scanner := bufio.NewScanner(stdout)
0000000000000000000000000000000000000000;;		for scanner.Scan() { // Splits on newlines by default
0000000000000000000000000000000000000000;;			buf = append(buf, scanner.Text())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := cmd.Wait(); err != nil { // Clean up
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO(#34726:1.8.0): Remove the opt-in for failing when swap is enabled.
0000000000000000000000000000000000000000;;		//     Running with swap enabled should be considered an error, but in order to maintain legacy
0000000000000000000000000000000000000000;;		//     behavior we have to require an opt-in to this error for a period of time.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If there is more than one line (table headers) in /proc/swaps, swap is enabled and we should error out.
0000000000000000000000000000000000000000;;		if len(buf) > 1 {
0000000000000000000000000000000000000000;;			if failSwapOn {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("Running with swap on is not supported, please disable swap! /proc/swaps contained: %v", buf)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Warningf("Running with swap on is not supported, please disable swap! " +
0000000000000000000000000000000000000000;;				"This will be a fatal error by default starting in K8s v1.6! " +
0000000000000000000000000000000000000000;;				"In the meantime, you can opt-in to making this a fatal error by enabling --experimental-fail-swap-on.")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var capacity = v1.ResourceList{}
0000000000000000000000000000000000000000;;		// It is safe to invoke `MachineInfo` on cAdvisor before logically initializing cAdvisor here because
0000000000000000000000000000000000000000;;		// machine info is computed and cached once as part of cAdvisor object creation.
0000000000000000000000000000000000000000;;		// But `RootFsInfo` and `ImagesFsInfo` are not available at this moment so they will be called later during manager starts
0000000000000000000000000000000000000000;;		if info, err := cadvisorInterface.MachineInfo(); err == nil {
0000000000000000000000000000000000000000;;			capacity = cadvisor.CapacityFromMachineInfo(info)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cgroupRoot := nodeConfig.CgroupRoot
0000000000000000000000000000000000000000;;		cgroupManager := NewCgroupManager(subsystems, nodeConfig.CgroupDriver)
0000000000000000000000000000000000000000;;		// Check if Cgroup-root actually exists on the node
0000000000000000000000000000000000000000;;		if nodeConfig.CgroupsPerQOS {
0000000000000000000000000000000000000000;;			// this does default to / when enabled, but this tests against regressions.
0000000000000000000000000000000000000000;;			if nodeConfig.CgroupRoot == "" {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("invalid configuration: cgroups-per-qos was specified and cgroup-root was not specified. To enable the QoS cgroup hierarchy you need to specify a valid cgroup-root")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// we need to check that the cgroup root actually exists for each subsystem
0000000000000000000000000000000000000000;;			// of note, we always use the cgroupfs driver when performing this check since
0000000000000000000000000000000000000000;;			// the input is provided in that format.
0000000000000000000000000000000000000000;;			// this is important because we do not want any name conversion to occur.
0000000000000000000000000000000000000000;;			if !cgroupManager.Exists(CgroupName(cgroupRoot)) {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("invalid configuration: cgroup-root %q doesn't exist: %v", cgroupRoot, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("container manager verified user specified cgroup-root exists: %v", cgroupRoot)
0000000000000000000000000000000000000000;;			// Include the the top level cgroup for enforcing node allocatable into cgroup-root.
0000000000000000000000000000000000000000;;			// This way, all sub modules can avoid having to understand the concept of node allocatable.
0000000000000000000000000000000000000000;;			cgroupRoot = path.Join(cgroupRoot, defaultNodeAllocatableCgroupName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.Infof("Creating Container Manager object based on Node Config: %+v", nodeConfig)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		qosContainerManager, err := NewQOSContainerManager(subsystems, cgroupRoot, nodeConfig)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return &containerManagerImpl{
0000000000000000000000000000000000000000;;			cadvisorInterface:   cadvisorInterface,
0000000000000000000000000000000000000000;;			mountUtil:           mountUtil,
0000000000000000000000000000000000000000;;			NodeConfig:          nodeConfig,
0000000000000000000000000000000000000000;;			subsystems:          subsystems,
0000000000000000000000000000000000000000;;			cgroupManager:       cgroupManager,
0000000000000000000000000000000000000000;;			capacity:            capacity,
0000000000000000000000000000000000000000;;			cgroupRoot:          cgroupRoot,
0000000000000000000000000000000000000000;;			recorder:            recorder,
0000000000000000000000000000000000000000;;			qosContainerManager: qosContainerManager,
0000000000000000000000000000000000000000;;		}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewPodContainerManager is a factory method returns a PodContainerManager object
0000000000000000000000000000000000000000;;	// If qosCgroups are enabled then it returns the general pod container manager
0000000000000000000000000000000000000000;;	// otherwise it returns a no-op manager which essentially does nothing
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) NewPodContainerManager() PodContainerManager {
0000000000000000000000000000000000000000;;		if cm.NodeConfig.CgroupsPerQOS {
0000000000000000000000000000000000000000;;			return &podContainerManagerImpl{
0000000000000000000000000000000000000000;;				qosContainersInfo: cm.GetQOSContainersInfo(),
0000000000000000000000000000000000000000;;				subsystems:        cm.subsystems,
0000000000000000000000000000000000000000;;				cgroupManager:     cm.cgroupManager,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return &podContainerManagerNoop{
0000000000000000000000000000000000000000;;			cgroupRoot: CgroupName(cm.cgroupRoot),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Create a cgroup container manager.
0000000000000000000000000000000000000000;;	func createManager(containerName string) *fs.Manager {
0000000000000000000000000000000000000000;;		allowAllDevices := true
0000000000000000000000000000000000000000;;		return &fs.Manager{
0000000000000000000000000000000000000000;;			Cgroups: &configs.Cgroup{
0000000000000000000000000000000000000000;;				Parent: "/",
0000000000000000000000000000000000000000;;				Name:   containerName,
0000000000000000000000000000000000000000;;				Resources: &configs.Resources{
0000000000000000000000000000000000000000;;					AllowAllDevices: &allowAllDevices,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type KernelTunableBehavior string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		KernelTunableWarn   KernelTunableBehavior = "warn"
0000000000000000000000000000000000000000;;		KernelTunableError  KernelTunableBehavior = "error"
0000000000000000000000000000000000000000;;		KernelTunableModify KernelTunableBehavior = "modify"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// setupKernelTunables validates kernel tunable flags are set as expected
0000000000000000000000000000000000000000;;	// depending upon the specified option, it will either warn, error, or modify the kernel tunable flags
0000000000000000000000000000000000000000;;	func setupKernelTunables(option KernelTunableBehavior) error {
0000000000000000000000000000000000000000;;		desiredState := map[string]int{
0000000000000000000000000000000000000000;;			utilsysctl.VmOvercommitMemory: utilsysctl.VmOvercommitMemoryAlways,
0000000000000000000000000000000000000000;;			utilsysctl.VmPanicOnOOM:       utilsysctl.VmPanicOnOOMInvokeOOMKiller,
0000000000000000000000000000000000000000;;			utilsysctl.KernelPanic:        utilsysctl.KernelPanicRebootTimeout,
0000000000000000000000000000000000000000;;			utilsysctl.KernelPanicOnOops:  utilsysctl.KernelPanicOnOopsAlways,
0000000000000000000000000000000000000000;;			utilsysctl.RootMaxKeys:        utilsysctl.RootMaxKeysSetting,
0000000000000000000000000000000000000000;;			utilsysctl.RootMaxBytes:       utilsysctl.RootMaxBytesSetting,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sysctl := utilsysctl.New()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		errList := []error{}
0000000000000000000000000000000000000000;;		for flag, expectedValue := range desiredState {
0000000000000000000000000000000000000000;;			val, err := sysctl.GetSysctl(flag)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				errList = append(errList, err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if val == expectedValue {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			switch option {
0000000000000000000000000000000000000000;;			case KernelTunableError:
0000000000000000000000000000000000000000;;				errList = append(errList, fmt.Errorf("Invalid kernel flag: %v, expected value: %v, actual value: %v", flag, expectedValue, val))
0000000000000000000000000000000000000000;;			case KernelTunableWarn:
0000000000000000000000000000000000000000;;				glog.V(2).Infof("Invalid kernel flag: %v, expected value: %v, actual value: %v", flag, expectedValue, val)
0000000000000000000000000000000000000000;;			case KernelTunableModify:
0000000000000000000000000000000000000000;;				glog.V(2).Infof("Updating kernel flag: %v, expected value: %v, actual value: %v", flag, expectedValue, val)
0000000000000000000000000000000000000000;;				err = sysctl.SetSysctl(flag, expectedValue)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					errList = append(errList, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return utilerrors.NewAggregate(errList)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) setupNode(activePods ActivePodsFunc) error {
0000000000000000000000000000000000000000;;		f, err := validateSystemRequirements(cm.mountUtil)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !f.cpuHardcapping {
0000000000000000000000000000000000000000;;			cm.status.SoftRequirements = fmt.Errorf("CPU hardcapping unsupported")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		b := KernelTunableModify
0000000000000000000000000000000000000000;;		if cm.GetNodeConfig().ProtectKernelDefaults {
0000000000000000000000000000000000000000;;			b = KernelTunableError
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := setupKernelTunables(b); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Setup top level qos containers only if CgroupsPerQOS flag is specified as true
0000000000000000000000000000000000000000;;		if cm.NodeConfig.CgroupsPerQOS {
0000000000000000000000000000000000000000;;			if err := cm.createNodeAllocatableCgroups(); err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			err = cm.qosContainerManager.Start(cm.getNodeAllocatableAbsolute, activePods)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("failed to initialize top level QOS containers: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Enforce Node Allocatable (if required)
0000000000000000000000000000000000000000;;		if err := cm.enforceNodeAllocatableCgroups(); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		systemContainers := []*systemContainer{}
0000000000000000000000000000000000000000;;		if cm.ContainerRuntime == "docker" {
0000000000000000000000000000000000000000;;			// With the docker-CRI integration, dockershim will manage the cgroups
0000000000000000000000000000000000000000;;			// and oom score for the docker processes.
0000000000000000000000000000000000000000;;			// In the future, NodeSpec should mandate the cgroup that the
0000000000000000000000000000000000000000;;			// runtime processes need to be in. For now, we still check the
0000000000000000000000000000000000000000;;			// cgroup for docker periodically, so that kubelet can recognize
0000000000000000000000000000000000000000;;			// the cgroup for docker and serve stats for the runtime.
0000000000000000000000000000000000000000;;			// TODO(#27097): Fix this after NodeSpec is clearly defined.
0000000000000000000000000000000000000000;;			cm.periodicTasks = append(cm.periodicTasks, func() {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("[ContainerManager]: Adding periodic tasks for docker CRI integration")
0000000000000000000000000000000000000000;;				cont, err := getContainerNameForProcess(dockerProcessName, dockerPidFile)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					glog.Error(err)
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				glog.V(2).Infof("[ContainerManager]: Discovered runtime cgroups name: %s", cont)
0000000000000000000000000000000000000000;;				cm.Lock()
0000000000000000000000000000000000000000;;				defer cm.Unlock()
0000000000000000000000000000000000000000;;				cm.RuntimeCgroupsName = cont
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if cm.SystemCgroupsName != "" {
0000000000000000000000000000000000000000;;			if cm.SystemCgroupsName == "/" {
0000000000000000000000000000000000000000;;				return fmt.Errorf("system container cannot be root (\"/\")")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			cont := newSystemCgroups(cm.SystemCgroupsName)
0000000000000000000000000000000000000000;;			cont.ensureStateFunc = func(manager *fs.Manager) error {
0000000000000000000000000000000000000000;;				return ensureSystemCgroups("/", manager)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			systemContainers = append(systemContainers, cont)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if cm.KubeletCgroupsName != "" {
0000000000000000000000000000000000000000;;			cont := newSystemCgroups(cm.KubeletCgroupsName)
0000000000000000000000000000000000000000;;			allowAllDevices := true
0000000000000000000000000000000000000000;;			manager := fs.Manager{
0000000000000000000000000000000000000000;;				Cgroups: &configs.Cgroup{
0000000000000000000000000000000000000000;;					Parent: "/",
0000000000000000000000000000000000000000;;					Name:   cm.KubeletCgroupsName,
0000000000000000000000000000000000000000;;					Resources: &configs.Resources{
0000000000000000000000000000000000000000;;						AllowAllDevices: &allowAllDevices,
0000000000000000000000000000000000000000;;					},
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			cont.ensureStateFunc = func(_ *fs.Manager) error {
0000000000000000000000000000000000000000;;				return ensureProcessInContainerWithOOMScore(os.Getpid(), qos.KubeletOOMScoreAdj, &manager)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			systemContainers = append(systemContainers, cont)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			cm.periodicTasks = append(cm.periodicTasks, func() {
0000000000000000000000000000000000000000;;				if err := ensureProcessInContainerWithOOMScore(os.Getpid(), qos.KubeletOOMScoreAdj, nil); err != nil {
0000000000000000000000000000000000000000;;					glog.Error(err)
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				cont, err := getContainer(os.Getpid())
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					glog.Errorf("failed to find cgroups of kubelet - %v", err)
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				cm.Lock()
0000000000000000000000000000000000000000;;				defer cm.Unlock()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				cm.KubeletCgroupsName = cont
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cm.systemContainers = systemContainers
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getContainerNameForProcess(name, pidFile string) (string, error) {
0000000000000000000000000000000000000000;;		pids, err := getPidsForProcess(name, pidFile)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", fmt.Errorf("failed to detect process id for %q - %v", name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(pids) == 0 {
0000000000000000000000000000000000000000;;			return "", nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		cont, err := getContainer(pids[0])
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return cont, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) GetNodeConfig() NodeConfig {
0000000000000000000000000000000000000000;;		cm.RLock()
0000000000000000000000000000000000000000;;		defer cm.RUnlock()
0000000000000000000000000000000000000000;;		return cm.NodeConfig
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) GetMountedSubsystems() *CgroupSubsystems {
0000000000000000000000000000000000000000;;		return cm.subsystems
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) GetQOSContainersInfo() QOSContainersInfo {
0000000000000000000000000000000000000000;;		return cm.qosContainerManager.GetQOSContainersInfo()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) UpdateQOSCgroups() error {
0000000000000000000000000000000000000000;;		return cm.qosContainerManager.UpdateCgroups()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) Status() Status {
0000000000000000000000000000000000000000;;		cm.RLock()
0000000000000000000000000000000000000000;;		defer cm.RUnlock()
0000000000000000000000000000000000000000;;		return cm.status
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) Start(node *v1.Node, activePods ActivePodsFunc) error {
0000000000000000000000000000000000000000;;		// cache the node Info including resource capacity and
0000000000000000000000000000000000000000;;		// allocatable of the node
0000000000000000000000000000000000000000;;		cm.nodeInfo = node
0000000000000000000000000000000000000000;;		// Setup the node
0000000000000000000000000000000000000000;;		if err := cm.setupNode(activePods); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Ensure that node allocatable configuration is valid.
0000000000000000000000000000000000000000;;		if err := cm.validateNodeAllocatable(); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Don't run a background thread if there are no ensureStateFuncs.
0000000000000000000000000000000000000000;;		hasEnsureStateFuncs := false
0000000000000000000000000000000000000000;;		for _, cont := range cm.systemContainers {
0000000000000000000000000000000000000000;;			if cont.ensureStateFunc != nil {
0000000000000000000000000000000000000000;;				hasEnsureStateFuncs = true
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if hasEnsureStateFuncs {
0000000000000000000000000000000000000000;;			// Run ensure state functions every minute.
0000000000000000000000000000000000000000;;			go wait.Until(func() {
0000000000000000000000000000000000000000;;				for _, cont := range cm.systemContainers {
0000000000000000000000000000000000000000;;					if cont.ensureStateFunc != nil {
0000000000000000000000000000000000000000;;						if err := cont.ensureStateFunc(cont.manager); err != nil {
0000000000000000000000000000000000000000;;							glog.Warningf("[ContainerManager] Failed to ensure state of %q: %v", cont.name, err)
0000000000000000000000000000000000000000;;						}
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}, time.Minute, wait.NeverStop)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(cm.periodicTasks) > 0 {
0000000000000000000000000000000000000000;;			go wait.Until(func() {
0000000000000000000000000000000000000000;;				for _, task := range cm.periodicTasks {
0000000000000000000000000000000000000000;;					if task != nil {
0000000000000000000000000000000000000000;;						task()
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}, 5*time.Minute, wait.NeverStop)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Local storage filesystem information from `RootFsInfo` and `ImagesFsInfo` is available at a later time
0000000000000000000000000000000000000000;;		// depending on the time when cadvisor manager updates container stats. Therefore use a go routine to keep
0000000000000000000000000000000000000000;;		// retrieving the information until it is available.
0000000000000000000000000000000000000000;;		stopChan := make(chan struct{})
0000000000000000000000000000000000000000;;		go wait.Until(func() {
0000000000000000000000000000000000000000;;			if err := cm.setFsCapacity(); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("[ContainerManager]: %v", err)
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			close(stopChan)
0000000000000000000000000000000000000000;;		}, time.Second, stopChan)
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) setFsCapacity() error {
0000000000000000000000000000000000000000;;		rootfs, err := cm.cadvisorInterface.RootFsInfo()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Fail to get rootfs information %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		hasDedicatedImageFs, _ := cm.cadvisorInterface.HasDedicatedImageFs()
0000000000000000000000000000000000000000;;		var imagesfs cadvisorapiv2.FsInfo
0000000000000000000000000000000000000000;;		if hasDedicatedImageFs {
0000000000000000000000000000000000000000;;			imagesfs, err = cm.cadvisorInterface.ImagesFsInfo()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("Fail to get imagefs information %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cm.Lock()
0000000000000000000000000000000000000000;;		for rName, rCap := range cadvisor.StorageScratchCapacityFromFsInfo(rootfs) {
0000000000000000000000000000000000000000;;			cm.capacity[rName] = rCap
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if hasDedicatedImageFs {
0000000000000000000000000000000000000000;;			for rName, rCap := range cadvisor.StorageOverlayCapacityFromFsInfo(imagesfs) {
0000000000000000000000000000000000000000;;				cm.capacity[rName] = rCap
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		cm.Unlock()
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) SystemCgroupsLimit() v1.ResourceList {
0000000000000000000000000000000000000000;;		cpuLimit := int64(0)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Sum up resources of all external containers.
0000000000000000000000000000000000000000;;		for _, cont := range cm.systemContainers {
0000000000000000000000000000000000000000;;			cpuLimit += cont.cpuMillicores
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return v1.ResourceList{
0000000000000000000000000000000000000000;;			v1.ResourceCPU: *resource.NewMilliQuantity(
0000000000000000000000000000000000000000;;				cpuLimit,
0000000000000000000000000000000000000000;;				resource.DecimalSI),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func isProcessRunningInHost(pid int) (bool, error) {
0000000000000000000000000000000000000000;;		// Get init pid namespace.
0000000000000000000000000000000000000000;;		initPidNs, err := os.Readlink("/proc/1/ns/pid")
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, fmt.Errorf("failed to find pid namespace of init process")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(10).Infof("init pid ns is %q", initPidNs)
0000000000000000000000000000000000000000;;		processPidNs, err := os.Readlink(fmt.Sprintf("/proc/%d/ns/pid", pid))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, fmt.Errorf("failed to find pid namespace of process %q", pid)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(10).Infof("Pid %d pid ns is %q", pid, processPidNs)
0000000000000000000000000000000000000000;;		return initPidNs == processPidNs, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getPidFromPidFile(pidFile string) (int, error) {
0000000000000000000000000000000000000000;;		file, err := os.Open(pidFile)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return 0, fmt.Errorf("error opening pid file %s: %v", pidFile, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		defer file.Close()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		data, err := ioutil.ReadAll(file)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return 0, fmt.Errorf("error reading pid file %s: %v", pidFile, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pid, err := strconv.Atoi(string(data))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return 0, fmt.Errorf("error parsing %s as a number: %v", string(data), err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return pid, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func getPidsForProcess(name, pidFile string) ([]int, error) {
0000000000000000000000000000000000000000;;		if len(pidFile) == 0 {
0000000000000000000000000000000000000000;;			return procfs.PidOf(name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		pid, err := getPidFromPidFile(pidFile)
0000000000000000000000000000000000000000;;		if err == nil {
0000000000000000000000000000000000000000;;			return []int{pid}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Try to lookup pid by process name
0000000000000000000000000000000000000000;;		pids, err2 := procfs.PidOf(name)
0000000000000000000000000000000000000000;;		if err2 == nil {
0000000000000000000000000000000000000000;;			return pids, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Return error from getPidFromPidFile since that should have worked
0000000000000000000000000000000000000000;;		// and is the real source of the problem.
0000000000000000000000000000000000000000;;		glog.V(4).Infof("unable to get pid from %s: %v", pidFile, err)
0000000000000000000000000000000000000000;;		return []int{}, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Ensures that the Docker daemon is in the desired container.
0000000000000000000000000000000000000000;;	// Temporarily export the function to be used by dockershim.
0000000000000000000000000000000000000000;;	// TODO(yujuhong): Move this function to dockershim once kubelet migrates to
0000000000000000000000000000000000000000;;	// dockershim as the default.
0000000000000000000000000000000000000000;;	func EnsureDockerInContainer(dockerAPIVersion *utilversion.Version, oomScoreAdj int, manager *fs.Manager) error {
0000000000000000000000000000000000000000;;		type process struct{ name, file string }
0000000000000000000000000000000000000000;;		dockerProcs := []process{{dockerProcessName, dockerPidFile}}
0000000000000000000000000000000000000000;;		if dockerAPIVersion.AtLeast(containerdAPIVersion) {
0000000000000000000000000000000000000000;;			dockerProcs = append(dockerProcs, process{containerdProcessName, containerdPidFile})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var errs []error
0000000000000000000000000000000000000000;;		for _, proc := range dockerProcs {
0000000000000000000000000000000000000000;;			pids, err := getPidsForProcess(proc.name, proc.file)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				errs = append(errs, fmt.Errorf("failed to get pids for %q: %v", proc.name, err))
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Move if the pid is not already in the desired container.
0000000000000000000000000000000000000000;;			for _, pid := range pids {
0000000000000000000000000000000000000000;;				if err := ensureProcessInContainerWithOOMScore(pid, oomScoreAdj, manager); err != nil {
0000000000000000000000000000000000000000;;					errs = append(errs, fmt.Errorf("errors moving %q pid: %v", proc.name, err))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return utilerrors.NewAggregate(errs)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ensureProcessInContainerWithOOMScore(pid int, oomScoreAdj int, manager *fs.Manager) error {
0000000000000000000000000000000000000000;;		if runningInHost, err := isProcessRunningInHost(pid); err != nil {
0000000000000000000000000000000000000000;;			// Err on the side of caution. Avoid moving the docker daemon unless we are able to identify its context.
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		} else if !runningInHost {
0000000000000000000000000000000000000000;;			// Process is running inside a container. Don't touch that.
0000000000000000000000000000000000000000;;			glog.V(2).Infof("pid %d is not running in the host namespaces", pid)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		var errs []error
0000000000000000000000000000000000000000;;		if manager != nil {
0000000000000000000000000000000000000000;;			cont, err := getContainer(pid)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				errs = append(errs, fmt.Errorf("failed to find container of PID %d: %v", pid, err))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if cont != manager.Cgroups.Name {
0000000000000000000000000000000000000000;;				err = manager.Apply(pid)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					errs = append(errs, fmt.Errorf("failed to move PID %d (in %q) to %q: %v", pid, cont, manager.Cgroups.Name, err))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Also apply oom-score-adj to processes
0000000000000000000000000000000000000000;;		oomAdjuster := oom.NewOOMAdjuster()
0000000000000000000000000000000000000000;;		glog.V(5).Infof("attempting to apply oom_score_adj of %d to pid %d", oomScoreAdj, pid)
0000000000000000000000000000000000000000;;		if err := oomAdjuster.ApplyOOMScoreAdj(pid, oomScoreAdj); err != nil {
0000000000000000000000000000000000000000;;			glog.V(3).Infof("Failed to apply oom_score_adj %d for pid %d: %v", oomScoreAdj, pid, err)
0000000000000000000000000000000000000000;;			errs = append(errs, fmt.Errorf("failed to apply oom score %d to PID %d: %v", oomScoreAdj, pid, err))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return utilerrors.NewAggregate(errs)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getContainer returns the cgroup associated with the specified pid.
0000000000000000000000000000000000000000;;	// It enforces a unified hierarchy for memory and cpu cgroups.
0000000000000000000000000000000000000000;;	// On systemd environments, it uses the name=systemd cgroup for the specified pid.
0000000000000000000000000000000000000000;;	func getContainer(pid int) (string, error) {
0000000000000000000000000000000000000000;;		cgs, err := cgroups.ParseCgroupFile(fmt.Sprintf("/proc/%d/cgroup", pid))
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return "", err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cpu, found := cgs["cpu"]
0000000000000000000000000000000000000000;;		if !found {
0000000000000000000000000000000000000000;;			return "", cgroups.NewNotFoundError("cpu")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		memory, found := cgs["memory"]
0000000000000000000000000000000000000000;;		if !found {
0000000000000000000000000000000000000000;;			return "", cgroups.NewNotFoundError("memory")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// since we use this container for accounting, we need to ensure its a unified hierarchy.
0000000000000000000000000000000000000000;;		if cpu != memory {
0000000000000000000000000000000000000000;;			return "", fmt.Errorf("cpu and memory cgroup hierarchy not unified.  cpu: %s, memory: %s", cpu, memory)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// on systemd, every pid is in a unified cgroup hierarchy (name=systemd as seen in systemd-cgls)
0000000000000000000000000000000000000000;;		// cpu and memory accounting is off by default, users may choose to enable it per unit or globally.
0000000000000000000000000000000000000000;;		// users could enable CPU and memory accounting globally via /etc/systemd/system.conf (DefaultCPUAccounting=true DefaultMemoryAccounting=true).
0000000000000000000000000000000000000000;;		// users could also enable CPU and memory accounting per unit via CPUAccounting=true and MemoryAccounting=true
0000000000000000000000000000000000000000;;		// we only warn if accounting is not enabled for CPU or memory so as to not break local development flows where kubelet is launched in a terminal.
0000000000000000000000000000000000000000;;		// for example, the cgroup for the user session will be something like /user.slice/user-X.slice/session-X.scope, but the cpu and memory
0000000000000000000000000000000000000000;;		// cgroup will be the closest ancestor where accounting is performed (most likely /) on systems that launch docker containers.
0000000000000000000000000000000000000000;;		// as a result, on those systems, you will not get cpu or memory accounting statistics for kubelet.
0000000000000000000000000000000000000000;;		// in addition, you would not get memory or cpu accounting for the runtime unless accounting was enabled on its unit (or globally).
0000000000000000000000000000000000000000;;		if systemd, found := cgs["name=systemd"]; found {
0000000000000000000000000000000000000000;;			if systemd != cpu {
0000000000000000000000000000000000000000;;				glog.Warningf("CPUAccounting not enabled for pid: %d", pid)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if systemd != memory {
0000000000000000000000000000000000000000;;				glog.Warningf("MemoryAccounting not enabled for pid: %d", pid)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return systemd, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return cpu, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Ensures the system container is created and all non-kernel threads and process 1
0000000000000000000000000000000000000000;;	// without a container are moved to it.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// The reason of leaving kernel threads at root cgroup is that we don't want to tie the
0000000000000000000000000000000000000000;;	// execution of these threads with to-be defined /system quota and create priority inversions.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	func ensureSystemCgroups(rootCgroupPath string, manager *fs.Manager) error {
0000000000000000000000000000000000000000;;		// Move non-kernel PIDs to the system container.
0000000000000000000000000000000000000000;;		attemptsRemaining := 10
0000000000000000000000000000000000000000;;		var errs []error
0000000000000000000000000000000000000000;;		for attemptsRemaining >= 0 {
0000000000000000000000000000000000000000;;			// Only keep errors on latest attempt.
0000000000000000000000000000000000000000;;			errs = []error{}
0000000000000000000000000000000000000000;;			attemptsRemaining--
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			allPids, err := cmutil.GetPids(rootCgroupPath)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				errs = append(errs, fmt.Errorf("failed to list PIDs for root: %v", err))
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Remove kernel pids and other protected PIDs (pid 1, PIDs already in system & kubelet containers)
0000000000000000000000000000000000000000;;			pids := make([]int, 0, len(allPids))
0000000000000000000000000000000000000000;;			for _, pid := range allPids {
0000000000000000000000000000000000000000;;				if pid == 1 || isKernelPid(pid) {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				pids = append(pids, pid)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.Infof("Found %d PIDs in root, %d of them are not to be moved", len(allPids), len(allPids)-len(pids))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Check if we have moved all the non-kernel PIDs.
0000000000000000000000000000000000000000;;			if len(pids) == 0 {
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			glog.Infof("Moving non-kernel processes: %v", pids)
0000000000000000000000000000000000000000;;			for _, pid := range pids {
0000000000000000000000000000000000000000;;				err := manager.Apply(pid)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					errs = append(errs, fmt.Errorf("failed to move PID %d into the system container %q: %v", pid, manager.Cgroups.Name, err))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if attemptsRemaining < 0 {
0000000000000000000000000000000000000000;;			errs = append(errs, fmt.Errorf("ran out of attempts to create system containers %q", manager.Cgroups.Name))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return utilerrors.NewAggregate(errs)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Determines whether the specified PID is a kernel PID.
0000000000000000000000000000000000000000;;	func isKernelPid(pid int) bool {
0000000000000000000000000000000000000000;;		// Kernel threads have no associated executable.
0000000000000000000000000000000000000000;;		_, err := os.Readlink(fmt.Sprintf("/proc/%d/exe", pid))
0000000000000000000000000000000000000000;;		return err != nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Helper for getting the docker API version.
0000000000000000000000000000000000000000;;	func getDockerAPIVersion(cadvisor cadvisor.Interface) *utilversion.Version {
0000000000000000000000000000000000000000;;		versions, err := cadvisor.VersionInfo()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Error requesting cAdvisor VersionInfo: %v", err)
0000000000000000000000000000000000000000;;			return utilversion.MustParseSemantic("0.0")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		dockerAPIVersion, err := utilversion.ParseGeneric(versions.DockerAPIVersion)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Error parsing docker version %q: %v", versions.DockerVersion, err)
0000000000000000000000000000000000000000;;			return utilversion.MustParseSemantic("0.0")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return dockerAPIVersion
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) GetCapacity() v1.ResourceList {
0000000000000000000000000000000000000000;;		cm.RLock()
0000000000000000000000000000000000000000;;		defer cm.RUnlock()
0000000000000000000000000000000000000000;;		return cm.capacity
0000000000000000000000000000000000000000;;	}
