0000000000000000000000000000000000000000;;	// +build linux
d6ca921ffb1bc599e9a0a94394050c22915d22e7;pkg/kubelet/cm/node_allocatable.go[pkg/kubelet/cm/node_allocatable.go][pkg/kubelet/cm/node_container_manager.go];	
0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2017 The Kubernetes Authors.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package cm
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		clientv1 "k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/events"
0000000000000000000000000000000000000000;;		evictionapi "k8s.io/kubernetes/pkg/kubelet/eviction/api"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		defaultNodeAllocatableCgroupName = "kubepods"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) createNodeAllocatableCgroups() error {
0000000000000000000000000000000000000000;;		cgroupConfig := &CgroupConfig{
0000000000000000000000000000000000000000;;			Name: CgroupName(cm.cgroupRoot),
0000000000000000000000000000000000000000;;			// The default limits for cpu shares can be very low which can lead to CPU starvation for pods.
0000000000000000000000000000000000000000;;			ResourceParameters: getCgroupConfig(cm.capacity),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if cm.cgroupManager.Exists(cgroupConfig.Name) {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := cm.cgroupManager.Create(cgroupConfig); err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Failed to create %q cgroup", cm.cgroupRoot)
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Enforce Node Allocatable Cgroup settings.
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) enforceNodeAllocatableCgroups() error {
0000000000000000000000000000000000000000;;		nc := cm.NodeConfig.NodeAllocatableConfig
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// We need to update limits on node allocatable cgroup no matter what because
0000000000000000000000000000000000000000;;		// default cpu shares on cgroups are low and can cause cpu starvation.
0000000000000000000000000000000000000000;;		nodeAllocatable := cm.capacity
0000000000000000000000000000000000000000;;		// Use Node Allocatable limits instead of capacity if the user requested enforcing node allocatable.
0000000000000000000000000000000000000000;;		if cm.CgroupsPerQOS && nc.EnforceNodeAllocatable.Has(NodeAllocatableEnforcementKey) {
0000000000000000000000000000000000000000;;			nodeAllocatable = cm.getNodeAllocatableAbsolute()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Attempting to enforce Node Allocatable with config: %+v", nc)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		cgroupConfig := &CgroupConfig{
0000000000000000000000000000000000000000;;			Name:               CgroupName(cm.cgroupRoot),
0000000000000000000000000000000000000000;;			ResourceParameters: getCgroupConfig(nodeAllocatable),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Using ObjectReference for events as the node maybe not cached; refer to #42701 for detail.
0000000000000000000000000000000000000000;;		nodeRef := &clientv1.ObjectReference{
0000000000000000000000000000000000000000;;			Kind:      "Node",
0000000000000000000000000000000000000000;;			Name:      cm.nodeInfo.Name,
0000000000000000000000000000000000000000;;			UID:       types.UID(cm.nodeInfo.Name),
0000000000000000000000000000000000000000;;			Namespace: "",
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If Node Allocatable is enforced on a node that has not been drained or is updated on an existing node to a lower value,
0000000000000000000000000000000000000000;;		// existing memory usage across pods might be higher that current Node Allocatable Memory Limits.
0000000000000000000000000000000000000000;;		// Pod Evictions are expected to bring down memory usage to below Node Allocatable limits.
0000000000000000000000000000000000000000;;		// Until evictions happen retry cgroup updates.
0000000000000000000000000000000000000000;;		// Update limits on non root cgroup-root to be safe since the default limits for CPU can be too low.
0000000000000000000000000000000000000000;;		if cm.cgroupRoot != "/" {
0000000000000000000000000000000000000000;;			go func() {
0000000000000000000000000000000000000000;;				for {
0000000000000000000000000000000000000000;;					err := cm.cgroupManager.Update(cgroupConfig)
0000000000000000000000000000000000000000;;					if err == nil {
0000000000000000000000000000000000000000;;						cm.recorder.Event(nodeRef, v1.EventTypeNormal, events.SuccessfulNodeAllocatableEnforcement, "Updated Node Allocatable limit across pods")
0000000000000000000000000000000000000000;;						return
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					message := fmt.Sprintf("Failed to update Node Allocatable Limits %q: %v", cm.cgroupRoot, err)
0000000000000000000000000000000000000000;;					cm.recorder.Event(nodeRef, v1.EventTypeWarning, events.FailedNodeAllocatableEnforcement, message)
0000000000000000000000000000000000000000;;					time.Sleep(time.Minute)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Now apply kube reserved and system reserved limits if required.
0000000000000000000000000000000000000000;;		if nc.EnforceNodeAllocatable.Has(SystemReservedEnforcementKey) {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("Enforcing System reserved on cgroup %q with limits: %+v", nc.SystemReservedCgroupName, nc.SystemReserved)
0000000000000000000000000000000000000000;;			if err := enforceExistingCgroup(cm.cgroupManager, nc.SystemReservedCgroupName, nc.SystemReserved); err != nil {
0000000000000000000000000000000000000000;;				message := fmt.Sprintf("Failed to enforce System Reserved Cgroup Limits on %q: %v", nc.SystemReservedCgroupName, err)
0000000000000000000000000000000000000000;;				cm.recorder.Event(nodeRef, v1.EventTypeWarning, events.FailedNodeAllocatableEnforcement, message)
0000000000000000000000000000000000000000;;				return fmt.Errorf(message)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			cm.recorder.Eventf(nodeRef, v1.EventTypeNormal, events.SuccessfulNodeAllocatableEnforcement, "Updated limits on system reserved cgroup %v", nc.SystemReservedCgroupName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if nc.EnforceNodeAllocatable.Has(KubeReservedEnforcementKey) {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("Enforcing kube reserved on cgroup %q with limits: %+v", nc.KubeReservedCgroupName, nc.KubeReserved)
0000000000000000000000000000000000000000;;			if err := enforceExistingCgroup(cm.cgroupManager, nc.KubeReservedCgroupName, nc.KubeReserved); err != nil {
0000000000000000000000000000000000000000;;				message := fmt.Sprintf("Failed to enforce Kube Reserved Cgroup Limits on %q: %v", nc.KubeReservedCgroupName, err)
0000000000000000000000000000000000000000;;				cm.recorder.Event(nodeRef, v1.EventTypeWarning, events.FailedNodeAllocatableEnforcement, message)
0000000000000000000000000000000000000000;;				return fmt.Errorf(message)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			cm.recorder.Eventf(nodeRef, v1.EventTypeNormal, events.SuccessfulNodeAllocatableEnforcement, "Updated limits on kube reserved cgroup %v", nc.KubeReservedCgroupName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// enforceExistingCgroup updates the limits `rl` on existing cgroup `cName` using `cgroupManager` interface.
0000000000000000000000000000000000000000;;	func enforceExistingCgroup(cgroupManager CgroupManager, cName string, rl v1.ResourceList) error {
0000000000000000000000000000000000000000;;		cgroupConfig := &CgroupConfig{
0000000000000000000000000000000000000000;;			Name:               CgroupName(cName),
0000000000000000000000000000000000000000;;			ResourceParameters: getCgroupConfig(rl),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Enforcing limits on cgroup %q with %d cpu shares and %d bytes of memory", cName, cgroupConfig.ResourceParameters.CpuShares, cgroupConfig.ResourceParameters.Memory)
0000000000000000000000000000000000000000;;		if !cgroupManager.Exists(cgroupConfig.Name) {
0000000000000000000000000000000000000000;;			return fmt.Errorf("%q cgroup does not exist", cgroupConfig.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if err := cgroupManager.Update(cgroupConfig); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Returns a ResourceConfig object that can be used to create or update cgroups via CgroupManager interface.
0000000000000000000000000000000000000000;;	func getCgroupConfig(rl v1.ResourceList) *ResourceConfig {
0000000000000000000000000000000000000000;;		// TODO(vishh): Set CPU Quota if necessary.
0000000000000000000000000000000000000000;;		if rl == nil {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var rc ResourceConfig
0000000000000000000000000000000000000000;;		if q, exists := rl[v1.ResourceMemory]; exists {
0000000000000000000000000000000000000000;;			// Memory is defined in bytes.
0000000000000000000000000000000000000000;;			val := q.Value()
0000000000000000000000000000000000000000;;			rc.Memory = &val
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if q, exists := rl[v1.ResourceCPU]; exists {
0000000000000000000000000000000000000000;;			// CPU is defined in milli-cores.
0000000000000000000000000000000000000000;;			val := MilliCPUToShares(q.MilliValue())
0000000000000000000000000000000000000000;;			rc.CpuShares = &val
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return &rc
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getNodeAllocatableAbsolute returns the absolute value of Node Allocatable which is primarily useful for enforcement.
0000000000000000000000000000000000000000;;	// Note that not all resources that are available on the node are included in the returned list of resources.
0000000000000000000000000000000000000000;;	// Returns a ResourceList.
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) getNodeAllocatableAbsolute() v1.ResourceList {
0000000000000000000000000000000000000000;;		result := make(v1.ResourceList)
0000000000000000000000000000000000000000;;		for k, v := range cm.capacity {
0000000000000000000000000000000000000000;;			value := *(v.Copy())
0000000000000000000000000000000000000000;;			if cm.NodeConfig.SystemReserved != nil {
0000000000000000000000000000000000000000;;				value.Sub(cm.NodeConfig.SystemReserved[k])
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if cm.NodeConfig.KubeReserved != nil {
0000000000000000000000000000000000000000;;				value.Sub(cm.NodeConfig.KubeReserved[k])
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if value.Sign() < 0 {
0000000000000000000000000000000000000000;;				// Negative Allocatable resources don't make sense.
0000000000000000000000000000000000000000;;				value.Set(0)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			result[k] = value
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetNodeAllocatable returns amount of compute or storage resource that have to be reserved on this node from scheduling.
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) GetNodeAllocatableReservation() v1.ResourceList {
0000000000000000000000000000000000000000;;		evictionReservation := hardEvictionReservation(cm.HardEvictionThresholds, cm.capacity)
0000000000000000000000000000000000000000;;		result := make(v1.ResourceList)
0000000000000000000000000000000000000000;;		for k := range cm.capacity {
0000000000000000000000000000000000000000;;			value := resource.NewQuantity(0, resource.DecimalSI)
0000000000000000000000000000000000000000;;			if cm.NodeConfig.SystemReserved != nil {
0000000000000000000000000000000000000000;;				value.Add(cm.NodeConfig.SystemReserved[k])
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if cm.NodeConfig.KubeReserved != nil {
0000000000000000000000000000000000000000;;				value.Add(cm.NodeConfig.KubeReserved[k])
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if evictionReservation != nil {
0000000000000000000000000000000000000000;;				value.Add(evictionReservation[k])
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if !value.IsZero() {
0000000000000000000000000000000000000000;;				result[k] = *value
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return result
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// hardEvictionReservation returns a resourcelist that includes reservation of resources based on hard eviction thresholds.
0000000000000000000000000000000000000000;;	func hardEvictionReservation(thresholds []evictionapi.Threshold, capacity v1.ResourceList) v1.ResourceList {
0000000000000000000000000000000000000000;;		if len(thresholds) == 0 {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ret := v1.ResourceList{}
0000000000000000000000000000000000000000;;		for _, threshold := range thresholds {
0000000000000000000000000000000000000000;;			if threshold.Operator != evictionapi.OpLessThan {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			switch threshold.Signal {
0000000000000000000000000000000000000000;;			case evictionapi.SignalMemoryAvailable:
0000000000000000000000000000000000000000;;				memoryCapacity := capacity[v1.ResourceMemory]
0000000000000000000000000000000000000000;;				value := evictionapi.GetThresholdQuantity(threshold.Value, &memoryCapacity)
0000000000000000000000000000000000000000;;				ret[v1.ResourceMemory] = *value
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return ret
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// validateNodeAllocatable ensures that the user specified Node Allocatable Configuration doesn't reserve more than the node capacity.
0000000000000000000000000000000000000000;;	// Returns error if the configuration is invalid, nil otherwise.
0000000000000000000000000000000000000000;;	func (cm *containerManagerImpl) validateNodeAllocatable() error {
0000000000000000000000000000000000000000;;		na := cm.GetNodeAllocatableReservation()
0000000000000000000000000000000000000000;;		zeroValue := resource.MustParse("0")
0000000000000000000000000000000000000000;;		var errors []string
0000000000000000000000000000000000000000;;		for key, val := range na {
0000000000000000000000000000000000000000;;			if val.Cmp(zeroValue) <= 0 {
0000000000000000000000000000000000000000;;				errors = append(errors, fmt.Sprintf("Resource %q has an allocatable of %v", key, val))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(errors) > 0 {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Invalid Node Allocatable configuration. %s", strings.Join(errors, " "))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
