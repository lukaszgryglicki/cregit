0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2017 The Kubernetes Authors.
f0b15e9483967270553d61443d068dcf8582dbd6;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package nvidia
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"io/ioutil"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"path"
0000000000000000000000000000000000000000;;		"regexp"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/dockershim/libdocker"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/gpu"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO: rework to use Nvidia's NVML, which is more complex, but also provides more fine-grained information and stats.
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// All NVIDIA GPUs cards should be mounted with nvidiactl and nvidia-uvm
0000000000000000000000000000000000000000;;		// If the driver installed correctly, the 2 devices will be there.
0000000000000000000000000000000000000000;;		nvidiaCtlDevice string = "/dev/nvidiactl"
0000000000000000000000000000000000000000;;		nvidiaUVMDevice string = "/dev/nvidia-uvm"
0000000000000000000000000000000000000000;;		// Optional device.
0000000000000000000000000000000000000000;;		nvidiaUVMToolsDevice string = "/dev/nvidia-uvm-tools"
0000000000000000000000000000000000000000;;		devDirectory                = "/dev"
0000000000000000000000000000000000000000;;		nvidiaDeviceRE              = `^nvidia[0-9]*$`
0000000000000000000000000000000000000000;;		nvidiaFullpathRE            = `^/dev/nvidia[0-9]*$`
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type activePodsLister interface {
0000000000000000000000000000000000000000;;		// Returns a list of active pods on the node.
0000000000000000000000000000000000000000;;		GetActivePods() []*v1.Pod
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// nvidiaGPUManager manages nvidia gpu devices.
0000000000000000000000000000000000000000;;	type nvidiaGPUManager struct {
0000000000000000000000000000000000000000;;		sync.Mutex
0000000000000000000000000000000000000000;;		// All gpus available on the Node
0000000000000000000000000000000000000000;;		allGPUs        sets.String
0000000000000000000000000000000000000000;;		allocated      *podGPUs
0000000000000000000000000000000000000000;;		defaultDevices []string
0000000000000000000000000000000000000000;;		// The interface which could get GPU mapping from all the containers.
0000000000000000000000000000000000000000;;		// TODO: Should make this independent of Docker in the future.
0000000000000000000000000000000000000000;;		dockerClient     libdocker.Interface
0000000000000000000000000000000000000000;;		activePodsLister activePodsLister
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewNvidiaGPUManager returns a GPUManager that manages local Nvidia GPUs.
0000000000000000000000000000000000000000;;	// TODO: Migrate to use pod level cgroups and make it generic to all runtimes.
0000000000000000000000000000000000000000;;	func NewNvidiaGPUManager(activePodsLister activePodsLister, dockerClient libdocker.Interface) (gpu.GPUManager, error) {
0000000000000000000000000000000000000000;;		if dockerClient == nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("invalid docker client specified")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return &nvidiaGPUManager{
0000000000000000000000000000000000000000;;			allGPUs:          sets.NewString(),
0000000000000000000000000000000000000000;;			dockerClient:     dockerClient,
0000000000000000000000000000000000000000;;			activePodsLister: activePodsLister,
0000000000000000000000000000000000000000;;		}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Initialize the GPU devices, so far only needed to discover the GPU paths.
0000000000000000000000000000000000000000;;	func (ngm *nvidiaGPUManager) Start() error {
0000000000000000000000000000000000000000;;		if ngm.dockerClient == nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Invalid docker client specified in GPU Manager")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ngm.Lock()
0000000000000000000000000000000000000000;;		defer ngm.Unlock()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if _, err := os.Stat(nvidiaCtlDevice); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if _, err := os.Stat(nvidiaUVMDevice); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ngm.defaultDevices = []string{nvidiaCtlDevice, nvidiaUVMDevice}
0000000000000000000000000000000000000000;;		_, err := os.Stat(nvidiaUVMToolsDevice)
0000000000000000000000000000000000000000;;		if !os.IsNotExist(err) {
0000000000000000000000000000000000000000;;			ngm.defaultDevices = append(ngm.defaultDevices, nvidiaUVMToolsDevice)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err := ngm.discoverGPUs(); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// We ignore errors when identifying allocated GPUs because it is possible that the runtime interfaces may be not be logically up.
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Get how many GPU cards we have.
0000000000000000000000000000000000000000;;	func (ngm *nvidiaGPUManager) Capacity() v1.ResourceList {
0000000000000000000000000000000000000000;;		gpus := resource.NewQuantity(int64(len(ngm.allGPUs)), resource.DecimalSI)
0000000000000000000000000000000000000000;;		return v1.ResourceList{
0000000000000000000000000000000000000000;;			v1.ResourceNvidiaGPU: *gpus,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// AllocateGPUs returns `num` GPUs if available, error otherwise.
0000000000000000000000000000000000000000;;	// Allocation is made thread safe using the following logic.
0000000000000000000000000000000000000000;;	// A list of all GPUs allocated is maintained along with their respective Pod UIDs.
0000000000000000000000000000000000000000;;	// It is expected that the list of active pods will not return any false positives.
0000000000000000000000000000000000000000;;	// As part of initialization or allocation, the list of GPUs in use will be computed once.
0000000000000000000000000000000000000000;;	// Whenever an allocation happens, the list of GPUs allocated is updated based on the list of currently active pods.
0000000000000000000000000000000000000000;;	// GPUs allocated to terminated pods are freed up lazily as part of allocation.
0000000000000000000000000000000000000000;;	// GPUs are allocated based on the internal list of allocatedGPUs.
0000000000000000000000000000000000000000;;	// It is not safe to generate a list of GPUs in use by inspecting active containers because of the delay between GPU allocation and container creation.
0000000000000000000000000000000000000000;;	// A GPU allocated to a container might be re-allocated to a subsequent container because the original container wasn't started quick enough.
0000000000000000000000000000000000000000;;	// The current algorithm scans containers only once and then uses a list of active pods to track GPU usage.
0000000000000000000000000000000000000000;;	// This is a sub-optimal solution and a better alternative would be that of using pod level cgroups instead.
0000000000000000000000000000000000000000;;	// GPUs allocated to containers should be reflected in pod level device cgroups before completing allocations.
0000000000000000000000000000000000000000;;	// The pod level cgroups will then serve as a checkpoint of GPUs in use.
0000000000000000000000000000000000000000;;	func (ngm *nvidiaGPUManager) AllocateGPU(pod *v1.Pod, container *v1.Container) ([]string, error) {
0000000000000000000000000000000000000000;;		gpusNeeded := container.Resources.Limits.NvidiaGPU().Value()
0000000000000000000000000000000000000000;;		if gpusNeeded == 0 {
0000000000000000000000000000000000000000;;			return []string{}, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ngm.Lock()
0000000000000000000000000000000000000000;;		defer ngm.Unlock()
0000000000000000000000000000000000000000;;		if ngm.allocated == nil {
0000000000000000000000000000000000000000;;			// Initialization is not complete. Try now. Failures can no longer be tolerated.
0000000000000000000000000000000000000000;;			ngm.allocated = ngm.gpusInUse()
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// update internal list of GPUs in use prior to allocating new GPUs.
0000000000000000000000000000000000000000;;			ngm.updateAllocatedGPUs()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Check if GPUs have already been allocated. If so return them right away.
0000000000000000000000000000000000000000;;		// This can happen if a container restarts for example.
0000000000000000000000000000000000000000;;		if devices := ngm.allocated.getGPUs(string(pod.UID), container.Name); devices != nil {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("Found pre-allocated GPUs for container %q in Pod %q: %v", container.Name, pod.UID, devices.List())
0000000000000000000000000000000000000000;;			return append(devices.List(), ngm.defaultDevices...), nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Get GPU devices in use.
0000000000000000000000000000000000000000;;		devicesInUse := ngm.allocated.devices()
0000000000000000000000000000000000000000;;		glog.V(5).Infof("gpus in use: %v", devicesInUse.List())
0000000000000000000000000000000000000000;;		// Get a list of available GPUs.
0000000000000000000000000000000000000000;;		available := ngm.allGPUs.Difference(devicesInUse)
0000000000000000000000000000000000000000;;		glog.V(5).Infof("gpus available: %v", available.List())
0000000000000000000000000000000000000000;;		if int64(available.Len()) < gpusNeeded {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("requested number of GPUs unavailable. Requested: %d, Available: %d", gpusNeeded, available.Len())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ret := available.UnsortedList()[:gpusNeeded]
0000000000000000000000000000000000000000;;		for _, device := range ret {
0000000000000000000000000000000000000000;;			// Update internal allocated GPU cache.
0000000000000000000000000000000000000000;;			ngm.allocated.insert(string(pod.UID), container.Name, device)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Add standard devices files that needs to be exposed.
0000000000000000000000000000000000000000;;		ret = append(ret, ngm.defaultDevices...)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return ret, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// updateAllocatedGPUs updates the list of GPUs in use.
0000000000000000000000000000000000000000;;	// It gets a list of active pods and then frees any GPUs that are bound to terminated pods.
0000000000000000000000000000000000000000;;	// Returns error on failure.
0000000000000000000000000000000000000000;;	func (ngm *nvidiaGPUManager) updateAllocatedGPUs() {
0000000000000000000000000000000000000000;;		activePods := ngm.activePodsLister.GetActivePods()
0000000000000000000000000000000000000000;;		activePodUids := sets.NewString()
0000000000000000000000000000000000000000;;		for _, pod := range activePods {
0000000000000000000000000000000000000000;;			activePodUids.Insert(string(pod.UID))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		allocatedPodUids := ngm.allocated.pods()
0000000000000000000000000000000000000000;;		podsToBeRemoved := allocatedPodUids.Difference(activePodUids)
0000000000000000000000000000000000000000;;		glog.V(5).Infof("pods to be removed: %v", podsToBeRemoved.List())
0000000000000000000000000000000000000000;;		ngm.allocated.delete(podsToBeRemoved.List())
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// discoverGPUs identifies allGPUs NVIDIA GPU devices available on the local node by walking `/dev` directory.
0000000000000000000000000000000000000000;;	// TODO: Without NVML support we only can check whether there has GPU devices, but
0000000000000000000000000000000000000000;;	// could not give a health check or get more information like GPU cores, memory, or
0000000000000000000000000000000000000000;;	// family name. Need to support NVML in the future. But we do not need NVML until
0000000000000000000000000000000000000000;;	// we want more features, features like schedule containers according to GPU family
0000000000000000000000000000000000000000;;	// name.
0000000000000000000000000000000000000000;;	func (ngm *nvidiaGPUManager) discoverGPUs() error {
0000000000000000000000000000000000000000;;		reg := regexp.MustCompile(nvidiaDeviceRE)
0000000000000000000000000000000000000000;;		files, err := ioutil.ReadDir(devDirectory)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for _, f := range files {
0000000000000000000000000000000000000000;;			if f.IsDir() {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if reg.MatchString(f.Name()) {
0000000000000000000000000000000000000000;;				glog.V(2).Infof("Found Nvidia GPU %q", f.Name())
0000000000000000000000000000000000000000;;				ngm.allGPUs.Insert(path.Join(devDirectory, f.Name()))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// gpusInUse returns a list of GPUs in use along with the respective pods that are using it.
0000000000000000000000000000000000000000;;	func (ngm *nvidiaGPUManager) gpusInUse() *podGPUs {
0000000000000000000000000000000000000000;;		pods := ngm.activePodsLister.GetActivePods()
0000000000000000000000000000000000000000;;		type containerIdentifier struct {
0000000000000000000000000000000000000000;;			id   string
0000000000000000000000000000000000000000;;			name string
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		type podContainers struct {
0000000000000000000000000000000000000000;;			uid        string
0000000000000000000000000000000000000000;;			containers []containerIdentifier
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// List of containers to inspect.
0000000000000000000000000000000000000000;;		podContainersToInspect := []podContainers{}
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			containers := sets.NewString()
0000000000000000000000000000000000000000;;			for _, container := range pod.Spec.Containers {
0000000000000000000000000000000000000000;;				// GPUs are expected to be specified only in limits.
0000000000000000000000000000000000000000;;				if !container.Resources.Limits.NvidiaGPU().IsZero() {
0000000000000000000000000000000000000000;;					containers.Insert(container.Name)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// If no GPUs were requested skip this pod.
0000000000000000000000000000000000000000;;			if containers.Len() == 0 {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// TODO: If kubelet restarts right after allocating a GPU to a pod, the container might not have started yet and so container status might not be available yet.
0000000000000000000000000000000000000000;;			// Use an internal checkpoint instead or try using the CRI if its checkpoint is reliable.
0000000000000000000000000000000000000000;;			var containersToInspect []containerIdentifier
0000000000000000000000000000000000000000;;			for _, container := range pod.Status.ContainerStatuses {
0000000000000000000000000000000000000000;;				if containers.Has(container.Name) {
0000000000000000000000000000000000000000;;					containersToInspect = append(containersToInspect, containerIdentifier{strings.Replace(container.ContainerID, "docker://", "", 1), container.Name})
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// add the pod and its containers that need to be inspected.
0000000000000000000000000000000000000000;;			podContainersToInspect = append(podContainersToInspect, podContainers{string(pod.UID), containersToInspect})
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		ret := newPodGPUs()
0000000000000000000000000000000000000000;;		for _, podContainer := range podContainersToInspect {
0000000000000000000000000000000000000000;;			for _, containerIdentifier := range podContainer.containers {
0000000000000000000000000000000000000000;;				containerJSON, err := ngm.dockerClient.InspectContainer(containerIdentifier.id)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					glog.V(3).Infof("Failed to inspect container %q in pod %q while attempting to reconcile nvidia gpus in use", containerIdentifier.id, podContainer.uid)
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				devices := containerJSON.HostConfig.Devices
0000000000000000000000000000000000000000;;				if devices == nil {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				for _, device := range devices {
0000000000000000000000000000000000000000;;					if isValidPath(device.PathOnHost) {
0000000000000000000000000000000000000000;;						glog.V(4).Infof("Nvidia GPU %q is in use by Docker Container: %q", device.PathOnHost, containerJSON.ID)
0000000000000000000000000000000000000000;;						ret.insert(podContainer.uid, containerIdentifier.name, device.PathOnHost)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return ret
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func isValidPath(path string) bool {
0000000000000000000000000000000000000000;;		return regexp.MustCompile(nvidiaFullpathRE).MatchString(path)
0000000000000000000000000000000000000000;;	}
