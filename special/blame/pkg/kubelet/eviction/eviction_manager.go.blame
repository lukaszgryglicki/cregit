0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
164ccd9ffae9b8f5c6e7dc54a3e47e56446b691d;pkg/kubelet/eviction/manager.go[pkg/kubelet/eviction/manager.go][pkg/kubelet/eviction/eviction_manager.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package eviction
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		clientv1 "k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/resource"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/clock"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		utilfeature "k8s.io/apiserver/pkg/util/feature"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		v1qos "k8s.io/kubernetes/pkg/api/v1/helper/qos"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/features"
0000000000000000000000000000000000000000;;		statsapi "k8s.io/kubernetes/pkg/kubelet/apis/stats/v1alpha1"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/cm"
0000000000000000000000000000000000000000;;		evictionapi "k8s.io/kubernetes/pkg/kubelet/eviction/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/lifecycle"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/metrics"
0000000000000000000000000000000000000000;;		kubepod "k8s.io/kubernetes/pkg/kubelet/pod"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/server/stats"
0000000000000000000000000000000000000000;;		kubelettypes "k8s.io/kubernetes/pkg/kubelet/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/format"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		podCleanupTimeout  = 30 * time.Second
0000000000000000000000000000000000000000;;		podCleanupPollFreq = time.Second
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// managerImpl implements Manager
0000000000000000000000000000000000000000;;	type managerImpl struct {
0000000000000000000000000000000000000000;;		//  used to track time
0000000000000000000000000000000000000000;;		clock clock.Clock
0000000000000000000000000000000000000000;;		// config is how the manager is configured
0000000000000000000000000000000000000000;;		config Config
0000000000000000000000000000000000000000;;		// the function to invoke to kill a pod
0000000000000000000000000000000000000000;;		killPodFunc KillPodFunc
0000000000000000000000000000000000000000;;		// the interface that knows how to do image gc
0000000000000000000000000000000000000000;;		imageGC ImageGC
0000000000000000000000000000000000000000;;		// the interface that knows how to do image gc
0000000000000000000000000000000000000000;;		containerGC ContainerGC
0000000000000000000000000000000000000000;;		// protects access to internal state
0000000000000000000000000000000000000000;;		sync.RWMutex
0000000000000000000000000000000000000000;;		// node conditions are the set of conditions present
0000000000000000000000000000000000000000;;		nodeConditions []v1.NodeConditionType
0000000000000000000000000000000000000000;;		// captures when a node condition was last observed based on a threshold being met
0000000000000000000000000000000000000000;;		nodeConditionsLastObservedAt nodeConditionsObservedAt
0000000000000000000000000000000000000000;;		// nodeRef is a reference to the node
0000000000000000000000000000000000000000;;		nodeRef *clientv1.ObjectReference
0000000000000000000000000000000000000000;;		// used to record events about the node
0000000000000000000000000000000000000000;;		recorder record.EventRecorder
0000000000000000000000000000000000000000;;		// used to measure usage stats on system
0000000000000000000000000000000000000000;;		summaryProvider stats.SummaryProvider
0000000000000000000000000000000000000000;;		// records when a threshold was first observed
0000000000000000000000000000000000000000;;		thresholdsFirstObservedAt thresholdsObservedAt
0000000000000000000000000000000000000000;;		// records the set of thresholds that have been met (including graceperiod) but not yet resolved
0000000000000000000000000000000000000000;;		thresholdsMet []evictionapi.Threshold
0000000000000000000000000000000000000000;;		// resourceToRankFunc maps a resource to ranking function for that resource.
0000000000000000000000000000000000000000;;		resourceToRankFunc map[v1.ResourceName]rankFunc
0000000000000000000000000000000000000000;;		// resourceToNodeReclaimFuncs maps a resource to an ordered list of functions that know how to reclaim that resource.
0000000000000000000000000000000000000000;;		resourceToNodeReclaimFuncs map[v1.ResourceName]nodeReclaimFuncs
0000000000000000000000000000000000000000;;		// last observations from synchronize
0000000000000000000000000000000000000000;;		lastObservations signalObservations
0000000000000000000000000000000000000000;;		// notifiersInitialized indicates if the threshold notifiers have been initialized (i.e. synchronize() has been called once)
0000000000000000000000000000000000000000;;		notifiersInitialized bool
0000000000000000000000000000000000000000;;		// dedicatedImageFs indicates if imagefs is on a separate device from the rootfs
0000000000000000000000000000000000000000;;		dedicatedImageFs *bool
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// ensure it implements the required interface
0000000000000000000000000000000000000000;;	var _ Manager = &managerImpl{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewManager returns a configured Manager and an associated admission handler to enforce eviction configuration.
0000000000000000000000000000000000000000;;	func NewManager(
0000000000000000000000000000000000000000;;		summaryProvider stats.SummaryProvider,
0000000000000000000000000000000000000000;;		config Config,
0000000000000000000000000000000000000000;;		killPodFunc KillPodFunc,
0000000000000000000000000000000000000000;;		imageGC ImageGC,
0000000000000000000000000000000000000000;;		containerGC ContainerGC,
0000000000000000000000000000000000000000;;		recorder record.EventRecorder,
0000000000000000000000000000000000000000;;		nodeRef *clientv1.ObjectReference,
0000000000000000000000000000000000000000;;		clock clock.Clock) (Manager, lifecycle.PodAdmitHandler) {
0000000000000000000000000000000000000000;;		manager := &managerImpl{
0000000000000000000000000000000000000000;;			clock:           clock,
0000000000000000000000000000000000000000;;			killPodFunc:     killPodFunc,
0000000000000000000000000000000000000000;;			imageGC:         imageGC,
0000000000000000000000000000000000000000;;			containerGC:     containerGC,
0000000000000000000000000000000000000000;;			config:          config,
0000000000000000000000000000000000000000;;			recorder:        recorder,
0000000000000000000000000000000000000000;;			summaryProvider: summaryProvider,
0000000000000000000000000000000000000000;;			nodeRef:         nodeRef,
0000000000000000000000000000000000000000;;			nodeConditionsLastObservedAt: nodeConditionsObservedAt{},
0000000000000000000000000000000000000000;;			thresholdsFirstObservedAt:    thresholdsObservedAt{},
0000000000000000000000000000000000000000;;			dedicatedImageFs:             nil,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return manager, manager
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Admit rejects a pod if its not safe to admit for node stability.
0000000000000000000000000000000000000000;;	func (m *managerImpl) Admit(attrs *lifecycle.PodAdmitAttributes) lifecycle.PodAdmitResult {
0000000000000000000000000000000000000000;;		m.RLock()
0000000000000000000000000000000000000000;;		defer m.RUnlock()
0000000000000000000000000000000000000000;;		if len(m.nodeConditions) == 0 {
0000000000000000000000000000000000000000;;			return lifecycle.PodAdmitResult{Admit: true}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Admit Critical pods even under resource pressure since they are required for system stability.
0000000000000000000000000000000000000000;;		// https://github.com/kubernetes/kubernetes/issues/40573 has more details.
0000000000000000000000000000000000000000;;		if utilfeature.DefaultFeatureGate.Enabled(features.ExperimentalCriticalPodAnnotation) && kubelettypes.IsCriticalPod(attrs.Pod) {
0000000000000000000000000000000000000000;;			return lifecycle.PodAdmitResult{Admit: true}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// the node has memory pressure, admit if not best-effort
0000000000000000000000000000000000000000;;		if hasNodeCondition(m.nodeConditions, v1.NodeMemoryPressure) {
0000000000000000000000000000000000000000;;			notBestEffort := v1.PodQOSBestEffort != v1qos.GetPodQOS(attrs.Pod)
0000000000000000000000000000000000000000;;			if notBestEffort {
0000000000000000000000000000000000000000;;				return lifecycle.PodAdmitResult{Admit: true}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// reject pods when under memory pressure (if pod is best effort), or if under disk pressure.
0000000000000000000000000000000000000000;;		glog.Warningf("Failed to admit pod %s - node has conditions: %v", format.Pod(attrs.Pod), m.nodeConditions)
0000000000000000000000000000000000000000;;		return lifecycle.PodAdmitResult{
0000000000000000000000000000000000000000;;			Admit:   false,
0000000000000000000000000000000000000000;;			Reason:  reason,
0000000000000000000000000000000000000000;;			Message: fmt.Sprintf(message, m.nodeConditions),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Start starts the control loop to observe and response to low compute resources.
0000000000000000000000000000000000000000;;	func (m *managerImpl) Start(diskInfoProvider DiskInfoProvider, podFunc ActivePodsFunc, podCleanedUpFunc PodCleanedUpFunc, nodeProvider NodeProvider, monitoringInterval time.Duration) {
0000000000000000000000000000000000000000;;		// start the eviction manager monitoring
0000000000000000000000000000000000000000;;		go func() {
0000000000000000000000000000000000000000;;			for {
0000000000000000000000000000000000000000;;				if evictedPods := m.synchronize(diskInfoProvider, podFunc, nodeProvider); evictedPods != nil {
0000000000000000000000000000000000000000;;					glog.Infof("eviction manager: pods %s evicted, waiting for pod to be cleaned up", format.Pods(evictedPods))
0000000000000000000000000000000000000000;;					m.waitForPodsCleanup(podCleanedUpFunc, evictedPods)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					time.Sleep(monitoringInterval)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// IsUnderMemoryPressure returns true if the node is under memory pressure.
0000000000000000000000000000000000000000;;	func (m *managerImpl) IsUnderMemoryPressure() bool {
0000000000000000000000000000000000000000;;		m.RLock()
0000000000000000000000000000000000000000;;		defer m.RUnlock()
0000000000000000000000000000000000000000;;		return hasNodeCondition(m.nodeConditions, v1.NodeMemoryPressure)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// IsUnderDiskPressure returns true if the node is under disk pressure.
0000000000000000000000000000000000000000;;	func (m *managerImpl) IsUnderDiskPressure() bool {
0000000000000000000000000000000000000000;;		m.RLock()
0000000000000000000000000000000000000000;;		defer m.RUnlock()
0000000000000000000000000000000000000000;;		return hasNodeCondition(m.nodeConditions, v1.NodeDiskPressure)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func startMemoryThresholdNotifier(thresholds []evictionapi.Threshold, observations signalObservations, hard bool, handler thresholdNotifierHandlerFunc) error {
0000000000000000000000000000000000000000;;		for _, threshold := range thresholds {
0000000000000000000000000000000000000000;;			if threshold.Signal != evictionapi.SignalMemoryAvailable || hard != isHardEvictionThreshold(threshold) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			observed, found := observations[evictionapi.SignalMemoryAvailable]
0000000000000000000000000000000000000000;;			if !found {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			cgroups, err := cm.GetCgroupSubsystems()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// TODO add support for eviction from --cgroup-root
0000000000000000000000000000000000000000;;			cgpath, found := cgroups.MountPoints["memory"]
0000000000000000000000000000000000000000;;			if !found || len(cgpath) == 0 {
0000000000000000000000000000000000000000;;				return fmt.Errorf("memory cgroup mount point not found")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			attribute := "memory.usage_in_bytes"
0000000000000000000000000000000000000000;;			quantity := evictionapi.GetThresholdQuantity(threshold.Value, observed.capacity)
0000000000000000000000000000000000000000;;			usageThreshold := resource.NewQuantity(observed.capacity.Value(), resource.DecimalSI)
0000000000000000000000000000000000000000;;			usageThreshold.Sub(*quantity)
0000000000000000000000000000000000000000;;			description := fmt.Sprintf("<%s available", formatThresholdValue(threshold.Value))
0000000000000000000000000000000000000000;;			memcgThresholdNotifier, err := NewMemCGThresholdNotifier(cgpath, attribute, usageThreshold.String(), description, handler)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			go memcgThresholdNotifier.Start(wait.NeverStop)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// synchronize is the main control loop that enforces eviction thresholds.
0000000000000000000000000000000000000000;;	// Returns the pod that was killed, or nil if no pod was killed.
0000000000000000000000000000000000000000;;	func (m *managerImpl) synchronize(diskInfoProvider DiskInfoProvider, podFunc ActivePodsFunc, nodeProvider NodeProvider) []*v1.Pod {
0000000000000000000000000000000000000000;;		// if we have nothing to do, just return
0000000000000000000000000000000000000000;;		thresholds := m.config.Thresholds
0000000000000000000000000000000000000000;;		if len(thresholds) == 0 {
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(3).Infof("eviction manager: synchronize housekeeping")
0000000000000000000000000000000000000000;;		// build the ranking functions (if not yet known)
0000000000000000000000000000000000000000;;		// TODO: have a function in cadvisor that lets us know if global housekeeping has completed
0000000000000000000000000000000000000000;;		if m.dedicatedImageFs == nil {
0000000000000000000000000000000000000000;;			hasImageFs, ok := diskInfoProvider.HasDedicatedImageFs()
0000000000000000000000000000000000000000;;			if ok != nil {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			m.dedicatedImageFs = &hasImageFs
0000000000000000000000000000000000000000;;			m.resourceToRankFunc = buildResourceToRankFunc(hasImageFs)
0000000000000000000000000000000000000000;;			m.resourceToNodeReclaimFuncs = buildResourceToNodeReclaimFuncs(m.imageGC, m.containerGC, hasImageFs)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		activePods := podFunc()
0000000000000000000000000000000000000000;;		// make observations and get a function to derive pod usage stats relative to those observations.
0000000000000000000000000000000000000000;;		observations, statsFunc, err := makeSignalObservations(m.summaryProvider, nodeProvider, activePods, *m.dedicatedImageFs)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("eviction manager: unexpected err: %v", err)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		debugLogObservations("observations", observations)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// attempt to create a threshold notifier to improve eviction response time
0000000000000000000000000000000000000000;;		if m.config.KernelMemcgNotification && !m.notifiersInitialized {
0000000000000000000000000000000000000000;;			glog.Infof("eviction manager attempting to integrate with kernel memcg notification api")
0000000000000000000000000000000000000000;;			m.notifiersInitialized = true
0000000000000000000000000000000000000000;;			// start soft memory notification
0000000000000000000000000000000000000000;;			err = startMemoryThresholdNotifier(m.config.Thresholds, observations, false, func(desc string) {
0000000000000000000000000000000000000000;;				glog.Infof("soft memory eviction threshold crossed at %s", desc)
0000000000000000000000000000000000000000;;				// TODO wait grace period for soft memory limit
0000000000000000000000000000000000000000;;				m.synchronize(diskInfoProvider, podFunc, nodeProvider)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Warningf("eviction manager: failed to create hard memory threshold notifier: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// start hard memory notification
0000000000000000000000000000000000000000;;			err = startMemoryThresholdNotifier(m.config.Thresholds, observations, true, func(desc string) {
0000000000000000000000000000000000000000;;				glog.Infof("hard memory eviction threshold crossed at %s", desc)
0000000000000000000000000000000000000000;;				m.synchronize(diskInfoProvider, podFunc, nodeProvider)
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Warningf("eviction manager: failed to create soft memory threshold notifier: %v", err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// determine the set of thresholds met independent of grace period
0000000000000000000000000000000000000000;;		thresholds = thresholdsMet(thresholds, observations, false)
0000000000000000000000000000000000000000;;		debugLogThresholdsWithObservation("thresholds - ignoring grace period", thresholds, observations)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// determine the set of thresholds previously met that have not yet satisfied the associated min-reclaim
0000000000000000000000000000000000000000;;		if len(m.thresholdsMet) > 0 {
0000000000000000000000000000000000000000;;			thresholdsNotYetResolved := thresholdsMet(m.thresholdsMet, observations, true)
0000000000000000000000000000000000000000;;			thresholds = mergeThresholds(thresholds, thresholdsNotYetResolved)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		debugLogThresholdsWithObservation("thresholds - reclaim not satisfied", thresholds, observations)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// determine the set of thresholds whose stats have been updated since the last sync
0000000000000000000000000000000000000000;;		thresholds = thresholdsUpdatedStats(thresholds, observations, m.lastObservations)
0000000000000000000000000000000000000000;;		debugLogThresholdsWithObservation("thresholds - updated stats", thresholds, observations)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// track when a threshold was first observed
0000000000000000000000000000000000000000;;		now := m.clock.Now()
0000000000000000000000000000000000000000;;		thresholdsFirstObservedAt := thresholdsFirstObservedAt(thresholds, m.thresholdsFirstObservedAt, now)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// the set of node conditions that are triggered by currently observed thresholds
0000000000000000000000000000000000000000;;		nodeConditions := nodeConditions(thresholds)
0000000000000000000000000000000000000000;;		if len(nodeConditions) > 0 {
0000000000000000000000000000000000000000;;			glog.V(3).Infof("eviction manager: node conditions - observed: %v", nodeConditions)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// track when a node condition was last observed
0000000000000000000000000000000000000000;;		nodeConditionsLastObservedAt := nodeConditionsLastObservedAt(nodeConditions, m.nodeConditionsLastObservedAt, now)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// node conditions report true if it has been observed within the transition period window
0000000000000000000000000000000000000000;;		nodeConditions = nodeConditionsObservedSince(nodeConditionsLastObservedAt, m.config.PressureTransitionPeriod, now)
0000000000000000000000000000000000000000;;		if len(nodeConditions) > 0 {
0000000000000000000000000000000000000000;;			glog.V(3).Infof("eviction manager: node conditions - transition period not met: %v", nodeConditions)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// determine the set of thresholds we need to drive eviction behavior (i.e. all grace periods are met)
0000000000000000000000000000000000000000;;		thresholds = thresholdsMetGracePeriod(thresholdsFirstObservedAt, now)
0000000000000000000000000000000000000000;;		debugLogThresholdsWithObservation("thresholds - grace periods satisified", thresholds, observations)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// update internal state
0000000000000000000000000000000000000000;;		m.Lock()
0000000000000000000000000000000000000000;;		m.nodeConditions = nodeConditions
0000000000000000000000000000000000000000;;		m.thresholdsFirstObservedAt = thresholdsFirstObservedAt
0000000000000000000000000000000000000000;;		m.nodeConditionsLastObservedAt = nodeConditionsLastObservedAt
0000000000000000000000000000000000000000;;		m.thresholdsMet = thresholds
0000000000000000000000000000000000000000;;		m.lastObservations = observations
0000000000000000000000000000000000000000;;		m.Unlock()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// evict pods if there is a resource uage violation from local volume temporary storage
0000000000000000000000000000000000000000;;		// If eviction happenes in localVolumeEviction function, skip the rest of eviction action
0000000000000000000000000000000000000000;;		if utilfeature.DefaultFeatureGate.Enabled(features.LocalStorageCapacityIsolation) {
0000000000000000000000000000000000000000;;			if evictedPods := m.localStorageEviction(activePods); len(evictedPods) > 0 {
0000000000000000000000000000000000000000;;				return evictedPods
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// determine the set of resources under starvation
0000000000000000000000000000000000000000;;		starvedResources := getStarvedResources(thresholds)
0000000000000000000000000000000000000000;;		if len(starvedResources) == 0 {
0000000000000000000000000000000000000000;;			glog.V(3).Infof("eviction manager: no resources are starved")
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// rank the resources to reclaim by eviction priority
0000000000000000000000000000000000000000;;		sort.Sort(byEvictionPriority(starvedResources))
0000000000000000000000000000000000000000;;		resourceToReclaim := starvedResources[0]
0000000000000000000000000000000000000000;;		glog.Warningf("eviction manager: attempting to reclaim %v", resourceToReclaim)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// determine if this is a soft or hard eviction associated with the resource
0000000000000000000000000000000000000000;;		softEviction := isSoftEvictionThresholds(thresholds, resourceToReclaim)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// record an event about the resources we are now attempting to reclaim via eviction
0000000000000000000000000000000000000000;;		m.recorder.Eventf(m.nodeRef, v1.EventTypeWarning, "EvictionThresholdMet", "Attempting to reclaim %s", resourceToReclaim)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// check if there are node-level resources we can reclaim to reduce pressure before evicting end-user pods.
0000000000000000000000000000000000000000;;		if m.reclaimNodeLevelResources(resourceToReclaim, observations) {
0000000000000000000000000000000000000000;;			glog.Infof("eviction manager: able to reduce %v pressure without evicting pods.", resourceToReclaim)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.Infof("eviction manager: must evict pod(s) to reclaim %v", resourceToReclaim)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// rank the pods for eviction
0000000000000000000000000000000000000000;;		rank, ok := m.resourceToRankFunc[resourceToReclaim]
0000000000000000000000000000000000000000;;		if !ok {
0000000000000000000000000000000000000000;;			glog.Errorf("eviction manager: no ranking function for resource %s", resourceToReclaim)
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// the only candidates viable for eviction are those pods that had anything running.
0000000000000000000000000000000000000000;;		if len(activePods) == 0 {
0000000000000000000000000000000000000000;;			glog.Errorf("eviction manager: eviction thresholds have been met, but no pods are active to evict")
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// rank the running pods for eviction for the specified resource
0000000000000000000000000000000000000000;;		rank(activePods, statsFunc)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.Infof("eviction manager: pods ranked for eviction: %s", format.Pods(activePods))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		//record age of metrics for met thresholds that we are using for evictions.
0000000000000000000000000000000000000000;;		for _, t := range thresholds {
0000000000000000000000000000000000000000;;			timeObserved := observations[t.Signal].time
0000000000000000000000000000000000000000;;			if !timeObserved.IsZero() {
0000000000000000000000000000000000000000;;				metrics.EvictionStatsAge.WithLabelValues(string(t.Signal)).Observe(metrics.SinceInMicroseconds(timeObserved.Time))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// we kill at most a single pod during each eviction interval
0000000000000000000000000000000000000000;;		for i := range activePods {
0000000000000000000000000000000000000000;;			pod := activePods[i]
0000000000000000000000000000000000000000;;			// If the pod is marked as critical and static, and support for critical pod annotations is enabled,
0000000000000000000000000000000000000000;;			// do not evict such pods. Static pods are not re-admitted after evictions.
0000000000000000000000000000000000000000;;			// https://github.com/kubernetes/kubernetes/issues/40573 has more details.
0000000000000000000000000000000000000000;;			if utilfeature.DefaultFeatureGate.Enabled(features.ExperimentalCriticalPodAnnotation) &&
0000000000000000000000000000000000000000;;				kubelettypes.IsCriticalPod(pod) && kubepod.IsStaticPod(pod) {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			status := v1.PodStatus{
0000000000000000000000000000000000000000;;				Phase:   v1.PodFailed,
0000000000000000000000000000000000000000;;				Message: fmt.Sprintf(message, resourceToReclaim),
0000000000000000000000000000000000000000;;				Reason:  reason,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// record that we are evicting the pod
0000000000000000000000000000000000000000;;			m.recorder.Eventf(pod, v1.EventTypeWarning, reason, fmt.Sprintf(message, resourceToReclaim))
0000000000000000000000000000000000000000;;			gracePeriodOverride := int64(0)
0000000000000000000000000000000000000000;;			if softEviction {
0000000000000000000000000000000000000000;;				gracePeriodOverride = m.config.MaxPodGracePeriodSeconds
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// this is a blocking call and should only return when the pod and its containers are killed.
0000000000000000000000000000000000000000;;			err := m.killPodFunc(pod, status, &gracePeriodOverride)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Warningf("eviction manager: error while evicting pod %s: %v", format.Pod(pod), err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return []*v1.Pod{pod}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.Infof("eviction manager: unable to evict any pods from the node")
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *managerImpl) waitForPodsCleanup(podCleanedUpFunc PodCleanedUpFunc, pods []*v1.Pod) {
0000000000000000000000000000000000000000;;		timeout := m.clock.NewTimer(podCleanupTimeout)
0000000000000000000000000000000000000000;;		tick := m.clock.Tick(podCleanupPollFreq)
0000000000000000000000000000000000000000;;		for {
0000000000000000000000000000000000000000;;			select {
0000000000000000000000000000000000000000;;			case <-timeout.C():
0000000000000000000000000000000000000000;;				glog.Warningf("eviction manager: timed out waiting for pods %s to be cleaned up", format.Pods(pods))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			case <-tick:
0000000000000000000000000000000000000000;;				for i, pod := range pods {
0000000000000000000000000000000000000000;;					if !podCleanedUpFunc(pod) {
0000000000000000000000000000000000000000;;						break
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if i == len(pods)-1 {
0000000000000000000000000000000000000000;;						glog.Infof("eviction manager: pods %s successfully cleaned up", format.Pods(pods))
0000000000000000000000000000000000000000;;						return
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// reclaimNodeLevelResources attempts to reclaim node level resources.  returns true if thresholds were satisfied and no pod eviction is required.
0000000000000000000000000000000000000000;;	func (m *managerImpl) reclaimNodeLevelResources(resourceToReclaim v1.ResourceName, observations signalObservations) bool {
0000000000000000000000000000000000000000;;		nodeReclaimFuncs := m.resourceToNodeReclaimFuncs[resourceToReclaim]
0000000000000000000000000000000000000000;;		for _, nodeReclaimFunc := range nodeReclaimFuncs {
0000000000000000000000000000000000000000;;			// attempt to reclaim the pressured resource.
0000000000000000000000000000000000000000;;			reclaimed, err := nodeReclaimFunc()
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Warningf("eviction manager: unexpected error when attempting to reduce %v pressure: %v", resourceToReclaim, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// update our local observations based on the amount reported to have been reclaimed.
0000000000000000000000000000000000000000;;			// note: this is optimistic, other things could have been still consuming the pressured resource in the interim.
0000000000000000000000000000000000000000;;			signal := resourceToSignal[resourceToReclaim]
0000000000000000000000000000000000000000;;			value, ok := observations[signal]
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				glog.Errorf("eviction manager: unable to find value associated with signal %v", signal)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			value.available.Add(*reclaimed)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// evaluate all current thresholds to see if with adjusted observations, we think we have met min reclaim goals
0000000000000000000000000000000000000000;;			if len(thresholdsMet(m.thresholdsMet, observations, true)) == 0 {
0000000000000000000000000000000000000000;;				return true
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// localStorageEviction checks the EmptyDir volume usage for each pod and determine whether it exceeds the specified limit and needs
0000000000000000000000000000000000000000;;	// to be evicted. It also checks every container in the pod, if the container overlay usage exceeds the limit, the pod will be evicted too.
0000000000000000000000000000000000000000;;	func (m *managerImpl) localStorageEviction(pods []*v1.Pod) []*v1.Pod {
0000000000000000000000000000000000000000;;		summary, err := m.summaryProvider.Get()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Could not get summary provider")
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		statsFunc := cachedStatsFunc(summary.Pods)
0000000000000000000000000000000000000000;;		evicted := []*v1.Pod{}
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			podStats, ok := statsFunc(pod)
0000000000000000000000000000000000000000;;			if !ok {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if m.emptyDirLimitEviction(podStats, pod) {
0000000000000000000000000000000000000000;;				evicted = append(evicted, pod)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if m.containerOverlayLimitEviction(podStats, pod) {
0000000000000000000000000000000000000000;;				evicted = append(evicted, pod)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return evicted
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *managerImpl) emptyDirLimitEviction(podStats statsapi.PodStats, pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		podVolumeUsed := make(map[string]*resource.Quantity)
0000000000000000000000000000000000000000;;		for _, volume := range podStats.VolumeStats {
0000000000000000000000000000000000000000;;			podVolumeUsed[volume.Name] = resource.NewQuantity(int64(*volume.UsedBytes), resource.BinarySI)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := range pod.Spec.Volumes {
0000000000000000000000000000000000000000;;			source := &pod.Spec.Volumes[i].VolumeSource
0000000000000000000000000000000000000000;;			if source.EmptyDir != nil {
0000000000000000000000000000000000000000;;				size := source.EmptyDir.SizeLimit
0000000000000000000000000000000000000000;;				used := podVolumeUsed[pod.Spec.Volumes[i].Name]
0000000000000000000000000000000000000000;;				if used != nil && size.Sign() == 1 && used.Cmp(size) > 0 {
0000000000000000000000000000000000000000;;					// the emptyDir usage exceeds the size limit, evict the pod
0000000000000000000000000000000000000000;;					return m.evictPod(pod, v1.ResourceName("EmptyDir"), fmt.Sprintf("emptyDir usage exceeds the limit %q", size.String()))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *managerImpl) containerOverlayLimitEviction(podStats statsapi.PodStats, pod *v1.Pod) bool {
0000000000000000000000000000000000000000;;		thresholdsMap := make(map[string]*resource.Quantity)
0000000000000000000000000000000000000000;;		for _, container := range pod.Spec.Containers {
0000000000000000000000000000000000000000;;			overlayLimit := container.Resources.Limits.StorageOverlay()
0000000000000000000000000000000000000000;;			if overlayLimit != nil && overlayLimit.Value() != 0 {
0000000000000000000000000000000000000000;;				thresholdsMap[container.Name] = overlayLimit
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, containerStat := range podStats.Containers {
0000000000000000000000000000000000000000;;			rootfs := diskUsage(containerStat.Rootfs)
0000000000000000000000000000000000000000;;			if overlayThreshold, ok := thresholdsMap[containerStat.Name]; ok {
0000000000000000000000000000000000000000;;				if overlayThreshold.Cmp(*rootfs) < 0 {
0000000000000000000000000000000000000000;;					return m.evictPod(pod, v1.ResourceName("containerOverlay"), fmt.Sprintf("container's overlay usage exceeds the limit %q", overlayThreshold.String()))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *managerImpl) evictPod(pod *v1.Pod, resourceName v1.ResourceName, evictMsg string) bool {
0000000000000000000000000000000000000000;;		if utilfeature.DefaultFeatureGate.Enabled(features.ExperimentalCriticalPodAnnotation) &&
0000000000000000000000000000000000000000;;			kubelettypes.IsCriticalPod(pod) && kubepod.IsStaticPod(pod) {
0000000000000000000000000000000000000000;;			glog.Errorf("eviction manager: cannot evict a critical pod %s", format.Pod(pod))
0000000000000000000000000000000000000000;;			return false
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		status := v1.PodStatus{
0000000000000000000000000000000000000000;;			Phase:   v1.PodFailed,
0000000000000000000000000000000000000000;;			Message: fmt.Sprintf(message, resourceName),
0000000000000000000000000000000000000000;;			Reason:  reason,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// record that we are evicting the pod
0000000000000000000000000000000000000000;;		m.recorder.Eventf(pod, v1.EventTypeWarning, reason, evictMsg)
0000000000000000000000000000000000000000;;		gracePeriod := int64(0)
0000000000000000000000000000000000000000;;		err := m.killPodFunc(pod, status, &gracePeriod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("eviction manager: pod %s failed to evict %v", format.Pod(pod), err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.Infof("eviction manager: pod %s is evicted successfully", format.Pod(pod))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return true
0000000000000000000000000000000000000000;;	}
