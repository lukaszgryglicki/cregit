0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
5464e5a5d7a8fd40e9826bae7a1dc78c936c2109;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package kuberuntime
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"errors"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;		cadvisorapi "github.com/google/cadvisor/info/v1"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		kubetypes "k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		utilruntime "k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/tools/record"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/flowcontrol"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api/v1/ref"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/credentialprovider"
0000000000000000000000000000000000000000;;		internalapi "k8s.io/kubernetes/pkg/kubelet/apis/cri"
0000000000000000000000000000000000000000;;		runtimeapi "k8s.io/kubernetes/pkg/kubelet/apis/cri/v1alpha1/runtime"
0000000000000000000000000000000000000000;;		kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/events"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/images"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/lifecycle"
0000000000000000000000000000000000000000;;		proberesults "k8s.io/kubernetes/pkg/kubelet/prober/results"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/cache"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/format"
0000000000000000000000000000000000000000;;		utilversion "k8s.io/kubernetes/pkg/util/version"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// The api version of kubelet runtime api
0000000000000000000000000000000000000000;;		kubeRuntimeAPIVersion = "0.1.0"
0000000000000000000000000000000000000000;;		// The root directory for pod logs
0000000000000000000000000000000000000000;;		podLogsRootDirectory = "/var/log/pods"
0000000000000000000000000000000000000000;;		// A minimal shutdown window for avoiding unnecessary SIGKILLs
0000000000000000000000000000000000000000;;		minimumGracePeriodInSeconds = 2
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The expiration time of version cache.
0000000000000000000000000000000000000000;;		versionCacheTTL = 60 * time.Second
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var (
0000000000000000000000000000000000000000;;		// ErrVersionNotSupported is returned when the api version of runtime interface is not supported
0000000000000000000000000000000000000000;;		ErrVersionNotSupported = errors.New("Runtime api version is not supported")
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// A subset of the pod.Manager interface extracted for garbage collection purposes.
0000000000000000000000000000000000000000;;	type podGetter interface {
0000000000000000000000000000000000000000;;		GetPodByUID(kubetypes.UID) (*v1.Pod, bool)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type kubeGenericRuntimeManager struct {
0000000000000000000000000000000000000000;;		runtimeName         string
0000000000000000000000000000000000000000;;		recorder            record.EventRecorder
0000000000000000000000000000000000000000;;		osInterface         kubecontainer.OSInterface
0000000000000000000000000000000000000000;;		containerRefManager *kubecontainer.RefManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// machineInfo contains the machine information.
0000000000000000000000000000000000000000;;		machineInfo *cadvisorapi.MachineInfo
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Container GC manager
0000000000000000000000000000000000000000;;		containerGC *containerGC
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Keyring for pulling images
0000000000000000000000000000000000000000;;		keyring credentialprovider.DockerKeyring
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Runner of lifecycle events.
0000000000000000000000000000000000000000;;		runner kubecontainer.HandlerRunner
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// RuntimeHelper that wraps kubelet to generate runtime container options.
0000000000000000000000000000000000000000;;		runtimeHelper kubecontainer.RuntimeHelper
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Health check results.
0000000000000000000000000000000000000000;;		livenessManager proberesults.Manager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If true, enforce container cpu limits with CFS quota support
0000000000000000000000000000000000000000;;		cpuCFSQuota bool
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// wrapped image puller.
0000000000000000000000000000000000000000;;		imagePuller images.ImageManager
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// gRPC service clients
0000000000000000000000000000000000000000;;		runtimeService internalapi.RuntimeService
0000000000000000000000000000000000000000;;		imageService   internalapi.ImageManagerService
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The version cache of runtime daemon.
0000000000000000000000000000000000000000;;		versionCache *cache.ObjectCache
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type KubeGenericRuntime interface {
0000000000000000000000000000000000000000;;		kubecontainer.Runtime
0000000000000000000000000000000000000000;;		kubecontainer.IndirectStreamingRuntime
0000000000000000000000000000000000000000;;		kubecontainer.ContainerCommandRunner
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewKubeGenericRuntimeManager creates a new kubeGenericRuntimeManager
0000000000000000000000000000000000000000;;	func NewKubeGenericRuntimeManager(
0000000000000000000000000000000000000000;;		recorder record.EventRecorder,
0000000000000000000000000000000000000000;;		livenessManager proberesults.Manager,
0000000000000000000000000000000000000000;;		containerRefManager *kubecontainer.RefManager,
0000000000000000000000000000000000000000;;		machineInfo *cadvisorapi.MachineInfo,
0000000000000000000000000000000000000000;;		podGetter podGetter,
0000000000000000000000000000000000000000;;		osInterface kubecontainer.OSInterface,
0000000000000000000000000000000000000000;;		runtimeHelper kubecontainer.RuntimeHelper,
0000000000000000000000000000000000000000;;		httpClient types.HttpGetter,
0000000000000000000000000000000000000000;;		imageBackOff *flowcontrol.Backoff,
0000000000000000000000000000000000000000;;		serializeImagePulls bool,
0000000000000000000000000000000000000000;;		imagePullQPS float32,
0000000000000000000000000000000000000000;;		imagePullBurst int,
0000000000000000000000000000000000000000;;		cpuCFSQuota bool,
0000000000000000000000000000000000000000;;		runtimeService internalapi.RuntimeService,
0000000000000000000000000000000000000000;;		imageService internalapi.ImageManagerService,
0000000000000000000000000000000000000000;;	) (KubeGenericRuntime, error) {
0000000000000000000000000000000000000000;;		kubeRuntimeManager := &kubeGenericRuntimeManager{
0000000000000000000000000000000000000000;;			recorder:            recorder,
0000000000000000000000000000000000000000;;			cpuCFSQuota:         cpuCFSQuota,
0000000000000000000000000000000000000000;;			livenessManager:     livenessManager,
0000000000000000000000000000000000000000;;			containerRefManager: containerRefManager,
0000000000000000000000000000000000000000;;			machineInfo:         machineInfo,
0000000000000000000000000000000000000000;;			osInterface:         osInterface,
0000000000000000000000000000000000000000;;			runtimeHelper:       runtimeHelper,
0000000000000000000000000000000000000000;;			runtimeService:      newInstrumentedRuntimeService(runtimeService),
0000000000000000000000000000000000000000;;			imageService:        newInstrumentedImageManagerService(imageService),
0000000000000000000000000000000000000000;;			keyring:             credentialprovider.NewDockerKeyring(),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		typedVersion, err := kubeRuntimeManager.runtimeService.Version(kubeRuntimeAPIVersion)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Get runtime version failed: %v", err)
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Only matching kubeRuntimeAPIVersion is supported now
0000000000000000000000000000000000000000;;		// TODO: Runtime API machinery is under discussion at https://github.com/kubernetes/kubernetes/issues/28642
0000000000000000000000000000000000000000;;		if typedVersion.Version != kubeRuntimeAPIVersion {
0000000000000000000000000000000000000000;;			glog.Errorf("Runtime api version %s is not supported, only %s is supported now",
0000000000000000000000000000000000000000;;				typedVersion.Version,
0000000000000000000000000000000000000000;;				kubeRuntimeAPIVersion)
0000000000000000000000000000000000000000;;			return nil, ErrVersionNotSupported
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubeRuntimeManager.runtimeName = typedVersion.RuntimeName
0000000000000000000000000000000000000000;;		glog.Infof("Container runtime %s initialized, version: %s, apiVersion: %s",
0000000000000000000000000000000000000000;;			typedVersion.RuntimeName,
0000000000000000000000000000000000000000;;			typedVersion.RuntimeVersion,
0000000000000000000000000000000000000000;;			typedVersion.RuntimeApiVersion)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If the container logs directory does not exist, create it.
0000000000000000000000000000000000000000;;		// TODO: create podLogsRootDirectory at kubelet.go when kubelet is refactored to
0000000000000000000000000000000000000000;;		// new runtime interface
0000000000000000000000000000000000000000;;		if _, err := osInterface.Stat(podLogsRootDirectory); os.IsNotExist(err) {
0000000000000000000000000000000000000000;;			if err := osInterface.MkdirAll(podLogsRootDirectory, 0755); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Failed to create directory %q: %v", podLogsRootDirectory, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubeRuntimeManager.imagePuller = images.NewImageManager(
0000000000000000000000000000000000000000;;			kubecontainer.FilterEventRecorder(recorder),
0000000000000000000000000000000000000000;;			kubeRuntimeManager,
0000000000000000000000000000000000000000;;			imageBackOff,
0000000000000000000000000000000000000000;;			serializeImagePulls,
0000000000000000000000000000000000000000;;			imagePullQPS,
0000000000000000000000000000000000000000;;			imagePullBurst)
0000000000000000000000000000000000000000;;		kubeRuntimeManager.runner = lifecycle.NewHandlerRunner(httpClient, kubeRuntimeManager, kubeRuntimeManager)
0000000000000000000000000000000000000000;;		kubeRuntimeManager.containerGC = NewContainerGC(runtimeService, podGetter, kubeRuntimeManager)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		kubeRuntimeManager.versionCache = cache.NewObjectCache(
0000000000000000000000000000000000000000;;			func() (interface{}, error) {
0000000000000000000000000000000000000000;;				return kubeRuntimeManager.getTypedVersion()
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			versionCacheTTL,
0000000000000000000000000000000000000000;;		)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return kubeRuntimeManager, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Type returns the type of the container runtime.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) Type() string {
0000000000000000000000000000000000000000;;		return m.runtimeName
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func newRuntimeVersion(version string) (*utilversion.Version, error) {
0000000000000000000000000000000000000000;;		return utilversion.ParseSemantic(version)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) getTypedVersion() (*runtimeapi.VersionResponse, error) {
0000000000000000000000000000000000000000;;		typedVersion, err := m.runtimeService.Version(kubeRuntimeAPIVersion)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Get remote runtime typed version failed: %v", err)
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return typedVersion, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Version returns the version information of the container runtime.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) Version() (kubecontainer.Version, error) {
0000000000000000000000000000000000000000;;		typedVersion, err := m.runtimeService.Version(kubeRuntimeAPIVersion)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Get remote runtime version failed: %v", err)
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return newRuntimeVersion(typedVersion.RuntimeVersion)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// APIVersion returns the cached API version information of the container
0000000000000000000000000000000000000000;;	// runtime. Implementation is expected to update this cache periodically.
0000000000000000000000000000000000000000;;	// This may be different from the runtime engine's version.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) APIVersion() (kubecontainer.Version, error) {
0000000000000000000000000000000000000000;;		versionObject, err := m.versionCache.Get(m.machineInfo.MachineID)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		typedVersion := versionObject.(*runtimeapi.VersionResponse)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return newRuntimeVersion(typedVersion.RuntimeApiVersion)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Status returns the status of the runtime. An error is returned if the Status
0000000000000000000000000000000000000000;;	// function itself fails, nil otherwise.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) Status() (*kubecontainer.RuntimeStatus, error) {
0000000000000000000000000000000000000000;;		status, err := m.runtimeService.Status()
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return toKubeRuntimeStatus(status), nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetPods returns a list of containers grouped by pods. The boolean parameter
0000000000000000000000000000000000000000;;	// specifies whether the runtime returns all containers including those already
0000000000000000000000000000000000000000;;	// exited and dead containers (used for garbage collection).
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) GetPods(all bool) ([]*kubecontainer.Pod, error) {
0000000000000000000000000000000000000000;;		pods := make(map[kubetypes.UID]*kubecontainer.Pod)
0000000000000000000000000000000000000000;;		sandboxes, err := m.getKubeletSandboxes(all)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := range sandboxes {
0000000000000000000000000000000000000000;;			s := sandboxes[i]
0000000000000000000000000000000000000000;;			if s.Metadata == nil {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Sandbox does not have metadata: %+v", s)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			podUID := kubetypes.UID(s.Metadata.Uid)
0000000000000000000000000000000000000000;;			if _, ok := pods[podUID]; !ok {
0000000000000000000000000000000000000000;;				pods[podUID] = &kubecontainer.Pod{
0000000000000000000000000000000000000000;;					ID:        podUID,
0000000000000000000000000000000000000000;;					Name:      s.Metadata.Name,
0000000000000000000000000000000000000000;;					Namespace: s.Metadata.Namespace,
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			p := pods[podUID]
0000000000000000000000000000000000000000;;			converted, err := m.sandboxToKubeContainer(s)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Convert %q sandbox %v of pod %q failed: %v", m.runtimeName, s, podUID, err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			p.Sandboxes = append(p.Sandboxes, converted)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		containers, err := m.getKubeletContainers(all)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for i := range containers {
0000000000000000000000000000000000000000;;			c := containers[i]
0000000000000000000000000000000000000000;;			if c.Metadata == nil {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Container does not have metadata: %+v", c)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			labelledInfo := getContainerInfoFromLabels(c.Labels)
0000000000000000000000000000000000000000;;			pod, found := pods[labelledInfo.PodUID]
0000000000000000000000000000000000000000;;			if !found {
0000000000000000000000000000000000000000;;				pod = &kubecontainer.Pod{
0000000000000000000000000000000000000000;;					ID:        labelledInfo.PodUID,
0000000000000000000000000000000000000000;;					Name:      labelledInfo.PodName,
0000000000000000000000000000000000000000;;					Namespace: labelledInfo.PodNamespace,
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				pods[labelledInfo.PodUID] = pod
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			converted, err := m.toKubeContainer(c)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Convert %s container %v of pod %q failed: %v", m.runtimeName, c, labelledInfo.PodUID, err)
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			pod.Containers = append(pod.Containers, converted)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Convert map to list.
0000000000000000000000000000000000000000;;		var result []*kubecontainer.Pod
0000000000000000000000000000000000000000;;		for _, pod := range pods {
0000000000000000000000000000000000000000;;			result = append(result, pod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return result, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// containerToKillInfo contains necessary information to kill a container.
0000000000000000000000000000000000000000;;	type containerToKillInfo struct {
0000000000000000000000000000000000000000;;		// The spec of the container.
0000000000000000000000000000000000000000;;		container *v1.Container
0000000000000000000000000000000000000000;;		// The name of the container.
0000000000000000000000000000000000000000;;		name string
0000000000000000000000000000000000000000;;		// The message indicates why the container will be killed.
0000000000000000000000000000000000000000;;		message string
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// podContainerSpecChanges keeps information on changes that need to happen for a pod.
0000000000000000000000000000000000000000;;	type podContainerSpecChanges struct {
0000000000000000000000000000000000000000;;		// Whether need to create a new sandbox.
0000000000000000000000000000000000000000;;		CreateSandbox bool
0000000000000000000000000000000000000000;;		// The id of existing sandbox. It is used for starting containers in ContainersToStart.
0000000000000000000000000000000000000000;;		SandboxID string
0000000000000000000000000000000000000000;;		// The attempt number of creating sandboxes for the pod.
0000000000000000000000000000000000000000;;		Attempt uint32
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// ContainersToStart keeps a map of containers that need to be started, note that
0000000000000000000000000000000000000000;;		// the key is index of the container inside pod.Spec.Containers, while
0000000000000000000000000000000000000000;;		// the value is a message indicates why the container needs to start.
0000000000000000000000000000000000000000;;		ContainersToStart map[int]string
0000000000000000000000000000000000000000;;		// ContainersToKeep keeps a map of containers that need to be kept as is, note that
0000000000000000000000000000000000000000;;		// the key is the container ID of the container, while
0000000000000000000000000000000000000000;;		// the value is index of the container inside pod.Spec.Containers.
0000000000000000000000000000000000000000;;		ContainersToKeep map[kubecontainer.ContainerID]int
0000000000000000000000000000000000000000;;		// ContainersToKill keeps a map of containers that need to be killed, note that
0000000000000000000000000000000000000000;;		// the key is the container ID of the container, while
0000000000000000000000000000000000000000;;		// the value contains necessary information to kill a container.
0000000000000000000000000000000000000000;;		ContainersToKill map[kubecontainer.ContainerID]containerToKillInfo
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// InitFailed indicates whether init containers are failed.
0000000000000000000000000000000000000000;;		InitFailed bool
0000000000000000000000000000000000000000;;		// InitContainersToKeep keeps a map of init containers that need to be kept as
0000000000000000000000000000000000000000;;		// is, note that the key is the container ID of the container, while
0000000000000000000000000000000000000000;;		// the value is index of the container inside pod.Spec.InitContainers.
0000000000000000000000000000000000000000;;		InitContainersToKeep map[kubecontainer.ContainerID]int
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// podSandboxChanged checks whether the spec of the pod is changed and returns
0000000000000000000000000000000000000000;;	// (changed, new attempt, original sandboxID if exist).
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) podSandboxChanged(pod *v1.Pod, podStatus *kubecontainer.PodStatus) (bool, uint32, string) {
0000000000000000000000000000000000000000;;		if len(podStatus.SandboxStatuses) == 0 {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("No sandbox for pod %q can be found. Need to start a new one", format.Pod(pod))
0000000000000000000000000000000000000000;;			return true, 0, ""
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		readySandboxCount := 0
0000000000000000000000000000000000000000;;		for _, s := range podStatus.SandboxStatuses {
0000000000000000000000000000000000000000;;			if s.State == runtimeapi.PodSandboxState_SANDBOX_READY {
0000000000000000000000000000000000000000;;				readySandboxCount++
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Needs to create a new sandbox when readySandboxCount > 1 or the ready sandbox is not the latest one.
0000000000000000000000000000000000000000;;		sandboxStatus := podStatus.SandboxStatuses[0]
0000000000000000000000000000000000000000;;		if readySandboxCount > 1 {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("More than 1 sandboxes for pod %q are ready. Need to reconcile them", format.Pod(pod))
0000000000000000000000000000000000000000;;			return true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if sandboxStatus.State != runtimeapi.PodSandboxState_SANDBOX_READY {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("No ready sandbox for pod %q can be found. Need to start a new one", format.Pod(pod))
0000000000000000000000000000000000000000;;			return true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Needs to create a new sandbox when network namespace changed.
0000000000000000000000000000000000000000;;		if sandboxStatus.Linux != nil && sandboxStatus.Linux.Namespaces != nil && sandboxStatus.Linux.Namespaces.Options != nil &&
0000000000000000000000000000000000000000;;			sandboxStatus.Linux.Namespaces.Options.HostNetwork != kubecontainer.IsHostNetworkPod(pod) {
0000000000000000000000000000000000000000;;			glog.V(2).Infof("Sandbox for pod %q has changed. Need to start a new one", format.Pod(pod))
0000000000000000000000000000000000000000;;			return true, sandboxStatus.Metadata.Attempt + 1, ""
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return false, sandboxStatus.Metadata.Attempt, sandboxStatus.Id
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// checkAndKeepInitContainers keeps all successfully completed init containers. If there
0000000000000000000000000000000000000000;;	// are failing containers, only keep the first failing one.
0000000000000000000000000000000000000000;;	func checkAndKeepInitContainers(pod *v1.Pod, podStatus *kubecontainer.PodStatus, initContainersToKeep map[kubecontainer.ContainerID]int) bool {
0000000000000000000000000000000000000000;;		initFailed := false
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i, container := range pod.Spec.InitContainers {
0000000000000000000000000000000000000000;;			containerStatus := podStatus.FindContainerStatusByName(container.Name)
0000000000000000000000000000000000000000;;			if containerStatus == nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if containerStatus.State == kubecontainer.ContainerStateRunning {
0000000000000000000000000000000000000000;;				initContainersToKeep[containerStatus.ID] = i
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if containerStatus.State == kubecontainer.ContainerStateExited {
0000000000000000000000000000000000000000;;				initContainersToKeep[containerStatus.ID] = i
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if isContainerFailed(containerStatus) {
0000000000000000000000000000000000000000;;				initFailed = true
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return initFailed
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// computePodContainerChanges checks whether the pod spec has changed and returns the changes if true.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) computePodContainerChanges(pod *v1.Pod, podStatus *kubecontainer.PodStatus) podContainerSpecChanges {
0000000000000000000000000000000000000000;;		glog.V(5).Infof("Syncing Pod %q: %+v", format.Pod(pod), pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sandboxChanged, attempt, sandboxID := m.podSandboxChanged(pod, podStatus)
0000000000000000000000000000000000000000;;		changes := podContainerSpecChanges{
0000000000000000000000000000000000000000;;			CreateSandbox:        sandboxChanged,
0000000000000000000000000000000000000000;;			SandboxID:            sandboxID,
0000000000000000000000000000000000000000;;			Attempt:              attempt,
0000000000000000000000000000000000000000;;			ContainersToStart:    make(map[int]string),
0000000000000000000000000000000000000000;;			ContainersToKeep:     make(map[kubecontainer.ContainerID]int),
0000000000000000000000000000000000000000;;			InitContainersToKeep: make(map[kubecontainer.ContainerID]int),
0000000000000000000000000000000000000000;;			ContainersToKill:     make(map[kubecontainer.ContainerID]containerToKillInfo),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// check the status of init containers.
0000000000000000000000000000000000000000;;		initFailed := false
0000000000000000000000000000000000000000;;		// always reset the init containers if the sandbox is changed.
0000000000000000000000000000000000000000;;		if !sandboxChanged {
0000000000000000000000000000000000000000;;			// Keep all successfully completed containers. If there are failing containers,
0000000000000000000000000000000000000000;;			// only keep the first failing one.
0000000000000000000000000000000000000000;;			initFailed = checkAndKeepInitContainers(pod, podStatus, changes.InitContainersToKeep)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		changes.InitFailed = initFailed
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// check the status of containers.
0000000000000000000000000000000000000000;;		for index, container := range pod.Spec.Containers {
0000000000000000000000000000000000000000;;			containerStatus := podStatus.FindContainerStatusByName(container.Name)
0000000000000000000000000000000000000000;;			if containerStatus == nil || containerStatus.State != kubecontainer.ContainerStateRunning {
0000000000000000000000000000000000000000;;				if kubecontainer.ShouldContainerBeRestarted(&container, pod, podStatus) {
0000000000000000000000000000000000000000;;					message := fmt.Sprintf("Container %+v is dead, but RestartPolicy says that we should restart it.", container)
0000000000000000000000000000000000000000;;					glog.Info(message)
0000000000000000000000000000000000000000;;					changes.ContainersToStart[index] = message
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if sandboxChanged {
0000000000000000000000000000000000000000;;				if pod.Spec.RestartPolicy != v1.RestartPolicyNever {
0000000000000000000000000000000000000000;;					message := fmt.Sprintf("Container %+v's pod sandbox is dead, the container will be recreated.", container)
0000000000000000000000000000000000000000;;					glog.Info(message)
0000000000000000000000000000000000000000;;					changes.ContainersToStart[index] = message
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if initFailed {
0000000000000000000000000000000000000000;;				// Initialization failed and Container exists.
0000000000000000000000000000000000000000;;				// If we have an initialization failure everything will be killed anyway.
0000000000000000000000000000000000000000;;				// If RestartPolicy is Always or OnFailure we restart containers that were running before.
0000000000000000000000000000000000000000;;				if pod.Spec.RestartPolicy != v1.RestartPolicyNever {
0000000000000000000000000000000000000000;;					message := fmt.Sprintf("Failed to initialize pod. %q will be restarted.", container.Name)
0000000000000000000000000000000000000000;;					glog.V(1).Info(message)
0000000000000000000000000000000000000000;;					changes.ContainersToStart[index] = message
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			expectedHash := kubecontainer.HashContainer(&container)
0000000000000000000000000000000000000000;;			containerChanged := containerStatus.Hash != expectedHash
0000000000000000000000000000000000000000;;			if containerChanged {
0000000000000000000000000000000000000000;;				message := fmt.Sprintf("Pod %q container %q hash changed (%d vs %d), it will be killed and re-created.",
0000000000000000000000000000000000000000;;					pod.Name, container.Name, containerStatus.Hash, expectedHash)
0000000000000000000000000000000000000000;;				glog.Info(message)
0000000000000000000000000000000000000000;;				changes.ContainersToStart[index] = message
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			liveness, found := m.livenessManager.Get(containerStatus.ID)
0000000000000000000000000000000000000000;;			if !found || liveness == proberesults.Success {
0000000000000000000000000000000000000000;;				changes.ContainersToKeep[containerStatus.ID] = index
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if pod.Spec.RestartPolicy != v1.RestartPolicyNever {
0000000000000000000000000000000000000000;;				message := fmt.Sprintf("pod %q container %q is unhealthy, it will be killed and re-created.", format.Pod(pod), container.Name)
0000000000000000000000000000000000000000;;				glog.Info(message)
0000000000000000000000000000000000000000;;				changes.ContainersToStart[index] = message
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Don't keep init containers if they are the only containers to keep.
0000000000000000000000000000000000000000;;		if !sandboxChanged && len(changes.ContainersToStart) == 0 && len(changes.ContainersToKeep) == 0 {
0000000000000000000000000000000000000000;;			changes.InitContainersToKeep = make(map[kubecontainer.ContainerID]int)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// compute containers to be killed
0000000000000000000000000000000000000000;;		runningContainerStatuses := podStatus.GetRunningContainerStatuses()
0000000000000000000000000000000000000000;;		for _, containerStatus := range runningContainerStatuses {
0000000000000000000000000000000000000000;;			_, keep := changes.ContainersToKeep[containerStatus.ID]
0000000000000000000000000000000000000000;;			_, keepInit := changes.InitContainersToKeep[containerStatus.ID]
0000000000000000000000000000000000000000;;			if !keep && !keepInit {
0000000000000000000000000000000000000000;;				var podContainer *v1.Container
0000000000000000000000000000000000000000;;				var killMessage string
0000000000000000000000000000000000000000;;				for i, c := range pod.Spec.Containers {
0000000000000000000000000000000000000000;;					if c.Name == containerStatus.Name {
0000000000000000000000000000000000000000;;						podContainer = &pod.Spec.Containers[i]
0000000000000000000000000000000000000000;;						killMessage = changes.ContainersToStart[i]
0000000000000000000000000000000000000000;;						break
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				changes.ContainersToKill[containerStatus.ID] = containerToKillInfo{
0000000000000000000000000000000000000000;;					name:      containerStatus.Name,
0000000000000000000000000000000000000000;;					container: podContainer,
0000000000000000000000000000000000000000;;					message:   killMessage,
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return changes
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// SyncPod syncs the running pod into the desired pod by executing following steps:
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	//  1. Compute sandbox and container changes.
0000000000000000000000000000000000000000;;	//  2. Kill pod sandbox if necessary.
0000000000000000000000000000000000000000;;	//  3. Kill any containers that should not be running.
0000000000000000000000000000000000000000;;	//  4. Create sandbox if necessary.
0000000000000000000000000000000000000000;;	//  5. Create init containers.
0000000000000000000000000000000000000000;;	//  6. Create normal containers.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {
0000000000000000000000000000000000000000;;		// Step 1: Compute sandbox and container changes.
0000000000000000000000000000000000000000;;		podContainerChanges := m.computePodContainerChanges(pod, podStatus)
0000000000000000000000000000000000000000;;		glog.V(3).Infof("computePodContainerChanges got %+v for pod %q", podContainerChanges, format.Pod(pod))
0000000000000000000000000000000000000000;;		if podContainerChanges.CreateSandbox {
0000000000000000000000000000000000000000;;			ref, err := ref.GetReference(api.Scheme, pod)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Couldn't make a ref to pod %q: '%v'", format.Pod(pod), err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if podContainerChanges.SandboxID != "" {
0000000000000000000000000000000000000000;;				m.recorder.Eventf(ref, v1.EventTypeNormal, "SandboxChanged", "Pod sandbox changed, it will be killed and re-created.")
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("SyncPod received new pod %q, will create a new sandbox for it", format.Pod(pod))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Step 2: Kill the pod if the sandbox has changed.
0000000000000000000000000000000000000000;;		if podContainerChanges.CreateSandbox || (len(podContainerChanges.ContainersToKeep) == 0 && len(podContainerChanges.ContainersToStart) == 0) {
0000000000000000000000000000000000000000;;			if len(podContainerChanges.ContainersToKeep) == 0 && len(podContainerChanges.ContainersToStart) == 0 {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Stopping PodSandbox for %q because all other containers are dead.", format.Pod(pod))
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Stopping PodSandbox for %q, will start new one", format.Pod(pod))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil)
0000000000000000000000000000000000000000;;			result.AddPodSyncResult(killResult)
0000000000000000000000000000000000000000;;			if killResult.Error() != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("killPodWithSyncResult failed: %v", killResult.Error())
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// Step 3: kill any running containers in this pod which are not to keep.
0000000000000000000000000000000000000000;;			for containerID, containerInfo := range podContainerChanges.ContainersToKill {
0000000000000000000000000000000000000000;;				glog.V(3).Infof("Killing unwanted container %q(id=%q) for pod %q", containerInfo.name, containerID, format.Pod(pod))
0000000000000000000000000000000000000000;;				killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name)
0000000000000000000000000000000000000000;;				result.AddSyncResult(killContainerResult)
0000000000000000000000000000000000000000;;				if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil {
0000000000000000000000000000000000000000;;					killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error())
0000000000000000000000000000000000000000;;					glog.Errorf("killContainer %q(id=%q) for pod %q failed: %v", containerInfo.name, containerID, format.Pod(pod), err)
0000000000000000000000000000000000000000;;					return
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Keep terminated init containers fairly aggressively controlled
0000000000000000000000000000000000000000;;		m.pruneInitContainersBeforeStart(pod, podStatus, podContainerChanges.InitContainersToKeep)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// We pass the value of the podIP down to generatePodSandboxConfig and
0000000000000000000000000000000000000000;;		// generateContainerConfig, which in turn passes it to various other
0000000000000000000000000000000000000000;;		// functions, in order to facilitate functionality that requires this
0000000000000000000000000000000000000000;;		// value (hosts file and downward API) and avoid races determining
0000000000000000000000000000000000000000;;		// the pod IP in cases where a container requires restart but the
0000000000000000000000000000000000000000;;		// podIP isn't in the status manager yet.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// We default to the IP in the passed-in pod status, and overwrite it if the
0000000000000000000000000000000000000000;;		// sandbox needs to be (re)started.
0000000000000000000000000000000000000000;;		podIP := ""
0000000000000000000000000000000000000000;;		if podStatus != nil {
0000000000000000000000000000000000000000;;			podIP = podStatus.IP
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Step 4: Create a sandbox for the pod if necessary.
0000000000000000000000000000000000000000;;		podSandboxID := podContainerChanges.SandboxID
0000000000000000000000000000000000000000;;		if podContainerChanges.CreateSandbox && len(podContainerChanges.ContainersToStart) > 0 {
0000000000000000000000000000000000000000;;			var msg string
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Creating sandbox for pod %q", format.Pod(pod))
0000000000000000000000000000000000000000;;			createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod))
0000000000000000000000000000000000000000;;			result.AddSyncResult(createSandboxResult)
0000000000000000000000000000000000000000;;			podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				createSandboxResult.Fail(kubecontainer.ErrCreatePodSandbox, msg)
0000000000000000000000000000000000000000;;				glog.Errorf("createPodSandbox for pod %q failed: %v", format.Pod(pod), err)
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Created PodSandbox %q for pod %q", podSandboxID, format.Pod(pod))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			podSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("Failed to get pod sandbox status: %v; Skipping pod %q", err, format.Pod(pod))
0000000000000000000000000000000000000000;;				result.Fail(err)
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// If we ever allow updating a pod from non-host-network to
0000000000000000000000000000000000000000;;			// host-network, we may use a stale IP.
0000000000000000000000000000000000000000;;			if !kubecontainer.IsHostNetworkPod(pod) {
0000000000000000000000000000000000000000;;				// Overwrite the podIP passed in the pod status, since we just started the pod sandbox.
0000000000000000000000000000000000000000;;				podIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus)
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Determined the ip %q for pod %q after sandbox changed", podIP, format.Pod(pod))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Get podSandboxConfig for containers to start.
0000000000000000000000000000000000000000;;		configPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)
0000000000000000000000000000000000000000;;		result.AddSyncResult(configPodSandboxResult)
0000000000000000000000000000000000000000;;		podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			message := fmt.Sprintf("GeneratePodSandboxConfig for pod %q failed: %v", format.Pod(pod), err)
0000000000000000000000000000000000000000;;			glog.Error(message)
0000000000000000000000000000000000000000;;			configPodSandboxResult.Fail(kubecontainer.ErrConfigPodSandbox, message)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Step 5: start init containers.
0000000000000000000000000000000000000000;;		status, next, done := findNextInitContainerToRun(pod, podStatus)
0000000000000000000000000000000000000000;;		if status != nil && status.ExitCode != 0 {
0000000000000000000000000000000000000000;;			// container initialization has failed, flag the pod as failed
0000000000000000000000000000000000000000;;			initContainerResult := kubecontainer.NewSyncResult(kubecontainer.InitContainer, status.Name)
0000000000000000000000000000000000000000;;			initContainerResult.Fail(kubecontainer.ErrRunInitContainer, fmt.Sprintf("init container %q exited with %d", status.Name, status.ExitCode))
0000000000000000000000000000000000000000;;			result.AddSyncResult(initContainerResult)
0000000000000000000000000000000000000000;;			if pod.Spec.RestartPolicy == v1.RestartPolicyNever {
0000000000000000000000000000000000000000;;				utilruntime.HandleError(fmt.Errorf("error running pod %q init container %q, restart=Never: %#v", format.Pod(pod), status.Name, status))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			utilruntime.HandleError(fmt.Errorf("Error running pod %q init container %q, restarting: %#v", format.Pod(pod), status.Name, status))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if next != nil {
0000000000000000000000000000000000000000;;			if len(podContainerChanges.ContainersToStart) == 0 {
0000000000000000000000000000000000000000;;				glog.V(4).Infof("No containers to start, stopping at init container %+v in pod %v", next.Name, format.Pod(pod))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// If we need to start the next container, do so now then exit
0000000000000000000000000000000000000000;;			container := next
0000000000000000000000000000000000000000;;			startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name)
0000000000000000000000000000000000000000;;			result.AddSyncResult(startContainerResult)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff)
0000000000000000000000000000000000000000;;			if isInBackOff {
0000000000000000000000000000000000000000;;				startContainerResult.Fail(err, msg)
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Backing Off restarting init container %+v in pod %v", container, format.Pod(pod))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Creating init container %+v in pod %v", container, format.Pod(pod))
0000000000000000000000000000000000000000;;			if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP); err != nil {
0000000000000000000000000000000000000000;;				startContainerResult.Fail(err, msg)
0000000000000000000000000000000000000000;;				utilruntime.HandleError(fmt.Errorf("init container start failed: %v: %s", err, msg))
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Successfully started the container; clear the entry in the failure
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Completed init container %q for pod %q", container.Name, format.Pod(pod))
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if !done {
0000000000000000000000000000000000000000;;			// init container still running
0000000000000000000000000000000000000000;;			glog.V(4).Infof("An init container is still running in pod %v", format.Pod(pod))
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if podContainerChanges.InitFailed {
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Not all init containers have succeeded for pod %v", format.Pod(pod))
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Step 6: start containers in podContainerChanges.ContainersToStart.
0000000000000000000000000000000000000000;;		for idx := range podContainerChanges.ContainersToStart {
0000000000000000000000000000000000000000;;			container := &pod.Spec.Containers[idx]
0000000000000000000000000000000000000000;;			startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name)
0000000000000000000000000000000000000000;;			result.AddSyncResult(startContainerResult)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff)
0000000000000000000000000000000000000000;;			if isInBackOff {
0000000000000000000000000000000000000000;;				startContainerResult.Fail(err, msg)
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Backing Off restarting container %+v in pod %v", container, format.Pod(pod))
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			glog.V(4).Infof("Creating container %+v in pod %v", container, format.Pod(pod))
0000000000000000000000000000000000000000;;			if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP); err != nil {
0000000000000000000000000000000000000000;;				startContainerResult.Fail(err, msg)
0000000000000000000000000000000000000000;;				utilruntime.HandleError(fmt.Errorf("container start failed: %v: %s", err, msg))
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// If a container is still in backoff, the function will return a brief backoff error and
0000000000000000000000000000000000000000;;	// a detailed error message.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) doBackOff(pod *v1.Pod, container *v1.Container, podStatus *kubecontainer.PodStatus, backOff *flowcontrol.Backoff) (bool, string, error) {
0000000000000000000000000000000000000000;;		var cStatus *kubecontainer.ContainerStatus
0000000000000000000000000000000000000000;;		for _, c := range podStatus.ContainerStatuses {
0000000000000000000000000000000000000000;;			if c.Name == container.Name && c.State == kubecontainer.ContainerStateExited {
0000000000000000000000000000000000000000;;				cStatus = c
0000000000000000000000000000000000000000;;				break
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if cStatus == nil {
0000000000000000000000000000000000000000;;			return false, "", nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.Infof("checking backoff for container %q in pod %q", container.Name, format.Pod(pod))
0000000000000000000000000000000000000000;;		// Use the finished time of the latest exited container as the start point to calculate whether to do back-off.
0000000000000000000000000000000000000000;;		ts := cStatus.FinishedAt
0000000000000000000000000000000000000000;;		// backOff requires a unique key to identify the container.
0000000000000000000000000000000000000000;;		key := getStableKey(pod, container)
0000000000000000000000000000000000000000;;		if backOff.IsInBackOffSince(key, ts) {
0000000000000000000000000000000000000000;;			if ref, err := kubecontainer.GenerateContainerRef(pod, container); err == nil {
0000000000000000000000000000000000000000;;				m.recorder.Eventf(ref, v1.EventTypeWarning, events.BackOffStartContainer, "Back-off restarting failed container")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			err := fmt.Errorf("Back-off %s restarting failed container=%s pod=%s", backOff.Get(key), container.Name, format.Pod(pod))
0000000000000000000000000000000000000000;;			glog.Infof("%s", err.Error())
0000000000000000000000000000000000000000;;			return true, err.Error(), kubecontainer.ErrCrashLoopBackOff
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		backOff.Next(key, ts)
0000000000000000000000000000000000000000;;		return false, "", nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// KillPod kills all the containers of a pod. Pod may be nil, running pod must not be.
0000000000000000000000000000000000000000;;	// gracePeriodOverride if specified allows the caller to override the pod default grace period.
0000000000000000000000000000000000000000;;	// only hard kill paths are allowed to specify a gracePeriodOverride in the kubelet in order to not corrupt user data.
0000000000000000000000000000000000000000;;	// it is useful when doing SIGKILL for hard eviction scenarios, or max grace period during soft eviction scenarios.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) KillPod(pod *v1.Pod, runningPod kubecontainer.Pod, gracePeriodOverride *int64) error {
0000000000000000000000000000000000000000;;		err := m.killPodWithSyncResult(pod, runningPod, gracePeriodOverride)
0000000000000000000000000000000000000000;;		return err.Error()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// killPodWithSyncResult kills a runningPod and returns SyncResult.
0000000000000000000000000000000000000000;;	// Note: The pod passed in could be *nil* when kubelet restarted.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) killPodWithSyncResult(pod *v1.Pod, runningPod kubecontainer.Pod, gracePeriodOverride *int64) (result kubecontainer.PodSyncResult) {
0000000000000000000000000000000000000000;;		killContainerResults := m.killContainersWithSyncResult(pod, runningPod, gracePeriodOverride)
0000000000000000000000000000000000000000;;		for _, containerResult := range killContainerResults {
0000000000000000000000000000000000000000;;			result.AddSyncResult(containerResult)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// stop sandbox, the sandbox will be removed in GarbageCollect
0000000000000000000000000000000000000000;;		killSandboxResult := kubecontainer.NewSyncResult(kubecontainer.KillPodSandbox, runningPod.ID)
0000000000000000000000000000000000000000;;		result.AddSyncResult(killSandboxResult)
0000000000000000000000000000000000000000;;		// Stop all sandboxes belongs to same pod
0000000000000000000000000000000000000000;;		for _, podSandbox := range runningPod.Sandboxes {
0000000000000000000000000000000000000000;;			if err := m.runtimeService.StopPodSandbox(podSandbox.ID.ID); err != nil {
0000000000000000000000000000000000000000;;				killSandboxResult.Fail(kubecontainer.ErrKillPodSandbox, err.Error())
0000000000000000000000000000000000000000;;				glog.Errorf("Failed to stop sandbox %q", podSandbox.ID)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// isHostNetwork checks whether the pod is running in host-network mode.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) isHostNetwork(podSandBoxID string, pod *v1.Pod) (bool, error) {
0000000000000000000000000000000000000000;;		if pod != nil {
0000000000000000000000000000000000000000;;			return kubecontainer.IsHostNetworkPod(pod), nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podStatus, err := m.runtimeService.PodSandboxStatus(podSandBoxID)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if nsOpts := podStatus.GetLinux().GetNamespaces().GetOptions(); nsOpts != nil {
0000000000000000000000000000000000000000;;			return nsOpts.HostNetwork, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return false, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetPodStatus retrieves the status of the pod, including the
0000000000000000000000000000000000000000;;	// information of all containers in the pod that are visible in Runtime.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) GetPodStatus(uid kubetypes.UID, name, namespace string) (*kubecontainer.PodStatus, error) {
0000000000000000000000000000000000000000;;		// Now we retain restart count of container as a container label. Each time a container
0000000000000000000000000000000000000000;;		// restarts, pod will read the restart count from the registered dead container, increment
0000000000000000000000000000000000000000;;		// it to get the new restart count, and then add a label with the new restart count on
0000000000000000000000000000000000000000;;		// the newly started container.
0000000000000000000000000000000000000000;;		// However, there are some limitations of this method:
0000000000000000000000000000000000000000;;		//	1. When all dead containers were garbage collected, the container status could
0000000000000000000000000000000000000000;;		//	not get the historical value and would be *inaccurate*. Fortunately, the chance
0000000000000000000000000000000000000000;;		//	is really slim.
0000000000000000000000000000000000000000;;		//	2. When working with old version containers which have no restart count label,
0000000000000000000000000000000000000000;;		//	we can only assume their restart count is 0.
0000000000000000000000000000000000000000;;		// Anyhow, we only promised "best-effort" restart count reporting, we can just ignore
0000000000000000000000000000000000000000;;		// these limitations now.
0000000000000000000000000000000000000000;;		// TODO: move this comment to SyncPod.
0000000000000000000000000000000000000000;;		podSandboxIDs, err := m.getSandboxIDByPodUID(uid, nil)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		podFullName := format.Pod(&v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				Name:      name,
0000000000000000000000000000000000000000;;				Namespace: namespace,
0000000000000000000000000000000000000000;;				UID:       uid,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		glog.V(4).Infof("getSandboxIDByPodUID got sandbox IDs %q for pod %q", podSandboxIDs, podFullName)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sandboxStatuses := make([]*runtimeapi.PodSandboxStatus, len(podSandboxIDs))
0000000000000000000000000000000000000000;;		podIP := ""
0000000000000000000000000000000000000000;;		for idx, podSandboxID := range podSandboxIDs {
0000000000000000000000000000000000000000;;			podSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("PodSandboxStatus of sandbox %q for pod %q error: %v", podSandboxID, podFullName, err)
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			sandboxStatuses[idx] = podSandboxStatus
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Only get pod IP from latest sandbox
0000000000000000000000000000000000000000;;			if idx == 0 && podSandboxStatus.State == runtimeapi.PodSandboxState_SANDBOX_READY {
0000000000000000000000000000000000000000;;				podIP = m.determinePodSandboxIP(namespace, name, podSandboxStatus)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Get statuses of all containers visible in the pod.
0000000000000000000000000000000000000000;;		containerStatuses, err := m.getPodContainerStatuses(uid, name, namespace)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("getPodContainerStatuses for pod %q failed: %v", podFullName, err)
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return &kubecontainer.PodStatus{
0000000000000000000000000000000000000000;;			ID:                uid,
0000000000000000000000000000000000000000;;			Name:              name,
0000000000000000000000000000000000000000;;			Namespace:         namespace,
0000000000000000000000000000000000000000;;			IP:                podIP,
0000000000000000000000000000000000000000;;			SandboxStatuses:   sandboxStatuses,
0000000000000000000000000000000000000000;;			ContainerStatuses: containerStatuses,
0000000000000000000000000000000000000000;;		}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Returns the filesystem path of the pod's network namespace.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// For CRI, container network is handled by the runtime completely and this
0000000000000000000000000000000000000000;;	// function should never be called.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) GetNetNS(_ kubecontainer.ContainerID) (string, error) {
0000000000000000000000000000000000000000;;		return "", fmt.Errorf("not supported")
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GarbageCollect removes dead containers using the specified container gc policy.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) GarbageCollect(gcPolicy kubecontainer.ContainerGCPolicy, allSourcesReady bool, evictNonDeletedPods bool) error {
0000000000000000000000000000000000000000;;		return m.containerGC.GarbageCollect(gcPolicy, allSourcesReady, evictNonDeletedPods)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetPodContainerID gets pod sandbox ID
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) GetPodContainerID(pod *kubecontainer.Pod) (kubecontainer.ContainerID, error) {
0000000000000000000000000000000000000000;;		formattedPod := kubecontainer.FormatPod(pod)
0000000000000000000000000000000000000000;;		if len(pod.Sandboxes) == 0 {
0000000000000000000000000000000000000000;;			glog.Errorf("No sandboxes are found for pod %q", formattedPod)
0000000000000000000000000000000000000000;;			return kubecontainer.ContainerID{}, fmt.Errorf("sandboxes for pod %q not found", formattedPod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// return sandboxID of the first sandbox since it is the latest one
0000000000000000000000000000000000000000;;		return pod.Sandboxes[0].ID, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// UpdatePodCIDR is just a passthrough method to update the runtimeConfig of the shim
0000000000000000000000000000000000000000;;	// with the podCIDR supplied by the kubelet.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) UpdatePodCIDR(podCIDR string) error {
0000000000000000000000000000000000000000;;		// TODO(#35531): do we really want to write a method on this manager for each
0000000000000000000000000000000000000000;;		// field of the config?
0000000000000000000000000000000000000000;;		glog.Infof("updating runtime config through cri with podcidr %v", podCIDR)
0000000000000000000000000000000000000000;;		return m.runtimeService.UpdateRuntimeConfig(
0000000000000000000000000000000000000000;;			&runtimeapi.RuntimeConfig{
0000000000000000000000000000000000000000;;				NetworkConfig: &runtimeapi.NetworkConfig{
0000000000000000000000000000000000000000;;					PodCidr: podCIDR,
0000000000000000000000000000000000000000;;				},
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;	}
