0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
5464e5a5d7a8fd40e9826bae7a1dc78c936c2109;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package kuberuntime
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"errors"
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"io"
0000000000000000000000000000000000000000;;		"math/rand"
0000000000000000000000000000000000000000;;		"net/url"
0000000000000000000000000000000000000000;;		"os"
0000000000000000000000000000000000000000;;		"path/filepath"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"sync"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"google.golang.org/grpc"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"github.com/armon/circbuf"
0000000000000000000000000000000000000000;;		"github.com/golang/glog"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		kubetypes "k8s.io/apimachinery/pkg/types"
0000000000000000000000000000000000000000;;		utilruntime "k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/sets"
0000000000000000000000000000000000000000;;		runtimeapi "k8s.io/kubernetes/pkg/kubelet/apis/cri/v1alpha1/runtime"
0000000000000000000000000000000000000000;;		kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/events"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/qos"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/types"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubelet/util/format"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/selinux"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/util/tail"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	var (
0000000000000000000000000000000000000000;;		ErrCreateContainerConfig = errors.New("CreateContainerConfigError")
0000000000000000000000000000000000000000;;		ErrCreateContainer       = errors.New("CreateContainerError")
0000000000000000000000000000000000000000;;		ErrPostStartHook         = errors.New("PostStartHookError")
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// recordContainerEvent should be used by the runtime manager for all container related events.
0000000000000000000000000000000000000000;;	// it has sanity checks to ensure that we do not write events that can abuse our masters.
0000000000000000000000000000000000000000;;	// in particular, it ensures that a containerID never appears in an event message as that
0000000000000000000000000000000000000000;;	// is prone to causing a lot of distinct events that do not count well.
0000000000000000000000000000000000000000;;	// it replaces any reference to a containerID with the containerName which is stable, and is what users know.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) recordContainerEvent(pod *v1.Pod, container *v1.Container, containerID, eventType, reason, message string, args ...interface{}) {
0000000000000000000000000000000000000000;;		ref, err := kubecontainer.GenerateContainerRef(pod, container)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Can't make a ref to pod %q, container %v: %v", format.Pod(pod), container.Name, err)
0000000000000000000000000000000000000000;;			return
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		eventMessage := message
0000000000000000000000000000000000000000;;		if len(args) > 0 {
0000000000000000000000000000000000000000;;			eventMessage = fmt.Sprintf(message, args...)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// this is a hack, but often the error from the runtime includes the containerID
0000000000000000000000000000000000000000;;		// which kills our ability to deduplicate events.  this protection makes a huge
0000000000000000000000000000000000000000;;		// difference in the number of unique events
0000000000000000000000000000000000000000;;		if containerID != "" {
0000000000000000000000000000000000000000;;			eventMessage = strings.Replace(eventMessage, containerID, container.Name, -1)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		m.recorder.Event(ref, eventType, reason, eventMessage)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// startContainer starts a container and returns a message indicates why it is failed on error.
0000000000000000000000000000000000000000;;	// It starts the container through the following steps:
0000000000000000000000000000000000000000;;	// * pull the image
0000000000000000000000000000000000000000;;	// * create the container
0000000000000000000000000000000000000000;;	// * start the container
0000000000000000000000000000000000000000;;	// * run the post start lifecycle hooks (if applicable)
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string) (string, error) {
0000000000000000000000000000000000000000;;		// Step 1: pull the image.
0000000000000000000000000000000000000000;;		imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return msg, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Step 2: create the container.
0000000000000000000000000000000000000000;;		ref, err := kubecontainer.GenerateContainerRef(pod, container)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Can't make a ref to pod %q, container %v: %v", format.Pod(pod), container.Name, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Generating ref for container %s: %#v", container.Name, ref)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// For a new container, the RestartCount should be 0
0000000000000000000000000000000000000000;;		restartCount := 0
0000000000000000000000000000000000000000;;		containerStatus := podStatus.FindContainerStatusByName(container.Name)
0000000000000000000000000000000000000000;;		if containerStatus != nil {
0000000000000000000000000000000000000000;;			restartCount = containerStatus.RestartCount + 1
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		containerConfig, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			m.recordContainerEvent(pod, container, "", v1.EventTypeWarning, events.FailedToCreateContainer, "Error: %v", grpc.ErrorDesc(err))
0000000000000000000000000000000000000000;;			return grpc.ErrorDesc(err), ErrCreateContainerConfig
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToCreateContainer, "Error: %v", grpc.ErrorDesc(err))
0000000000000000000000000000000000000000;;			return grpc.ErrorDesc(err), ErrCreateContainer
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, "Created container")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if ref != nil {
0000000000000000000000000000000000000000;;			m.containerRefManager.SetRef(kubecontainer.ContainerID{
0000000000000000000000000000000000000000;;				Type: m.runtimeName,
0000000000000000000000000000000000000000;;				ID:   containerID,
0000000000000000000000000000000000000000;;			}, ref)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Step 3: start the container.
0000000000000000000000000000000000000000;;		err = m.runtimeService.StartContainer(containerID)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, "Error: %v", grpc.ErrorDesc(err))
0000000000000000000000000000000000000000;;			return grpc.ErrorDesc(err), kubecontainer.ErrRunContainer
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.StartedContainer, "Started container")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Symlink container logs to the legacy container log location for cluster logging
0000000000000000000000000000000000000000;;		// support.
0000000000000000000000000000000000000000;;		// TODO(random-liu): Remove this after cluster logging supports CRI container log path.
0000000000000000000000000000000000000000;;		containerMeta := containerConfig.GetMetadata()
0000000000000000000000000000000000000000;;		sandboxMeta := podSandboxConfig.GetMetadata()
0000000000000000000000000000000000000000;;		legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name,
0000000000000000000000000000000000000000;;			sandboxMeta.Namespace)
0000000000000000000000000000000000000000;;		containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath)
0000000000000000000000000000000000000000;;		if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Failed to create legacy symbolic link %q to container %q log %q: %v",
0000000000000000000000000000000000000000;;				legacySymlink, containerID, containerLog, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Step 4: execute the post start hook.
0000000000000000000000000000000000000000;;		if container.Lifecycle != nil && container.Lifecycle.PostStart != nil {
0000000000000000000000000000000000000000;;			kubeContainerID := kubecontainer.ContainerID{
0000000000000000000000000000000000000000;;				Type: m.runtimeName,
0000000000000000000000000000000000000000;;				ID:   containerID,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)
0000000000000000000000000000000000000000;;			if handlerErr != nil {
0000000000000000000000000000000000000000;;				m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg)
0000000000000000000000000000000000000000;;				if err := m.killContainer(pod, kubeContainerID, container.Name, "FailedPostStartHook", nil); err != nil {
0000000000000000000000000000000000000000;;					glog.Errorf("Failed to kill container %q(id=%q) in pod %q: %v, %v",
0000000000000000000000000000000000000000;;						container.Name, kubeContainerID.String(), format.Pod(pod), ErrPostStartHook, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return msg, ErrPostStartHook
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return "", nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// generateContainerConfig generates container config for kubelet runtime v1.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) generateContainerConfig(container *v1.Container, pod *v1.Pod, restartCount int, podIP, imageRef string) (*runtimeapi.ContainerConfig, error) {
0000000000000000000000000000000000000000;;		opts, _, err := m.runtimeHelper.GenerateRunContainerOptions(pod, container, podIP)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		uid, username, err := m.getImageUser(container.Image)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if uid != nil {
0000000000000000000000000000000000000000;;			// Verify RunAsNonRoot. Non-root verification only supports numeric user.
0000000000000000000000000000000000000000;;			if err := verifyRunAsNonRoot(pod, container, *uid); err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else if username != "" {
0000000000000000000000000000000000000000;;			glog.Warningf("Non-root verification doesn't support non-numeric user (%s)", username)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		command, args := kubecontainer.ExpandContainerCommandAndArgs(container, opts.Envs)
0000000000000000000000000000000000000000;;		containerLogsPath := buildContainerLogsPath(container.Name, restartCount)
0000000000000000000000000000000000000000;;		restartCountUint32 := uint32(restartCount)
0000000000000000000000000000000000000000;;		config := &runtimeapi.ContainerConfig{
0000000000000000000000000000000000000000;;			Metadata: &runtimeapi.ContainerMetadata{
0000000000000000000000000000000000000000;;				Name:    container.Name,
0000000000000000000000000000000000000000;;				Attempt: restartCountUint32,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Image:       &runtimeapi.ImageSpec{Image: imageRef},
0000000000000000000000000000000000000000;;			Command:     command,
0000000000000000000000000000000000000000;;			Args:        args,
0000000000000000000000000000000000000000;;			WorkingDir:  container.WorkingDir,
0000000000000000000000000000000000000000;;			Labels:      newContainerLabels(container, pod),
0000000000000000000000000000000000000000;;			Annotations: newContainerAnnotations(container, pod, restartCount),
0000000000000000000000000000000000000000;;			Devices:     makeDevices(opts),
0000000000000000000000000000000000000000;;			Mounts:      m.makeMounts(opts, container),
0000000000000000000000000000000000000000;;			LogPath:     containerLogsPath,
0000000000000000000000000000000000000000;;			Stdin:       container.Stdin,
0000000000000000000000000000000000000000;;			StdinOnce:   container.StdinOnce,
0000000000000000000000000000000000000000;;			Tty:         container.TTY,
0000000000000000000000000000000000000000;;			Linux:       m.generateLinuxContainerConfig(container, pod, uid, username),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// set environment variables
0000000000000000000000000000000000000000;;		envs := make([]*runtimeapi.KeyValue, len(opts.Envs))
0000000000000000000000000000000000000000;;		for idx := range opts.Envs {
0000000000000000000000000000000000000000;;			e := opts.Envs[idx]
0000000000000000000000000000000000000000;;			envs[idx] = &runtimeapi.KeyValue{
0000000000000000000000000000000000000000;;				Key:   e.Name,
0000000000000000000000000000000000000000;;				Value: e.Value,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		config.Envs = envs
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return config, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// generateLinuxContainerConfig generates linux container config for kubelet runtime v1.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) generateLinuxContainerConfig(container *v1.Container, pod *v1.Pod, uid *int64, username string) *runtimeapi.LinuxContainerConfig {
0000000000000000000000000000000000000000;;		lc := &runtimeapi.LinuxContainerConfig{
0000000000000000000000000000000000000000;;			Resources:       &runtimeapi.LinuxContainerResources{},
0000000000000000000000000000000000000000;;			SecurityContext: m.determineEffectiveSecurityContext(pod, container, uid, username),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// set linux container resources
0000000000000000000000000000000000000000;;		var cpuShares int64
0000000000000000000000000000000000000000;;		cpuRequest := container.Resources.Requests.Cpu()
0000000000000000000000000000000000000000;;		cpuLimit := container.Resources.Limits.Cpu()
0000000000000000000000000000000000000000;;		memoryLimit := container.Resources.Limits.Memory().Value()
0000000000000000000000000000000000000000;;		oomScoreAdj := int64(qos.GetContainerOOMScoreAdjust(pod, container,
0000000000000000000000000000000000000000;;			int64(m.machineInfo.MemoryCapacity)))
0000000000000000000000000000000000000000;;		// If request is not specified, but limit is, we want request to default to limit.
0000000000000000000000000000000000000000;;		// API server does this for new containers, but we repeat this logic in Kubelet
0000000000000000000000000000000000000000;;		// for containers running on existing Kubernetes clusters.
0000000000000000000000000000000000000000;;		if cpuRequest.IsZero() && !cpuLimit.IsZero() {
0000000000000000000000000000000000000000;;			cpuShares = milliCPUToShares(cpuLimit.MilliValue())
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// if cpuRequest.Amount is nil, then milliCPUToShares will return the minimal number
0000000000000000000000000000000000000000;;			// of CPU shares.
0000000000000000000000000000000000000000;;			cpuShares = milliCPUToShares(cpuRequest.MilliValue())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		lc.Resources.CpuShares = cpuShares
0000000000000000000000000000000000000000;;		if memoryLimit != 0 {
0000000000000000000000000000000000000000;;			lc.Resources.MemoryLimitInBytes = memoryLimit
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Set OOM score of the container based on qos policy. Processes in lower-priority pods should
0000000000000000000000000000000000000000;;		// be killed first if the system runs out of memory.
0000000000000000000000000000000000000000;;		lc.Resources.OomScoreAdj = oomScoreAdj
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if m.cpuCFSQuota {
0000000000000000000000000000000000000000;;			// if cpuLimit.Amount is nil, then the appropriate default value is returned
0000000000000000000000000000000000000000;;			// to allow full usage of cpu resource.
0000000000000000000000000000000000000000;;			cpuQuota, cpuPeriod := milliCPUToQuota(cpuLimit.MilliValue())
0000000000000000000000000000000000000000;;			lc.Resources.CpuQuota = cpuQuota
0000000000000000000000000000000000000000;;			lc.Resources.CpuPeriod = cpuPeriod
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return lc
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// makeDevices generates container devices for kubelet runtime v1.
0000000000000000000000000000000000000000;;	func makeDevices(opts *kubecontainer.RunContainerOptions) []*runtimeapi.Device {
0000000000000000000000000000000000000000;;		devices := make([]*runtimeapi.Device, len(opts.Devices))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for idx := range opts.Devices {
0000000000000000000000000000000000000000;;			device := opts.Devices[idx]
0000000000000000000000000000000000000000;;			devices[idx] = &runtimeapi.Device{
0000000000000000000000000000000000000000;;				HostPath:      device.PathOnHost,
0000000000000000000000000000000000000000;;				ContainerPath: device.PathInContainer,
0000000000000000000000000000000000000000;;				Permissions:   device.Permissions,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return devices
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// makeMounts generates container volume mounts for kubelet runtime v1.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) makeMounts(opts *kubecontainer.RunContainerOptions, container *v1.Container) []*runtimeapi.Mount {
0000000000000000000000000000000000000000;;		volumeMounts := []*runtimeapi.Mount{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for idx := range opts.Mounts {
0000000000000000000000000000000000000000;;			v := opts.Mounts[idx]
0000000000000000000000000000000000000000;;			selinuxRelabel := v.SELinuxRelabel && selinux.SELinuxEnabled()
0000000000000000000000000000000000000000;;			mount := &runtimeapi.Mount{
0000000000000000000000000000000000000000;;				HostPath:       v.HostPath,
0000000000000000000000000000000000000000;;				ContainerPath:  v.ContainerPath,
0000000000000000000000000000000000000000;;				Readonly:       v.ReadOnly,
0000000000000000000000000000000000000000;;				SelinuxRelabel: selinuxRelabel,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			volumeMounts = append(volumeMounts, mount)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The reason we create and mount the log file in here (not in kubelet) is because
0000000000000000000000000000000000000000;;		// the file's location depends on the ID of the container, and we need to create and
0000000000000000000000000000000000000000;;		// mount the file before actually starting the container.
0000000000000000000000000000000000000000;;		if opts.PodContainerDir != "" && len(container.TerminationMessagePath) != 0 {
0000000000000000000000000000000000000000;;			// Because the PodContainerDir contains pod uid and container name which is unique enough,
0000000000000000000000000000000000000000;;			// here we just add a random id to make the path unique for different instances
0000000000000000000000000000000000000000;;			// of the same container.
0000000000000000000000000000000000000000;;			cid := makeUID()
0000000000000000000000000000000000000000;;			containerLogPath := filepath.Join(opts.PodContainerDir, cid)
0000000000000000000000000000000000000000;;			fs, err := m.osInterface.Create(containerLogPath)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				utilruntime.HandleError(fmt.Errorf("error on creating termination-log file %q: %v", containerLogPath, err))
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				fs.Close()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// Chmod is needed because ioutil.WriteFile() ends up calling
0000000000000000000000000000000000000000;;				// open(2) to create the file, so the final mode used is "mode &
0000000000000000000000000000000000000000;;				// ~umask". But we want to make sure the specified mode is used
0000000000000000000000000000000000000000;;				// in the file no matter what the umask is.
0000000000000000000000000000000000000000;;				if err := m.osInterface.Chmod(containerLogPath, 0666); err != nil {
0000000000000000000000000000000000000000;;					utilruntime.HandleError(fmt.Errorf("unable to set termination-log file permissions %q: %v", containerLogPath, err))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				selinuxRelabel := selinux.SELinuxEnabled()
0000000000000000000000000000000000000000;;				volumeMounts = append(volumeMounts, &runtimeapi.Mount{
0000000000000000000000000000000000000000;;					HostPath:       containerLogPath,
0000000000000000000000000000000000000000;;					ContainerPath:  container.TerminationMessagePath,
0000000000000000000000000000000000000000;;					SelinuxRelabel: selinuxRelabel,
0000000000000000000000000000000000000000;;				})
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return volumeMounts
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getKubeletContainers lists containers managed by kubelet.
0000000000000000000000000000000000000000;;	// The boolean parameter specifies whether returns all containers including
0000000000000000000000000000000000000000;;	// those already exited and dead containers (used for garbage collection).
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) getKubeletContainers(allContainers bool) ([]*runtimeapi.Container, error) {
0000000000000000000000000000000000000000;;		filter := &runtimeapi.ContainerFilter{}
0000000000000000000000000000000000000000;;		if !allContainers {
0000000000000000000000000000000000000000;;			runningState := runtimeapi.ContainerState_CONTAINER_RUNNING
0000000000000000000000000000000000000000;;			filter.State = &runtimeapi.ContainerStateValue{
0000000000000000000000000000000000000000;;				State: runningState,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		containers, err := m.getContainersHelper(filter)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("getKubeletContainers failed: %v", err)
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return containers, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getContainers lists containers by filter.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) getContainersHelper(filter *runtimeapi.ContainerFilter) ([]*runtimeapi.Container, error) {
0000000000000000000000000000000000000000;;		resp, err := m.runtimeService.ListContainers(filter)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return resp, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// makeUID returns a randomly generated string.
0000000000000000000000000000000000000000;;	func makeUID() string {
0000000000000000000000000000000000000000;;		return fmt.Sprintf("%08x", rand.Uint32())
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getTerminationMessage looks on the filesystem for the provided termination message path, returning a limited
0000000000000000000000000000000000000000;;	// amount of those bytes, or returns true if the logs should be checked.
0000000000000000000000000000000000000000;;	func getTerminationMessage(status *runtimeapi.ContainerStatus, terminationMessagePath string, fallbackToLogs bool) (string, bool) {
0000000000000000000000000000000000000000;;		if len(terminationMessagePath) != 0 {
0000000000000000000000000000000000000000;;			for _, mount := range status.Mounts {
0000000000000000000000000000000000000000;;				if mount.ContainerPath != terminationMessagePath {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				path := mount.HostPath
0000000000000000000000000000000000000000;;				data, _, err := tail.ReadAtMost(path, kubecontainer.MaxContainerTerminationMessageLength)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return fmt.Sprintf("Error on reading termination log %s: %v", path, err), false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if !fallbackToLogs || len(data) != 0 {
0000000000000000000000000000000000000000;;					return string(data), false
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return "", fallbackToLogs
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// readLastStringFromContainerLogs attempts to read up to the max log length from the end of the CRI log represented
0000000000000000000000000000000000000000;;	// by path. It reads up to max log lines.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) readLastStringFromContainerLogs(path string) string {
0000000000000000000000000000000000000000;;		value := int64(kubecontainer.MaxContainerTerminationMessageLogLines)
0000000000000000000000000000000000000000;;		buf, _ := circbuf.NewBuffer(kubecontainer.MaxContainerTerminationMessageLogLength)
0000000000000000000000000000000000000000;;		if err := m.ReadLogs(path, "", &v1.PodLogOptions{TailLines: &value}, buf, buf); err != nil {
0000000000000000000000000000000000000000;;			return fmt.Sprintf("Error on reading termination message from logs: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return buf.String()
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getPodContainerStatuses gets all containers' statuses for the pod.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) getPodContainerStatuses(uid kubetypes.UID, name, namespace string) ([]*kubecontainer.ContainerStatus, error) {
0000000000000000000000000000000000000000;;		// Select all containers of the given pod.
0000000000000000000000000000000000000000;;		containers, err := m.runtimeService.ListContainers(&runtimeapi.ContainerFilter{
0000000000000000000000000000000000000000;;			LabelSelector: map[string]string{types.KubernetesPodUIDLabel: string(uid)},
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("ListContainers error: %v", err)
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		statuses := make([]*kubecontainer.ContainerStatus, len(containers))
0000000000000000000000000000000000000000;;		// TODO: optimization: set maximum number of containers per container name to examine.
0000000000000000000000000000000000000000;;		for i, c := range containers {
0000000000000000000000000000000000000000;;			status, err := m.runtimeService.ContainerStatus(c.Id)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("ContainerStatus for %s error: %v", c.Id, err)
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			cStatus := toKubeContainerStatus(status, m.runtimeName)
0000000000000000000000000000000000000000;;			if status.State == runtimeapi.ContainerState_CONTAINER_EXITED {
0000000000000000000000000000000000000000;;				// Populate the termination message if needed.
0000000000000000000000000000000000000000;;				annotatedInfo := getContainerInfoFromAnnotations(status.Annotations)
0000000000000000000000000000000000000000;;				labeledInfo := getContainerInfoFromLabels(status.Labels)
0000000000000000000000000000000000000000;;				fallbackToLogs := annotatedInfo.TerminationMessagePolicy == v1.TerminationMessageFallbackToLogsOnError && (cStatus.ExitCode != 0 || cStatus.Reason == "OOMKilled")
0000000000000000000000000000000000000000;;				tMessage, checkLogs := getTerminationMessage(status, annotatedInfo.TerminationMessagePath, fallbackToLogs)
0000000000000000000000000000000000000000;;				if checkLogs {
0000000000000000000000000000000000000000;;					path := buildFullContainerLogsPath(uid, labeledInfo.ContainerName, annotatedInfo.RestartCount)
0000000000000000000000000000000000000000;;					tMessage = m.readLastStringFromContainerLogs(path)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// Use the termination message written by the application is not empty
0000000000000000000000000000000000000000;;				if len(tMessage) != 0 {
0000000000000000000000000000000000000000;;					cStatus.Message = tMessage
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			statuses[i] = cStatus
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		sort.Sort(containerStatusByCreated(statuses))
0000000000000000000000000000000000000000;;		return statuses, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func toKubeContainerStatus(status *runtimeapi.ContainerStatus, runtimeName string) *kubecontainer.ContainerStatus {
0000000000000000000000000000000000000000;;		annotatedInfo := getContainerInfoFromAnnotations(status.Annotations)
0000000000000000000000000000000000000000;;		labeledInfo := getContainerInfoFromLabels(status.Labels)
0000000000000000000000000000000000000000;;		cStatus := &kubecontainer.ContainerStatus{
0000000000000000000000000000000000000000;;			ID: kubecontainer.ContainerID{
0000000000000000000000000000000000000000;;				Type: runtimeName,
0000000000000000000000000000000000000000;;				ID:   status.Id,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Name:         labeledInfo.ContainerName,
0000000000000000000000000000000000000000;;			Image:        status.Image.Image,
0000000000000000000000000000000000000000;;			ImageID:      status.ImageRef,
0000000000000000000000000000000000000000;;			Hash:         annotatedInfo.Hash,
0000000000000000000000000000000000000000;;			RestartCount: annotatedInfo.RestartCount,
0000000000000000000000000000000000000000;;			State:        toKubeContainerState(status.State),
0000000000000000000000000000000000000000;;			CreatedAt:    time.Unix(0, status.CreatedAt),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if status.State != runtimeapi.ContainerState_CONTAINER_CREATED {
0000000000000000000000000000000000000000;;			// If container is not in the created state, we have tried and
0000000000000000000000000000000000000000;;			// started the container. Set the StartedAt time.
0000000000000000000000000000000000000000;;			cStatus.StartedAt = time.Unix(0, status.StartedAt)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if status.State == runtimeapi.ContainerState_CONTAINER_EXITED {
0000000000000000000000000000000000000000;;			cStatus.Reason = status.Reason
0000000000000000000000000000000000000000;;			cStatus.Message = status.Message
0000000000000000000000000000000000000000;;			cStatus.ExitCode = int(status.ExitCode)
0000000000000000000000000000000000000000;;			cStatus.FinishedAt = time.Unix(0, status.FinishedAt)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return cStatus
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// executePreStopHook runs the pre-stop lifecycle hooks if applicable and returns the duration it takes.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) executePreStopHook(pod *v1.Pod, containerID kubecontainer.ContainerID, containerSpec *v1.Container, gracePeriod int64) int64 {
0000000000000000000000000000000000000000;;		glog.V(3).Infof("Running preStop hook for container %q", containerID.String())
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		start := metav1.Now()
0000000000000000000000000000000000000000;;		done := make(chan struct{})
0000000000000000000000000000000000000000;;		go func() {
0000000000000000000000000000000000000000;;			defer close(done)
0000000000000000000000000000000000000000;;			defer utilruntime.HandleCrash()
0000000000000000000000000000000000000000;;			if msg, err := m.runner.Run(containerID, pod, containerSpec, containerSpec.Lifecycle.PreStop); err != nil {
0000000000000000000000000000000000000000;;				glog.Errorf("preStop hook for container %q failed: %v", containerSpec.Name, err)
0000000000000000000000000000000000000000;;				m.recordContainerEvent(pod, containerSpec, containerID.ID, v1.EventTypeWarning, events.FailedPreStopHook, msg)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		select {
0000000000000000000000000000000000000000;;		case <-time.After(time.Duration(gracePeriod) * time.Second):
0000000000000000000000000000000000000000;;			glog.V(2).Infof("preStop hook for container %q did not complete in %d seconds", containerID, gracePeriod)
0000000000000000000000000000000000000000;;		case <-done:
0000000000000000000000000000000000000000;;			glog.V(3).Infof("preStop hook for container %q completed", containerID)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return int64(metav1.Now().Sub(start.Time).Seconds())
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// restoreSpecsFromContainerLabels restores all information needed for killing a container. In some
0000000000000000000000000000000000000000;;	// case we may not have pod and container spec when killing a container, e.g. pod is deleted during
0000000000000000000000000000000000000000;;	// kubelet restart.
0000000000000000000000000000000000000000;;	// To solve this problem, we've already written necessary information into container labels. Here we
0000000000000000000000000000000000000000;;	// just need to retrieve them from container labels and restore the specs.
0000000000000000000000000000000000000000;;	// TODO(random-liu): Add a node e2e test to test this behaviour.
0000000000000000000000000000000000000000;;	// TODO(random-liu): Change the lifecycle handler to just accept information needed, so that we can
0000000000000000000000000000000000000000;;	// just pass the needed function not create the fake object.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) restoreSpecsFromContainerLabels(containerID kubecontainer.ContainerID) (*v1.Pod, *v1.Container, error) {
0000000000000000000000000000000000000000;;		var pod *v1.Pod
0000000000000000000000000000000000000000;;		var container *v1.Container
0000000000000000000000000000000000000000;;		s, err := m.runtimeService.ContainerStatus(containerID.ID)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		l := getContainerInfoFromLabels(s.Labels)
0000000000000000000000000000000000000000;;		a := getContainerInfoFromAnnotations(s.Annotations)
0000000000000000000000000000000000000000;;		// Notice that the followings are not full spec. The container killing code should not use
0000000000000000000000000000000000000000;;		// un-restored fields.
0000000000000000000000000000000000000000;;		pod = &v1.Pod{
0000000000000000000000000000000000000000;;			ObjectMeta: metav1.ObjectMeta{
0000000000000000000000000000000000000000;;				UID:                        l.PodUID,
0000000000000000000000000000000000000000;;				Name:                       l.PodName,
0000000000000000000000000000000000000000;;				Namespace:                  l.PodNamespace,
0000000000000000000000000000000000000000;;				DeletionGracePeriodSeconds: a.PodDeletionGracePeriod,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;			Spec: v1.PodSpec{
0000000000000000000000000000000000000000;;				TerminationGracePeriodSeconds: a.PodTerminationGracePeriod,
0000000000000000000000000000000000000000;;			},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		container = &v1.Container{
0000000000000000000000000000000000000000;;			Name:  l.ContainerName,
0000000000000000000000000000000000000000;;			Ports: a.ContainerPorts,
0000000000000000000000000000000000000000;;			TerminationMessagePath: a.TerminationMessagePath,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if a.PreStopHandler != nil {
0000000000000000000000000000000000000000;;			container.Lifecycle = &v1.Lifecycle{
0000000000000000000000000000000000000000;;				PreStop: a.PreStopHandler,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pod, container, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// killContainer kills a container through the following steps:
0000000000000000000000000000000000000000;;	// * Run the pre-stop lifecycle hooks (if applicable).
0000000000000000000000000000000000000000;;	// * Stop the container.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) killContainer(pod *v1.Pod, containerID kubecontainer.ContainerID, containerName string, reason string, gracePeriodOverride *int64) error {
0000000000000000000000000000000000000000;;		var containerSpec *v1.Container
0000000000000000000000000000000000000000;;		if pod != nil {
0000000000000000000000000000000000000000;;			if containerSpec = kubecontainer.GetContainerSpec(pod, containerName); containerSpec == nil {
0000000000000000000000000000000000000000;;				return fmt.Errorf("failed to get containerSpec %q(id=%q) in pod %q when killing container for reason %q",
0000000000000000000000000000000000000000;;					containerName, containerID.String(), format.Pod(pod), reason)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// Restore necessary information if one of the specs is nil.
0000000000000000000000000000000000000000;;			restoredPod, restoredContainer, err := m.restoreSpecsFromContainerLabels(containerID)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			pod, containerSpec = restoredPod, restoredContainer
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// From this point , pod and container must be non-nil.
0000000000000000000000000000000000000000;;		gracePeriod := int64(minimumGracePeriodInSeconds)
0000000000000000000000000000000000000000;;		switch {
0000000000000000000000000000000000000000;;		case pod.DeletionGracePeriodSeconds != nil:
0000000000000000000000000000000000000000;;			gracePeriod = *pod.DeletionGracePeriodSeconds
0000000000000000000000000000000000000000;;		case pod.Spec.TerminationGracePeriodSeconds != nil:
0000000000000000000000000000000000000000;;			gracePeriod = *pod.Spec.TerminationGracePeriodSeconds
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		glog.V(2).Infof("Killing container %q with %d second grace period", containerID.String(), gracePeriod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Run the pre-stop lifecycle hooks if applicable.
0000000000000000000000000000000000000000;;		if containerSpec.Lifecycle != nil && containerSpec.Lifecycle.PreStop != nil {
0000000000000000000000000000000000000000;;			gracePeriod = gracePeriod - m.executePreStopHook(pod, containerID, containerSpec, gracePeriod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// always give containers a minimal shutdown window to avoid unnecessary SIGKILLs
0000000000000000000000000000000000000000;;		if gracePeriod < minimumGracePeriodInSeconds {
0000000000000000000000000000000000000000;;			gracePeriod = minimumGracePeriodInSeconds
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if gracePeriodOverride != nil {
0000000000000000000000000000000000000000;;			gracePeriod = *gracePeriodOverride
0000000000000000000000000000000000000000;;			glog.V(3).Infof("Killing container %q, but using %d second grace period override", containerID, gracePeriod)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := m.runtimeService.StopContainer(containerID.ID, gracePeriod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			glog.Errorf("Container %q termination failed with gracePeriod %d: %v", containerID.String(), gracePeriod, err)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			glog.V(3).Infof("Container %q exited normally", containerID.String())
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		message := fmt.Sprintf("Killing container with id %s", containerID.String())
0000000000000000000000000000000000000000;;		if reason != "" {
0000000000000000000000000000000000000000;;			message = fmt.Sprint(message, ":", reason)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		m.recordContainerEvent(pod, containerSpec, containerID.ID, v1.EventTypeNormal, events.KillingContainer, message)
0000000000000000000000000000000000000000;;		m.containerRefManager.ClearRef(containerID)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// killContainersWithSyncResult kills all pod's containers with sync results.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) killContainersWithSyncResult(pod *v1.Pod, runningPod kubecontainer.Pod, gracePeriodOverride *int64) (syncResults []*kubecontainer.SyncResult) {
0000000000000000000000000000000000000000;;		containerResults := make(chan *kubecontainer.SyncResult, len(runningPod.Containers))
0000000000000000000000000000000000000000;;		wg := sync.WaitGroup{}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		wg.Add(len(runningPod.Containers))
0000000000000000000000000000000000000000;;		for _, container := range runningPod.Containers {
0000000000000000000000000000000000000000;;			go func(container *kubecontainer.Container) {
0000000000000000000000000000000000000000;;				defer utilruntime.HandleCrash()
0000000000000000000000000000000000000000;;				defer wg.Done()
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, container.Name)
0000000000000000000000000000000000000000;;				if err := m.killContainer(pod, container.ID, container.Name, "Need to kill Pod", gracePeriodOverride); err != nil {
0000000000000000000000000000000000000000;;					killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error())
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				containerResults <- killContainerResult
0000000000000000000000000000000000000000;;			}(container)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		wg.Wait()
0000000000000000000000000000000000000000;;		close(containerResults)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for containerResult := range containerResults {
0000000000000000000000000000000000000000;;			syncResults = append(syncResults, containerResult)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// pruneInitContainers ensures that before we begin creating init containers, we have reduced the number
0000000000000000000000000000000000000000;;	// of outstanding init containers still present. This reduces load on the container garbage collector
0000000000000000000000000000000000000000;;	// by only preserving the most recent terminated init container.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) pruneInitContainersBeforeStart(pod *v1.Pod, podStatus *kubecontainer.PodStatus, initContainersToKeep map[kubecontainer.ContainerID]int) {
0000000000000000000000000000000000000000;;		// only the last execution of each init container should be preserved, and only preserve it if it is in the
0000000000000000000000000000000000000000;;		// list of init containers to keep.
0000000000000000000000000000000000000000;;		initContainerNames := sets.NewString()
0000000000000000000000000000000000000000;;		for _, container := range pod.Spec.InitContainers {
0000000000000000000000000000000000000000;;			initContainerNames.Insert(container.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for name := range initContainerNames {
0000000000000000000000000000000000000000;;			count := 0
0000000000000000000000000000000000000000;;			for _, status := range podStatus.ContainerStatuses {
0000000000000000000000000000000000000000;;				if status.Name != name || !initContainerNames.Has(status.Name) || status.State != kubecontainer.ContainerStateExited {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				count++
0000000000000000000000000000000000000000;;				// keep the first init container for this name
0000000000000000000000000000000000000000;;				if count == 1 {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// if there is a reason to preserve the older container, do so
0000000000000000000000000000000000000000;;				if _, ok := initContainersToKeep[status.ID]; ok {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// prune all other init containers that match this container name
0000000000000000000000000000000000000000;;				glog.V(4).Infof("Removing init container %q instance %q %d", status.Name, status.ID.ID, count)
0000000000000000000000000000000000000000;;				if err := m.runtimeService.RemoveContainer(status.ID.ID); err != nil {
0000000000000000000000000000000000000000;;					utilruntime.HandleError(fmt.Errorf("failed to remove pod init container %q: %v; Skipping pod %q", status.Name, err, format.Pod(pod)))
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// remove any references to this container
0000000000000000000000000000000000000000;;				if _, ok := m.containerRefManager.GetRef(status.ID); ok {
0000000000000000000000000000000000000000;;					m.containerRefManager.ClearRef(status.ID)
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					glog.Warningf("No ref for container %q", status.ID)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// findNextInitContainerToRun returns the status of the last failed container, the
0000000000000000000000000000000000000000;;	// next init container to start, or done if there are no further init containers.
0000000000000000000000000000000000000000;;	// Status is only returned if an init container is failed, in which case next will
0000000000000000000000000000000000000000;;	// point to the current container.
0000000000000000000000000000000000000000;;	func findNextInitContainerToRun(pod *v1.Pod, podStatus *kubecontainer.PodStatus) (status *kubecontainer.ContainerStatus, next *v1.Container, done bool) {
0000000000000000000000000000000000000000;;		if len(pod.Spec.InitContainers) == 0 {
0000000000000000000000000000000000000000;;			return nil, nil, true
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If there are failed containers, return the status of the last failed one.
0000000000000000000000000000000000000000;;		for i := len(pod.Spec.InitContainers) - 1; i >= 0; i-- {
0000000000000000000000000000000000000000;;			container := &pod.Spec.InitContainers[i]
0000000000000000000000000000000000000000;;			status := podStatus.FindContainerStatusByName(container.Name)
0000000000000000000000000000000000000000;;			if status != nil && isContainerFailed(status) {
0000000000000000000000000000000000000000;;				return status, container, false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// There are no failed containers now.
0000000000000000000000000000000000000000;;		for i := len(pod.Spec.InitContainers) - 1; i >= 0; i-- {
0000000000000000000000000000000000000000;;			container := &pod.Spec.InitContainers[i]
0000000000000000000000000000000000000000;;			status := podStatus.FindContainerStatusByName(container.Name)
0000000000000000000000000000000000000000;;			if status == nil {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// container is still running, return not done.
0000000000000000000000000000000000000000;;			if status.State == kubecontainer.ContainerStateRunning {
0000000000000000000000000000000000000000;;				return nil, nil, false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if status.State == kubecontainer.ContainerStateExited {
0000000000000000000000000000000000000000;;				// all init containers successful
0000000000000000000000000000000000000000;;				if i == (len(pod.Spec.InitContainers) - 1) {
0000000000000000000000000000000000000000;;					return nil, nil, true
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;				// all containers up to i successful, go to i+1
0000000000000000000000000000000000000000;;				return nil, &pod.Spec.InitContainers[i+1], false
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return nil, &pod.Spec.InitContainers[0], false
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetContainerLogs returns logs of a specific container.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) GetContainerLogs(pod *v1.Pod, containerID kubecontainer.ContainerID, logOptions *v1.PodLogOptions, stdout, stderr io.Writer) (err error) {
0000000000000000000000000000000000000000;;		status, err := m.runtimeService.ContainerStatus(containerID.ID)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to get container status %q: %v", containerID, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		labeledInfo := getContainerInfoFromLabels(status.Labels)
0000000000000000000000000000000000000000;;		annotatedInfo := getContainerInfoFromAnnotations(status.Annotations)
0000000000000000000000000000000000000000;;		path := buildFullContainerLogsPath(pod.UID, labeledInfo.ContainerName, annotatedInfo.RestartCount)
0000000000000000000000000000000000000000;;		return m.ReadLogs(path, containerID.ID, logOptions, stdout, stderr)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetExec gets the endpoint the runtime will serve the exec request from.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) GetExec(id kubecontainer.ContainerID, cmd []string, stdin, stdout, stderr, tty bool) (*url.URL, error) {
0000000000000000000000000000000000000000;;		req := &runtimeapi.ExecRequest{
0000000000000000000000000000000000000000;;			ContainerId: id.ID,
0000000000000000000000000000000000000000;;			Cmd:         cmd,
0000000000000000000000000000000000000000;;			Tty:         tty,
0000000000000000000000000000000000000000;;			Stdin:       stdin,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		resp, err := m.runtimeService.Exec(req)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return url.Parse(resp.Url)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// GetAttach gets the endpoint the runtime will serve the attach request from.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) GetAttach(id kubecontainer.ContainerID, stdin, stdout, stderr, tty bool) (*url.URL, error) {
0000000000000000000000000000000000000000;;		req := &runtimeapi.AttachRequest{
0000000000000000000000000000000000000000;;			ContainerId: id.ID,
0000000000000000000000000000000000000000;;			Stdin:       stdin,
0000000000000000000000000000000000000000;;			Tty:         tty,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		resp, err := m.runtimeService.Attach(req)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return url.Parse(resp.Url)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunInContainer synchronously executes the command in the container, and returns the output.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) RunInContainer(id kubecontainer.ContainerID, cmd []string, timeout time.Duration) ([]byte, error) {
0000000000000000000000000000000000000000;;		stdout, stderr, err := m.runtimeService.ExecSync(id.ID, cmd, 0)
0000000000000000000000000000000000000000;;		// NOTE(tallclair): This does not correctly interleave stdout & stderr, but should be sufficient
0000000000000000000000000000000000000000;;		// for logging purposes. A combined output option will need to be added to the ExecSyncRequest
0000000000000000000000000000000000000000;;		// if more precise output ordering is ever required.
0000000000000000000000000000000000000000;;		return append(stdout, stderr...), err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// removeContainer removes the container and the container logs.
0000000000000000000000000000000000000000;;	// Notice that we remove the container logs first, so that container will not be removed if
0000000000000000000000000000000000000000;;	// container logs are failed to be removed, and kubelet will retry this later. This guarantees
0000000000000000000000000000000000000000;;	// that container logs to be removed with the container.
0000000000000000000000000000000000000000;;	// Notice that we assume that the container should only be removed in non-running state, and
0000000000000000000000000000000000000000;;	// it will not write container logs anymore in that state.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) removeContainer(containerID string) error {
0000000000000000000000000000000000000000;;		glog.V(4).Infof("Removing container %q", containerID)
0000000000000000000000000000000000000000;;		// Remove the container log.
0000000000000000000000000000000000000000;;		// TODO: Separate log and container lifecycle management.
0000000000000000000000000000000000000000;;		if err := m.removeContainerLog(containerID); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Remove the container.
0000000000000000000000000000000000000000;;		return m.runtimeService.RemoveContainer(containerID)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// removeContainerLog removes the container log.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) removeContainerLog(containerID string) error {
0000000000000000000000000000000000000000;;		// Remove the container log.
0000000000000000000000000000000000000000;;		status, err := m.runtimeService.ContainerStatus(containerID)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to get container status %q: %v", containerID, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		labeledInfo := getContainerInfoFromLabels(status.Labels)
0000000000000000000000000000000000000000;;		annotatedInfo := getContainerInfoFromAnnotations(status.Annotations)
0000000000000000000000000000000000000000;;		path := buildFullContainerLogsPath(labeledInfo.PodUID, labeledInfo.ContainerName, annotatedInfo.RestartCount)
0000000000000000000000000000000000000000;;		if err := m.osInterface.Remove(path); err != nil && !os.IsNotExist(err) {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to remove container %q log %q: %v", containerID, path, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Remove the legacy container log symlink.
0000000000000000000000000000000000000000;;		// TODO(random-liu): Remove this after cluster logging supports CRI container log path.
0000000000000000000000000000000000000000;;		legacySymlink := legacyLogSymlink(containerID, labeledInfo.ContainerName, labeledInfo.PodName,
0000000000000000000000000000000000000000;;			labeledInfo.PodNamespace)
0000000000000000000000000000000000000000;;		if err := m.osInterface.Remove(legacySymlink); err != nil && !os.IsNotExist(err) {
0000000000000000000000000000000000000000;;			return fmt.Errorf("failed to remove container %q log legacy symbolic link %q: %v",
0000000000000000000000000000000000000000;;				containerID, legacySymlink, err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// DeleteContainer removes a container.
0000000000000000000000000000000000000000;;	func (m *kubeGenericRuntimeManager) DeleteContainer(containerID kubecontainer.ContainerID) error {
0000000000000000000000000000000000000000;;		return m.removeContainer(containerID.ID)
0000000000000000000000000000000000000000;;	}
