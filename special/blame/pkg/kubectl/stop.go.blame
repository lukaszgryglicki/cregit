0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2014 The Kubernetes Authors.
a18cddc2602cbfa22d60f258fe3bda28bcd48577;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package kubectl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime/schema"
0000000000000000000000000000000000000000;;		utilerrors "k8s.io/apimachinery/pkg/util/errors"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/uuid"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/apps"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/batch"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/apis/extensions"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset"
0000000000000000000000000000000000000000;;		appsclient "k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset/typed/apps/internalversion"
0000000000000000000000000000000000000000;;		batchclient "k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset/typed/batch/internalversion"
0000000000000000000000000000000000000000;;		coreclient "k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset/typed/core/internalversion"
0000000000000000000000000000000000000000;;		extensionsclient "k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset/typed/extensions/internalversion"
0000000000000000000000000000000000000000;;		deploymentutil "k8s.io/kubernetes/pkg/controller/deployment/util"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		Interval = time.Second * 1
0000000000000000000000000000000000000000;;		Timeout  = time.Minute * 5
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// A Reaper terminates an object as gracefully as possible.
0000000000000000000000000000000000000000;;	type Reaper interface {
0000000000000000000000000000000000000000;;		// Stop a given object within a namespace. timeout is how long we'll
0000000000000000000000000000000000000000;;		// wait for the termination to be successful. gracePeriod is time given
0000000000000000000000000000000000000000;;		// to an API object for it to delete itself cleanly (e.g., pod
0000000000000000000000000000000000000000;;		// shutdown). It may or may not be supported by the API object.
0000000000000000000000000000000000000000;;		Stop(namespace, name string, timeout time.Duration, gracePeriod *metav1.DeleteOptions) error
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type NoSuchReaperError struct {
0000000000000000000000000000000000000000;;		kind schema.GroupKind
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (n *NoSuchReaperError) Error() string {
0000000000000000000000000000000000000000;;		return fmt.Sprintf("no reaper has been implemented for %v", n.kind)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func IsNoSuchReaperError(err error) bool {
0000000000000000000000000000000000000000;;		_, ok := err.(*NoSuchReaperError)
0000000000000000000000000000000000000000;;		return ok
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ReaperFor(kind schema.GroupKind, c internalclientset.Interface) (Reaper, error) {
0000000000000000000000000000000000000000;;		switch kind {
0000000000000000000000000000000000000000;;		case api.Kind("ReplicationController"):
0000000000000000000000000000000000000000;;			return &ReplicationControllerReaper{c.Core(), Interval, Timeout}, nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case extensions.Kind("ReplicaSet"):
0000000000000000000000000000000000000000;;			return &ReplicaSetReaper{c.Extensions(), Interval, Timeout}, nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case extensions.Kind("DaemonSet"):
0000000000000000000000000000000000000000;;			return &DaemonSetReaper{c.Extensions(), Interval, Timeout}, nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case api.Kind("Pod"):
0000000000000000000000000000000000000000;;			return &PodReaper{c.Core()}, nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case batch.Kind("Job"):
0000000000000000000000000000000000000000;;			return &JobReaper{c.Batch(), c.Core(), Interval, Timeout}, nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case apps.Kind("StatefulSet"):
0000000000000000000000000000000000000000;;			return &StatefulSetReaper{c.Apps(), c.Core(), Interval, Timeout}, nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		case extensions.Kind("Deployment"), apps.Kind("Deployment"):
0000000000000000000000000000000000000000;;			return &DeploymentReaper{c.Extensions(), c.Extensions(), Interval, Timeout}, nil
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil, &NoSuchReaperError{kind}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func ReaperForReplicationController(rcClient coreclient.ReplicationControllersGetter, timeout time.Duration) (Reaper, error) {
0000000000000000000000000000000000000000;;		return &ReplicationControllerReaper{rcClient, Interval, timeout}, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type ReplicationControllerReaper struct {
0000000000000000000000000000000000000000;;		client                coreclient.ReplicationControllersGetter
0000000000000000000000000000000000000000;;		pollInterval, timeout time.Duration
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	type ReplicaSetReaper struct {
0000000000000000000000000000000000000000;;		client                extensionsclient.ReplicaSetsGetter
0000000000000000000000000000000000000000;;		pollInterval, timeout time.Duration
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	type DaemonSetReaper struct {
0000000000000000000000000000000000000000;;		client                extensionsclient.DaemonSetsGetter
0000000000000000000000000000000000000000;;		pollInterval, timeout time.Duration
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	type JobReaper struct {
0000000000000000000000000000000000000000;;		client                batchclient.JobsGetter
0000000000000000000000000000000000000000;;		podClient             coreclient.PodsGetter
0000000000000000000000000000000000000000;;		pollInterval, timeout time.Duration
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	type DeploymentReaper struct {
0000000000000000000000000000000000000000;;		dClient               extensionsclient.DeploymentsGetter
0000000000000000000000000000000000000000;;		rsClient              extensionsclient.ReplicaSetsGetter
0000000000000000000000000000000000000000;;		pollInterval, timeout time.Duration
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	type PodReaper struct {
0000000000000000000000000000000000000000;;		client coreclient.PodsGetter
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	type StatefulSetReaper struct {
0000000000000000000000000000000000000000;;		client                appsclient.StatefulSetsGetter
0000000000000000000000000000000000000000;;		podClient             coreclient.PodsGetter
0000000000000000000000000000000000000000;;		pollInterval, timeout time.Duration
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getOverlappingControllers finds rcs that this controller overlaps, as well as rcs overlapping this controller.
0000000000000000000000000000000000000000;;	func getOverlappingControllers(rcClient coreclient.ReplicationControllerInterface, rc *api.ReplicationController) ([]api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		rcs, err := rcClient.List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("error getting replication controllers: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		var matchingRCs []api.ReplicationController
0000000000000000000000000000000000000000;;		rcLabels := labels.Set(rc.Spec.Selector)
0000000000000000000000000000000000000000;;		for _, controller := range rcs.Items {
0000000000000000000000000000000000000000;;			newRCLabels := labels.Set(controller.Spec.Selector)
0000000000000000000000000000000000000000;;			if labels.SelectorFromSet(newRCLabels).Matches(rcLabels) || labels.SelectorFromSet(rcLabels).Matches(newRCLabels) {
0000000000000000000000000000000000000000;;				matchingRCs = append(matchingRCs, controller)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return matchingRCs, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (reaper *ReplicationControllerReaper) Stop(namespace, name string, timeout time.Duration, gracePeriod *metav1.DeleteOptions) error {
0000000000000000000000000000000000000000;;		rc := reaper.client.ReplicationControllers(namespace)
0000000000000000000000000000000000000000;;		scaler := &ReplicationControllerScaler{reaper.client}
0000000000000000000000000000000000000000;;		ctrl, err := rc.Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if timeout == 0 {
0000000000000000000000000000000000000000;;			timeout = Timeout + time.Duration(10*ctrl.Spec.Replicas)*time.Second
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The rc manager will try and detect all matching rcs for a pod's labels,
0000000000000000000000000000000000000000;;		// and only sync the oldest one. This means if we have a pod with labels
0000000000000000000000000000000000000000;;		// [(k1: v1), (k2: v2)] and two rcs: rc1 with selector [(k1=v1)], and rc2 with selector [(k1=v1),(k2=v2)],
0000000000000000000000000000000000000000;;		// the rc manager will sync the older of the two rcs.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// If there are rcs with a superset of labels, eg:
0000000000000000000000000000000000000000;;		// deleting: (k1=v1), superset: (k2=v2, k1=v1)
0000000000000000000000000000000000000000;;		//	- It isn't safe to delete the rc because there could be a pod with labels
0000000000000000000000000000000000000000;;		//	  (k1=v1) that isn't managed by the superset rc. We can't scale it down
0000000000000000000000000000000000000000;;		//	  either, because there could be a pod (k2=v2, k1=v1) that it deletes
0000000000000000000000000000000000000000;;		//	  causing a fight with the superset rc.
0000000000000000000000000000000000000000;;		// If there are rcs with a subset of labels, eg:
0000000000000000000000000000000000000000;;		// deleting: (k2=v2, k1=v1), subset: (k1=v1), superset: (k2=v2, k1=v1, k3=v3)
0000000000000000000000000000000000000000;;		//  - Even if it's safe to delete this rc without a scale down because all it's pods
0000000000000000000000000000000000000000;;		//	  are being controlled by the subset rc the code returns an error.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// In theory, creating overlapping controllers is user error, so the loop below
0000000000000000000000000000000000000000;;		// tries to account for this logic only in the common case, where we end up
0000000000000000000000000000000000000000;;		// with multiple rcs that have an exact match on selectors.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		overlappingCtrls, err := getOverlappingControllers(rc, ctrl)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error getting replication controllers: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		exactMatchRCs := []api.ReplicationController{}
0000000000000000000000000000000000000000;;		overlapRCs := []string{}
0000000000000000000000000000000000000000;;		for _, overlappingRC := range overlappingCtrls {
0000000000000000000000000000000000000000;;			if len(overlappingRC.Spec.Selector) == len(ctrl.Spec.Selector) {
0000000000000000000000000000000000000000;;				exactMatchRCs = append(exactMatchRCs, overlappingRC)
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				overlapRCs = append(overlapRCs, overlappingRC.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(overlapRCs) > 0 {
0000000000000000000000000000000000000000;;			return fmt.Errorf(
0000000000000000000000000000000000000000;;				"Detected overlapping controllers for rc %v: %v, please manage deletion individually with --cascade=false.",
0000000000000000000000000000000000000000;;				ctrl.Name, strings.Join(overlapRCs, ","))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(exactMatchRCs) == 1 {
0000000000000000000000000000000000000000;;			// No overlapping controllers.
0000000000000000000000000000000000000000;;			retry := NewRetryParams(reaper.pollInterval, reaper.timeout)
0000000000000000000000000000000000000000;;			waitForReplicas := NewRetryParams(reaper.pollInterval, timeout)
0000000000000000000000000000000000000000;;			if err = scaler.Scale(namespace, name, 0, nil, retry, waitForReplicas); err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		falseVar := false
0000000000000000000000000000000000000000;;		deleteOptions := &metav1.DeleteOptions{OrphanDependents: &falseVar}
0000000000000000000000000000000000000000;;		return rc.Delete(name, deleteOptions)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// TODO(madhusudancs): Implement it when controllerRef is implemented - https://github.com/kubernetes/kubernetes/issues/2210
0000000000000000000000000000000000000000;;	// getOverlappingReplicaSets finds ReplicaSets that this ReplicaSet overlaps, as well as ReplicaSets overlapping this ReplicaSet.
0000000000000000000000000000000000000000;;	func getOverlappingReplicaSets(c extensionsclient.ReplicaSetInterface, rs *extensions.ReplicaSet) ([]extensions.ReplicaSet, []extensions.ReplicaSet, error) {
0000000000000000000000000000000000000000;;		var overlappingRSs, exactMatchRSs []extensions.ReplicaSet
0000000000000000000000000000000000000000;;		return overlappingRSs, exactMatchRSs, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (reaper *ReplicaSetReaper) Stop(namespace, name string, timeout time.Duration, gracePeriod *metav1.DeleteOptions) error {
0000000000000000000000000000000000000000;;		rsc := reaper.client.ReplicaSets(namespace)
0000000000000000000000000000000000000000;;		scaler := &ReplicaSetScaler{reaper.client}
0000000000000000000000000000000000000000;;		rs, err := rsc.Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if timeout == 0 {
0000000000000000000000000000000000000000;;			timeout = Timeout + time.Duration(10*rs.Spec.Replicas)*time.Second
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// The ReplicaSet controller will try and detect all matching ReplicaSets
0000000000000000000000000000000000000000;;		// for a pod's labels, and only sync the oldest one. This means if we have
0000000000000000000000000000000000000000;;		// a pod with labels [(k1: v1), (k2: v2)] and two ReplicaSets: rs1 with
0000000000000000000000000000000000000000;;		// selector [(k1=v1)], and rs2 with selector [(k1=v1),(k2=v2)], the
0000000000000000000000000000000000000000;;		// ReplicaSet controller will sync the older of the two ReplicaSets.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// If there are ReplicaSets with a superset of labels, eg:
0000000000000000000000000000000000000000;;		// deleting: (k1=v1), superset: (k2=v2, k1=v1)
0000000000000000000000000000000000000000;;		//	- It isn't safe to delete the ReplicaSet because there could be a pod
0000000000000000000000000000000000000000;;		//    with labels (k1=v1) that isn't managed by the superset ReplicaSet.
0000000000000000000000000000000000000000;;		//    We can't scale it down either, because there could be a pod
0000000000000000000000000000000000000000;;		//    (k2=v2, k1=v1) that it deletes causing a fight with the superset
0000000000000000000000000000000000000000;;		//    ReplicaSet.
0000000000000000000000000000000000000000;;		// If there are ReplicaSets with a subset of labels, eg:
0000000000000000000000000000000000000000;;		// deleting: (k2=v2, k1=v1), subset: (k1=v1), superset: (k2=v2, k1=v1, k3=v3)
0000000000000000000000000000000000000000;;		//  - Even if it's safe to delete this ReplicaSet without a scale down because
0000000000000000000000000000000000000000;;		//    all it's pods are being controlled by the subset ReplicaSet the code
0000000000000000000000000000000000000000;;		//    returns an error.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// In theory, creating overlapping ReplicaSets is user error, so the loop below
0000000000000000000000000000000000000000;;		// tries to account for this logic only in the common case, where we end up
0000000000000000000000000000000000000000;;		// with multiple ReplicaSets that have an exact match on selectors.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO(madhusudancs): Re-evaluate again when controllerRef is implemented -
0000000000000000000000000000000000000000;;		// https://github.com/kubernetes/kubernetes/issues/2210
0000000000000000000000000000000000000000;;		overlappingRSs, exactMatchRSs, err := getOverlappingReplicaSets(rsc, rs)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("error getting ReplicaSets: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(overlappingRSs) > 0 {
0000000000000000000000000000000000000000;;			var names []string
0000000000000000000000000000000000000000;;			for _, overlappingRS := range overlappingRSs {
0000000000000000000000000000000000000000;;				names = append(names, overlappingRS.Name)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return fmt.Errorf(
0000000000000000000000000000000000000000;;				"Detected overlapping ReplicaSets for ReplicaSet %v: %v, please manage deletion individually with --cascade=false.",
0000000000000000000000000000000000000000;;				rs.Name, strings.Join(names, ","))
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(exactMatchRSs) == 0 {
0000000000000000000000000000000000000000;;			// No overlapping ReplicaSets.
0000000000000000000000000000000000000000;;			retry := NewRetryParams(reaper.pollInterval, reaper.timeout)
0000000000000000000000000000000000000000;;			waitForReplicas := NewRetryParams(reaper.pollInterval, timeout)
0000000000000000000000000000000000000000;;			if err = scaler.Scale(namespace, name, 0, nil, retry, waitForReplicas); err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		falseVar := false
0000000000000000000000000000000000000000;;		deleteOptions := &metav1.DeleteOptions{OrphanDependents: &falseVar}
0000000000000000000000000000000000000000;;		return rsc.Delete(name, deleteOptions)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (reaper *DaemonSetReaper) Stop(namespace, name string, timeout time.Duration, gracePeriod *metav1.DeleteOptions) error {
0000000000000000000000000000000000000000;;		ds, err := reaper.client.DaemonSets(namespace).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// We set the nodeSelector to a random label. This label is nearly guaranteed
0000000000000000000000000000000000000000;;		// to not be set on any node so the DameonSetController will start deleting
0000000000000000000000000000000000000000;;		// daemon pods. Once it's done deleting the daemon pods, it's safe to delete
0000000000000000000000000000000000000000;;		// the DaemonSet.
0000000000000000000000000000000000000000;;		ds.Spec.Template.Spec.NodeSelector = map[string]string{
0000000000000000000000000000000000000000;;			string(uuid.NewUUID()): string(uuid.NewUUID()),
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// force update to avoid version conflict
0000000000000000000000000000000000000000;;		ds.ResourceVersion = ""
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if ds, err = reaper.client.DaemonSets(namespace).Update(ds); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Wait for the daemon set controller to kill all the daemon pods.
0000000000000000000000000000000000000000;;		if err := wait.Poll(reaper.pollInterval, reaper.timeout, func() (bool, error) {
0000000000000000000000000000000000000000;;			updatedDS, err := reaper.client.DaemonSets(namespace).Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			return updatedDS.Status.CurrentNumberScheduled+updatedDS.Status.NumberMisscheduled == 0, nil
0000000000000000000000000000000000000000;;		}); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		falseVar := false
0000000000000000000000000000000000000000;;		deleteOptions := &metav1.DeleteOptions{OrphanDependents: &falseVar}
0000000000000000000000000000000000000000;;		return reaper.client.DaemonSets(namespace).Delete(name, deleteOptions)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (reaper *StatefulSetReaper) Stop(namespace, name string, timeout time.Duration, gracePeriod *metav1.DeleteOptions) error {
0000000000000000000000000000000000000000;;		statefulsets := reaper.client.StatefulSets(namespace)
0000000000000000000000000000000000000000;;		scaler := &StatefulSetScaler{reaper.client}
0000000000000000000000000000000000000000;;		ss, err := statefulsets.Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if timeout == 0 {
0000000000000000000000000000000000000000;;			numReplicas := ss.Spec.Replicas
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// BUG: this timeout is never used.
0000000000000000000000000000000000000000;;			timeout = Timeout + time.Duration(10*numReplicas)*time.Second
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		retry := NewRetryParams(reaper.pollInterval, reaper.timeout)
0000000000000000000000000000000000000000;;		waitForStatefulSet := NewRetryParams(reaper.pollInterval, reaper.timeout)
0000000000000000000000000000000000000000;;		if err = scaler.Scale(namespace, name, 0, nil, retry, waitForStatefulSet); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: Cleanup volumes? We don't want to accidentally delete volumes from
0000000000000000000000000000000000000000;;		// stop, so just leave this up to the statefulset.
0000000000000000000000000000000000000000;;		falseVar := false
0000000000000000000000000000000000000000;;		deleteOptions := &metav1.DeleteOptions{OrphanDependents: &falseVar}
0000000000000000000000000000000000000000;;		return statefulsets.Delete(name, deleteOptions)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (reaper *JobReaper) Stop(namespace, name string, timeout time.Duration, gracePeriod *metav1.DeleteOptions) error {
0000000000000000000000000000000000000000;;		jobs := reaper.client.Jobs(namespace)
0000000000000000000000000000000000000000;;		pods := reaper.podClient.Pods(namespace)
0000000000000000000000000000000000000000;;		scaler := &JobScaler{reaper.client}
0000000000000000000000000000000000000000;;		job, err := jobs.Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if timeout == 0 {
0000000000000000000000000000000000000000;;			// we will never have more active pods than job.Spec.Parallelism
0000000000000000000000000000000000000000;;			parallelism := *job.Spec.Parallelism
0000000000000000000000000000000000000000;;			timeout = Timeout + time.Duration(10*parallelism)*time.Second
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// TODO: handle overlapping jobs
0000000000000000000000000000000000000000;;		retry := NewRetryParams(reaper.pollInterval, reaper.timeout)
0000000000000000000000000000000000000000;;		waitForJobs := NewRetryParams(reaper.pollInterval, timeout)
0000000000000000000000000000000000000000;;		if err = scaler.Scale(namespace, name, 0, nil, retry, waitForJobs); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// at this point only dead pods are left, that should be removed
0000000000000000000000000000000000000000;;		selector, _ := metav1.LabelSelectorAsSelector(job.Spec.Selector)
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{LabelSelector: selector.String()}
0000000000000000000000000000000000000000;;		podList, err := pods.List(options)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		errList := []error{}
0000000000000000000000000000000000000000;;		for _, pod := range podList.Items {
0000000000000000000000000000000000000000;;			if err := pods.Delete(pod.Name, gracePeriod); err != nil {
0000000000000000000000000000000000000000;;				// ignores the error when the pod isn't found
0000000000000000000000000000000000000000;;				if !errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;					errList = append(errList, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(errList) > 0 {
0000000000000000000000000000000000000000;;			return utilerrors.NewAggregate(errList)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// once we have all the pods removed we can safely remove the job itself.
0000000000000000000000000000000000000000;;		falseVar := false
0000000000000000000000000000000000000000;;		deleteOptions := &metav1.DeleteOptions{OrphanDependents: &falseVar}
0000000000000000000000000000000000000000;;		return jobs.Delete(name, deleteOptions)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (reaper *DeploymentReaper) Stop(namespace, name string, timeout time.Duration, gracePeriod *metav1.DeleteOptions) error {
0000000000000000000000000000000000000000;;		deployments := reaper.dClient.Deployments(namespace)
0000000000000000000000000000000000000000;;		rsReaper := &ReplicaSetReaper{reaper.rsClient, reaper.pollInterval, reaper.timeout}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		deployment, err := reaper.updateDeploymentWithRetries(namespace, name, func(d *extensions.Deployment) {
0000000000000000000000000000000000000000;;			// set deployment's history and scale to 0
0000000000000000000000000000000000000000;;			// TODO replace with patch when available: https://github.com/kubernetes/kubernetes/issues/20527
0000000000000000000000000000000000000000;;			rhl := int32(0)
0000000000000000000000000000000000000000;;			d.Spec.RevisionHistoryLimit = &rhl
0000000000000000000000000000000000000000;;			d.Spec.Replicas = 0
0000000000000000000000000000000000000000;;			d.Spec.Paused = true
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Use observedGeneration to determine if the deployment controller noticed the pause.
0000000000000000000000000000000000000000;;		if err := deploymentutil.WaitForObservedDeploymentInternal(func() (*extensions.Deployment, error) {
0000000000000000000000000000000000000000;;			return deployments.Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		}, deployment.Generation, 1*time.Second, 1*time.Minute); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Stop all replica sets belonging to this Deployment.
0000000000000000000000000000000000000000;;		rss, err := deploymentutil.ListReplicaSetsInternal(deployment,
0000000000000000000000000000000000000000;;			func(namespace string, options metav1.ListOptions) ([]*extensions.ReplicaSet, error) {
0000000000000000000000000000000000000000;;				rsList, err := reaper.rsClient.ReplicaSets(namespace).List(options)
0000000000000000000000000000000000000000;;				if err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				rss := make([]*extensions.ReplicaSet, 0, len(rsList.Items))
0000000000000000000000000000000000000000;;				for i := range rsList.Items {
0000000000000000000000000000000000000000;;					rss = append(rss, &rsList.Items[i])
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				return rss, nil
0000000000000000000000000000000000000000;;			})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		errList := []error{}
0000000000000000000000000000000000000000;;		for _, rs := range rss {
0000000000000000000000000000000000000000;;			if err := rsReaper.Stop(rs.Namespace, rs.Name, timeout, gracePeriod); err != nil {
0000000000000000000000000000000000000000;;				scaleGetErr, ok := err.(ScaleError)
0000000000000000000000000000000000000000;;				if errors.IsNotFound(err) || (ok && errors.IsNotFound(scaleGetErr.ActualError)) {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				errList = append(errList, err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if len(errList) > 0 {
0000000000000000000000000000000000000000;;			return utilerrors.NewAggregate(errList)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Delete deployment at the end.
0000000000000000000000000000000000000000;;		// Note: We delete deployment at the end so that if removing RSs fails, we at least have the deployment to retry.
0000000000000000000000000000000000000000;;		var falseVar = false
0000000000000000000000000000000000000000;;		nonOrphanOption := metav1.DeleteOptions{OrphanDependents: &falseVar}
0000000000000000000000000000000000000000;;		return deployments.Delete(name, &nonOrphanOption)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type updateDeploymentFunc func(d *extensions.Deployment)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (reaper *DeploymentReaper) updateDeploymentWithRetries(namespace, name string, applyUpdate updateDeploymentFunc) (deployment *extensions.Deployment, err error) {
0000000000000000000000000000000000000000;;		deployments := reaper.dClient.Deployments(namespace)
0000000000000000000000000000000000000000;;		err = wait.Poll(10*time.Millisecond, 1*time.Minute, func() (bool, error) {
0000000000000000000000000000000000000000;;			if deployment, err = deployments.Get(name, metav1.GetOptions{}); err != nil {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Apply the update, then attempt to push it to the apiserver.
0000000000000000000000000000000000000000;;			applyUpdate(deployment)
0000000000000000000000000000000000000000;;			if deployment, err = deployments.Update(deployment); err == nil {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Retry only on update conflict.
0000000000000000000000000000000000000000;;			if errors.IsConflict(err) {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return false, err
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		return deployment, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (reaper *PodReaper) Stop(namespace, name string, timeout time.Duration, gracePeriod *metav1.DeleteOptions) error {
0000000000000000000000000000000000000000;;		pods := reaper.client.Pods(namespace)
0000000000000000000000000000000000000000;;		_, err := pods.Get(name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return pods.Delete(name, gracePeriod)
0000000000000000000000000000000000000000;;	}
