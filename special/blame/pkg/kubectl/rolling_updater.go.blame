0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2014 The Kubernetes Authors.
fd833fbcbbd4c596f1cda70445f373eb91366319;;	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package kubectl
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"io"
0000000000000000000000000000000000000000;;		"strconv"
0000000000000000000000000000000000000000;;		"strings"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/api/core/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/labels"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/intstr"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/client-go/util/integer"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		k8s_api_v1 "k8s.io/kubernetes/pkg/api/v1"
0000000000000000000000000000000000000000;;		podutil "k8s.io/kubernetes/pkg/api/v1/pod"
0000000000000000000000000000000000000000;;		coreclient "k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset/typed/core/internalversion"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/retry"
0000000000000000000000000000000000000000;;		client "k8s.io/kubernetes/pkg/client/unversioned"
0000000000000000000000000000000000000000;;		deploymentutil "k8s.io/kubernetes/pkg/controller/deployment/util"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/kubectl/util"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		sourceIdAnnotation         = kubectlAnnotationPrefix + "update-source-id"
0000000000000000000000000000000000000000;;		desiredReplicasAnnotation  = kubectlAnnotationPrefix + "desired-replicas"
0000000000000000000000000000000000000000;;		originalReplicasAnnotation = kubectlAnnotationPrefix + "original-replicas"
0000000000000000000000000000000000000000;;		nextControllerAnnotation   = kubectlAnnotationPrefix + "next-controller-id"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RollingUpdaterConfig is the configuration for a rolling deployment process.
0000000000000000000000000000000000000000;;	type RollingUpdaterConfig struct {
0000000000000000000000000000000000000000;;		// Out is a writer for progress output.
0000000000000000000000000000000000000000;;		Out io.Writer
0000000000000000000000000000000000000000;;		// OldRC is an existing controller to be replaced.
0000000000000000000000000000000000000000;;		OldRc *api.ReplicationController
0000000000000000000000000000000000000000;;		// NewRc is a controller that will take ownership of updated pods (will be
0000000000000000000000000000000000000000;;		// created if needed).
0000000000000000000000000000000000000000;;		NewRc *api.ReplicationController
0000000000000000000000000000000000000000;;		// UpdatePeriod is the time to wait between individual pod updates.
0000000000000000000000000000000000000000;;		UpdatePeriod time.Duration
0000000000000000000000000000000000000000;;		// Interval is the time to wait between polling controller status after
0000000000000000000000000000000000000000;;		// update.
0000000000000000000000000000000000000000;;		Interval time.Duration
0000000000000000000000000000000000000000;;		// Timeout is the time to wait for controller updates before giving up.
0000000000000000000000000000000000000000;;		Timeout time.Duration
0000000000000000000000000000000000000000;;		// MinReadySeconds is the number of seconds to wait after the pods are ready
0000000000000000000000000000000000000000;;		MinReadySeconds int32
0000000000000000000000000000000000000000;;		// CleanupPolicy defines the cleanup action to take after the deployment is
0000000000000000000000000000000000000000;;		// complete.
0000000000000000000000000000000000000000;;		CleanupPolicy RollingUpdaterCleanupPolicy
0000000000000000000000000000000000000000;;		// MaxUnavailable is the maximum number of pods that can be unavailable during the update.
0000000000000000000000000000000000000000;;		// Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%).
0000000000000000000000000000000000000000;;		// Absolute number is calculated from percentage by rounding up.
0000000000000000000000000000000000000000;;		// This can not be 0 if MaxSurge is 0.
0000000000000000000000000000000000000000;;		// By default, a fixed value of 1 is used.
0000000000000000000000000000000000000000;;		// Example: when this is set to 30%, the old RC can be scaled down to 70% of desired pods
0000000000000000000000000000000000000000;;		// immediately when the rolling update starts. Once new pods are ready, old RC
0000000000000000000000000000000000000000;;		// can be scaled down further, followed by scaling up the new RC, ensuring
0000000000000000000000000000000000000000;;		// that the total number of pods available at all times during the update is at
0000000000000000000000000000000000000000;;		// least 70% of desired pods.
0000000000000000000000000000000000000000;;		MaxUnavailable intstr.IntOrString
0000000000000000000000000000000000000000;;		// MaxSurge is the maximum number of pods that can be scheduled above the desired number of pods.
0000000000000000000000000000000000000000;;		// Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%).
0000000000000000000000000000000000000000;;		// This can not be 0 if MaxUnavailable is 0.
0000000000000000000000000000000000000000;;		// Absolute number is calculated from percentage by rounding up.
0000000000000000000000000000000000000000;;		// By default, a value of 1 is used.
0000000000000000000000000000000000000000;;		// Example: when this is set to 30%, the new RC can be scaled up immediately
0000000000000000000000000000000000000000;;		// when the rolling update starts, such that the total number of old and new pods do not exceed
0000000000000000000000000000000000000000;;		// 130% of desired pods. Once old pods have been killed, new RC can be scaled up
0000000000000000000000000000000000000000;;		// further, ensuring that total number of pods running at any time during
0000000000000000000000000000000000000000;;		// the update is atmost 130% of desired pods.
0000000000000000000000000000000000000000;;		MaxSurge intstr.IntOrString
0000000000000000000000000000000000000000;;		// OnProgress is invoked if set during each scale cycle, to allow the caller to perform additional logic or
0000000000000000000000000000000000000000;;		// abort the scale. If an error is returned the cleanup method will not be invoked. The percentage value
0000000000000000000000000000000000000000;;		// is a synthetic "progress" calculation that represents the approximate percentage completion.
0000000000000000000000000000000000000000;;		OnProgress func(oldRc, newRc *api.ReplicationController, percentage int) error
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RollingUpdaterCleanupPolicy is a cleanup action to take after the
0000000000000000000000000000000000000000;;	// deployment is complete.
0000000000000000000000000000000000000000;;	type RollingUpdaterCleanupPolicy string
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	const (
0000000000000000000000000000000000000000;;		// DeleteRollingUpdateCleanupPolicy means delete the old controller.
0000000000000000000000000000000000000000;;		DeleteRollingUpdateCleanupPolicy RollingUpdaterCleanupPolicy = "Delete"
0000000000000000000000000000000000000000;;		// PreserveRollingUpdateCleanupPolicy means keep the old controller.
0000000000000000000000000000000000000000;;		PreserveRollingUpdateCleanupPolicy RollingUpdaterCleanupPolicy = "Preserve"
0000000000000000000000000000000000000000;;		// RenameRollingUpdateCleanupPolicy means delete the old controller, and rename
0000000000000000000000000000000000000000;;		// the new controller to the name of the old controller.
0000000000000000000000000000000000000000;;		RenameRollingUpdateCleanupPolicy RollingUpdaterCleanupPolicy = "Rename"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RollingUpdater provides methods for updating replicated pods in a predictable,
0000000000000000000000000000000000000000;;	// fault-tolerant way.
0000000000000000000000000000000000000000;;	type RollingUpdater struct {
0000000000000000000000000000000000000000;;		rcClient  coreclient.ReplicationControllersGetter
0000000000000000000000000000000000000000;;		podClient coreclient.PodsGetter
0000000000000000000000000000000000000000;;		// Namespace for resources
0000000000000000000000000000000000000000;;		ns string
0000000000000000000000000000000000000000;;		// scaleAndWait scales a controller and returns its updated state.
0000000000000000000000000000000000000000;;		scaleAndWait func(rc *api.ReplicationController, retry *RetryParams, wait *RetryParams) (*api.ReplicationController, error)
0000000000000000000000000000000000000000;;		//getOrCreateTargetController gets and validates an existing controller or
0000000000000000000000000000000000000000;;		//makes a new one.
0000000000000000000000000000000000000000;;		getOrCreateTargetController func(controller *api.ReplicationController, sourceId string) (*api.ReplicationController, bool, error)
0000000000000000000000000000000000000000;;		// cleanup performs post deployment cleanup tasks for newRc and oldRc.
0000000000000000000000000000000000000000;;		cleanup func(oldRc, newRc *api.ReplicationController, config *RollingUpdaterConfig) error
0000000000000000000000000000000000000000;;		// getReadyPods returns the amount of old and new ready pods.
0000000000000000000000000000000000000000;;		getReadyPods func(oldRc, newRc *api.ReplicationController, minReadySeconds int32) (int32, int32, error)
0000000000000000000000000000000000000000;;		// nowFn returns the current time used to calculate the minReadySeconds
0000000000000000000000000000000000000000;;		nowFn func() metav1.Time
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewRollingUpdater creates a RollingUpdater from a client.
0000000000000000000000000000000000000000;;	func NewRollingUpdater(namespace string, rcClient coreclient.ReplicationControllersGetter, podClient coreclient.PodsGetter) *RollingUpdater {
0000000000000000000000000000000000000000;;		updater := &RollingUpdater{
0000000000000000000000000000000000000000;;			rcClient:  rcClient,
0000000000000000000000000000000000000000;;			podClient: podClient,
0000000000000000000000000000000000000000;;			ns:        namespace,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Inject real implementations.
0000000000000000000000000000000000000000;;		updater.scaleAndWait = updater.scaleAndWaitWithScaler
0000000000000000000000000000000000000000;;		updater.getOrCreateTargetController = updater.getOrCreateTargetControllerWithClient
0000000000000000000000000000000000000000;;		updater.getReadyPods = updater.readyPods
0000000000000000000000000000000000000000;;		updater.cleanup = updater.cleanupWithClients
0000000000000000000000000000000000000000;;		updater.nowFn = func() metav1.Time { return metav1.Now() }
0000000000000000000000000000000000000000;;		return updater
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Update all pods for a ReplicationController (oldRc) by creating a new
0000000000000000000000000000000000000000;;	// controller (newRc) with 0 replicas, and synchronously scaling oldRc and
0000000000000000000000000000000000000000;;	// newRc until oldRc has 0 replicas and newRc has the original # of desired
0000000000000000000000000000000000000000;;	// replicas. Cleanup occurs based on a RollingUpdaterCleanupPolicy.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Each interval, the updater will attempt to make progress however it can
0000000000000000000000000000000000000000;;	// without violating any availability constraints defined by the config. This
0000000000000000000000000000000000000000;;	// means the amount scaled up or down each interval will vary based on the
0000000000000000000000000000000000000000;;	// timeliness of readiness and the updater will always try to make progress,
0000000000000000000000000000000000000000;;	// even slowly.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// If an update from newRc to oldRc is already in progress, we attempt to
0000000000000000000000000000000000000000;;	// drive it to completion. If an error occurs at any step of the update, the
0000000000000000000000000000000000000000;;	// error will be returned.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// A scaling event (either up or down) is considered progress; if no progress
0000000000000000000000000000000000000000;;	// is made within the config.Timeout, an error is returned.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// TODO: make this handle performing a rollback of a partially completed
0000000000000000000000000000000000000000;;	// rollout.
0000000000000000000000000000000000000000;;	func (r *RollingUpdater) Update(config *RollingUpdaterConfig) error {
0000000000000000000000000000000000000000;;		out := config.Out
0000000000000000000000000000000000000000;;		oldRc := config.OldRc
0000000000000000000000000000000000000000;;		scaleRetryParams := NewRetryParams(config.Interval, config.Timeout)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Find an existing controller (for continuing an interrupted update) or
0000000000000000000000000000000000000000;;		// create a new one if necessary.
0000000000000000000000000000000000000000;;		sourceId := fmt.Sprintf("%s:%s", oldRc.Name, oldRc.UID)
0000000000000000000000000000000000000000;;		newRc, existed, err := r.getOrCreateTargetController(config.NewRc, sourceId)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if existed {
0000000000000000000000000000000000000000;;			fmt.Fprintf(out, "Continuing update with existing controller %s.\n", newRc.Name)
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			fmt.Fprintf(out, "Created %s\n", newRc.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Extract the desired replica count from the controller.
0000000000000000000000000000000000000000;;		desiredAnnotation, err := strconv.Atoi(newRc.Annotations[desiredReplicasAnnotation])
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("Unable to parse annotation for %s: %s=%s",
0000000000000000000000000000000000000000;;				newRc.Name, desiredReplicasAnnotation, newRc.Annotations[desiredReplicasAnnotation])
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		desired := int32(desiredAnnotation)
0000000000000000000000000000000000000000;;		// Extract the original replica count from the old controller, adding the
0000000000000000000000000000000000000000;;		// annotation if it doesn't yet exist.
0000000000000000000000000000000000000000;;		_, hasOriginalAnnotation := oldRc.Annotations[originalReplicasAnnotation]
0000000000000000000000000000000000000000;;		if !hasOriginalAnnotation {
0000000000000000000000000000000000000000;;			existing, err := r.rcClient.ReplicationControllers(oldRc.Namespace).Get(oldRc.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			originReplicas := strconv.Itoa(int(existing.Spec.Replicas))
0000000000000000000000000000000000000000;;			applyUpdate := func(rc *api.ReplicationController) {
0000000000000000000000000000000000000000;;				if rc.Annotations == nil {
0000000000000000000000000000000000000000;;					rc.Annotations = map[string]string{}
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				rc.Annotations[originalReplicasAnnotation] = originReplicas
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if oldRc, err = updateRcWithRetries(r.rcClient, existing.Namespace, existing, applyUpdate); err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// maxSurge is the maximum scaling increment and maxUnavailable are the maximum pods
0000000000000000000000000000000000000000;;		// that can be unavailable during a rollout.
0000000000000000000000000000000000000000;;		maxSurge, maxUnavailable, err := deploymentutil.ResolveFenceposts(&config.MaxSurge, &config.MaxUnavailable, desired)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Validate maximums.
0000000000000000000000000000000000000000;;		if desired > 0 && maxUnavailable == 0 && maxSurge == 0 {
0000000000000000000000000000000000000000;;			return fmt.Errorf("one of maxSurge or maxUnavailable must be specified")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// The minimum pods which must remain available throughout the update
0000000000000000000000000000000000000000;;		// calculated for internal convenience.
0000000000000000000000000000000000000000;;		minAvailable := int32(integer.IntMax(0, int(desired-maxUnavailable)))
0000000000000000000000000000000000000000;;		// If the desired new scale is 0, then the max unavailable is necessarily
0000000000000000000000000000000000000000;;		// the effective scale of the old RC regardless of the configuration
0000000000000000000000000000000000000000;;		// (equivalent to 100% maxUnavailable).
0000000000000000000000000000000000000000;;		if desired == 0 {
0000000000000000000000000000000000000000;;			maxUnavailable = oldRc.Spec.Replicas
0000000000000000000000000000000000000000;;			minAvailable = 0
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fmt.Fprintf(out, "Scaling up %s from %d to %d, scaling down %s from %d to 0 (keep %d pods available, don't exceed %d pods)\n",
0000000000000000000000000000000000000000;;			newRc.Name, newRc.Spec.Replicas, desired, oldRc.Name, oldRc.Spec.Replicas, minAvailable, desired+maxSurge)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// give a caller incremental notification and allow them to exit early
0000000000000000000000000000000000000000;;		goal := desired - newRc.Spec.Replicas
0000000000000000000000000000000000000000;;		if goal < 0 {
0000000000000000000000000000000000000000;;			goal = -goal
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		progress := func(complete bool) error {
0000000000000000000000000000000000000000;;			if config.OnProgress == nil {
0000000000000000000000000000000000000000;;				return nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			progress := desired - newRc.Spec.Replicas
0000000000000000000000000000000000000000;;			if progress < 0 {
0000000000000000000000000000000000000000;;				progress = -progress
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			percentage := 100
0000000000000000000000000000000000000000;;			if !complete && goal > 0 {
0000000000000000000000000000000000000000;;				percentage = int((goal - progress) * 100 / goal)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return config.OnProgress(oldRc, newRc, percentage)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Scale newRc and oldRc until newRc has the desired number of replicas and
0000000000000000000000000000000000000000;;		// oldRc has 0 replicas.
0000000000000000000000000000000000000000;;		progressDeadline := time.Now().UnixNano() + config.Timeout.Nanoseconds()
0000000000000000000000000000000000000000;;		for newRc.Spec.Replicas != desired || oldRc.Spec.Replicas != 0 {
0000000000000000000000000000000000000000;;			// Store the existing replica counts for progress timeout tracking.
0000000000000000000000000000000000000000;;			newReplicas := newRc.Spec.Replicas
0000000000000000000000000000000000000000;;			oldReplicas := oldRc.Spec.Replicas
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Scale up as much as possible.
0000000000000000000000000000000000000000;;			scaledRc, err := r.scaleUp(newRc, oldRc, desired, maxSurge, maxUnavailable, scaleRetryParams, config)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			newRc = scaledRc
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// notify the caller if necessary
0000000000000000000000000000000000000000;;			if err := progress(false); err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Wait between scaling operations for things to settle.
0000000000000000000000000000000000000000;;			time.Sleep(config.UpdatePeriod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// Scale down as much as possible.
0000000000000000000000000000000000000000;;			scaledRc, err = r.scaleDown(newRc, oldRc, desired, minAvailable, maxUnavailable, maxSurge, config)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			oldRc = scaledRc
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// notify the caller if necessary
0000000000000000000000000000000000000000;;			if err := progress(false); err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			// If we are making progress, continue to advance the progress deadline.
0000000000000000000000000000000000000000;;			// Otherwise, time out with an error.
0000000000000000000000000000000000000000;;			progressMade := (newRc.Spec.Replicas != newReplicas) || (oldRc.Spec.Replicas != oldReplicas)
0000000000000000000000000000000000000000;;			if progressMade {
0000000000000000000000000000000000000000;;				progressDeadline = time.Now().UnixNano() + config.Timeout.Nanoseconds()
0000000000000000000000000000000000000000;;			} else if time.Now().UnixNano() > progressDeadline {
0000000000000000000000000000000000000000;;				return fmt.Errorf("timed out waiting for any update progress to be made")
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// notify the caller if necessary
0000000000000000000000000000000000000000;;		if err := progress(true); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Housekeeping and cleanup policy execution.
0000000000000000000000000000000000000000;;		return r.cleanup(oldRc, newRc, config)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// scaleUp scales up newRc to desired by whatever increment is possible given
0000000000000000000000000000000000000000;;	// the configured surge threshold. scaleUp will safely no-op as necessary when
0000000000000000000000000000000000000000;;	// it detects redundancy or other relevant conditions.
0000000000000000000000000000000000000000;;	func (r *RollingUpdater) scaleUp(newRc, oldRc *api.ReplicationController, desired, maxSurge, maxUnavailable int32, scaleRetryParams *RetryParams, config *RollingUpdaterConfig) (*api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		// If we're already at the desired, do nothing.
0000000000000000000000000000000000000000;;		if newRc.Spec.Replicas == desired {
0000000000000000000000000000000000000000;;			return newRc, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Scale up as far as we can based on the surge limit.
0000000000000000000000000000000000000000;;		increment := (desired + maxSurge) - (oldRc.Spec.Replicas + newRc.Spec.Replicas)
0000000000000000000000000000000000000000;;		// If the old is already scaled down, go ahead and scale all the way up.
0000000000000000000000000000000000000000;;		if oldRc.Spec.Replicas == 0 {
0000000000000000000000000000000000000000;;			increment = desired - newRc.Spec.Replicas
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// We can't scale up without violating the surge limit, so do nothing.
0000000000000000000000000000000000000000;;		if increment <= 0 {
0000000000000000000000000000000000000000;;			return newRc, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Increase the replica count, and deal with fenceposts.
0000000000000000000000000000000000000000;;		newRc.Spec.Replicas += increment
0000000000000000000000000000000000000000;;		if newRc.Spec.Replicas > desired {
0000000000000000000000000000000000000000;;			newRc.Spec.Replicas = desired
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Perform the scale-up.
0000000000000000000000000000000000000000;;		fmt.Fprintf(config.Out, "Scaling %s up to %d\n", newRc.Name, newRc.Spec.Replicas)
0000000000000000000000000000000000000000;;		scaledRc, err := r.scaleAndWait(newRc, scaleRetryParams, scaleRetryParams)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return scaledRc, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// scaleDown scales down oldRc to 0 at whatever decrement possible given the
0000000000000000000000000000000000000000;;	// thresholds defined on the config. scaleDown will safely no-op as necessary
0000000000000000000000000000000000000000;;	// when it detects redundancy or other relevant conditions.
0000000000000000000000000000000000000000;;	func (r *RollingUpdater) scaleDown(newRc, oldRc *api.ReplicationController, desired, minAvailable, maxUnavailable, maxSurge int32, config *RollingUpdaterConfig) (*api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		// Already scaled down; do nothing.
0000000000000000000000000000000000000000;;		if oldRc.Spec.Replicas == 0 {
0000000000000000000000000000000000000000;;			return oldRc, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Get ready pods. We shouldn't block, otherwise in case both old and new
0000000000000000000000000000000000000000;;		// pods are unavailable then the rolling update process blocks.
0000000000000000000000000000000000000000;;		// Timeout-wise we are already covered by the progress check.
0000000000000000000000000000000000000000;;		_, newAvailable, err := r.getReadyPods(oldRc, newRc, config.MinReadySeconds)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// The old controller is considered as part of the total because we want to
0000000000000000000000000000000000000000;;		// maintain minimum availability even with a volatile old controller.
0000000000000000000000000000000000000000;;		// Scale down as much as possible while maintaining minimum availability
0000000000000000000000000000000000000000;;		allPods := oldRc.Spec.Replicas + newRc.Spec.Replicas
0000000000000000000000000000000000000000;;		newUnavailable := newRc.Spec.Replicas - newAvailable
0000000000000000000000000000000000000000;;		decrement := allPods - minAvailable - newUnavailable
0000000000000000000000000000000000000000;;		// The decrement normally shouldn't drop below 0 because the available count
0000000000000000000000000000000000000000;;		// always starts below the old replica count, but the old replica count can
0000000000000000000000000000000000000000;;		// decrement due to externalities like pods death in the replica set. This
0000000000000000000000000000000000000000;;		// will be considered a transient condition; do nothing and try again later
0000000000000000000000000000000000000000;;		// with new readiness values.
0000000000000000000000000000000000000000;;		//
0000000000000000000000000000000000000000;;		// If the most we can scale is 0, it means we can't scale down without
0000000000000000000000000000000000000000;;		// violating the minimum. Do nothing and try again later when conditions may
0000000000000000000000000000000000000000;;		// have changed.
0000000000000000000000000000000000000000;;		if decrement <= 0 {
0000000000000000000000000000000000000000;;			return oldRc, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Reduce the replica count, and deal with fenceposts.
0000000000000000000000000000000000000000;;		oldRc.Spec.Replicas -= decrement
0000000000000000000000000000000000000000;;		if oldRc.Spec.Replicas < 0 {
0000000000000000000000000000000000000000;;			oldRc.Spec.Replicas = 0
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// If the new is already fully scaled and available up to the desired size, go
0000000000000000000000000000000000000000;;		// ahead and scale old all the way down.
0000000000000000000000000000000000000000;;		if newRc.Spec.Replicas == desired && newAvailable == desired {
0000000000000000000000000000000000000000;;			oldRc.Spec.Replicas = 0
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Perform the scale-down.
0000000000000000000000000000000000000000;;		fmt.Fprintf(config.Out, "Scaling %s down to %d\n", oldRc.Name, oldRc.Spec.Replicas)
0000000000000000000000000000000000000000;;		retryWait := &RetryParams{config.Interval, config.Timeout}
0000000000000000000000000000000000000000;;		scaledRc, err := r.scaleAndWait(oldRc, retryWait, retryWait)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return scaledRc, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// scalerScaleAndWait scales a controller using a Scaler and a real client.
0000000000000000000000000000000000000000;;	func (r *RollingUpdater) scaleAndWaitWithScaler(rc *api.ReplicationController, retry *RetryParams, wait *RetryParams) (*api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		scaler := &ReplicationControllerScaler{r.rcClient}
0000000000000000000000000000000000000000;;		if err := scaler.Scale(rc.Namespace, rc.Name, uint(rc.Spec.Replicas), &ScalePrecondition{-1, ""}, retry, wait); err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return r.rcClient.ReplicationControllers(rc.Namespace).Get(rc.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// readyPods returns the old and new ready counts for their pods.
0000000000000000000000000000000000000000;;	// If a pod is observed as being ready, it's considered ready even
0000000000000000000000000000000000000000;;	// if it later becomes notReady.
0000000000000000000000000000000000000000;;	func (r *RollingUpdater) readyPods(oldRc, newRc *api.ReplicationController, minReadySeconds int32) (int32, int32, error) {
0000000000000000000000000000000000000000;;		controllers := []*api.ReplicationController{oldRc, newRc}
0000000000000000000000000000000000000000;;		oldReady := int32(0)
0000000000000000000000000000000000000000;;		newReady := int32(0)
0000000000000000000000000000000000000000;;		if r.nowFn == nil {
0000000000000000000000000000000000000000;;			r.nowFn = func() metav1.Time { return metav1.Now() }
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for i := range controllers {
0000000000000000000000000000000000000000;;			controller := controllers[i]
0000000000000000000000000000000000000000;;			selector := labels.Set(controller.Spec.Selector).AsSelector()
0000000000000000000000000000000000000000;;			options := metav1.ListOptions{LabelSelector: selector.String()}
0000000000000000000000000000000000000000;;			pods, err := r.podClient.Pods(controller.Namespace).List(options)
0000000000000000000000000000000000000000;;			if err != nil {
0000000000000000000000000000000000000000;;				return 0, 0, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			for _, pod := range pods.Items {
0000000000000000000000000000000000000000;;				v1Pod := &v1.Pod{}
0000000000000000000000000000000000000000;;				if err := k8s_api_v1.Convert_api_Pod_To_v1_Pod(&pod, v1Pod, nil); err != nil {
0000000000000000000000000000000000000000;;					return 0, 0, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				// Do not count deleted pods as ready
0000000000000000000000000000000000000000;;				if v1Pod.DeletionTimestamp != nil {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				if !podutil.IsPodAvailable(v1Pod, minReadySeconds, r.nowFn()) {
0000000000000000000000000000000000000000;;					continue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;				switch controller.Name {
0000000000000000000000000000000000000000;;				case oldRc.Name:
0000000000000000000000000000000000000000;;					oldReady++
0000000000000000000000000000000000000000;;				case newRc.Name:
0000000000000000000000000000000000000000;;					newReady++
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return oldReady, newReady, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// getOrCreateTargetControllerWithClient looks for an existing controller with
0000000000000000000000000000000000000000;;	// sourceId. If found, the existing controller is returned with true
0000000000000000000000000000000000000000;;	// indicating that the controller already exists. If the controller isn't
0000000000000000000000000000000000000000;;	// found, a new one is created and returned along with false indicating the
0000000000000000000000000000000000000000;;	// controller was created.
0000000000000000000000000000000000000000;;	//
0000000000000000000000000000000000000000;;	// Existing controllers are validated to ensure their sourceIdAnnotation
0000000000000000000000000000000000000000;;	// matches sourceId; if there's a mismatch, an error is returned.
0000000000000000000000000000000000000000;;	func (r *RollingUpdater) getOrCreateTargetControllerWithClient(controller *api.ReplicationController, sourceId string) (*api.ReplicationController, bool, error) {
0000000000000000000000000000000000000000;;		existingRc, err := r.existingController(controller)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			if !errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;				// There was an error trying to find the controller; don't assume we
0000000000000000000000000000000000000000;;				// should create it.
0000000000000000000000000000000000000000;;				return nil, false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if controller.Spec.Replicas <= 0 {
0000000000000000000000000000000000000000;;				return nil, false, fmt.Errorf("Invalid controller spec for %s; required: > 0 replicas, actual: %d\n", controller.Name, controller.Spec.Replicas)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// The controller wasn't found, so create it.
0000000000000000000000000000000000000000;;			if controller.Annotations == nil {
0000000000000000000000000000000000000000;;				controller.Annotations = map[string]string{}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			controller.Annotations[desiredReplicasAnnotation] = fmt.Sprintf("%d", controller.Spec.Replicas)
0000000000000000000000000000000000000000;;			controller.Annotations[sourceIdAnnotation] = sourceId
0000000000000000000000000000000000000000;;			controller.Spec.Replicas = 0
0000000000000000000000000000000000000000;;			newRc, err := r.rcClient.ReplicationControllers(r.ns).Create(controller)
0000000000000000000000000000000000000000;;			return newRc, false, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Validate and use the existing controller.
0000000000000000000000000000000000000000;;		annotations := existingRc.Annotations
0000000000000000000000000000000000000000;;		source := annotations[sourceIdAnnotation]
0000000000000000000000000000000000000000;;		_, ok := annotations[desiredReplicasAnnotation]
0000000000000000000000000000000000000000;;		if source != sourceId || !ok {
0000000000000000000000000000000000000000;;			return nil, false, fmt.Errorf("Missing/unexpected annotations for controller %s, expected %s : %s", controller.Name, sourceId, annotations)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return existingRc, true, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// existingController verifies if the controller already exists
0000000000000000000000000000000000000000;;	func (r *RollingUpdater) existingController(controller *api.ReplicationController) (*api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		// without rc name but generate name, there's no existing rc
0000000000000000000000000000000000000000;;		if len(controller.Name) == 0 && len(controller.GenerateName) > 0 {
0000000000000000000000000000000000000000;;			return nil, errors.NewNotFound(api.Resource("replicationcontrollers"), controller.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// controller name is required to get rc back
0000000000000000000000000000000000000000;;		return r.rcClient.ReplicationControllers(controller.Namespace).Get(controller.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// cleanupWithClients performs cleanup tasks after the rolling update. Update
0000000000000000000000000000000000000000;;	// process related annotations are removed from oldRc and newRc. The
0000000000000000000000000000000000000000;;	// CleanupPolicy on config is executed.
0000000000000000000000000000000000000000;;	func (r *RollingUpdater) cleanupWithClients(oldRc, newRc *api.ReplicationController, config *RollingUpdaterConfig) error {
0000000000000000000000000000000000000000;;		// Clean up annotations
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		newRc, err = r.rcClient.ReplicationControllers(r.ns).Get(newRc.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		applyUpdate := func(rc *api.ReplicationController) {
0000000000000000000000000000000000000000;;			delete(rc.Annotations, sourceIdAnnotation)
0000000000000000000000000000000000000000;;			delete(rc.Annotations, desiredReplicasAnnotation)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if newRc, err = updateRcWithRetries(r.rcClient, r.ns, newRc, applyUpdate); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err = wait.Poll(config.Interval, config.Timeout, client.ControllerHasDesiredReplicas(r.rcClient, newRc)); err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		newRc, err = r.rcClient.ReplicationControllers(r.ns).Get(newRc.Name, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		switch config.CleanupPolicy {
0000000000000000000000000000000000000000;;		case DeleteRollingUpdateCleanupPolicy:
0000000000000000000000000000000000000000;;			// delete old rc
0000000000000000000000000000000000000000;;			fmt.Fprintf(config.Out, "Update succeeded. Deleting %s\n", oldRc.Name)
0000000000000000000000000000000000000000;;			return r.rcClient.ReplicationControllers(r.ns).Delete(oldRc.Name, nil)
0000000000000000000000000000000000000000;;		case RenameRollingUpdateCleanupPolicy:
0000000000000000000000000000000000000000;;			// delete old rc
0000000000000000000000000000000000000000;;			fmt.Fprintf(config.Out, "Update succeeded. Deleting old controller: %s\n", oldRc.Name)
0000000000000000000000000000000000000000;;			if err := r.rcClient.ReplicationControllers(r.ns).Delete(oldRc.Name, nil); err != nil {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			fmt.Fprintf(config.Out, "Renaming %s to %s\n", newRc.Name, oldRc.Name)
0000000000000000000000000000000000000000;;			return Rename(r.rcClient, newRc, oldRc.Name)
0000000000000000000000000000000000000000;;		case PreserveRollingUpdateCleanupPolicy:
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		default:
0000000000000000000000000000000000000000;;			return nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func Rename(c coreclient.ReplicationControllersGetter, rc *api.ReplicationController, newName string) error {
0000000000000000000000000000000000000000;;		oldName := rc.Name
0000000000000000000000000000000000000000;;		rc.Name = newName
0000000000000000000000000000000000000000;;		rc.ResourceVersion = ""
0000000000000000000000000000000000000000;;		// First delete the oldName RC and orphan its pods.
0000000000000000000000000000000000000000;;		trueVar := true
0000000000000000000000000000000000000000;;		err := c.ReplicationControllers(rc.Namespace).Delete(oldName, &metav1.DeleteOptions{OrphanDependents: &trueVar})
0000000000000000000000000000000000000000;;		if err != nil && !errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		err = wait.Poll(5*time.Second, 60*time.Second, func() (bool, error) {
0000000000000000000000000000000000000000;;			_, err := c.ReplicationControllers(rc.Namespace).Get(oldName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;			if err == nil {
0000000000000000000000000000000000000000;;				return false, nil
0000000000000000000000000000000000000000;;			} else if errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;				return true, nil
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				return false, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Then create the same RC with the new name.
0000000000000000000000000000000000000000;;		_, err = c.ReplicationControllers(rc.Namespace).Create(rc)
0000000000000000000000000000000000000000;;		return err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func LoadExistingNextReplicationController(c coreclient.ReplicationControllersGetter, namespace, newName string) (*api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		if len(newName) == 0 {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		newRc, err := c.ReplicationControllers(namespace).Get(newName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil && errors.IsNotFound(err) {
0000000000000000000000000000000000000000;;			return nil, nil
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return newRc, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type NewControllerConfig struct {
0000000000000000000000000000000000000000;;		Namespace        string
0000000000000000000000000000000000000000;;		OldName, NewName string
0000000000000000000000000000000000000000;;		Image            string
0000000000000000000000000000000000000000;;		Container        string
0000000000000000000000000000000000000000;;		DeploymentKey    string
0000000000000000000000000000000000000000;;		PullPolicy       api.PullPolicy
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func CreateNewControllerFromCurrentController(rcClient coreclient.ReplicationControllersGetter, codec runtime.Codec, cfg *NewControllerConfig) (*api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		containerIndex := 0
0000000000000000000000000000000000000000;;		// load the old RC into the "new" RC
0000000000000000000000000000000000000000;;		newRc, err := rcClient.ReplicationControllers(cfg.Namespace).Get(cfg.OldName, metav1.GetOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(cfg.Container) != 0 {
0000000000000000000000000000000000000000;;			containerFound := false
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for i, c := range newRc.Spec.Template.Spec.Containers {
0000000000000000000000000000000000000000;;				if c.Name == cfg.Container {
0000000000000000000000000000000000000000;;					containerIndex = i
0000000000000000000000000000000000000000;;					containerFound = true
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			if !containerFound {
0000000000000000000000000000000000000000;;				return nil, fmt.Errorf("container %s not found in pod", cfg.Container)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(newRc.Spec.Template.Spec.Containers) > 1 && len(cfg.Container) == 0 {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("must specify container to update when updating a multi-container pod")
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(newRc.Spec.Template.Spec.Containers) == 0 {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("pod has no containers! (%v)", newRc)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		newRc.Spec.Template.Spec.Containers[containerIndex].Image = cfg.Image
0000000000000000000000000000000000000000;;		if len(cfg.PullPolicy) != 0 {
0000000000000000000000000000000000000000;;			newRc.Spec.Template.Spec.Containers[containerIndex].ImagePullPolicy = cfg.PullPolicy
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newHash, err := util.HashObject(newRc, codec)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if len(cfg.NewName) == 0 {
0000000000000000000000000000000000000000;;			cfg.NewName = fmt.Sprintf("%s-%s", newRc.Name, newHash)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		newRc.Name = cfg.NewName
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		newRc.Spec.Selector[cfg.DeploymentKey] = newHash
0000000000000000000000000000000000000000;;		newRc.Spec.Template.Labels[cfg.DeploymentKey] = newHash
0000000000000000000000000000000000000000;;		// Clear resource version after hashing so that identical updates get different hashes.
0000000000000000000000000000000000000000;;		newRc.ResourceVersion = ""
0000000000000000000000000000000000000000;;		return newRc, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func AbortRollingUpdate(c *RollingUpdaterConfig) error {
0000000000000000000000000000000000000000;;		// Swap the controllers
0000000000000000000000000000000000000000;;		tmp := c.OldRc
0000000000000000000000000000000000000000;;		c.OldRc = c.NewRc
0000000000000000000000000000000000000000;;		c.NewRc = tmp
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if c.NewRc.Annotations == nil {
0000000000000000000000000000000000000000;;			c.NewRc.Annotations = map[string]string{}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		c.NewRc.Annotations[sourceIdAnnotation] = fmt.Sprintf("%s:%s", c.OldRc.Name, c.OldRc.UID)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Use the original value since the replica count change from old to new
0000000000000000000000000000000000000000;;		// could be asymmetric. If we don't know the original count, we can't safely
0000000000000000000000000000000000000000;;		// roll back to a known good size.
0000000000000000000000000000000000000000;;		originalSize, foundOriginal := tmp.Annotations[originalReplicasAnnotation]
0000000000000000000000000000000000000000;;		if !foundOriginal {
0000000000000000000000000000000000000000;;			return fmt.Errorf("couldn't find original replica count of %q", tmp.Name)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		fmt.Fprintf(c.Out, "Setting %q replicas to %s\n", c.NewRc.Name, originalSize)
0000000000000000000000000000000000000000;;		c.NewRc.Annotations[desiredReplicasAnnotation] = originalSize
0000000000000000000000000000000000000000;;		c.CleanupPolicy = DeleteRollingUpdateCleanupPolicy
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func GetNextControllerAnnotation(rc *api.ReplicationController) (string, bool) {
0000000000000000000000000000000000000000;;		res, found := rc.Annotations[nextControllerAnnotation]
0000000000000000000000000000000000000000;;		return res, found
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func SetNextControllerAnnotation(rc *api.ReplicationController, name string) {
0000000000000000000000000000000000000000;;		if rc.Annotations == nil {
0000000000000000000000000000000000000000;;			rc.Annotations = map[string]string{}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		rc.Annotations[nextControllerAnnotation] = name
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func UpdateExistingReplicationController(rcClient coreclient.ReplicationControllersGetter, podClient coreclient.PodsGetter, oldRc *api.ReplicationController, namespace, newName, deploymentKey, deploymentValue string, out io.Writer) (*api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		if _, found := oldRc.Spec.Selector[deploymentKey]; !found {
0000000000000000000000000000000000000000;;			SetNextControllerAnnotation(oldRc, newName)
0000000000000000000000000000000000000000;;			return AddDeploymentKeyToReplicationController(oldRc, rcClient, podClient, deploymentKey, deploymentValue, namespace, out)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If we didn't need to update the controller for the deployment key, we still need to write
0000000000000000000000000000000000000000;;		// the "next" controller.
0000000000000000000000000000000000000000;;		applyUpdate := func(rc *api.ReplicationController) {
0000000000000000000000000000000000000000;;			SetNextControllerAnnotation(rc, newName)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return updateRcWithRetries(rcClient, namespace, oldRc, applyUpdate)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func AddDeploymentKeyToReplicationController(oldRc *api.ReplicationController, rcClient coreclient.ReplicationControllersGetter, podClient coreclient.PodsGetter, deploymentKey, deploymentValue, namespace string, out io.Writer) (*api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		var err error
0000000000000000000000000000000000000000;;		// First, update the template label.  This ensures that any newly created pods will have the new label
0000000000000000000000000000000000000000;;		applyUpdate := func(rc *api.ReplicationController) {
0000000000000000000000000000000000000000;;			if rc.Spec.Template.Labels == nil {
0000000000000000000000000000000000000000;;				rc.Spec.Template.Labels = map[string]string{}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			rc.Spec.Template.Labels[deploymentKey] = deploymentValue
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		if oldRc, err = updateRcWithRetries(rcClient, namespace, oldRc, applyUpdate); err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Update all pods managed by the rc to have the new hash label, so they are correctly adopted
0000000000000000000000000000000000000000;;		// TODO: extract the code from the label command and re-use it here.
0000000000000000000000000000000000000000;;		selector := labels.SelectorFromSet(oldRc.Spec.Selector)
0000000000000000000000000000000000000000;;		options := metav1.ListOptions{LabelSelector: selector.String()}
0000000000000000000000000000000000000000;;		podList, err := podClient.Pods(namespace).List(options)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for ix := range podList.Items {
0000000000000000000000000000000000000000;;			pod := &podList.Items[ix]
0000000000000000000000000000000000000000;;			applyUpdate := func(p *api.Pod) {
0000000000000000000000000000000000000000;;				if p.Labels == nil {
0000000000000000000000000000000000000000;;					p.Labels = map[string]string{
0000000000000000000000000000000000000000;;						deploymentKey: deploymentValue,
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					p.Labels[deploymentKey] = deploymentValue
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			if pod, err = updatePodWithRetries(podClient, namespace, pod, applyUpdate); err != nil {
0000000000000000000000000000000000000000;;				return nil, err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if oldRc.Spec.Selector == nil {
0000000000000000000000000000000000000000;;			oldRc.Spec.Selector = map[string]string{}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Copy the old selector, so that we can scrub out any orphaned pods
0000000000000000000000000000000000000000;;		selectorCopy := map[string]string{}
0000000000000000000000000000000000000000;;		for k, v := range oldRc.Spec.Selector {
0000000000000000000000000000000000000000;;			selectorCopy[k] = v
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		applyUpdate = func(rc *api.ReplicationController) {
0000000000000000000000000000000000000000;;			rc.Spec.Selector[deploymentKey] = deploymentValue
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Update the selector of the rc so it manages all the pods we updated above
0000000000000000000000000000000000000000;;		if oldRc, err = updateRcWithRetries(rcClient, namespace, oldRc, applyUpdate); err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Clean up any orphaned pods that don't have the new label, this can happen if the rc manager
0000000000000000000000000000000000000000;;		// doesn't see the update to its pod template and creates a new pod with the old labels after
0000000000000000000000000000000000000000;;		// we've finished re-adopting existing pods to the rc.
0000000000000000000000000000000000000000;;		selector = labels.SelectorFromSet(selectorCopy)
0000000000000000000000000000000000000000;;		options = metav1.ListOptions{LabelSelector: selector.String()}
0000000000000000000000000000000000000000;;		podList, err = podClient.Pods(namespace).List(options)
0000000000000000000000000000000000000000;;		for ix := range podList.Items {
0000000000000000000000000000000000000000;;			pod := &podList.Items[ix]
0000000000000000000000000000000000000000;;			if value, found := pod.Labels[deploymentKey]; !found || value != deploymentValue {
0000000000000000000000000000000000000000;;				if err := podClient.Pods(namespace).Delete(pod.Name, nil); err != nil {
0000000000000000000000000000000000000000;;					return nil, err
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		return oldRc, nil
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type updateRcFunc func(controller *api.ReplicationController)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// updateRcWithRetries retries updating the given rc on conflict with the following steps:
0000000000000000000000000000000000000000;;	// 1. Get latest resource
0000000000000000000000000000000000000000;;	// 2. applyUpdate
0000000000000000000000000000000000000000;;	// 3. Update the resource
0000000000000000000000000000000000000000;;	func updateRcWithRetries(rcClient coreclient.ReplicationControllersGetter, namespace string, rc *api.ReplicationController, applyUpdate updateRcFunc) (*api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		// Deep copy the rc in case we failed on Get during retry loop
0000000000000000000000000000000000000000;;		obj, err := api.Scheme.Copy(rc)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed to deep copy rc before updating it: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		oldRc := obj.(*api.ReplicationController)
0000000000000000000000000000000000000000;;		err = retry.RetryOnConflict(retry.DefaultBackoff, func() (e error) {
0000000000000000000000000000000000000000;;			// Apply the update, then attempt to push it to the apiserver.
0000000000000000000000000000000000000000;;			applyUpdate(rc)
0000000000000000000000000000000000000000;;			if rc, e = rcClient.ReplicationControllers(namespace).Update(rc); e == nil {
0000000000000000000000000000000000000000;;				// rc contains the latest controller post update
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			updateErr := e
0000000000000000000000000000000000000000;;			// Update the controller with the latest resource version, if the update failed we
0000000000000000000000000000000000000000;;			// can't trust rc so use oldRc.Name.
0000000000000000000000000000000000000000;;			if rc, e = rcClient.ReplicationControllers(namespace).Get(oldRc.Name, metav1.GetOptions{}); e != nil {
0000000000000000000000000000000000000000;;				// The Get failed: Value in rc cannot be trusted.
0000000000000000000000000000000000000000;;				rc = oldRc
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Only return the error from update
0000000000000000000000000000000000000000;;			return updateErr
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		// If the error is non-nil the returned controller cannot be trusted, if it is nil, the returned
0000000000000000000000000000000000000000;;		// controller contains the applied update.
0000000000000000000000000000000000000000;;		return rc, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type updatePodFunc func(controller *api.Pod)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// updatePodWithRetries retries updating the given pod on conflict with the following steps:
0000000000000000000000000000000000000000;;	// 1. Get latest resource
0000000000000000000000000000000000000000;;	// 2. applyUpdate
0000000000000000000000000000000000000000;;	// 3. Update the resource
0000000000000000000000000000000000000000;;	func updatePodWithRetries(podClient coreclient.PodsGetter, namespace string, pod *api.Pod, applyUpdate updatePodFunc) (*api.Pod, error) {
0000000000000000000000000000000000000000;;		// Deep copy the pod in case we failed on Get during retry loop
0000000000000000000000000000000000000000;;		obj, err := api.Scheme.Copy(pod)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, fmt.Errorf("failed to deep copy pod before updating it: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		oldPod := obj.(*api.Pod)
0000000000000000000000000000000000000000;;		err = retry.RetryOnConflict(retry.DefaultBackoff, func() (e error) {
0000000000000000000000000000000000000000;;			// Apply the update, then attempt to push it to the apiserver.
0000000000000000000000000000000000000000;;			applyUpdate(pod)
0000000000000000000000000000000000000000;;			if pod, e = podClient.Pods(namespace).Update(pod); e == nil {
0000000000000000000000000000000000000000;;				return
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			updateErr := e
0000000000000000000000000000000000000000;;			if pod, e = podClient.Pods(namespace).Get(oldPod.Name, metav1.GetOptions{}); e != nil {
0000000000000000000000000000000000000000;;				pod = oldPod
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			// Only return the error from update
0000000000000000000000000000000000000000;;			return updateErr
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		// If the error is non-nil the returned pod cannot be trusted, if it is nil, the returned
0000000000000000000000000000000000000000;;		// controller contains the applied update.
0000000000000000000000000000000000000000;;		return pod, err
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func FindSourceController(r coreclient.ReplicationControllersGetter, namespace, name string) (*api.ReplicationController, error) {
0000000000000000000000000000000000000000;;		list, err := r.ReplicationControllers(namespace).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return nil, err
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		for ix := range list.Items {
0000000000000000000000000000000000000000;;			rc := &list.Items[ix]
0000000000000000000000000000000000000000;;			if rc.Annotations != nil && strings.HasPrefix(rc.Annotations[sourceIdAnnotation], name) {
0000000000000000000000000000000000000000;;				return rc, nil
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil, fmt.Errorf("couldn't find a replication controller with source id == %s/%s", namespace, name)
0000000000000000000000000000000000000000;;	}
