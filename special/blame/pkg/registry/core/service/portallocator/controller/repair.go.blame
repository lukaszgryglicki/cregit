0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2015 The Kubernetes Authors.
b1eeaccc107052578d9c19418779718ae626f595;pkg/registry/service/portallocator/controller/repair.go[pkg/registry/service/portallocator/controller/repair.go][pkg/registry/core/service/portallocator/controller/repair.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package controller
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"fmt"
0000000000000000000000000000000000000000;;		"time"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/api/errors"
0000000000000000000000000000000000000000;;		metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/net"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/runtime"
0000000000000000000000000000000000000000;;		"k8s.io/apimachinery/pkg/util/wait"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/api"
0000000000000000000000000000000000000000;;		coreclient "k8s.io/kubernetes/pkg/client/clientset_generated/internalclientset/typed/core/internalversion"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/client/retry"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/registry/core/rangeallocation"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/registry/core/service"
0000000000000000000000000000000000000000;;		"k8s.io/kubernetes/pkg/registry/core/service/portallocator"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// See ipallocator/controller/repair.go; this is a copy for ports.
0000000000000000000000000000000000000000;;	type Repair struct {
0000000000000000000000000000000000000000;;		interval      time.Duration
0000000000000000000000000000000000000000;;		serviceClient coreclient.ServicesGetter
0000000000000000000000000000000000000000;;		portRange     net.PortRange
0000000000000000000000000000000000000000;;		alloc         rangeallocation.RangeRegistry
0000000000000000000000000000000000000000;;		leaks         map[int]int // counter per leaked port
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// How many times we need to detect a leak before we clean up.  This is to
0000000000000000000000000000000000000000;;	// avoid races between allocating a ports and using it.
0000000000000000000000000000000000000000;;	const numRepairsBeforeLeakCleanup = 3
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// NewRepair creates a controller that periodically ensures that all ports are uniquely allocated across the cluster
0000000000000000000000000000000000000000;;	// and generates informational warnings for a cluster that is not in sync.
0000000000000000000000000000000000000000;;	func NewRepair(interval time.Duration, serviceClient coreclient.ServicesGetter, portRange net.PortRange, alloc rangeallocation.RangeRegistry) *Repair {
0000000000000000000000000000000000000000;;		return &Repair{
0000000000000000000000000000000000000000;;			interval:      interval,
0000000000000000000000000000000000000000;;			serviceClient: serviceClient,
0000000000000000000000000000000000000000;;			portRange:     portRange,
0000000000000000000000000000000000000000;;			alloc:         alloc,
0000000000000000000000000000000000000000;;			leaks:         map[int]int{},
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunUntil starts the controller until the provided ch is closed.
0000000000000000000000000000000000000000;;	func (c *Repair) RunUntil(ch chan struct{}) {
0000000000000000000000000000000000000000;;		wait.Until(func() {
0000000000000000000000000000000000000000;;			if err := c.RunOnce(); err != nil {
0000000000000000000000000000000000000000;;				runtime.HandleError(err)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}, c.interval, ch)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// RunOnce verifies the state of the port allocations and returns an error if an unrecoverable problem occurs.
0000000000000000000000000000000000000000;;	func (c *Repair) RunOnce() error {
0000000000000000000000000000000000000000;;		return retry.RetryOnConflict(retry.DefaultBackoff, c.runOnce)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// runOnce verifies the state of the port allocations and returns an error if an unrecoverable problem occurs.
0000000000000000000000000000000000000000;;	func (c *Repair) runOnce() error {
0000000000000000000000000000000000000000;;		// TODO: (per smarterclayton) if Get() or ListServices() is a weak consistency read,
0000000000000000000000000000000000000000;;		// or if they are executed against different leaders,
0000000000000000000000000000000000000000;;		// the ordering guarantee required to ensure no port is allocated twice is violated.
0000000000000000000000000000000000000000;;		// ListServices must return a ResourceVersion higher than the etcd index Get triggers,
0000000000000000000000000000000000000000;;		// and the release code must not release services that have had ports allocated but not yet been created
0000000000000000000000000000000000000000;;		// See #8295
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// If etcd server is not running we should wait for some time and fail only then. This is particularly
0000000000000000000000000000000000000000;;		// important when we start apiserver and etcd at the same time.
0000000000000000000000000000000000000000;;		var snapshot *api.RangeAllocation
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		err := wait.PollImmediate(time.Second, 10*time.Second, func() (bool, error) {
0000000000000000000000000000000000000000;;			var err error
0000000000000000000000000000000000000000;;			snapshot, err = c.alloc.Get()
0000000000000000000000000000000000000000;;			return err == nil, err
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("unable to refresh the port allocations: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// If not yet initialized.
0000000000000000000000000000000000000000;;		if snapshot.Range == "" {
0000000000000000000000000000000000000000;;			snapshot.Range = c.portRange.String()
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		// Create an allocator because it is easy to use.
0000000000000000000000000000000000000000;;		stored, err := portallocator.NewFromSnapshot(snapshot)
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("unable to rebuild allocator from snapshot: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// We explicitly send no resource version, since the resource version
0000000000000000000000000000000000000000;;		// of 'snapshot' is from a different collection, it's not comparable to
0000000000000000000000000000000000000000;;		// the service collection. The caching layer keeps per-collection RVs,
0000000000000000000000000000000000000000;;		// and this is proper, since in theory the collections could be hosted
0000000000000000000000000000000000000000;;		// in separate etcd (or even non-etcd) instances.
0000000000000000000000000000000000000000;;		list, err := c.serviceClient.Services(metav1.NamespaceAll).List(metav1.ListOptions{})
0000000000000000000000000000000000000000;;		if err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("unable to refresh the port block: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		rebuilt := portallocator.NewPortAllocator(c.portRange)
0000000000000000000000000000000000000000;;		// Check every Service's ports, and rebuild the state as we think it should be.
0000000000000000000000000000000000000000;;		for i := range list.Items {
0000000000000000000000000000000000000000;;			svc := &list.Items[i]
0000000000000000000000000000000000000000;;			ports := service.CollectServiceNodePorts(svc)
0000000000000000000000000000000000000000;;			if len(ports) == 0 {
0000000000000000000000000000000000000000;;				continue
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, port := range ports {
0000000000000000000000000000000000000000;;				switch err := rebuilt.Allocate(port); err {
0000000000000000000000000000000000000000;;				case nil:
0000000000000000000000000000000000000000;;					if stored.Has(port) {
0000000000000000000000000000000000000000;;						// remove it from the old set, so we can find leaks
0000000000000000000000000000000000000000;;						stored.Release(port)
0000000000000000000000000000000000000000;;					} else {
0000000000000000000000000000000000000000;;						// doesn't seem to be allocated
0000000000000000000000000000000000000000;;						runtime.HandleError(fmt.Errorf("the node port %d for service %s/%s is not allocated; repairing", port, svc.Name, svc.Namespace))
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					delete(c.leaks, port) // it is used, so it can't be leaked
0000000000000000000000000000000000000000;;				case portallocator.ErrAllocated:
0000000000000000000000000000000000000000;;					// TODO: send event
0000000000000000000000000000000000000000;;					// port is duplicate, reallocate
0000000000000000000000000000000000000000;;					runtime.HandleError(fmt.Errorf("the node port %d for service %s/%s was assigned to multiple services; please recreate", port, svc.Name, svc.Namespace))
0000000000000000000000000000000000000000;;				case err.(*portallocator.ErrNotInRange):
0000000000000000000000000000000000000000;;					// TODO: send event
0000000000000000000000000000000000000000;;					// port is out of range, reallocate
0000000000000000000000000000000000000000;;					runtime.HandleError(fmt.Errorf("the port %d for service %s/%s is not within the port range %v; please recreate", port, svc.Name, svc.Namespace, c.portRange))
0000000000000000000000000000000000000000;;				case portallocator.ErrFull:
0000000000000000000000000000000000000000;;					// TODO: send event
0000000000000000000000000000000000000000;;					// somehow we are out of ports
0000000000000000000000000000000000000000;;					return fmt.Errorf("the port range %v is full; you must widen the port range in order to create new services", c.portRange)
0000000000000000000000000000000000000000;;				default:
0000000000000000000000000000000000000000;;					return fmt.Errorf("unable to allocate port %d for service %s/%s due to an unknown error, exiting: %v", port, svc.Name, svc.Namespace, err)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Check for ports that are left in the old set.  They appear to have been leaked.
0000000000000000000000000000000000000000;;		stored.ForEach(func(port int) {
0000000000000000000000000000000000000000;;			count, found := c.leaks[port]
0000000000000000000000000000000000000000;;			switch {
0000000000000000000000000000000000000000;;			case !found:
0000000000000000000000000000000000000000;;				// flag it to be cleaned up after any races (hopefully) are gone
0000000000000000000000000000000000000000;;				runtime.HandleError(fmt.Errorf("the node port %d may have leaked: flagging for later clean up", port))
0000000000000000000000000000000000000000;;				count = numRepairsBeforeLeakCleanup - 1
0000000000000000000000000000000000000000;;				fallthrough
0000000000000000000000000000000000000000;;			case count > 0:
0000000000000000000000000000000000000000;;				// pretend it is still in use until count expires
0000000000000000000000000000000000000000;;				c.leaks[port] = count - 1
0000000000000000000000000000000000000000;;				if err := rebuilt.Allocate(port); err != nil {
0000000000000000000000000000000000000000;;					runtime.HandleError(fmt.Errorf("the node port %d may have leaked, but can not be allocated: %v", port, err))
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			default:
0000000000000000000000000000000000000000;;				// do not add it to the rebuilt set, which means it will be available for reuse
0000000000000000000000000000000000000000;;				runtime.HandleError(fmt.Errorf("the node port %d appears to have leaked: cleaning up", port))
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		})
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Blast the rebuilt state into storage.
0000000000000000000000000000000000000000;;		if err := rebuilt.Snapshot(snapshot); err != nil {
0000000000000000000000000000000000000000;;			return fmt.Errorf("unable to snapshot the updated port allocations: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if err := c.alloc.CreateOrUpdate(snapshot); err != nil {
0000000000000000000000000000000000000000;;			if errors.IsConflict(err) {
0000000000000000000000000000000000000000;;				return err
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return fmt.Errorf("unable to persist the updated port allocations: %v", err)
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return nil
0000000000000000000000000000000000000000;;	}
