0000000000000000000000000000000000000000;;	/*
0000000000000000000000000000000000000000;;	Copyright 2016 The Kubernetes Authors.
834bb730525e247a2cf07a423e58424709ad0996;federation/pkg/federation-controller/replicaset/planner/planner.go[federation/pkg/federation-controller/replicaset/planner/planner.go][federation/pkg/federation-controller/util/planner/planner.go];	
0000000000000000000000000000000000000000;;	Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	limitations under the License.
0000000000000000000000000000000000000000;;	*/
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	package planner
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	import (
0000000000000000000000000000000000000000;;		"hash/fnv"
0000000000000000000000000000000000000000;;		"sort"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		fedapi "k8s.io/kubernetes/federation/apis/federation"
0000000000000000000000000000000000000000;;	)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Planner decides how many out of the given replicas should be placed in each of the
0000000000000000000000000000000000000000;;	// federated clusters.
0000000000000000000000000000000000000000;;	type Planner struct {
0000000000000000000000000000000000000000;;		preferences *fedapi.ReplicaAllocationPreferences
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type namedClusterPreferences struct {
0000000000000000000000000000000000000000;;		clusterName string
0000000000000000000000000000000000000000;;		hash        uint32
0000000000000000000000000000000000000000;;		fedapi.ClusterPreferences
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	type byWeight []*namedClusterPreferences
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func (a byWeight) Len() int      { return len(a) }
0000000000000000000000000000000000000000;;	func (a byWeight) Swap(i, j int) { a[i], a[j] = a[j], a[i] }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Preferences are sorted according by decreasing weight and increasing hash (built on top of cluster name and rs name).
0000000000000000000000000000000000000000;;	// Sorting is made by a hash to avoid assigning single-replica rs to the alphabetically smallest cluster.
0000000000000000000000000000000000000000;;	func (a byWeight) Less(i, j int) bool {
0000000000000000000000000000000000000000;;		return (a[i].Weight > a[j].Weight) || (a[i].Weight == a[j].Weight && a[i].hash < a[j].hash)
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func NewPlanner(preferences *fedapi.ReplicaAllocationPreferences) *Planner {
0000000000000000000000000000000000000000;;		return &Planner{
0000000000000000000000000000000000000000;;			preferences: preferences,
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	// Distribute the desired number of replicas among the given cluster according to the planner preferences.
0000000000000000000000000000000000000000;;	// The function tries its best to assign each cluster the preferred number of replicas, however if
0000000000000000000000000000000000000000;;	// sum of MinReplicas for all cluster is bigger thant replicasToDistribute then some cluster will not
0000000000000000000000000000000000000000;;	// have all of the replicas assigned. In such case a cluster with higher weight has priority over
0000000000000000000000000000000000000000;;	// cluster with lower weight (or with lexicographically smaller name in case of draw).
0000000000000000000000000000000000000000;;	// It can also use the current replica count and estimated capacity to provide better planning and
0000000000000000000000000000000000000000;;	// adhere to rebalance policy. To avoid prioritization of clusters with smaller lexicographical names
0000000000000000000000000000000000000000;;	// a semi-random string (like replica set name) can be provided.
0000000000000000000000000000000000000000;;	// Two maps are returned:
0000000000000000000000000000000000000000;;	// * a map that contains information how many replicas will be possible to run in a cluster.
0000000000000000000000000000000000000000;;	// * a map that contains information how many extra replicas would be nice to schedule in a cluster so,
0000000000000000000000000000000000000000;;	//   if by chance, they are scheduled we will be closer to the desired replicas layout.
0000000000000000000000000000000000000000;;	func (p *Planner) Plan(replicasToDistribute int64, availableClusters []string, currentReplicaCount map[string]int64,
0000000000000000000000000000000000000000;;		estimatedCapacity map[string]int64, replicaSetKey string) (map[string]int64, map[string]int64) {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		preferences := make([]*namedClusterPreferences, 0, len(availableClusters))
0000000000000000000000000000000000000000;;		plan := make(map[string]int64, len(preferences))
0000000000000000000000000000000000000000;;		overflow := make(map[string]int64, len(preferences))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		named := func(name string, pref fedapi.ClusterPreferences) *namedClusterPreferences {
0000000000000000000000000000000000000000;;			// Seems to work better than addler for our case.
0000000000000000000000000000000000000000;;			hasher := fnv.New32()
0000000000000000000000000000000000000000;;			hasher.Write([]byte(name))
0000000000000000000000000000000000000000;;			hasher.Write([]byte(replicaSetKey))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			return &namedClusterPreferences{
0000000000000000000000000000000000000000;;				clusterName:        name,
0000000000000000000000000000000000000000;;				hash:               hasher.Sum32(),
0000000000000000000000000000000000000000;;				ClusterPreferences: pref,
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		for _, cluster := range availableClusters {
0000000000000000000000000000000000000000;;			if localRSP, found := p.preferences.Clusters[cluster]; found {
0000000000000000000000000000000000000000;;				preferences = append(preferences, named(cluster, localRSP))
0000000000000000000000000000000000000000;;			} else {
0000000000000000000000000000000000000000;;				if localRSP, found := p.preferences.Clusters["*"]; found {
0000000000000000000000000000000000000000;;					preferences = append(preferences, named(cluster, localRSP))
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					plan[cluster] = int64(0)
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		sort.Sort(byWeight(preferences))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		remainingReplicas := replicasToDistribute
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// Assign each cluster the minimum number of replicas it requested.
0000000000000000000000000000000000000000;;		for _, preference := range preferences {
0000000000000000000000000000000000000000;;			min := minInt64(preference.MinReplicas, remainingReplicas)
0000000000000000000000000000000000000000;;			if capacity, hasCapacity := estimatedCapacity[preference.clusterName]; hasCapacity {
0000000000000000000000000000000000000000;;				min = minInt64(min, capacity)
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			remainingReplicas -= min
0000000000000000000000000000000000000000;;			plan[preference.clusterName] = min
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// This map contains information how many replicas were assigned to
0000000000000000000000000000000000000000;;		// the cluster based only on the current replica count and
0000000000000000000000000000000000000000;;		// rebalance=false preference. It will be later used in remaining replica
0000000000000000000000000000000000000000;;		// distribution code.
0000000000000000000000000000000000000000;;		preallocated := make(map[string]int64)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if p.preferences.Rebalance == false {
0000000000000000000000000000000000000000;;			for _, preference := range preferences {
0000000000000000000000000000000000000000;;				planned := plan[preference.clusterName]
0000000000000000000000000000000000000000;;				count, hasSome := currentReplicaCount[preference.clusterName]
0000000000000000000000000000000000000000;;				if hasSome && count > planned {
0000000000000000000000000000000000000000;;					target := count
0000000000000000000000000000000000000000;;					if preference.MaxReplicas != nil {
0000000000000000000000000000000000000000;;						target = minInt64(*preference.MaxReplicas, target)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if capacity, hasCapacity := estimatedCapacity[preference.clusterName]; hasCapacity {
0000000000000000000000000000000000000000;;						target = minInt64(capacity, target)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					extra := minInt64(target-planned, remainingReplicas)
0000000000000000000000000000000000000000;;					if extra < 0 {
0000000000000000000000000000000000000000;;						extra = 0
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					remainingReplicas -= extra
0000000000000000000000000000000000000000;;					preallocated[preference.clusterName] = extra
0000000000000000000000000000000000000000;;					plan[preference.clusterName] = extra + planned
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		modified := true
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		// It is possible single pass of the loop is not enough to distribute all replicas among clusters due
0000000000000000000000000000000000000000;;		// to weight, max and rounding corner cases. In such case we iterate until either
0000000000000000000000000000000000000000;;		// there is no replicas or no cluster gets any more replicas or the number
0000000000000000000000000000000000000000;;		// of attempts is less than available cluster count. If there is no preallocated pods
0000000000000000000000000000000000000000;;		// every loop either distributes all remainingReplicas or maxes out at least one cluster.
0000000000000000000000000000000000000000;;		// If there are preallocated then the replica spreading may take longer.
0000000000000000000000000000000000000000;;		// We reduce the number of pending preallocated replicas by at least half with each iteration so
0000000000000000000000000000000000000000;;		// we may need log(replicasAtStart) iterations.
0000000000000000000000000000000000000000;;		// TODO: Prove that clusterCount * log(replicas) iterations solves the problem or adjust the number.
0000000000000000000000000000000000000000;;		// TODO: This algorithm is O(clusterCount^2 * log(replicas)) which is good for up to 100 clusters.
0000000000000000000000000000000000000000;;		// Find something faster.
0000000000000000000000000000000000000000;;		for trial := 0; modified && remainingReplicas > 0; trial++ {
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			modified = false
0000000000000000000000000000000000000000;;			weightSum := int64(0)
0000000000000000000000000000000000000000;;			for _, preference := range preferences {
0000000000000000000000000000000000000000;;				weightSum += preference.Weight
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			newPreferences := make([]*namedClusterPreferences, 0, len(preferences))
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			distributeInThisLoop := remainingReplicas
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;			for _, preference := range preferences {
0000000000000000000000000000000000000000;;				if weightSum > 0 {
0000000000000000000000000000000000000000;;					start := plan[preference.clusterName]
0000000000000000000000000000000000000000;;					// Distribute the remaining replicas, rounding fractions always up.
0000000000000000000000000000000000000000;;					extra := (distributeInThisLoop*preference.Weight + weightSum - 1) / weightSum
0000000000000000000000000000000000000000;;					extra = minInt64(extra, remainingReplicas)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Account preallocated.
0000000000000000000000000000000000000000;;					prealloc := preallocated[preference.clusterName]
0000000000000000000000000000000000000000;;					usedPrealloc := minInt64(extra, prealloc)
0000000000000000000000000000000000000000;;					preallocated[preference.clusterName] = prealloc - usedPrealloc
0000000000000000000000000000000000000000;;					extra = extra - usedPrealloc
0000000000000000000000000000000000000000;;					if usedPrealloc > 0 {
0000000000000000000000000000000000000000;;						modified = true
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// In total there should be the amount that was there at start plus whatever is due
0000000000000000000000000000000000000000;;					// in this iteration
0000000000000000000000000000000000000000;;					total := start + extra
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Check if we don't overflow the cluster, and if yes don't consider this cluster
0000000000000000000000000000000000000000;;					// in any of the following iterations.
0000000000000000000000000000000000000000;;					full := false
0000000000000000000000000000000000000000;;					if preference.MaxReplicas != nil && total > *preference.MaxReplicas {
0000000000000000000000000000000000000000;;						total = *preference.MaxReplicas
0000000000000000000000000000000000000000;;						full = true
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;					if capacity, hasCapacity := estimatedCapacity[preference.clusterName]; hasCapacity && total > capacity {
0000000000000000000000000000000000000000;;						overflow[preference.clusterName] = total - capacity
0000000000000000000000000000000000000000;;						total = capacity
0000000000000000000000000000000000000000;;						full = true
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					if !full {
0000000000000000000000000000000000000000;;						newPreferences = append(newPreferences, preference)
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Only total-start replicas were actually taken.
0000000000000000000000000000000000000000;;					remainingReplicas -= (total - start)
0000000000000000000000000000000000000000;;					plan[preference.clusterName] = total
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;					// Something extra got scheduled on this cluster.
0000000000000000000000000000000000000000;;					if total > start {
0000000000000000000000000000000000000000;;						modified = true
0000000000000000000000000000000000000000;;					}
0000000000000000000000000000000000000000;;				} else {
0000000000000000000000000000000000000000;;					break
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			preferences = newPreferences
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;		if p.preferences.Rebalance {
0000000000000000000000000000000000000000;;			return plan, overflow
0000000000000000000000000000000000000000;;		} else {
0000000000000000000000000000000000000000;;			// If rebalance = false then overflow is trimmed at the level
0000000000000000000000000000000000000000;;			// of replicas that it failed to place somewhere.
0000000000000000000000000000000000000000;;			newOverflow := make(map[string]int64)
0000000000000000000000000000000000000000;;			for key, value := range overflow {
0000000000000000000000000000000000000000;;				value = minInt64(value, remainingReplicas)
0000000000000000000000000000000000000000;;				if value > 0 {
0000000000000000000000000000000000000000;;					newOverflow[key] = value
0000000000000000000000000000000000000000;;				}
0000000000000000000000000000000000000000;;			}
0000000000000000000000000000000000000000;;			return plan, newOverflow
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	func minInt64(a int64, b int64) int64 {
0000000000000000000000000000000000000000;;		if a < b {
0000000000000000000000000000000000000000;;			return a
0000000000000000000000000000000000000000;;		}
0000000000000000000000000000000000000000;;		return b
0000000000000000000000000000000000000000;;	}
