0000000000000000000000000000000000000000;;	#!/bin/bash
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Copyright 2016 The Kubernetes Authors.
0000000000000000000000000000000000000000;;	#
0000000000000000000000000000000000000000;;	# Licensed under the Apache License, Version 2.0 (the "License");
0000000000000000000000000000000000000000;;	# you may not use this file except in compliance with the License.
0000000000000000000000000000000000000000;;	# You may obtain a copy of the License at
0000000000000000000000000000000000000000;;	#
0000000000000000000000000000000000000000;;	#     http://www.apache.org/licenses/LICENSE-2.0
0000000000000000000000000000000000000000;;	#
0000000000000000000000000000000000000000;;	# Unless required by applicable law or agreed to in writing, software
0000000000000000000000000000000000000000;;	# distributed under the License is distributed on an "AS IS" BASIS,
0000000000000000000000000000000000000000;;	# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
0000000000000000000000000000000000000000;;	# See the License for the specific language governing permissions and
0000000000000000000000000000000000000000;;	# limitations under the License.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# This contains util code for testing kubectl.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	set -o errexit
0000000000000000000000000000000000000000;;	set -o nounset
0000000000000000000000000000000000000000;;	set -o pipefail
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Set locale to ensure english responses from kubectl commands
0000000000000000000000000000000000000000;;	export LANG=C
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	KUBE_ROOT=$(dirname "${BASH_SOURCE}")/../..
0000000000000000000000000000000000000000;;	# Expects the following has already been done by whatever sources this script
0000000000000000000000000000000000000000;;	# source "${KUBE_ROOT}/hack/lib/init.sh"
0000000000000000000000000000000000000000;;	# source "${KUBE_ROOT}/hack/lib/test.sh"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	ETCD_HOST=${ETCD_HOST:-127.0.0.1}
0000000000000000000000000000000000000000;;	ETCD_PORT=${ETCD_PORT:-2379}
0000000000000000000000000000000000000000;;	API_PORT=${API_PORT:-8080}
0000000000000000000000000000000000000000;;	SECURE_API_PORT=${SECURE_API_PORT:-6443}
0000000000000000000000000000000000000000;;	API_HOST=${API_HOST:-127.0.0.1}
0000000000000000000000000000000000000000;;	KUBELET_HEALTHZ_PORT=${KUBELET_HEALTHZ_PORT:-10248}
0000000000000000000000000000000000000000;;	CTLRMGR_PORT=${CTLRMGR_PORT:-10252}
0000000000000000000000000000000000000000;;	PROXY_HOST=127.0.0.1 # kubectl only serves on localhost.
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	IMAGE_NGINX="gcr.io/google-containers/nginx:1.7.9"
0000000000000000000000000000000000000000;;	IMAGE_DEPLOYMENT_R1="gcr.io/google-containers/nginx:test-cmd"  # deployment-revision1.yaml
0000000000000000000000000000000000000000;;	IMAGE_DEPLOYMENT_R2="$IMAGE_NGINX"  # deployment-revision2.yaml
0000000000000000000000000000000000000000;;	IMAGE_PERL="gcr.io/google-containers/perl"
0000000000000000000000000000000000000000;;	IMAGE_DAEMONSET_R1="gcr.io/google-containers/pause:2.0"
0000000000000000000000000000000000000000;;	IMAGE_DAEMONSET_R2="gcr.io/google-containers/pause:latest"
0000000000000000000000000000000000000000;;	IMAGE_DAEMONSET_R2_2="gcr.io/google-containers/nginx:test-cmd"  # rollingupdate-daemonset-rv2.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Expose kubectl directly for readability
0000000000000000000000000000000000000000;;	PATH="${KUBE_OUTPUT_HOSTBIN}":$PATH
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Define variables for resource types to prevent typos.
0000000000000000000000000000000000000000;;	clusterroles="clusterroles"
0000000000000000000000000000000000000000;;	configmaps="configmaps"
0000000000000000000000000000000000000000;;	csr="csr"
0000000000000000000000000000000000000000;;	deployments="deployments"
0000000000000000000000000000000000000000;;	horizontalpodautoscalers="horizontalpodautoscalers"
0000000000000000000000000000000000000000;;	metrics="metrics"
0000000000000000000000000000000000000000;;	namespaces="namespaces"
0000000000000000000000000000000000000000;;	nodes="nodes"
0000000000000000000000000000000000000000;;	persistentvolumeclaims="persistentvolumeclaims"
0000000000000000000000000000000000000000;;	persistentvolumes="persistentvolumes"
0000000000000000000000000000000000000000;;	pods="pods"
0000000000000000000000000000000000000000;;	podtemplates="podtemplates"
0000000000000000000000000000000000000000;;	replicasets="replicasets"
0000000000000000000000000000000000000000;;	replicationcontrollers="replicationcontrollers"
0000000000000000000000000000000000000000;;	roles="roles"
0000000000000000000000000000000000000000;;	secrets="secrets"
0000000000000000000000000000000000000000;;	serviceaccounts="serviceaccounts"
0000000000000000000000000000000000000000;;	services="services"
0000000000000000000000000000000000000000;;	statefulsets="statefulsets"
0000000000000000000000000000000000000000;;	static="static"
0000000000000000000000000000000000000000;;	storageclass="storageclass"
0000000000000000000000000000000000000000;;	subjectaccessreviews="subjectaccessreviews"
0000000000000000000000000000000000000000;;	thirdpartyresources="thirdpartyresources"
0000000000000000000000000000000000000000;;	customresourcedefinitions="customresourcedefinitions"
0000000000000000000000000000000000000000;;	daemonsets="daemonsets"
0000000000000000000000000000000000000000;;	controllerrevisions="controllerrevisions"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# include shell2junit library
0000000000000000000000000000000000000000;;	sh2ju="${KUBE_ROOT}/third_party/forked/shell2junit/sh2ju.sh"
0000000000000000000000000000000000000000;;	if [[ -f "${sh2ju}" ]]; then
0000000000000000000000000000000000000000;;	  source "${sh2ju}"
0000000000000000000000000000000000000000;;	else
0000000000000000000000000000000000000000;;	  echo "failed to find third_party/forked/shell2junit/sh2ju.sh"
0000000000000000000000000000000000000000;;	  exit 1
0000000000000000000000000000000000000000;;	fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# record_command runs the command and records its output/error messages in junit format
0000000000000000000000000000000000000000;;	# it expects the first to be the name of the command
0000000000000000000000000000000000000000;;	# Example:
0000000000000000000000000000000000000000;;	# record_command run_kubectl_tests
0000000000000000000000000000000000000000;;	#
0000000000000000000000000000000000000000;;	# WARNING: Variable changes in the command will NOT be effective after record_command returns.
0000000000000000000000000000000000000000;;	#          This is because the command runs in subshell.
0000000000000000000000000000000000000000;;	function record_command() {
0000000000000000000000000000000000000000;;	    set +o nounset
0000000000000000000000000000000000000000;;	    set +o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    local name="$1"
0000000000000000000000000000000000000000;;	    local output="${KUBE_JUNIT_REPORT_DIR:-/tmp/junit-results}"
0000000000000000000000000000000000000000;;	    echo "Recording: ${name}"
0000000000000000000000000000000000000000;;	    echo "Running command: $@"
0000000000000000000000000000000000000000;;	    juLog -output="${output}" -class="test-cmd" -name="${name}" "$@"
0000000000000000000000000000000000000000;;	    if [[ $? -ne 0 ]]; then
0000000000000000000000000000000000000000;;	      echo "Error when running ${name}"
0000000000000000000000000000000000000000;;	      foundError="True"
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    set -o nounset
0000000000000000000000000000000000000000;;	    set -o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Stops the running kubectl proxy, if there is one.
0000000000000000000000000000000000000000;;	function stop-proxy()
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  [[ -n "${PROXY_PORT-}" ]] && kube::log::status "Stopping proxy on port ${PROXY_PORT}"
0000000000000000000000000000000000000000;;	  [[ -n "${PROXY_PID-}" ]] && kill "${PROXY_PID}" 1>&2 2>/dev/null
0000000000000000000000000000000000000000;;	  [[ -n "${PROXY_PORT_FILE-}" ]] && rm -f ${PROXY_PORT_FILE}
0000000000000000000000000000000000000000;;	  PROXY_PID=
0000000000000000000000000000000000000000;;	  PROXY_PORT=
0000000000000000000000000000000000000000;;	  PROXY_PORT_FILE=
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Starts "kubect proxy" to test the client proxy. $1: api_prefix
0000000000000000000000000000000000000000;;	function start-proxy()
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  stop-proxy
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  PROXY_PORT_FILE=$(mktemp proxy-port.out.XXXXX)
0000000000000000000000000000000000000000;;	  kube::log::status "Starting kubectl proxy on random port; output file in ${PROXY_PORT_FILE}; args: ${1-}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if [ $# -eq 0 ]; then
0000000000000000000000000000000000000000;;	    kubectl proxy --port=0 --www=. 1>${PROXY_PORT_FILE} 2>&1 &
0000000000000000000000000000000000000000;;	  else
0000000000000000000000000000000000000000;;	    kubectl proxy --port=0 --www=. --api-prefix="$1" 1>${PROXY_PORT_FILE} 2>&1 &
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  PROXY_PID=$!
0000000000000000000000000000000000000000;;	  PROXY_PORT=
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  local attempts=0
0000000000000000000000000000000000000000;;	  while [[ -z ${PROXY_PORT} ]]; do
0000000000000000000000000000000000000000;;	    if (( ${attempts} > 9 )); then
0000000000000000000000000000000000000000;;	      kill "${PROXY_PID}"
0000000000000000000000000000000000000000;;	      kube::log::error_exit "Couldn't start proxy. Failed to read port after ${attempts} tries. Got: $(cat ${PROXY_PORT_FILE})"
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    sleep .5
0000000000000000000000000000000000000000;;	    kube::log::status "Attempt ${attempts} to read ${PROXY_PORT_FILE}..."
0000000000000000000000000000000000000000;;	    PROXY_PORT=$(sed 's/.*Starting to serve on 127.0.0.1:\([0-9]*\)$/\1/'< ${PROXY_PORT_FILE})
0000000000000000000000000000000000000000;;	    attempts=$((attempts+1))
0000000000000000000000000000000000000000;;	  done
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "kubectl proxy running on port ${PROXY_PORT}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # We try checking kubectl proxy 30 times with 1s delays to avoid occasional
0000000000000000000000000000000000000000;;	  # failures.
0000000000000000000000000000000000000000;;	  if [ $# -eq 0 ]; then
0000000000000000000000000000000000000000;;	    kube::util::wait_for_url "http://127.0.0.1:${PROXY_PORT}/healthz" "kubectl proxy"
0000000000000000000000000000000000000000;;	  else
0000000000000000000000000000000000000000;;	    kube::util::wait_for_url "http://127.0.0.1:${PROXY_PORT}/$1/healthz" "kubectl proxy --api-prefix=$1"
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	function cleanup()
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  [[ -n "${APISERVER_PID-}" ]] && kill "${APISERVER_PID}" 1>&2 2>/dev/null
0000000000000000000000000000000000000000;;	  [[ -n "${CTLRMGR_PID-}" ]] && kill "${CTLRMGR_PID}" 1>&2 2>/dev/null
0000000000000000000000000000000000000000;;	  [[ -n "${KUBELET_PID-}" ]] && kill "${KUBELET_PID}" 1>&2 2>/dev/null
0000000000000000000000000000000000000000;;	  stop-proxy
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::etcd::cleanup
0000000000000000000000000000000000000000;;	  rm -rf "${KUBE_TEMP}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Clean up complete"
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Executes curl against the proxy. $1 is the path to use, $2 is the desired
0000000000000000000000000000000000000000;;	# return code. Prints a helpful message on failure.
0000000000000000000000000000000000000000;;	function check-curl-proxy-code()
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  local status
0000000000000000000000000000000000000000;;	  local -r address=$1
0000000000000000000000000000000000000000;;	  local -r desired=$2
0000000000000000000000000000000000000000;;	  local -r full_address="${PROXY_HOST}:${PROXY_PORT}${address}"
0000000000000000000000000000000000000000;;	  status=$(curl -w "%{http_code}" --silent --output /dev/null "${full_address}")
0000000000000000000000000000000000000000;;	  if [ "${status}" == "${desired}" ]; then
0000000000000000000000000000000000000000;;	    return 0
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  echo "For address ${full_address}, got ${status} but wanted ${desired}"
0000000000000000000000000000000000000000;;	  return 1
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# TODO: Remove this function when we do the retry inside the kubectl commands. See #15333.
0000000000000000000000000000000000000000;;	function kubectl-with-retry()
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  ERROR_FILE="${KUBE_TEMP}/kubectl-error"
0000000000000000000000000000000000000000;;	  preserve_err_file=${PRESERVE_ERR_FILE-false}
0000000000000000000000000000000000000000;;	  for count in {0..3}; do
0000000000000000000000000000000000000000;;	    kubectl "$@" 2> ${ERROR_FILE} || true
0000000000000000000000000000000000000000;;	    if grep -q "the object has been modified" "${ERROR_FILE}"; then
0000000000000000000000000000000000000000;;	      kube::log::status "retry $1, error: $(cat ${ERROR_FILE})"
0000000000000000000000000000000000000000;;	      rm "${ERROR_FILE}"
0000000000000000000000000000000000000000;;	      sleep $((2**count))
0000000000000000000000000000000000000000;;	    else
0000000000000000000000000000000000000000;;	      if [ "$preserve_err_file" != true ] ; then
0000000000000000000000000000000000000000;;	        rm "${ERROR_FILE}"
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	      break
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  done
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Waits for the pods with the given label to match the list of names. Don't call
0000000000000000000000000000000000000000;;	# this function unless you know the exact pod names, or expect no pods.
0000000000000000000000000000000000000000;;	# $1: label to match
0000000000000000000000000000000000000000;;	# $2: list of pod names sorted by name
0000000000000000000000000000000000000000;;	# Example invocation:
0000000000000000000000000000000000000000;;	# wait-for-pods-with-label "app=foo" "nginx-0nginx-1"
0000000000000000000000000000000000000000;;	function wait-for-pods-with-label()
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  local i
0000000000000000000000000000000000000000;;	  for i in $(seq 1 10); do
0000000000000000000000000000000000000000;;	    kubeout=`kubectl get po -l $1 --template '{{range.items}}{{.metadata.name}}{{end}}' --sort-by metadata.name "${kube_flags[@]}"`
0000000000000000000000000000000000000000;;	    if [[ $kubeout = $2 ]]; then
0000000000000000000000000000000000000000;;	        return
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    echo Waiting for pods: $2, found $kubeout
0000000000000000000000000000000000000000;;	    sleep $i
0000000000000000000000000000000000000000;;	  done
0000000000000000000000000000000000000000;;	  kube::log::error_exit "Timeout waiting for pods with label $1"
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Code to be run before running the tests.
0000000000000000000000000000000000000000;;	setup() {
0000000000000000000000000000000000000000;;	  kube::util::trap_add cleanup EXIT SIGINT
0000000000000000000000000000000000000000;;	  kube::util::ensure-temp-dir
0000000000000000000000000000000000000000;;	  # ensure ~/.kube/config isn't loaded by tests
0000000000000000000000000000000000000000;;	  HOME="${KUBE_TEMP}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::etcd::start
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Find a standard sed instance for use with edit scripts
0000000000000000000000000000000000000000;;	  SED=sed
0000000000000000000000000000000000000000;;	  if which gsed &>/dev/null; then
0000000000000000000000000000000000000000;;	    SED=gsed
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  if ! ($SED --version 2>&1 | grep -q GNU); then
0000000000000000000000000000000000000000;;	    echo "!!! GNU sed is required.  If on OS X, use 'brew install gnu-sed'."
0000000000000000000000000000000000000000;;	    exit 1
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Building kubectl"
0000000000000000000000000000000000000000;;	  make -C "${KUBE_ROOT}" WHAT="cmd/kubectl"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Check kubectl
0000000000000000000000000000000000000000;;	  kube::log::status "Running kubectl with no options"
0000000000000000000000000000000000000000;;	  "${KUBE_OUTPUT_HOSTBIN}/kubectl"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # TODO: we need to note down the current default namespace and set back to this
0000000000000000000000000000000000000000;;	  # namespace after the tests are done.
0000000000000000000000000000000000000000;;	  kubectl config view
0000000000000000000000000000000000000000;;	  CONTEXT="test"
0000000000000000000000000000000000000000;;	  kubectl config set-context "${CONTEXT}"
0000000000000000000000000000000000000000;;	  kubectl config use-context "${CONTEXT}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Setup complete"
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	########################################################
0000000000000000000000000000000000000000;;	# Kubectl version (--short, --client, --output) #
0000000000000000000000000000000000000000;;	########################################################
0000000000000000000000000000000000000000;;	run_kubectl_version_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl version"
0000000000000000000000000000000000000000;;	  TEMP="${KUBE_TEMP}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl get "${kube_flags[@]}" --raw /version
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # create version files, one for the client, one for the server.
0000000000000000000000000000000000000000;;	  # these are the files we will use to ensure that the remainder output is correct
0000000000000000000000000000000000000000;;	  kube::test::version::object_to_file "Client" "" "${TEMP}/client_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::object_to_file "Server" "" "${TEMP}/server_version_test"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl version: check client only output matches expected output"
0000000000000000000000000000000000000000;;	  kube::test::version::object_to_file "Client" "--client" "${TEMP}/client_only_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::object_to_file "Client" "--client" "${TEMP}/server_client_only_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::diff_assert "${TEMP}/client_version_test" "eq" "${TEMP}/client_only_version_test" "the flag '--client' shows correct client info"
0000000000000000000000000000000000000000;;	  kube::test::version::diff_assert "${TEMP}/server_version_test" "ne" "${TEMP}/server_client_only_version_test" "the flag '--client' correctly has no server version info"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl version: verify json output"
0000000000000000000000000000000000000000;;	  kube::test::version::json_client_server_object_to_file "" "clientVersion" "${TEMP}/client_json_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::json_client_server_object_to_file "" "serverVersion" "${TEMP}/server_json_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::diff_assert "${TEMP}/client_version_test" "eq" "${TEMP}/client_json_version_test" "--output json has correct client info"
0000000000000000000000000000000000000000;;	  kube::test::version::diff_assert "${TEMP}/server_version_test" "eq" "${TEMP}/server_json_version_test" "--output json has correct server info"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl version: verify json output using additional --client flag does not contain serverVersion"
0000000000000000000000000000000000000000;;	  kube::test::version::json_client_server_object_to_file "--client" "clientVersion" "${TEMP}/client_only_json_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::json_client_server_object_to_file "--client" "serverVersion" "${TEMP}/server_client_only_json_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::diff_assert "${TEMP}/client_version_test" "eq" "${TEMP}/client_only_json_version_test" "--client --output json has correct client info"
0000000000000000000000000000000000000000;;	  kube::test::version::diff_assert "${TEMP}/server_version_test" "ne" "${TEMP}/server_client_only_json_version_test" "--client --output json has no server info"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl version: compare json output using additional --short flag"
0000000000000000000000000000000000000000;;	  kube::test::version::json_client_server_object_to_file "--short" "clientVersion" "${TEMP}/client_short_json_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::json_client_server_object_to_file "--short" "serverVersion" "${TEMP}/server_short_json_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::diff_assert "${TEMP}/client_version_test" "eq" "${TEMP}/client_short_json_version_test" "--short --output client json info is equal to non short result"
0000000000000000000000000000000000000000;;	  kube::test::version::diff_assert "${TEMP}/server_version_test" "eq" "${TEMP}/server_short_json_version_test" "--short --output server json info is equal to non short result"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl version: compare json output with yaml output"
0000000000000000000000000000000000000000;;	  kube::test::version::json_object_to_file "" "${TEMP}/client_server_json_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::yaml_object_to_file "" "${TEMP}/client_server_yaml_version_test"
0000000000000000000000000000000000000000;;	  kube::test::version::diff_assert "${TEMP}/client_server_json_version_test" "eq" "${TEMP}/client_server_yaml_version_test" "--output json/yaml has identical information"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Runs all pod related tests.
0000000000000000000000000000000000000000;;	run_pod_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:pods)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create POD valid-pod from JSON
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD is created
0000000000000000000000000000000000000000;;	  kubectl get "${kube_flags[@]}" pods -o json
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{$id_field}}" 'valid-pod'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod/valid-pod' "{{$id_field}}" 'valid-pod'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods/valid-pod' "{{$id_field}}" 'valid-pod'
0000000000000000000000000000000000000000;;	  # Repeat above test using jsonpath template
0000000000000000000000000000000000000000;;	  kube::test::get_object_jsonpath_assert pods "{.items[*]$id_field}" 'valid-pod'
0000000000000000000000000000000000000000;;	  kube::test::get_object_jsonpath_assert 'pod valid-pod' "{$id_field}" 'valid-pod'
0000000000000000000000000000000000000000;;	  kube::test::get_object_jsonpath_assert 'pod/valid-pod' "{$id_field}" 'valid-pod'
0000000000000000000000000000000000000000;;	  kube::test::get_object_jsonpath_assert 'pods/valid-pod' "{$id_field}" 'valid-pod'
0000000000000000000000000000000000000000;;	  # Describe command should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_object_assert pods 'valid-pod' "Name:" "Image:" "Node:" "Labels:" "Status:"
0000000000000000000000000000000000000000;;	  # Describe command should print events information by default
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert pods 'valid-pod'
0000000000000000000000000000000000000000;;	  # Describe command should not print events information when show-events=false
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert pods 'valid-pod' false
0000000000000000000000000000000000000000;;	  # Describe command should print events information when show-events=true
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert pods 'valid-pod' true
0000000000000000000000000000000000000000;;	  # Describe command (resource only) should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_assert pods "Name:" "Image:" "Node:" "Labels:" "Status:"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Describe command should print events information by default
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert pods
0000000000000000000000000000000000000000;;	  # Describe command should not print events information when show-events=false
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert pods false
0000000000000000000000000000000000000000;;	  # Describe command should print events information when show-events=true
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert pods true
0000000000000000000000000000000000000000;;	  ### Validate Export ###
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods/valid-pod' "{{.metadata.namespace}} {{.metadata.name}}" '<no value> valid-pod' "--export=true"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Dump current valid-pod POD
0000000000000000000000000000000000000000;;	  output_pod=$(kubectl get pod valid-pod -o yaml --output-version=v1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete POD valid-pod by id
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete pod valid-pod "${kube_flags[@]}" --grace-period=0 --force
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete POD valid-pod by id with --now
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete pod valid-pod "${kube_flags[@]}" --now
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete POD valid-pod by id with --grace-period=0
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command succeeds without --force by waiting
0000000000000000000000000000000000000000;;	  kubectl delete pod valid-pod "${kube_flags[@]}" --grace-period=0
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create POD valid-pod from dumped YAML
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  echo "${output_pod}" | $SED '/namespace:/d' | kubectl create -f - "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete POD valid-pod from JSON
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml "${kube_flags[@]}" --grace-period=0 --force
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create POD valid-pod from JSON
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete POD valid-pod with label
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert "pods -l'name in (valid-pod)'" '{{range.items}}{{$id_field}}:{{end}}' 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete pods -l'name in (valid-pod)' "${kube_flags[@]}" --grace-period=0 --force
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert "pods -l'name in (valid-pod)'" '{{range.items}}{{$id_field}}:{{end}}' ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create POD valid-pod from YAML
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete PODs with no parameter mustn't kill everything
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  ! kubectl delete pods "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete PODs with --all and a label selector is not permitted
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  ! kubectl delete --all pods -l'name in (valid-pod)' "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete all PODs
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete --all pods "${kube_flags[@]}" --grace-period=0 --force # --all remove all the pods
0000000000000000000000000000000000000000;;	  # Post-condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert "pods -l'name in (valid-pod)'" '{{range.items}}{{$id_field}}:{{end}}' ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Detailed tests for describe pod output
0000000000000000000000000000000000000000;;	    ### Create a new namespace
0000000000000000000000000000000000000000;;	  # Pre-condition: the test-secrets namespace does not exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'namespaces' '{{range.items}}{{ if eq $id_field \"test-kubectl-describe-pod\" }}found{{end}}{{end}}:' ':'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create namespace test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  # Post-condition: namespace 'test-secrets' is created.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'namespaces/test-kubectl-describe-pod' "{{$id_field}}" 'test-kubectl-describe-pod'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a generic secret
0000000000000000000000000000000000000000;;	  # Pre-condition: no SECRET exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secrets --namespace=test-kubectl-describe-pod' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create secret generic test-secret --from-literal=key-1=value1 --type=test-type --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  # Post-condition: secret exists and has expected values
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/test-secret --namespace=test-kubectl-describe-pod' "{{$id_field}}" 'test-secret'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/test-secret --namespace=test-kubectl-describe-pod' "{{$secret_type}}" 'test-type'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a generic configmap
0000000000000000000000000000000000000000;;	  # Pre-condition: no CONFIGMAP exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'configmaps --namespace=test-kubectl-describe-pod' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create configmap test-configmap --from-literal=key-2=value2 --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  # Post-condition: configmap exists and has expected values
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'configmap/test-configmap --namespace=test-kubectl-describe-pod' "{{$id_field}}" 'test-configmap'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a pod disruption budget with minAvailable
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create pdb test-pdb-1 --selector=app=rails --min-available=2 --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  # Post-condition: pdb exists and has expected values
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pdb/test-pdb-1 --namespace=test-kubectl-describe-pod' "{{$pdb_min_available}}" '2'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create pdb test-pdb-2 --selector=app=rails --min-available=50% --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  # Post-condition: pdb exists and has expected values
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pdb/test-pdb-2 --namespace=test-kubectl-describe-pod' "{{$pdb_min_available}}" '50%'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a pod disruption budget with maxUnavailable
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create pdb test-pdb-3 --selector=app=rails --max-unavailable=2 --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  # Post-condition: pdb exists and has expected values
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pdb/test-pdb-3 --namespace=test-kubectl-describe-pod' "{{$pdb_max_unavailable}}" '2'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create pdb test-pdb-4 --selector=app=rails --max-unavailable=50% --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  # Post-condition: pdb exists and has expected values
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pdb/test-pdb-4 --namespace=test-kubectl-describe-pod' "{{$pdb_max_unavailable}}" '50%'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Fail creating a pod disruption budget if both maxUnavailable and minAvailable specified
0000000000000000000000000000000000000000;;	  ! kubectl create pdb test-pdb --selector=app=rails --min-available=2 --max-unavailable=3 --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Create a pod that consumes secret, configmap, and downward API keys as envs
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods --namespace=test-kubectl-describe-pod' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/pod-with-api-env.yaml --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::test::describe_object_assert 'pods --namespace=test-kubectl-describe-pod' 'env-test-pod' "TEST_CMD_1" "<set to the key 'key-1' in secret 'test-secret'>" "TEST_CMD_2" "<set to the key 'key-2' of config map 'test-configmap'>" "TEST_CMD_3" "env-test-pod (v1:metadata.name)"
0000000000000000000000000000000000000000;;	  # Describe command (resource only) should print detailed information about environment variables
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_assert 'pods --namespace=test-kubectl-describe-pod' "TEST_CMD_1" "<set to the key 'key-1' in secret 'test-secret'>" "TEST_CMD_2" "<set to the key 'key-2' of config map 'test-configmap'>" "TEST_CMD_3" "env-test-pod (v1:metadata.name)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete pod env-test-pod --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  kubectl delete secret test-secret --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  kubectl delete configmap test-configmap --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  kubectl delete pdb/test-pdb-1 pdb/test-pdb-2 pdb/test-pdb-3 pdb/test-pdb-4 --namespace=test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	  kubectl delete namespace test-kubectl-describe-pod
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create two PODs
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl create -f examples/storage/redis/redis-master.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod and redis-master PODs are created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'redis-master:valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete multiple PODs at once
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod and redis-master PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'redis-master:valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete pods valid-pod redis-master "${kube_flags[@]}" --grace-period=0 --force # delete multiple pods at once
0000000000000000000000000000000000000000;;	  # Post-condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create valid-pod POD
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Label the valid-pod POD
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod is not labelled
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{range$labels_field}}{{.}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl label pods valid-pod new-name=new-valid-pod "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod is labelled
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{range$labels_field}}{{.}}:{{end}}" 'valid-pod:new-valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Label the valid-pod POD with empty label value
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod does not have label "emptylabel"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{range$labels_field}}{{.}}:{{end}}" 'valid-pod:new-valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl label pods valid-pod emptylabel="" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid pod contains "emptylabel" with no value
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{${labels_field}.emptylabel}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Annotate the valid-pod POD with empty annotation value
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod does not have annotation "emptyannotation"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{${annotations_field}.emptyannotation}}" '<no value>'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl annotate pods valid-pod emptyannotation="" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid pod contains "emptyannotation" with no value
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{${annotations_field}.emptyannotation}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Record label change
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod does not have record annotation
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{range.items}}{{$annotations_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl label pods valid-pod record-change=true --record=true "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod has record annotation
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{range$annotations_field}}{{.}}:{{end}}" ".*--record=true.*"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Do not record label change
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl label pods valid-pod no-record-change=true --record=false "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod's record annotation still contains command with --record=true
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{range$annotations_field}}{{.}}:{{end}}" ".*--record=true.*"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Record label change with unspecified flag and previous change already recorded
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl label pods valid-pod new-record-change=true "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod's record annotation contains new change
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{range$annotations_field}}{{.}}:{{end}}" ".*new-record-change=true.*"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete POD by label
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete pods -lnew-name=new-valid-pod --grace-period=0 --force "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create pod-with-precision POD
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD is running
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/pod-with-precision.json "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD is running
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'pod-with-precision:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Patch preserves precision
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl patch "${kube_flags[@]}" pod pod-with-precision -p='{"metadata":{"annotations":{"patchkey": "patchvalue"}}}'
0000000000000000000000000000000000000000;;	  # Post-condition: pod-with-precision POD has patched annotation
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod pod-with-precision' "{{${annotations_field}.patchkey}}" 'patchvalue'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl label pods pod-with-precision labelkey=labelvalue "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: pod-with-precision POD has label
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod pod-with-precision' "{{${labels_field}.labelkey}}" 'labelvalue'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl annotate pods pod-with-precision annotatekey=annotatevalue "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: pod-with-precision POD has annotation
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod pod-with-precision' "{{${annotations_field}.annotatekey}}" 'annotatevalue'
0000000000000000000000000000000000000000;;	  # Cleanup
0000000000000000000000000000000000000000;;	  kubectl delete pod pod-with-precision "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Annotate POD YAML file locally without effecting the live pod.
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl annotate -f hack/testdata/pod.yaml annotatekey=annotatevalue "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Pre-condition: annotationkey is annotationvalue
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod test-pod' "{{${annotations_field}.annotatekey}}" 'annotatevalue'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl annotate --local -f hack/testdata/pod.yaml annotatekey=localvalue -o yaml "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  echo $output_message
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Post-condition: annotationkey is still annotationvalue in the live pod, but command output is the new value
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod test-pod' "{{${annotations_field}.annotatekey}}" 'annotatevalue'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "localvalue"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Cleanup
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create valid-pod POD
0000000000000000000000000000000000000000;;	  # Pre-condition: no services and no rcs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert service "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  ## kubectl create --edit can update the label filed of multiple resources. tmp-editor.sh is a fake editor
0000000000000000000000000000000000000000;;	  TEMP=$(mktemp /tmp/tmp-editor-XXXXXXXX.sh)
0000000000000000000000000000000000000000;;	  echo -e "#!/bin/bash\n$SED -i \"s/mock/modified/g\" \$1" > ${TEMP}
0000000000000000000000000000000000000000;;	  chmod +x ${TEMP}
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  EDITOR=${TEMP} kubectl create --edit -f hack/testdata/multi-resource-json.json "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: service named modified and rc named modified are created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert service "{{range.items}}{{$id_field}}:{{end}}" 'modified:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'modified:'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete service/modified "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl delete rc/modified "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Pre-condition: no services and no rcs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert service "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  EDITOR=${TEMP} kubectl create --edit -f hack/testdata/multi-resource-list.json "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: service named modified and rc named modified are created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert service "{{range.items}}{{$id_field}}:{{end}}" 'modified:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'modified:'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  rm ${TEMP}
0000000000000000000000000000000000000000;;	  kubectl delete service/modified "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl delete rc/modified "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl create --edit won't create anything if user makes no changes
0000000000000000000000000000000000000000;;	  [ "$(EDITOR=cat kubectl create --edit -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml -o json 2>&1 | grep 'Edit cancelled')" ]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Create valid-pod POD
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Patch can modify a local object
0000000000000000000000000000000000000000;;	  kubectl patch --local -f pkg/api/validation/testdata/v1/validPod.yaml --patch='{"spec": {"restartPolicy":"Never"}}' -o jsonpath='{.spec.restartPolicy}' | grep -q "Never"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Patch pod can change image
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl patch "${kube_flags[@]}" pod valid-pod --record -p='{"spec":{"containers":[{"name": "kubernetes-serve-hostname", "image": "nginx"}]}}'
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD has image nginx
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$image_field}}:{{end}}" 'nginx:'
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod has the record annotation
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$annotations_field}}:{{end}}" "${change_cause_annotation}"
0000000000000000000000000000000000000000;;	  # prove that patch can use different types
0000000000000000000000000000000000000000;;	  kubectl patch "${kube_flags[@]}" pod valid-pod --type="json" -p='[{"op": "replace", "path": "/spec/containers/0/image", "value":"nginx2"}]'
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD has image nginx
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$image_field}}:{{end}}" 'nginx2:'
0000000000000000000000000000000000000000;;	  # prove that patch can use different types
0000000000000000000000000000000000000000;;	  kubectl patch "${kube_flags[@]}" pod valid-pod --type="json" -p='[{"op": "replace", "path": "/spec/containers/0/image", "value":"nginx"}]'
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD has image nginx
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$image_field}}:{{end}}" 'nginx:'
0000000000000000000000000000000000000000;;	  # prove that yaml input works too
0000000000000000000000000000000000000000;;	  YAML_PATCH=$'spec:\n  containers:\n  - name: kubernetes-serve-hostname\n    image: changed-with-yaml\n'
0000000000000000000000000000000000000000;;	  kubectl patch "${kube_flags[@]}" pod valid-pod -p="${YAML_PATCH}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD has image nginx
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$image_field}}:{{end}}" 'changed-with-yaml:'
0000000000000000000000000000000000000000;;	  ## Patch pod from JSON can change image
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl patch "${kube_flags[@]}" -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml -p='{"spec":{"containers":[{"name": "kubernetes-serve-hostname", "image": "gcr.io/google_containers/pause-amd64:3.0"}]}}'
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD has image gcr.io/google_containers/pause-amd64:3.0
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$image_field}}:{{end}}" 'gcr.io/google_containers/pause-amd64:3.0:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## If resourceVersion is specified in the patch, it will be treated as a precondition, i.e., if the resourceVersion is different from that is stored in the server, the Patch should be rejected
0000000000000000000000000000000000000000;;	  ERROR_FILE="${KUBE_TEMP}/conflict-error"
0000000000000000000000000000000000000000;;	  ## If the resourceVersion is the same as the one stored in the server, the patch will be applied.
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  # Needs to retry because other party may change the resource.
0000000000000000000000000000000000000000;;	  for count in {0..3}; do
0000000000000000000000000000000000000000;;	    resourceVersion=$(kubectl get "${kube_flags[@]}" pod valid-pod -o go-template='{{ .metadata.resourceVersion }}')
0000000000000000000000000000000000000000;;	    kubectl patch "${kube_flags[@]}" pod valid-pod -p='{"spec":{"containers":[{"name": "kubernetes-serve-hostname", "image": "nginx"}]},"metadata":{"resourceVersion":"'$resourceVersion'"}}' 2> "${ERROR_FILE}" || true
0000000000000000000000000000000000000000;;	    if grep -q "the object has been modified" "${ERROR_FILE}"; then
0000000000000000000000000000000000000000;;	      kube::log::status "retry $1, error: $(cat ${ERROR_FILE})"
0000000000000000000000000000000000000000;;	      rm "${ERROR_FILE}"
0000000000000000000000000000000000000000;;	      sleep $((2**count))
0000000000000000000000000000000000000000;;	    else
0000000000000000000000000000000000000000;;	      rm "${ERROR_FILE}"
0000000000000000000000000000000000000000;;	      kube::test::get_object_assert pods "{{range.items}}{{$image_field}}:{{end}}" 'nginx:'
0000000000000000000000000000000000000000;;	      break
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  done
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## If the resourceVersion is the different from the one stored in the server, the patch will be rejected.
0000000000000000000000000000000000000000;;	  resourceVersion=$(kubectl get "${kube_flags[@]}" pod valid-pod -o go-template='{{ .metadata.resourceVersion }}')
0000000000000000000000000000000000000000;;	  ((resourceVersion+=100))
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl patch "${kube_flags[@]}" pod valid-pod -p='{"spec":{"containers":[{"name": "kubernetes-serve-hostname", "image": "nginx"}]},"metadata":{"resourceVersion":"'$resourceVersion'"}}' 2> "${ERROR_FILE}" || true
0000000000000000000000000000000000000000;;	  # Post-condition: should get an error reporting the conflict
0000000000000000000000000000000000000000;;	  if grep -q "please apply your changes to the latest version and try again" "${ERROR_FILE}"; then
0000000000000000000000000000000000000000;;	    kube::log::status "\"kubectl patch with resourceVersion $resourceVersion\" returns error as expected: $(cat ${ERROR_FILE})"
0000000000000000000000000000000000000000;;	  else
0000000000000000000000000000000000000000;;	    kube::log::status "\"kubectl patch with resourceVersion $resourceVersion\" returns unexpected error or non-error: $(cat ${ERROR_FILE})"
0000000000000000000000000000000000000000;;	    exit 1
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  rm "${ERROR_FILE}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## --force replace pod can change other field, e.g., spec.container.name
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl get "${kube_flags[@]}" pod valid-pod -o json | $SED 's/"kubernetes-serve-hostname"/"replaced-k8s-serve-hostname"/g' > /tmp/tmp-valid-pod.json
0000000000000000000000000000000000000000;;	  kubectl replace "${kube_flags[@]}" --force -f /tmp/tmp-valid-pod.json
0000000000000000000000000000000000000000;;	  # Post-condition: spec.container.name = "replaced-k8s-serve-hostname"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{(index .spec.containers 0).name}}" 'replaced-k8s-serve-hostname'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## check replace --grace-period requires --force
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl replace "${kube_flags[@]}" --grace-period=1 -f /tmp/tmp-valid-pod.json 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" '\-\-grace-period must have \-\-force specified'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## check replace --timeout requires --force
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl replace "${kube_flags[@]}" --timeout=1s -f /tmp/tmp-valid-pod.json 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" '\-\-timeout must have \-\-force specified'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #cleaning
0000000000000000000000000000000000000000;;	  rm /tmp/tmp-valid-pod.json
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## replace of a cluster scoped resource can succeed
0000000000000000000000000000000000000000;;	  # Pre-condition: a node exists
0000000000000000000000000000000000000000;;	  kubectl create -f - "${kube_flags[@]}" << __EOF__
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  "kind": "Node",
0000000000000000000000000000000000000000;;	  "apiVersion": "v1",
0000000000000000000000000000000000000000;;	  "metadata": {
0000000000000000000000000000000000000000;;	    "name": "node-v1-test"
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	__EOF__
0000000000000000000000000000000000000000;;	  kubectl replace -f - "${kube_flags[@]}" << __EOF__
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  "kind": "Node",
0000000000000000000000000000000000000000;;	  "apiVersion": "v1",
0000000000000000000000000000000000000000;;	  "metadata": {
0000000000000000000000000000000000000000;;	    "name": "node-v1-test",
0000000000000000000000000000000000000000;;	    "annotations": {"a":"b"},
0000000000000000000000000000000000000000;;	    "resourceVersion": "0"
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	__EOF__
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Post-condition: the node command succeeds
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert "node node-v1-test" "{{.metadata.annotations.a}}" 'b'
0000000000000000000000000000000000000000;;	  kubectl delete node node-v1-test "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl edit can update the image field of a POD. tmp-editor.sh is a fake editor
0000000000000000000000000000000000000000;;	  echo -e "#!/bin/bash\n$SED -i \"s/nginx/gcr.io\/google_containers\/serve_hostname/g\" \$1" > /tmp/tmp-editor.sh
0000000000000000000000000000000000000000;;	  chmod +x /tmp/tmp-editor.sh
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD has image nginx
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$image_field}}:{{end}}" 'nginx:'
0000000000000000000000000000000000000000;;	  EDITOR=/tmp/tmp-editor.sh kubectl edit "${kube_flags[@]}" pods/valid-pod
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD has image gcr.io/google_containers/serve_hostname
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$image_field}}:{{end}}" 'gcr.io/google_containers/serve_hostname:'
0000000000000000000000000000000000000000;;	  # cleaning
0000000000000000000000000000000000000000;;	  rm /tmp/tmp-editor.sh
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl edit should work on Windows
0000000000000000000000000000000000000000;;	  [ "$(EDITOR=cat kubectl edit pod/valid-pod 2>&1 | grep 'Edit cancelled')" ]
0000000000000000000000000000000000000000;;	  [ "$(EDITOR=cat kubectl edit pod/valid-pod | grep 'name: valid-pod')" ]
0000000000000000000000000000000000000000;;	  [ "$(EDITOR=cat kubectl edit --windows-line-endings pod/valid-pod | file - | grep CRLF)" ]
0000000000000000000000000000000000000000;;	  [ ! "$(EDITOR=cat kubectl edit --windows-line-endings=false pod/valid-pod | file - | grep CRLF)" ]
0000000000000000000000000000000000000000;;	  [ "$(EDITOR=cat kubectl edit ns | grep 'kind: List')" ]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Label POD YAML file locally without effecting the live pod.
0000000000000000000000000000000000000000;;	  # Pre-condition: name is valid-pod
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{${labels_field}.name}}" 'valid-pod'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl label --local --overwrite -f hack/testdata/pod.yaml name=localonlyvalue -o yaml "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  echo $output_message
0000000000000000000000000000000000000000;;	  # Post-condition: name is still valid-pod in the live pod, but command output is the new value
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{${labels_field}.name}}" 'valid-pod'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "localonlyvalue"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Overwriting an existing label is not permitted
0000000000000000000000000000000000000000;;	  # Pre-condition: name is valid-pod
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{${labels_field}.name}}" 'valid-pod'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  ! kubectl label pods valid-pod name=valid-pod-super-sayan "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: name is still valid-pod
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{${labels_field}.name}}" 'valid-pod'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### --overwrite must be used to overwrite existing label, can be applied to all resources
0000000000000000000000000000000000000000;;	  # Pre-condition: name is valid-pod
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{${labels_field}.name}}" 'valid-pod'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl label --overwrite pods --all name=valid-pod-super-sayan "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: name is valid-pod-super-sayan
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pod valid-pod' "{{${labels_field}.name}}" 'valid-pod-super-sayan'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete POD by label
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete pods -l'name in (valid-pod-super-sayan)' --grace-period=0 --force "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create two PODs from 1 yaml file
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/multi-pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: redis-master and valid-pod PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'redis-master:valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete two PODs from 1 yaml file
0000000000000000000000000000000000000000;;	  # Pre-condition: redis-master and valid-pod PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'redis-master:valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete -f test/fixtures/doc-yaml/user-guide/multi-pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl apply should update configuration annotations only if apply is already called
0000000000000000000000000000000000000000;;	  ## 1. kubectl create doesn't set the annotation
0000000000000000000000000000000000000000;;	  # Pre-Condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command: create a pod "test-pod"
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods test-pod' "{{${labels_field}.name}}" 'test-pod-label'
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" doesn't have configuration annotation
0000000000000000000000000000000000000000;;	  ! [[ "$(kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  ## 2. kubectl replace doesn't set the annotation
0000000000000000000000000000000000000000;;	  kubectl get pods test-pod -o yaml "${kube_flags[@]}" | $SED 's/test-pod-label/test-pod-replaced/g' > "${KUBE_TEMP}"/test-pod-replace.yaml
0000000000000000000000000000000000000000;;	  # Command: replace the pod "test-pod"
0000000000000000000000000000000000000000;;	  kubectl replace -f "${KUBE_TEMP}"/test-pod-replace.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" is replaced
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods test-pod' "{{${labels_field}.name}}" 'test-pod-replaced'
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" doesn't have configuration annotation
0000000000000000000000000000000000000000;;	  ! [[ "$(kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  ## 3. kubectl apply does set the annotation
0000000000000000000000000000000000000000;;	  # Command: apply the pod "test-pod"
0000000000000000000000000000000000000000;;	  kubectl apply -f hack/testdata/pod-apply.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" is applied
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods test-pod' "{{${labels_field}.name}}" 'test-pod-applied'
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" has configuration annotation
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration > "${KUBE_TEMP}"/annotation-configuration
0000000000000000000000000000000000000000;;	  ## 4. kubectl replace updates an existing annotation
0000000000000000000000000000000000000000;;	  kubectl get pods test-pod -o yaml "${kube_flags[@]}" | $SED 's/test-pod-applied/test-pod-replaced/g' > "${KUBE_TEMP}"/test-pod-replace.yaml
0000000000000000000000000000000000000000;;	  # Command: replace the pod "test-pod"
0000000000000000000000000000000000000000;;	  kubectl replace -f "${KUBE_TEMP}"/test-pod-replace.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" is replaced
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods test-pod' "{{${labels_field}.name}}" 'test-pod-replaced'
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" has configuration annotation, and it's updated (different from the annotation when it's applied)
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration > "${KUBE_TEMP}"/annotation-configuration-replaced
0000000000000000000000000000000000000000;;	  ! [[ $(diff -q "${KUBE_TEMP}"/annotation-configuration "${KUBE_TEMP}"/annotation-configuration-replaced > /dev/null) ]]
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  rm "${KUBE_TEMP}"/test-pod-replace.yaml "${KUBE_TEMP}"/annotation-configuration "${KUBE_TEMP}"/annotation-configuration-replaced
0000000000000000000000000000000000000000;;	  kubectl delete pods test-pod "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Runs tests related to kubectl apply.
0000000000000000000000000000000000000000;;	run_kubectl_apply_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl apply"
0000000000000000000000000000000000000000;;	  ## kubectl apply should create the resource that doesn't exist yet
0000000000000000000000000000000000000000;;	  # Pre-Condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command: apply a pod "test-pod" (doesn't exist) should create this pod
0000000000000000000000000000000000000000;;	  kubectl apply -f hack/testdata/pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods test-pod' "{{${labels_field}.name}}" 'test-pod-label'
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" has configuration annotation
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete pods test-pod "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl apply -f with label selector should only apply matching objects
0000000000000000000000000000000000000000;;	  # Pre-Condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # apply
0000000000000000000000000000000000000000;;	  kubectl apply -l unique-label=bingbang -f hack/testdata/filter "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # check right pod exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods selector-test-pod' "{{${labels_field}.name}}" 'selector-test-pod'
0000000000000000000000000000000000000000;;	  # check wrong pod doesn't exist
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pods selector-test-pod-dont-apply 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'pods "selector-test-pod-dont-apply" not found'
0000000000000000000000000000000000000000;;	  # cleanup
0000000000000000000000000000000000000000;;	  kubectl delete pods selector-test-pod
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl apply --prune
0000000000000000000000000000000000000000;;	  # Pre-Condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # apply a
0000000000000000000000000000000000000000;;	  kubectl apply --prune -l prune-group=true -f hack/testdata/prune/a.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # check right pod exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods a' "{{${id_field}}}" 'a'
0000000000000000000000000000000000000000;;	  # check wrong pod doesn't exist
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pods b 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'pods "b" not found'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # apply b
0000000000000000000000000000000000000000;;	  kubectl apply --prune -l prune-group=true -f hack/testdata/prune/b.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # check right pod exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods b' "{{${id_field}}}" 'b'
0000000000000000000000000000000000000000;;	  # check wrong pod doesn't exist
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pods a 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'pods "a" not found'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # cleanup
0000000000000000000000000000000000000000;;	  kubectl delete pods b
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # same thing without prune for a sanity check
0000000000000000000000000000000000000000;;	  # Pre-Condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # apply a
0000000000000000000000000000000000000000;;	  kubectl apply -l prune-group=true -f hack/testdata/prune/a.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # check right pod exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods a' "{{${id_field}}}" 'a'
0000000000000000000000000000000000000000;;	  # check wrong pod doesn't exist
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pods b 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'pods "b" not found'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # apply b
0000000000000000000000000000000000000000;;	  kubectl apply -l prune-group=true -f hack/testdata/prune/b.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # check both pods exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods a' "{{${id_field}}}" 'a'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods b' "{{${id_field}}}" 'b'
0000000000000000000000000000000000000000;;	  # check wrong pod doesn't exist
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # cleanup
0000000000000000000000000000000000000000;;	  kubectl delete pod/a pod/b
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl apply --prune requires a --all flag to select everything
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl apply --prune -f hack/testdata/prune 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" \
0000000000000000000000000000000000000000;;	    'all resources selected for prune without explicitly passing --all'
0000000000000000000000000000000000000000;;	  # should apply everything
0000000000000000000000000000000000000000;;	  kubectl apply --all --prune -f hack/testdata/prune
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods a' "{{${id_field}}}" 'a'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods b' "{{${id_field}}}" 'b'
0000000000000000000000000000000000000000;;	  kubectl delete pod/a pod/b
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl apply --prune should fallback to delete for non reapable types
0000000000000000000000000000000000000000;;	  kubectl apply --all --prune -f hack/testdata/prune-reap/a.yml 2>&1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pvc a-pvc' "{{${id_field}}}" 'a-pvc'
0000000000000000000000000000000000000000;;	  kubectl apply --all --prune -f hack/testdata/prune-reap/b.yml 2>&1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pvc b-pvc' "{{${id_field}}}" 'b-pvc'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kubectl delete pvc b-pvc 2>&1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl apply --prune --prune-whitelist
0000000000000000000000000000000000000000;;	  # Pre-Condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # apply pod a
0000000000000000000000000000000000000000;;	  kubectl apply --prune -l prune-group=true -f hack/testdata/prune/a.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # check right pod exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods a' "{{${id_field}}}" 'a'
0000000000000000000000000000000000000000;;	  # apply svc and don't prune pod a by overwriting whitelist
0000000000000000000000000000000000000000;;	  kubectl apply --prune -l prune-group=true -f hack/testdata/prune/svc.yaml --prune-whitelist core/v1/Service 2>&1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service prune-svc' "{{${id_field}}}" 'prune-svc'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods a' "{{${id_field}}}" 'a'
0000000000000000000000000000000000000000;;	  # apply svc and prune pod a with default whitelist
0000000000000000000000000000000000000000;;	  kubectl apply --prune -l prune-group=true -f hack/testdata/prune/svc.yaml 2>&1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service prune-svc' "{{${id_field}}}" 'prune-svc'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # cleanup
0000000000000000000000000000000000000000;;	  kubectl delete svc prune-svc 2>&1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Runs tests related to kubectl create --filename(-f) --selector(-l).
0000000000000000000000000000000000000000;;	run_kubectl_create_filter_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl create filter"
0000000000000000000000000000000000000000;;	  ## kubectl create -f with label selector should only create matching objects
0000000000000000000000000000000000000000;;	  # Pre-Condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # create
0000000000000000000000000000000000000000;;	  kubectl create -l unique-label=bingbang -f hack/testdata/filter "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # check right pod exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods selector-test-pod' "{{${labels_field}.name}}" 'selector-test-pod'
0000000000000000000000000000000000000000;;	  # check wrong pod doesn't exist
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pods selector-test-pod-dont-apply 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'pods "selector-test-pod-dont-apply" not found'
0000000000000000000000000000000000000000;;	  # cleanup
0000000000000000000000000000000000000000;;	  kubectl delete pods selector-test-pod
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_apply_deployments_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl apply deployments"
0000000000000000000000000000000000000000;;	  ## kubectl apply should propagate user defined null values
0000000000000000000000000000000000000000;;	  # Pre-Condition: no Deployments, ReplicaSets, Pods exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployments "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert replicasets "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # apply base deployment
0000000000000000000000000000000000000000;;	  kubectl apply -f hack/testdata/null-propagation/deployment-l1.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # check right deployment exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployments my-depl' "{{${id_field}}}" 'my-depl'
0000000000000000000000000000000000000000;;	  # check right labels exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployments my-depl' "{{.spec.template.metadata.labels.l1}}" 'l1'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployments my-depl' "{{.spec.selector.matchLabels.l1}}" 'l1'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployments my-depl' "{{.metadata.labels.l1}}" 'l1'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # apply new deployment with new template labels
0000000000000000000000000000000000000000;;	  kubectl apply -f hack/testdata/null-propagation/deployment-l2.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # check right labels exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployments my-depl' "{{.spec.template.metadata.labels.l1}}" '<no value>'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployments my-depl' "{{.spec.selector.matchLabels.l1}}" '<no value>'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployments my-depl' "{{.metadata.labels.l1}}" '<no value>'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployments my-depl' "{{.spec.template.metadata.labels.l2}}" 'l2'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployments my-depl' "{{.spec.selector.matchLabels.l2}}" 'l2'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployments my-depl' "{{.metadata.labels.l2}}" 'l2'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # cleanup
0000000000000000000000000000000000000000;;	  # need to explicitly remove replicasets and pods because we changed the deployment selector and orphaned things
0000000000000000000000000000000000000000;;	  kubectl delete deployments,rs,pods --all --cascade=false --grace-period=0
0000000000000000000000000000000000000000;;	  # Post-Condition: no Deployments, ReplicaSets, Pods exist
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert deployments "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert replicasets "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	# Runs tests for --save-config tests.
0000000000000000000000000000000000000000;;	run_save_config_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl --save-config"
0000000000000000000000000000000000000000;;	  ## Configuration annotations should be set when --save-config is enabled
0000000000000000000000000000000000000000;;	  ## 1. kubectl create --save-config should generate configuration annotation
0000000000000000000000000000000000000000;;	  # Pre-Condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command: create a pod "test-pod"
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/pod.yaml --save-config "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" has configuration annotation
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  ## 2. kubectl edit --save-config should generate configuration annotation
0000000000000000000000000000000000000000;;	  # Pre-Condition: no POD exists, then create pod "test-pod", which shouldn't have configuration annotation
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  ! [[ "$(kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  # Command: edit the pod "test-pod"
0000000000000000000000000000000000000000;;	  temp_editor="${KUBE_TEMP}/tmp-editor.sh"
0000000000000000000000000000000000000000;;	  echo -e "#!/bin/bash\n$SED -i \"s/test-pod-label/test-pod-label-edited/g\" \$@" > "${temp_editor}"
0000000000000000000000000000000000000000;;	  chmod +x "${temp_editor}"
0000000000000000000000000000000000000000;;	  EDITOR=${temp_editor} kubectl edit pod test-pod --save-config "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" has configuration annotation
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  ## 3. kubectl replace --save-config should generate configuration annotation
0000000000000000000000000000000000000000;;	  # Pre-Condition: no POD exists, then create pod "test-pod", which shouldn't have configuration annotation
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  ! [[ "$(kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  # Command: replace the pod "test-pod"
0000000000000000000000000000000000000000;;	  kubectl replace -f hack/testdata/pod.yaml --save-config "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: pod "test-pod" has configuration annotation
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get pods test-pod -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  ## 4. kubectl run --save-config should generate configuration annotation
0000000000000000000000000000000000000000;;	  # Pre-Condition: no RC exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command: create the rc "nginx" with image nginx
0000000000000000000000000000000000000000;;	  kubectl run nginx "--image=$IMAGE_NGINX" --save-config --generator=run/v1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: rc "nginx" has configuration annotation
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get rc nginx -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  ## 5. kubectl expose --save-config should generate configuration annotation
0000000000000000000000000000000000000000;;	  # Pre-Condition: no service exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert svc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command: expose the rc "nginx"
0000000000000000000000000000000000000000;;	  kubectl expose rc nginx --save-config --port=80 --target-port=8000 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: service "nginx" has configuration annotation
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get svc nginx -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete rc,svc nginx
0000000000000000000000000000000000000000;;	  ## 6. kubectl autoscale --save-config should generate configuration annotation
0000000000000000000000000000000000000000;;	  # Pre-Condition: no RC exists, then create the rc "frontend", which shouldn't have configuration annotation
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  ! [[ "$(kubectl get rc frontend -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  # Command: autoscale rc "frontend"
0000000000000000000000000000000000000000;;	  kubectl autoscale -f hack/testdata/frontend-controller.yaml --save-config "${kube_flags[@]}" --max=2
0000000000000000000000000000000000000000;;	  # Post-Condition: hpa "frontend" has configuration annotation
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get hpa frontend -o yaml "${kube_flags[@]}" | grep kubectl.kubernetes.io/last-applied-configuration)" ]]
0000000000000000000000000000000000000000;;	  # Ensure we can interact with HPA objects in lists through autoscaling/v1 APIs
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get hpa -o=jsonpath='{.items[0].apiVersion}' 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'autoscaling/v1'
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get hpa.autoscaling -o=jsonpath='{.items[0].apiVersion}' 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'autoscaling/v1'
0000000000000000000000000000000000000000;;	  # tests kubectl group prefix matching
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get hpa.autoscal -o=jsonpath='{.items[0].apiVersion}' 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'autoscaling/v1'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  # Note that we should delete hpa first, otherwise it may fight with the rc reaper.
0000000000000000000000000000000000000000;;	  kubectl delete hpa frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl delete rc  frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_run_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl run"
0000000000000000000000000000000000000000;;	  ## kubectl run should create deployments or jobs
0000000000000000000000000000000000000000;;	  # Pre-Condition: no Job exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert jobs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl run pi --generator=job/v1 "--image=$IMAGE_PERL" --restart=OnFailure -- perl -Mbignum=bpi -wle 'print bpi(20)' "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: Job "pi" is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert jobs "{{range.items}}{{$id_field}}:{{end}}" 'pi:'
0000000000000000000000000000000000000000;;	  # Describe command (resource only) should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_assert pods "Name:" "Image:" "Node:" "Labels:" "Status:" "Created By"
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete jobs pi "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no pods exist.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Pre-Condition: no Deployment exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl run nginx-extensions "--image=$IMAGE_NGINX" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: Deployment "nginx" is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment.extensions "{{range.items}}{{$id_field}}:{{end}}" 'nginx-extensions:'
0000000000000000000000000000000000000000;;	  # and old generator was used, iow. old defaults are applied
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get deployment.extensions/nginx-extensions -o jsonpath='{.spec.revisionHistoryLimit}')
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" '2'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete deployment nginx-extensions "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl run nginx-apps "--image=$IMAGE_NGINX" --generator=deployment/apps.v1beta1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: Deployment "nginx" is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment.apps "{{range.items}}{{$id_field}}:{{end}}" 'nginx-apps:'
0000000000000000000000000000000000000000;;	  # and new generator was used, iow. new defaults are applied
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get deployment/nginx-apps -o jsonpath='{.spec.revisionHistoryLimit}')
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" '2'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete deployment nginx-apps "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_using_deprecated_commands_test() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl using deprecated commands"
0000000000000000000000000000000000000000;;	  ## `kubectl run-container` should function identical to `kubectl run`, but it
0000000000000000000000000000000000000000;;	  ## should also print a deprecation warning.
0000000000000000000000000000000000000000;;	  # Pre-Condition: no Job exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert jobs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl 2>&1 run-container pi --generator=job/v1 "--image=$IMAGE_PERL" --restart=OnFailure -- perl -Mbignum=bpi -wle 'print bpi(15)' "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Ensure that the user is warned their command is deprecated.
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'deprecated'
0000000000000000000000000000000000000000;;	  # Post-Condition: Job "pi" is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert jobs "{{range.items}}{{$id_field}}:{{end}}" 'pi:'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete jobs pi "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no pods exist.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_get_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl get"
0000000000000000000000000000000000000000;;	  ### Test retrieval of non-existing pods
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pods abc 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: POD abc should error since it doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'pods "abc" not found'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test retrieval of non-existing POD with output flag specified
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pods abc 2>&1 "${kube_flags[@]}" -o name)
0000000000000000000000000000000000000000;;	  # Post-condition: POD abc should error since it doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'pods "abc" not found'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test retrieval of pods when none exist with non-human readable output format flag specified
0000000000000000000000000000000000000000;;	  # Pre-condition: no pods exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods 2>&1 "${kube_flags[@]}" -o json)
0000000000000000000000000000000000000000;;	  # Post-condition: The text "No resources found" should not be part of the output
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'No resources found'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods 2>&1 "${kube_flags[@]}" -o yaml)
0000000000000000000000000000000000000000;;	  # Post-condition: The text "No resources found" should not be part of the output
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'No resources found'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods 2>&1 "${kube_flags[@]}" -o name)
0000000000000000000000000000000000000000;;	  # Post-condition: The text "No resources found" should not be part of the output
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'No resources found'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods 2>&1 "${kube_flags[@]}" -o jsonpath='{.items}')
0000000000000000000000000000000000000000;;	  # Post-condition: The text "No resources found" should not be part of the output
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'No resources found'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods 2>&1 "${kube_flags[@]}" -o go-template='{{.items}}')
0000000000000000000000000000000000000000;;	  # Post-condition: The text "No resources found" should not be part of the output
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'No resources found'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods 2>&1 "${kube_flags[@]}" -o custom-columns=NAME:.metadata.name)
0000000000000000000000000000000000000000;;	  # Post-condition: The text "No resources found" should not be part of the output
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'No resources found'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test retrieval of pods when none exist, with human-readable output format flag specified
0000000000000000000000000000000000000000;;	  # Pre-condition: no pods exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: The text "No resources found" should be part of the output
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'No resources found'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods --ignore-not-found 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: The text "No resources found" should not be part of the output
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'No resources found'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods 2>&1 "${kube_flags[@]}" -o wide)
0000000000000000000000000000000000000000;;	  # Post-condition: The text "No resources found" should be part of the output
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'No resources found'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test retrieval of non-existing POD with json output flag specified
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pods abc 2>&1 "${kube_flags[@]}" -o json)
0000000000000000000000000000000000000000;;	  # Post-condition: POD abc should error since it doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'pods "abc" not found'
0000000000000000000000000000000000000000;;	  # Post-condition: make sure we don't display an empty List
0000000000000000000000000000000000000000;;	  if kube::test::if_has_string "${output_message}" 'List'; then
0000000000000000000000000000000000000000;;	    echo 'Unexpected List output'
0000000000000000000000000000000000000000;;	    echo "${LINENO} $(basename $0)"
0000000000000000000000000000000000000000;;	    exit 1
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test kubectl get all
0000000000000000000000000000000000000000;;	  output_message=$(kubectl --v=6 --namespace default get all 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: Check if we get 200 OK from all the url(s)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "/api/v1/namespaces/default/pods 200 OK"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "/api/v1/namespaces/default/replicationcontrollers 200 OK"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "/api/v1/namespaces/default/services 200 OK"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "/apis/apps/v1beta1/namespaces/default/statefulsets 200 OK"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "/apis/autoscaling/v1/namespaces/default/horizontalpodautoscalers 200"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "/apis/batch/v1/namespaces/default/jobs 200 OK"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "/apis/extensions/v1beta1/namespaces/default/deployments 200 OK"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "/apis/extensions/v1beta1/namespaces/default/replicasets 200 OK"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test --allow-missing-template-keys
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD is created
0000000000000000000000000000000000000000;;	  kubectl get "${kube_flags[@]}" pods -o json
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## check --allow-missing-template-keys defaults to true for jsonpath templates
0000000000000000000000000000000000000000;;	  kubectl get "${kube_flags[@]}" pod valid-pod -o jsonpath='{.missing}'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## check --allow-missing-template-keys defaults to true for go templates
0000000000000000000000000000000000000000;;	  kubectl get "${kube_flags[@]}" pod valid-pod -o go-template='{{.missing}}'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## check --allow-missing-template-keys=false results in an error for a missing key with jsonpath
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pod valid-pod --allow-missing-template-keys=false -o jsonpath='{.missing}' "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'missing is not found'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## check --allow-missing-template-keys=false results in an error for a missing key with go
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pod valid-pod --allow-missing-template-keys=false -o go-template='{{.missing}}' "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'map has no entry for key "missing"'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test kubectl get watch
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods -w --request-timeout=1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'STATUS'    # headers
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'valid-pod' # pod details
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods/valid-pod -o name -w --request-timeout=1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'STATUS' # no headers
0000000000000000000000000000000000000000;;	  kube::test::if_has_string     "${output_message}" 'pods/valid-pod' # resource name
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pods/valid-pod -o yaml -w --request-timeout=1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'STATUS'          # no headers
0000000000000000000000000000000000000000;;	  kube::test::if_has_string     "${output_message}" 'name: valid-pod' # yaml
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pods/invalid-pod -w --request-timeout=1 "${kube_flags[@]}" 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" '"invalid-pod" not found'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # cleanup
0000000000000000000000000000000000000000;;	  kubectl delete pods valid-pod "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test 'kubectl get -f <file> -o <non default printer>' prints all the items in the file's list
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/multi-pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: PODs redis-master and valid-pod exist
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Check that all items in the list are printed
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get -f test/fixtures/doc-yaml/user-guide/multi-pod.yaml -o jsonpath="{..metadata.name}" "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "redis-master valid-pod"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # cleanup
0000000000000000000000000000000000000000;;	  kubectl delete pods redis-master valid-pod "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_request_timeout_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl request timeout"
0000000000000000000000000000000000000000;;	  ### Test global request timeout option
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod POD is created
0000000000000000000000000000000000000000;;	  kubectl get "${kube_flags[@]}" pods -o json
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## check --request-timeout on 'get pod'
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pod valid-pod --request-timeout=1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'valid-pod'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## check --request-timeout on 'get pod' with --watch
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pod valid-pod --request-timeout=1 --watch 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'Timeout exceeded while reading body'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## check --request-timeout value with no time unit
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get pod valid-pod --request-timeout=1 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'valid-pod'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## check --request-timeout value with invalid time unit
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pod valid-pod --request-timeout="1p" 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'Invalid timeout value'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # cleanup
0000000000000000000000000000000000000000;;	  kubectl delete pods valid-pod "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_crd_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl crd"
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags_with_token[@]}" create -f - << __EOF__
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  "kind": "CustomResourceDefinition",
0000000000000000000000000000000000000000;;	  "apiVersion": "apiextensions.k8s.io/v1beta1",
0000000000000000000000000000000000000000;;	  "metadata": {
0000000000000000000000000000000000000000;;	    "name": "foos.company.com"
0000000000000000000000000000000000000000;;	  },
0000000000000000000000000000000000000000;;	  "spec": {
0000000000000000000000000000000000000000;;	    "group": "company.com",
0000000000000000000000000000000000000000;;	    "version": "v1",
0000000000000000000000000000000000000000;;	    "names": {
0000000000000000000000000000000000000000;;	      "plural": "foos",
0000000000000000000000000000000000000000;;	      "kind": "Foo"
0000000000000000000000000000000000000000;;	    }
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	__EOF__
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Post-Condition: assertion object exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert customresourcedefinitions "{{range.items}}{{$id_field}}:{{end}}" 'foos.company.com:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags_with_token[@]}" create -f - << __EOF__
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  "kind": "CustomResourceDefinition",
0000000000000000000000000000000000000000;;	  "apiVersion": "apiextensions.k8s.io/v1beta1",
0000000000000000000000000000000000000000;;	  "metadata": {
0000000000000000000000000000000000000000;;	    "name": "bars.company.com"
0000000000000000000000000000000000000000;;	  },
0000000000000000000000000000000000000000;;	  "spec": {
0000000000000000000000000000000000000000;;	    "group": "company.com",
0000000000000000000000000000000000000000;;	    "version": "v1",
0000000000000000000000000000000000000000;;	    "names": {
0000000000000000000000000000000000000000;;	      "plural": "bars",
0000000000000000000000000000000000000000;;	      "kind": "Bar"
0000000000000000000000000000000000000000;;	    }
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	__EOF__
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Post-Condition: assertion object exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert customresourcedefinitions "{{range.items}}{{$id_field}}:{{end}}" 'bars.company.com:foos.company.com:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  run_non_native_resource_tests
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # teardown
0000000000000000000000000000000000000000;;	  kubectl delete customresourcedefinitions/foos.company.com "${kube_flags_with_token[@]}"
0000000000000000000000000000000000000000;;	  kubectl delete customresourcedefinitions/bars.company.com "${kube_flags_with_token[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	kube::util::non_native_resources() {
0000000000000000000000000000000000000000;;	  local times
0000000000000000000000000000000000000000;;	  local wait
0000000000000000000000000000000000000000;;	  local failed
0000000000000000000000000000000000000000;;	  times=30
0000000000000000000000000000000000000000;;	  wait=10
0000000000000000000000000000000000000000;;	  local i
0000000000000000000000000000000000000000;;	  for i in $(seq 1 $times); do
0000000000000000000000000000000000000000;;	    failed=""
0000000000000000000000000000000000000000;;	    kubectl "${kube_flags[@]}" get --raw '/apis/company.com/v1' || failed=true
0000000000000000000000000000000000000000;;	    kubectl "${kube_flags[@]}" get --raw '/apis/company.com/v1/foos' || failed=true
0000000000000000000000000000000000000000;;	    kubectl "${kube_flags[@]}" get --raw '/apis/company.com/v1/bars' || failed=true
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    if [ -z "${failed}" ]; then
0000000000000000000000000000000000000000;;	      return 0
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    sleep ${wait}
0000000000000000000000000000000000000000;;	  done
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::error "Timed out waiting for non-native-resources; tried ${times} waiting ${wait}s between each"
0000000000000000000000000000000000000000;;	  return 1
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_non_native_resource_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl non-native resources"
0000000000000000000000000000000000000000;;	  kube::util::non_native_resources
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we can list this new third party resource (foos)
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we can list this new third party resource (bars)
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we can create a new resource of type Foo
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" create -f hack/testdata/TPR/foo.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we can list this new third party resource
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" 'test:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test alternate forms
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foo                 "{{range.items}}{{$id_field}}:{{end}}" 'test:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos.company.com    "{{range.items}}{{$id_field}}:{{end}}" 'test:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos.v1.company.com "{{range.items}}{{$id_field}}:{{end}}" 'test:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test all printers, with lists and individual items
0000000000000000000000000000000000000000;;	  kube::log::status "Testing ThirdPartyResource printing"
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos/test
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos      -o name
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos/test -o name
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos      -o wide
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos/test -o wide
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos      -o json
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos/test -o json
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos      -o yaml
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos/test -o yaml
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos      -o "jsonpath={.items[*].someField}" --allow-missing-template-keys=false
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos/test -o "jsonpath={.someField}"          --allow-missing-template-keys=false
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos      -o "go-template={{range .items}}{{.someField}}{{end}}" --allow-missing-template-keys=false
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos/test -o "go-template={{.someField}}"                        --allow-missing-template-keys=false
0000000000000000000000000000000000000000;;	  output_message=$(kubectl "${kube_flags[@]}" get foos/test -o name)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'foos/test'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test patching
0000000000000000000000000000000000000000;;	  kube::log::status "Testing ThirdPartyResource patching"
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" patch foos/test -p '{"patched":"value1"}' --type=merge
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test "{{.patched}}" 'value1'
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" patch foos/test -p '{"patched":"value2"}' --type=merge --record
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test "{{.patched}}" 'value2'
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" patch foos/test -p '{"patched":null}' --type=merge --record
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test "{{.patched}}" '<no value>'
0000000000000000000000000000000000000000;;	  # Get local version
0000000000000000000000000000000000000000;;	  TPR_RESOURCE_FILE="${KUBE_TEMP}/tpr-foos-test.json"
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" get foos/test -o json > "${TPR_RESOURCE_FILE}"
0000000000000000000000000000000000000000;;	  # cannot apply strategic patch locally
0000000000000000000000000000000000000000;;	  TPR_PATCH_ERROR_FILE="${KUBE_TEMP}/tpr-foos-test-error"
0000000000000000000000000000000000000000;;	  ! kubectl "${kube_flags[@]}" patch --local -f "${TPR_RESOURCE_FILE}" -p '{"patched":"value3"}' 2> "${TPR_PATCH_ERROR_FILE}"
0000000000000000000000000000000000000000;;	  if grep -q "try --type merge" "${TPR_PATCH_ERROR_FILE}"; then
0000000000000000000000000000000000000000;;	    kube::log::status "\"kubectl patch --local\" returns error as expected for ThirdPartyResource: $(cat ${TPR_PATCH_ERROR_FILE})"
0000000000000000000000000000000000000000;;	  else
0000000000000000000000000000000000000000;;	    kube::log::status "\"kubectl patch --local\" returns unexpected error or non-error: $(cat ${TPR_PATCH_ERROR_FILE})"
0000000000000000000000000000000000000000;;	    exit 1
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  # can apply merge patch locally
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" patch --local -f "${TPR_RESOURCE_FILE}" -p '{"patched":"value3"}' --type=merge -o json
0000000000000000000000000000000000000000;;	  # can apply merge patch remotely
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" patch --record -f "${TPR_RESOURCE_FILE}" -p '{"patched":"value3"}' --type=merge -o json
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test "{{.patched}}" 'value3'
0000000000000000000000000000000000000000;;	  rm "${TPR_RESOURCE_FILE}"
0000000000000000000000000000000000000000;;	  rm "${TPR_PATCH_ERROR_FILE}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test labeling
0000000000000000000000000000000000000000;;	  kube::log::status "Testing ThirdPartyResource labeling"
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" label foos --all listlabel=true
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" label foo/test itemlabel=true
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test annotating
0000000000000000000000000000000000000000;;	  kube::log::status "Testing ThirdPartyResource annotating"
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" annotate foos --all listannotation=true
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" annotate foo/test itemannotation=true
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test describing
0000000000000000000000000000000000000000;;	  kube::log::status "Testing ThirdPartyResource describing"
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" describe foos
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" describe foos/test
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" describe foos | grep listlabel=true
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" describe foos | grep itemlabel=true
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Delete the resource with cascade.
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" delete foos test --cascade=true
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Make sure it's gone
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we can create a new resource of type Bar
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" create -f hack/testdata/TPR/bar.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we can list this new third party resource
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars "{{range.items}}{{$id_field}}:{{end}}" 'test:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we can watch the resource.
0000000000000000000000000000000000000000;;	  # Start watcher in background with process substitution,
0000000000000000000000000000000000000000;;	  # so we can read from stdout asynchronously.
0000000000000000000000000000000000000000;;	  kube::log::status "Testing ThirdPartyResource watching"
0000000000000000000000000000000000000000;;	  exec 3< <(kubectl "${kube_flags[@]}" get bars --request-timeout=1m --watch-only -o name & echo $! ; wait)
0000000000000000000000000000000000000000;;	  local watch_pid
0000000000000000000000000000000000000000;;	  read <&3 watch_pid
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # We can't be sure when the watch gets established,
0000000000000000000000000000000000000000;;	  # so keep triggering events (in the background) until something comes through.
0000000000000000000000000000000000000000;;	  local tries=0
0000000000000000000000000000000000000000;;	  while [ ${tries} -lt 10 ]; do
0000000000000000000000000000000000000000;;	    tries=$((tries+1))
0000000000000000000000000000000000000000;;	    kubectl "${kube_flags[@]}" patch bars/test -p "{\"patched\":\"${tries}\"}" --type=merge
0000000000000000000000000000000000000000;;	    sleep 1
0000000000000000000000000000000000000000;;	  done &
0000000000000000000000000000000000000000;;	  local patch_pid=$!
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Wait up to 30s for a complete line of output.
0000000000000000000000000000000000000000;;	  local watch_output
0000000000000000000000000000000000000000;;	  read <&3 -t 30 watch_output
0000000000000000000000000000000000000000;;	  # Stop the watcher and the patch loop.
0000000000000000000000000000000000000000;;	  kill -9 ${watch_pid}
0000000000000000000000000000000000000000;;	  kill -9 ${patch_pid}
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${watch_output}" 'bars/test'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Delete the resource without cascade.
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" delete bars test --cascade=false
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Make sure it's gone
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we can create single item via apply
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" apply -f hack/testdata/TPR/foo.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we have create a foo named test
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" 'test:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that the field has the expected value
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test '{{.someField}}' 'field1'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that apply an empty patch doesn't change fields
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" apply -f hack/testdata/TPR/foo.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that the field has the same value after re-apply
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test '{{.someField}}' 'field1'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that apply has updated the subfield
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test '{{.nestedField.someSubfield}}' 'subfield1'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Update a subfield and then apply the change
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" apply -f hack/testdata/TPR/foo-updated-subfield.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that apply has updated the subfield
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test '{{.nestedField.someSubfield}}' 'modifiedSubfield'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that the field has the expected value
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test '{{.nestedField.otherSubfield}}' 'subfield2'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Delete a subfield and then apply the change
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" apply -f hack/testdata/TPR/foo-deleted-subfield.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that apply has deleted the field
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test '{{.nestedField.otherSubfield}}' '<no value>'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that the field does not exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test '{{.nestedField.newSubfield}}' '<no value>'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Add a field and then apply the change
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" apply -f hack/testdata/TPR/foo-added-subfield.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that apply has added the field
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test '{{.nestedField.newSubfield}}' 'subfield3'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Delete the resource
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" delete -f hack/testdata/TPR/foo.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Make sure it's gone
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we can create list via apply
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" apply -f hack/testdata/TPR/multi-tpr-list.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that we have create a foo and a bar from a list
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" 'test-list:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars "{{range.items}}{{$id_field}}:{{end}}" 'test-list:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that the field has the expected value
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test-list '{{.someField}}' 'field1'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars/test-list '{{.someField}}' 'field1'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that re-apply an list doesn't change anything
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" apply -f hack/testdata/TPR/multi-tpr-list.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that the field has the same value after re-apply
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test-list '{{.someField}}' 'field1'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars/test-list '{{.someField}}' 'field1'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that the fields have the expected value
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test-list '{{.someField}}' 'field1'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars/test-list '{{.someField}}' 'field1'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Update fields and then apply the change
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" apply -f hack/testdata/TPR/multi-tpr-list-updated-field.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that apply has updated the fields
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test-list '{{.someField}}' 'modifiedField'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars/test-list '{{.someField}}' 'modifiedField'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that the field has the expected value
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test-list '{{.otherField}}' 'field2'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars/test-list '{{.otherField}}' 'field2'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Delete fields and then apply the change
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" apply -f hack/testdata/TPR/multi-tpr-list-deleted-field.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that apply has deleted the fields
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test-list '{{.otherField}}' '<no value>'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars/test-list '{{.otherField}}' '<no value>'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that the fields does not exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test-list '{{.newField}}' '<no value>'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars/test-list '{{.newField}}' '<no value>'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Add a field and then apply the change
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" apply -f hack/testdata/TPR/multi-tpr-list-added-field.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that apply has added the field
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos/test-list '{{.newField}}' 'field3'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars/test-list '{{.newField}}' 'field3'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Delete the resource
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" delete -f hack/testdata/TPR/multi-tpr-list.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Make sure it's gone
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl apply --prune
0000000000000000000000000000000000000000;;	  # Test that no foo or bar exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # apply --prune on foo.yaml that has foo/test
0000000000000000000000000000000000000000;;	  kubectl apply --prune -l pruneGroup=true -f hack/testdata/TPR/foo.yaml "${kube_flags[@]}" --prune-whitelist=company.com/v1/Foo --prune-whitelist=company.com/v1/Bar
0000000000000000000000000000000000000000;;	  # check right tprs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" 'test:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # apply --prune on bar.yaml that has bar/test
0000000000000000000000000000000000000000;;	  kubectl apply --prune -l pruneGroup=true -f hack/testdata/TPR/bar.yaml "${kube_flags[@]}" --prune-whitelist=company.com/v1/Foo --prune-whitelist=company.com/v1/Bar
0000000000000000000000000000000000000000;;	  # check right tprs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars "{{range.items}}{{$id_field}}:{{end}}" 'test:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Delete the resource
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" delete -f hack/testdata/TPR/bar.yaml
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Make sure it's gone
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert foos "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test 'kubectl create' with namespace, and namespace cleanup.
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" create namespace non-native-resources
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" create -f hack/testdata/TPR/bar.yaml --namespace=non-native-resources
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert bars '{{len .items}}' '1' --namespace=non-native-resources
0000000000000000000000000000000000000000;;	  kubectl "${kube_flags[@]}" delete namespace non-native-resources
0000000000000000000000000000000000000000;;	  # Make sure objects go away.
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert bars '{{len .items}}' '0' --namespace=non-native-resources
0000000000000000000000000000000000000000;;	  # Make sure namespace goes away.
0000000000000000000000000000000000000000;;	  local tries=0
0000000000000000000000000000000000000000;;	  while kubectl "${kube_flags[@]}" get namespace non-native-resources && [ ${tries} -lt 10 ]; do
0000000000000000000000000000000000000000;;	    tries=$((tries+1))
0000000000000000000000000000000000000000;;	    sleep ${tries}
0000000000000000000000000000000000000000;;	  done
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_recursive_resources_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing recursive resources"
0000000000000000000000000000000000000000;;	  ### Create multiple busybox PODs recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl create -f hack/testdata/recursive/pod --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are created, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'error validating data: kind not set'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Edit multiple busybox PODs by updating the image field of multiple PODs recursively from a directory. tmp-editor.sh is a fake editor
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  echo -e '#!/bin/bash\nsed -i "s/image: busybox/image: prom\/busybox/g" $1' > /tmp/tmp-editor.sh
0000000000000000000000000000000000000000;;	  chmod +x /tmp/tmp-editor.sh
0000000000000000000000000000000000000000;;	  output_message=$(! EDITOR=/tmp/tmp-editor.sh kubectl edit -f hack/testdata/recursive/pod --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are not edited, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  # The reason why busybox0 & busybox1 PODs are not edited is because the editor tries to load all objects in
0000000000000000000000000000000000000000;;	  # a list but since it contains invalid objects, it will never open.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$image_field}}:{{end}}" 'busybox:busybox:'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	  # cleaning
0000000000000000000000000000000000000000;;	  rm /tmp/tmp-editor.sh
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Replace multiple busybox PODs recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl replace -f hack/testdata/recursive/pod-modify --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are replaced, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{${labels_field}.status}}:{{end}}" 'replaced:replaced:'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'error validating data: kind not set'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Describe multiple busybox PODs recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl describe -f hack/testdata/recursive/pod --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are described, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "app=busybox0"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "app=busybox1"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Annotate multiple busybox PODs recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl annotate -f hack/testdata/recursive/pod annotatekey='annotatevalue' --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are annotated, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{${annotations_field}.annotatekey}}:{{end}}" 'annotatevalue:annotatevalue:'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Apply multiple busybox PODs recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl apply -f hack/testdata/recursive/pod-modify --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are updated, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{${labels_field}.status}}:{{end}}" 'replaced:replaced:'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'error validating data: kind not set'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Convert deployment YAML file locally without affecting the live deployment.
0000000000000000000000000000000000000000;;	  # Pre-condition: no deployments exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  # Create a deployment (revision 1)
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/deployment-revision1.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" 'nginx:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R1}:"
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(kubectl convert --local -f hack/testdata/deployment-revision1.yaml --output-version=apps/v1beta1 -o go-template='{{ .apiVersion }}' "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  echo $output_message
0000000000000000000000000000000000000000;;	  # Post-condition: apiVersion is still extensions/v1beta1 in the live deployment, but command output is the new value
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployment nginx' "{{ .apiVersion }}" 'extensions/v1beta1'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "apps/v1beta1"
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete deployment nginx "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Convert multiple busybox PODs recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl convert -f hack/testdata/recursive/pod --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are converted, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Get multiple busybox PODs recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get -f hack/testdata/recursive/pod --recursive 2>&1 "${kube_flags[@]}" -o go-template="{{range.items}}{{$id_field}}:{{end}}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are retrieved, but because busybox2 is malformed, it should not show up
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "busybox0:busybox1:"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Label multiple busybox PODs recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl label -f hack/testdata/recursive/pod mylabel='myvalue' --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  echo $output_message
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are labeled, but because busybox2 is malformed, it should not show up
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{${labels_field}.mylabel}}:{{end}}" 'myvalue:myvalue:'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Patch multiple busybox PODs recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl patch -f hack/testdata/recursive/pod -p='{"spec":{"containers":[{"name":"busybox","image":"prom/busybox"}]}}' --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  echo $output_message
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are patched, but because busybox2 is malformed, it should not show up
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$image_field}}:{{end}}" 'prom/busybox:prom/busybox:'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete multiple busybox PODs recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl delete -f hack/testdata/recursive/pod --recursive --grace-period=0 --force 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 PODs are deleted, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create replication controller recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: no replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  ! kubectl create -f hack/testdata/recursive/rc --recursive "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: frontend replication controller is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Autoscale multiple replication controllers recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 replication controllers exist & 1
0000000000000000000000000000000000000000;;	  # replica each
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc busybox0' "{{$rc_replicas_field}}" '1'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc busybox1' "{{$rc_replicas_field}}" '1'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl autoscale --min=1 --max=2 -f hack/testdata/recursive/rc --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox replication controllers are autoscaled
0000000000000000000000000000000000000000;;	  # with min. of 1 replica & max of 2 replicas, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'hpa busybox0' "{{$hpa_min_field}} {{$hpa_max_field}} {{$hpa_cpu_field}}" '1 2 80'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'hpa busybox1' "{{$hpa_min_field}} {{$hpa_max_field}} {{$hpa_cpu_field}}" '1 2 80'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	  kubectl delete hpa busybox0 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl delete hpa busybox1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Expose multiple replication controllers as service recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 replication controllers exist & 1
0000000000000000000000000000000000000000;;	  # replica each
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc busybox0' "{{$rc_replicas_field}}" '1'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc busybox1' "{{$rc_replicas_field}}" '1'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl expose -f hack/testdata/recursive/rc --recursive --port=80 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: service exists and the port is unnamed
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service busybox0' "{{$port_name}} {{$port_field}}" '<no value> 80'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service busybox1' "{{$port_name}} {{$port_field}}" '<no value> 80'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Scale multiple replication controllers recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 replication controllers exist & 1
0000000000000000000000000000000000000000;;	  # replica each
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc busybox0' "{{$rc_replicas_field}}" '1'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc busybox1' "{{$rc_replicas_field}}" '1'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl scale --current-replicas=1 --replicas=2 -f hack/testdata/recursive/rc --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 replication controllers are scaled to 2 replicas, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc busybox0' "{{$rc_replicas_field}}" '2'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc busybox1' "{{$rc_replicas_field}}" '2'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete multiple busybox replication controllers recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  # Pre-condition: busybox0 & busybox1 PODs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl delete -f hack/testdata/recursive/rc --recursive --grace-period=0 --force 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 replication controllers are deleted, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Rollout on multiple deployments recursively
0000000000000000000000000000000000000000;;	  # Pre-condition: no deployments exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  # Create deployments (revision 1) recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  ! kubectl create -f hack/testdata/recursive/deployment --recursive "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" 'nginx0-deployment:nginx1-deployment:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_NGINX}:${IMAGE_NGINX}:"
0000000000000000000000000000000000000000;;	  ## Rollback the deployments to revision 1 recursively
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl rollout undo -f hack/testdata/recursive/deployment --recursive --to-revision=1 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: nginx0 & nginx1 should be a no-op, and since nginx2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_NGINX}:${IMAGE_NGINX}:"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	  ## Pause the deployments recursively
0000000000000000000000000000000000000000;;	  PRESERVE_ERR_FILE=true
0000000000000000000000000000000000000000;;	  kubectl-with-retry rollout pause -f hack/testdata/recursive/deployment --recursive "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  output_message=$(cat ${ERROR_FILE})
0000000000000000000000000000000000000000;;	  # Post-condition: nginx0 & nginx1 should both have paused set to true, and since nginx2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{.spec.paused}}:{{end}}" "true:true:"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	  ## Resume the deployments recursively
0000000000000000000000000000000000000000;;	  kubectl-with-retry rollout resume -f hack/testdata/recursive/deployment --recursive "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  output_message=$(cat ${ERROR_FILE})
0000000000000000000000000000000000000000;;	  # Post-condition: nginx0 & nginx1 should both have paused set to nothing, and since nginx2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{.spec.paused}}:{{end}}" "<no value>:<no value>:"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	  ## Retrieve the rollout history of the deployments recursively
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl rollout history -f hack/testdata/recursive/deployment --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: nginx0 & nginx1 should both have a history, and since nginx2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "nginx0-deployment"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "nginx1-deployment"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  unset PRESERVE_ERR_FILE
0000000000000000000000000000000000000000;;	  rm "${ERROR_FILE}"
0000000000000000000000000000000000000000;;	  ! kubectl delete -f hack/testdata/recursive/deployment --recursive "${kube_flags[@]}" --grace-period=0 --force
0000000000000000000000000000000000000000;;	  sleep 1
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Rollout on multiple replication controllers recursively - these tests ensure that rollouts cannot be performed on resources that don't support it
0000000000000000000000000000000000000000;;	  # Pre-condition: no replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  # Create replication controllers recursively from directory of YAML files
0000000000000000000000000000000000000000;;	  ! kubectl create -f hack/testdata/recursive/rc --recursive "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'busybox0:busybox1:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  ## Attempt to rollback the replication controllers to revision 1 recursively
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl rollout undo -f hack/testdata/recursive/rc --recursive --to-revision=1 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 should error as they are RC's, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'no rollbacker has been implemented for {"" "ReplicationController"}'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	  ## Attempt to pause the replication controllers recursively
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl rollout pause -f hack/testdata/recursive/rc --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 should error as they are RC's, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'replicationcontrollers "busybox0" pausing is not supported'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'replicationcontrollers "busybox1" pausing is not supported'
0000000000000000000000000000000000000000;;	  ## Attempt to resume the replication controllers recursively
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl rollout resume -f hack/testdata/recursive/rc --recursive 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: busybox0 & busybox1 should error as they are RC's, and since busybox2 is malformed, it should error
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Object 'Kind' is missing"
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'replicationcontrollers "busybox0" resuming is not supported'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'replicationcontrollers "busybox0" resuming is not supported'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  ! kubectl delete -f hack/testdata/recursive/rc --recursive "${kube_flags[@]}" --grace-period=0 --force
0000000000000000000000000000000000000000;;	  sleep 1
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_namespace_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:namespaces)"
0000000000000000000000000000000000000000;;	  ### Create a new namespace
0000000000000000000000000000000000000000;;	  # Pre-condition: only the "default" namespace exists
0000000000000000000000000000000000000000;;	  # The Pre-condition doesn't hold anymore after we create and switch namespaces before creating pods with same name in the test.
0000000000000000000000000000000000000000;;	  # kube::test::get_object_assert namespaces "{{range.items}}{{$id_field}}:{{end}}" 'default:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create namespace my-namespace
0000000000000000000000000000000000000000;;	  # Post-condition: namespace 'my-namespace' is created.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'namespaces/my-namespace' "{{$id_field}}" 'my-namespace'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete namespace my-namespace
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	  # Pods in Namespaces #
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    ### Create a new namespace
0000000000000000000000000000000000000000;;	    # Pre-condition: the other namespace does not exist
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'namespaces' '{{range.items}}{{ if eq $id_field \"other\" }}found{{end}}{{end}}:' ':'
0000000000000000000000000000000000000000;;	    # Command
0000000000000000000000000000000000000000;;	    kubectl create namespace other
0000000000000000000000000000000000000000;;	    # Post-condition: namespace 'other' is created.
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'namespaces/other' "{{$id_field}}" 'other'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    ### Create POD valid-pod in specific namespace
0000000000000000000000000000000000000000;;	    # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'pods --namespace=other' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	    # Command
0000000000000000000000000000000000000000;;	    kubectl create "${kube_flags[@]}" --namespace=other -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml
0000000000000000000000000000000000000000;;	    # Post-condition: valid-pod POD is created
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'pods --namespace=other' "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	    # Post-condition: verify shorthand `-n other` has the same results as `--namespace=other`
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'pods -n other' "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	    # Post-condition: a resource cannot be retrieved by name across all namespaces
0000000000000000000000000000000000000000;;	    output_message=$(! kubectl get "${kube_flags[@]}" pod valid-pod --all-namespaces 2>&1)
0000000000000000000000000000000000000000;;	    kube::test::if_has_string "${output_message}" "a resource cannot be retrieved by name across all namespaces"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    ### Delete POD valid-pod in specific namespace
0000000000000000000000000000000000000000;;	    # Pre-condition: valid-pod POD exists
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'pods --namespace=other' "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	    # Command
0000000000000000000000000000000000000000;;	    kubectl delete "${kube_flags[@]}" pod --namespace=other valid-pod --grace-period=0 --force
0000000000000000000000000000000000000000;;	    # Post-condition: valid-pod POD doesn't exist
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'pods --namespace=other' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	    # Clean up
0000000000000000000000000000000000000000;;	    kubectl delete namespace other
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_secrets_test() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing secrets"
0000000000000000000000000000000000000000;;	  ### Create a new namespace
0000000000000000000000000000000000000000;;	  # Pre-condition: the test-secrets namespace does not exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'namespaces' '{{range.items}}{{ if eq $id_field \"test-secrets\" }}found{{end}}{{end}}:' ':'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create namespace test-secrets
0000000000000000000000000000000000000000;;	  # Post-condition: namespace 'test-secrets' is created.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'namespaces/test-secrets' "{{$id_field}}" 'test-secrets'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a generic secret in a specific namespace
0000000000000000000000000000000000000000;;	  # Pre-condition: no SECRET exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secrets --namespace=test-secrets' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create secret generic test-secret --from-literal=key1=value1 --type=test-type --namespace=test-secrets
0000000000000000000000000000000000000000;;	  # Post-condition: secret exists and has expected values
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/test-secret --namespace=test-secrets' "{{$id_field}}" 'test-secret'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/test-secret --namespace=test-secrets' "{{$secret_type}}" 'test-type'
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get secret/test-secret --namespace=test-secrets -o yaml "${kube_flags[@]}" | grep 'key1: dmFsdWUx')" ]]
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete secret test-secret --namespace=test-secrets
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a docker-registry secret in a specific namespace
0000000000000000000000000000000000000000;;	  if [[ "${WAIT_FOR_DELETION:-}" == "true" ]]; then
0000000000000000000000000000000000000000;;	    kube::test::wait_object_assert 'secrets --namespace=test-secrets' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  # Pre-condition: no SECRET exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secrets --namespace=test-secrets' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create secret docker-registry test-secret --docker-username=test-user --docker-password=test-password --docker-email='test-user@test.com' --namespace=test-secrets
0000000000000000000000000000000000000000;;	  # Post-condition: secret exists and has expected values
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/test-secret --namespace=test-secrets' "{{$id_field}}" 'test-secret'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/test-secret --namespace=test-secrets' "{{$secret_type}}" 'kubernetes.io/dockercfg'
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get secret/test-secret --namespace=test-secrets -o yaml "${kube_flags[@]}" | grep '.dockercfg:')" ]]
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete secret test-secret --namespace=test-secrets
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a tls secret
0000000000000000000000000000000000000000;;	  if [[ "${WAIT_FOR_DELETION:-}" == "true" ]]; then
0000000000000000000000000000000000000000;;	    kube::test::wait_object_assert 'secrets --namespace=test-secrets' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  # Pre-condition: no SECRET exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secrets --namespace=test-secrets' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create secret tls test-secret --namespace=test-secrets --key=hack/testdata/tls.key --cert=hack/testdata/tls.crt
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/test-secret --namespace=test-secrets' "{{$id_field}}" 'test-secret'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/test-secret --namespace=test-secrets' "{{$secret_type}}" 'kubernetes.io/tls'
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete secret test-secret --namespace=test-secrets
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Create a secret using stringData
0000000000000000000000000000000000000000;;	  kubectl create --namespace=test-secrets -f - "${kube_flags[@]}" << __EOF__
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  "kind": "Secret",
0000000000000000000000000000000000000000;;	  "apiVersion": "v1",
0000000000000000000000000000000000000000;;	  "metadata": {
0000000000000000000000000000000000000000;;	    "name": "secret-string-data"
0000000000000000000000000000000000000000;;	  },
0000000000000000000000000000000000000000;;	  "data": {
0000000000000000000000000000000000000000;;	    "k1":"djE=",
0000000000000000000000000000000000000000;;	    "k2":""
0000000000000000000000000000000000000000;;	  },
0000000000000000000000000000000000000000;;	  "stringData": {
0000000000000000000000000000000000000000;;	    "k2":"v2"
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	__EOF__
0000000000000000000000000000000000000000;;	  # Post-condition: secret-string-data secret is created with expected data, merged/overridden data from stringData, and a cleared stringData field
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/secret-string-data --namespace=test-secrets ' '{{.data}}' '.*k1:djE=.*'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/secret-string-data --namespace=test-secrets ' '{{.data}}' '.*k2:djI=.*'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secret/secret-string-data --namespace=test-secrets ' '{{.stringData}}' '<no value>'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete secret secret-string-data --namespace=test-secrets
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a secret using output flags
0000000000000000000000000000000000000000;;	  if [[ "${WAIT_FOR_DELETION:-}" == "true" ]]; then
0000000000000000000000000000000000000000;;	    kube::test::wait_object_assert 'secrets --namespace=test-secrets' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  # Pre-condition: no secret exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'secrets --namespace=test-secrets' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  [[ "$(kubectl create secret generic test-secret --namespace=test-secrets --from-literal=key1=value1 --output=go-template --template=\"{{.metadata.name}}:\" | grep 'test-secret:')" ]]
0000000000000000000000000000000000000000;;	  ## Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete secret test-secret --namespace=test-secrets
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete namespace test-secrets
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_configmap_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing configmaps"
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/configmap/configmap.yaml
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert configmap "{{range.items}}{{$id_field}}{{end}}" 'test-configmap'
0000000000000000000000000000000000000000;;	  kubectl delete configmap test-configmap "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a new namespace
0000000000000000000000000000000000000000;;	  # Pre-condition: the test-configmaps namespace does not exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'namespaces' '{{range.items}}{{ if eq $id_field \"test-configmaps\" }}found{{end}}{{end}}:' ':'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create namespace test-configmaps
0000000000000000000000000000000000000000;;	  # Post-condition: namespace 'test-configmaps' is created.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'namespaces/test-configmaps' "{{$id_field}}" 'test-configmaps'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a generic configmap in a specific namespace
0000000000000000000000000000000000000000;;	  # Pre-condition: no configmaps namespace exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'configmaps --namespace=test-configmaps' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create configmap test-configmap --from-literal=key1=value1 --namespace=test-configmaps
0000000000000000000000000000000000000000;;	  # Post-condition: configmap exists and has expected values
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'configmap/test-configmap --namespace=test-configmaps' "{{$id_field}}" 'test-configmap'
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get configmap/test-configmap --namespace=test-configmaps -o yaml "${kube_flags[@]}" | grep 'key1: value1')" ]]
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete configmap test-configmap --namespace=test-configmaps
0000000000000000000000000000000000000000;;	  kubectl delete namespace test-configmaps
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_service_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # switch back to the default namespace
0000000000000000000000000000000000000000;;	  kubectl config set-context "${CONTEXT}" --namespace=""
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:services)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create redis-master service from JSON
0000000000000000000000000000000000000000;;	  # Pre-condition: Only the default kubernetes services exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f examples/guestbook/redis-master-service.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: redis-master service exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:redis-master:'
0000000000000000000000000000000000000000;;	  # Describe command should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_object_assert services 'redis-master' "Name:" "Labels:" "Selector:" "IP:" "Port:" "Endpoints:" "Session Affinity:"
0000000000000000000000000000000000000000;;	  # Describe command should print events information by default
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert services 'redis-master'
0000000000000000000000000000000000000000;;	  # Describe command should not print events information when show-events=false
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert services 'redis-master' false
0000000000000000000000000000000000000000;;	  # Describe command should print events information when show-events=true
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert services 'redis-master' true
0000000000000000000000000000000000000000;;	  # Describe command (resource only) should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_assert services "Name:" "Labels:" "Selector:" "IP:" "Port:" "Endpoints:" "Session Affinity:"
0000000000000000000000000000000000000000;;	  # Describe command should print events information by default
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert services
0000000000000000000000000000000000000000;;	  # Describe command should not print events information when show-events=false
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert services false
0000000000000000000000000000000000000000;;	  # Describe command should print events information when show-events=true
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert services true
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### set selector
0000000000000000000000000000000000000000;;	  # prove role=master
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'services redis-master' "{{range$service_selector_field}}{{.}}:{{end}}" "redis:master:backend:"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Set selector of a local file without talking to the server
0000000000000000000000000000000000000000;;	  kubectl set selector -f examples/guestbook/redis-master-service.yaml role=padawan --local -o yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  ! kubectl set selector -f examples/guestbook/redis-master-service.yaml role=padawan --dry-run -o yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Set command to change the selector.
0000000000000000000000000000000000000000;;	  kubectl set selector -f examples/guestbook/redis-master-service.yaml role=padawan
0000000000000000000000000000000000000000;;	  # prove role=padawan
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'services redis-master' "{{range$service_selector_field}}{{.}}:{{end}}" "padawan:"
0000000000000000000000000000000000000000;;	  # Set command to reset the selector back to the original one.
0000000000000000000000000000000000000000;;	  kubectl set selector -f examples/guestbook/redis-master-service.yaml app=redis,role=master,tier=backend
0000000000000000000000000000000000000000;;	  # prove role=master
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'services redis-master' "{{range$service_selector_field}}{{.}}:{{end}}" "redis:master:backend:"
0000000000000000000000000000000000000000;;	  # Show dry-run works on running selector
0000000000000000000000000000000000000000;;	  kubectl set selector services redis-master role=padawan --dry-run -o yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  ! kubectl set selector services redis-master role=padawan --local -o yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'services redis-master' "{{range$service_selector_field}}{{.}}:{{end}}" "redis:master:backend:"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Dump current redis-master service
0000000000000000000000000000000000000000;;	  output_service=$(kubectl get service redis-master -o json --output-version=v1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete redis-master-service by id
0000000000000000000000000000000000000000;;	  # Pre-condition: redis-master service exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:redis-master:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete service redis-master "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  if [[ "${WAIT_FOR_DELETION:-}" == "true" ]]; then
0000000000000000000000000000000000000000;;	    kube::test::wait_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  # Post-condition: Only the default kubernetes services exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create redis-master-service from dumped JSON
0000000000000000000000000000000000000000;;	  # Pre-condition: Only the default kubernetes services exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  echo "${output_service}" | kubectl create -f - "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: redis-master service is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:redis-master:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create redis-master-v1-test service
0000000000000000000000000000000000000000;;	  # Pre-condition: redis-master-service service exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:redis-master:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f - "${kube_flags[@]}" << __EOF__
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  "kind": "Service",
0000000000000000000000000000000000000000;;	  "apiVersion": "v1",
0000000000000000000000000000000000000000;;	  "metadata": {
0000000000000000000000000000000000000000;;	    "name": "service-v1-test"
0000000000000000000000000000000000000000;;	  },
0000000000000000000000000000000000000000;;	  "spec": {
0000000000000000000000000000000000000000;;	    "ports": [
0000000000000000000000000000000000000000;;	      {
0000000000000000000000000000000000000000;;	        "protocol": "TCP",
0000000000000000000000000000000000000000;;	        "port": 80,
0000000000000000000000000000000000000000;;	        "targetPort": 80
0000000000000000000000000000000000000000;;	      }
0000000000000000000000000000000000000000;;	    ]
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	__EOF__
0000000000000000000000000000000000000000;;	  # Post-condition: service-v1-test service is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:redis-master:service-.*-test:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Identity
0000000000000000000000000000000000000000;;	  kubectl get service "${kube_flags[@]}" service-v1-test -o json | kubectl replace "${kube_flags[@]}" -f -
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete services by id
0000000000000000000000000000000000000000;;	  # Pre-condition: service-v1-test exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:redis-master:service-.*-test:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete service redis-master "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl delete service "service-v1-test" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  if [[ "${WAIT_FOR_DELETION:-}" == "true" ]]; then
0000000000000000000000000000000000000000;;	    kube::test::wait_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  # Post-condition: Only the default kubernetes services exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create two services
0000000000000000000000000000000000000000;;	  # Pre-condition: Only the default kubernetes services exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f examples/guestbook/redis-master-service.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl create -f examples/guestbook/redis-slave-service.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: redis-master and redis-slave services are created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:redis-master:redis-slave:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Custom columns can be specified
0000000000000000000000000000000000000000;;	  # Pre-condition: generate output using custom columns
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get services -o=custom-columns=NAME:.metadata.name,RSRC:.metadata.resourceVersion 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: should contain name column
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'redis-master'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete multiple services at once
0000000000000000000000000000000000000000;;	  # Pre-condition: redis-master and redis-slave services exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:redis-master:redis-slave:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete services redis-master redis-slave "${kube_flags[@]}" # delete multiple services at once
0000000000000000000000000000000000000000;;	  if [[ "${WAIT_FOR_DELETION:-}" == "true" ]]; then
0000000000000000000000000000000000000000;;	    kube::test::wait_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  # Post-condition: Only the default kubernetes services exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create an ExternalName service
0000000000000000000000000000000000000000;;	  # Pre-condition: Only the default kubernetes service exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create service externalname beep-boop --external-name bar.com
0000000000000000000000000000000000000000;;	  # Post-condition: beep-boop service is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'beep-boop:kubernetes:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete beep-boop service by id
0000000000000000000000000000000000000000;;	  # Pre-condition: beep-boop service exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'beep-boop:kubernetes:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete service beep-boop "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  if [[ "${WAIT_FOR_DELETION:-}" == "true" ]]; then
0000000000000000000000000000000000000000;;	    kube::test::wait_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  # Post-condition: Only the default kubernetes services exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'kubernetes:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_rc_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:replicationcontrollers)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create and stop controller, make sure it doesn't leak pods
0000000000000000000000000000000000000000;;	  # Pre-condition: no replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl delete rc frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no pods from frontend controller
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods -l "name=frontend"' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create replication controller frontend from JSON
0000000000000000000000000000000000000000;;	  # Pre-condition: no replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: frontend replication controller is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'frontend:'
0000000000000000000000000000000000000000;;	  # Describe command should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_object_assert rc 'frontend' "Name:" "Pod Template:" "Labels:" "Selector:" "Replicas:" "Pods Status:" "Volumes:" "GET_HOSTS_FROM:"
0000000000000000000000000000000000000000;;	  # Describe command should print events information by default
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert rc 'frontend'
0000000000000000000000000000000000000000;;	  # Describe command should not print events information when show-events=false
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert rc 'frontend' false
0000000000000000000000000000000000000000;;	  # Describe command should print events information when show-events=true
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert rc 'frontend' true
0000000000000000000000000000000000000000;;	  # Describe command (resource only) should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_assert rc "Name:" "Name:" "Pod Template:" "Labels:" "Selector:" "Replicas:" "Pods Status:" "Volumes:" "GET_HOSTS_FROM:"
0000000000000000000000000000000000000000;;	  # Describe command should print events information by default
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert rc
0000000000000000000000000000000000000000;;	  # Describe command should not print events information when show-events=false
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert rc false
0000000000000000000000000000000000000000;;	  # Describe command should print events information when show-events=true
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert rc true
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Scale replication controller frontend with current-replicas and replicas
0000000000000000000000000000000000000000;;	  # Pre-condition: 3 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc frontend' "{{$rc_replicas_field}}" '3'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl scale --current-replicas=3 --replicas=2 replicationcontrollers frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: 2 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc frontend' "{{$rc_replicas_field}}" '2'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Scale replication controller frontend with (wrong) current-replicas and replicas
0000000000000000000000000000000000000000;;	  # Pre-condition: 2 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc frontend' "{{$rc_replicas_field}}" '2'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  ! kubectl scale --current-replicas=3 --replicas=2 replicationcontrollers frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: nothing changed
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc frontend' "{{$rc_replicas_field}}" '2'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Scale replication controller frontend with replicas only
0000000000000000000000000000000000000000;;	  # Pre-condition: 2 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc frontend' "{{$rc_replicas_field}}" '2'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl scale  --replicas=3 replicationcontrollers frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: 3 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc frontend' "{{$rc_replicas_field}}" '3'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Scale replication controller from JSON with replicas only
0000000000000000000000000000000000000000;;	  # Pre-condition: 3 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc frontend' "{{$rc_replicas_field}}" '3'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl scale  --replicas=2 -f hack/testdata/frontend-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: 2 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc frontend' "{{$rc_replicas_field}}" '2'
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete rc frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Scale multiple replication controllers
0000000000000000000000000000000000000000;;	  kubectl create -f examples/guestbook/legacy/redis-master-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl create -f examples/guestbook/legacy/redis-slave-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl scale rc/redis-master rc/redis-slave --replicas=4 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: 4 replicas each
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc redis-master' "{{$rc_replicas_field}}" '4'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc redis-slave' "{{$rc_replicas_field}}" '4'
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete rc redis-{master,slave} "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Scale a job
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/job.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl scale --replicas=2 job/pi
0000000000000000000000000000000000000000;;	  # Post-condition: 2 replicas for pi
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'job pi' "{{$job_parallelism_field}}" '2'
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete job/pi "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Scale a deployment
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/deployment.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl scale --current-replicas=3 --replicas=1 deployment/nginx-deployment
0000000000000000000000000000000000000000;;	  # Post-condition: 1 replica for nginx-deployment
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployment nginx-deployment' "{{$deployment_replicas}}" '1'
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete deployment/nginx-deployment "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Expose a deployment as a service
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/deployment.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Pre-condition: 3 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deployment nginx-deployment' "{{$deployment_replicas}}" '3'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl expose deployment/nginx-deployment
0000000000000000000000000000000000000000;;	  # Post-condition: service exists and exposes deployment port (80)
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service nginx-deployment' "{{$port_field}}" '80'
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete deployment/nginx-deployment service/nginx-deployment "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Expose replication controller as service
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Pre-condition: 3 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rc frontend' "{{$rc_replicas_field}}" '3'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl expose rc frontend --port=80 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: service exists and the port is unnamed
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service frontend' "{{$port_name}} {{$port_field}}" '<no value> 80'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl expose service frontend --port=443 --name=frontend-2 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: service exists and the port is unnamed
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service frontend-2' "{{$port_name}} {{$port_field}}" '<no value> 443'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl expose pod valid-pod --port=444 --name=frontend-3 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: service exists and the port is unnamed
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service frontend-3' "{{$port_name}} {{$port_field}}" '<no value> 444'
0000000000000000000000000000000000000000;;	  # Create a service using service/v1 generator
0000000000000000000000000000000000000000;;	  kubectl expose rc frontend --port=80 --name=frontend-4 --generator=service/v1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: service exists and the port is named default.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service frontend-4' "{{$port_name}} {{$port_field}}" 'default 80'
0000000000000000000000000000000000000000;;	  # Verify that expose service works without specifying a port.
0000000000000000000000000000000000000000;;	  kubectl expose service frontend --name=frontend-5 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: service exists with the same port as the original service.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service frontend-5' "{{$port_field}}" '80'
0000000000000000000000000000000000000000;;	  # Cleanup services
0000000000000000000000000000000000000000;;	  kubectl delete pod valid-pod "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl delete service frontend{,-2,-3,-4,-5} "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Expose negative invalid resource test
0000000000000000000000000000000000000000;;	  # Pre-condition: don't need
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl expose nodes 127.0.0.1 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: the error message has "cannot expose" string
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'cannot expose'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Try to generate a service with invalid name (exceeding maximum valid size)
0000000000000000000000000000000000000000;;	  # Pre-condition: use --name flag
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl expose -f hack/testdata/pod-with-large-name.yaml --name=invalid-large-service-name-that-has-more-than-sixty-three-characters --port=8081 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: should fail due to invalid name
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'metadata.name: Invalid value'
0000000000000000000000000000000000000000;;	  # Pre-condition: default run without --name flag; should succeed by truncating the inherited name
0000000000000000000000000000000000000000;;	  output_message=$(kubectl expose -f hack/testdata/pod-with-large-name.yaml --port=8081 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: inherited name from pod has been truncated
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" '\"kubernetes-serve-hostname-testing-sixty-three-characters-in-len\" exposed'
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete svc kubernetes-serve-hostname-testing-sixty-three-characters-in-len "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Expose multiport object as a new service
0000000000000000000000000000000000000000;;	  # Pre-condition: don't use --port flag
0000000000000000000000000000000000000000;;	  output_message=$(kubectl expose -f test/fixtures/doc-yaml/admin/high-availability/etcd.yaml --selector=test=etcd 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  # Post-condition: expose succeeded
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" '\"etcd-server\" exposed'
0000000000000000000000000000000000000000;;	  # Post-condition: generated service has both ports from the exposed pod
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service etcd-server' "{{$port_name}} {{$port_field}}" 'port-1 2380'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service etcd-server' "{{$second_port_name}} {{$second_port_field}}" 'port-2 2379'
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete svc etcd-server "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete replication controller with id
0000000000000000000000000000000000000000;;	  # Pre-condition: frontend replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'frontend:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete rc frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create two replication controllers
0000000000000000000000000000000000000000;;	  # Pre-condition: no replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl create -f examples/guestbook/legacy/redis-slave-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: frontend and redis-slave
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'frontend:redis-slave:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete multiple controllers at once
0000000000000000000000000000000000000000;;	  # Pre-condition: frontend and redis-slave
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'frontend:redis-slave:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete rc frontend redis-slave "${kube_flags[@]}" # delete multiple controllers at once
0000000000000000000000000000000000000000;;	  # Post-condition: no replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Auto scale replication controller
0000000000000000000000000000000000000000;;	  # Pre-condition: no replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'frontend:'
0000000000000000000000000000000000000000;;	  # autoscale 1~2 pods, CPU utilization 70%, rc specified by file
0000000000000000000000000000000000000000;;	  kubectl autoscale -f hack/testdata/frontend-controller.yaml "${kube_flags[@]}" --max=2 --cpu-percent=70
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'hpa frontend' "{{$hpa_min_field}} {{$hpa_max_field}} {{$hpa_cpu_field}}" '1 2 70'
0000000000000000000000000000000000000000;;	  kubectl delete hpa frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # autoscale 2~3 pods, no CPU utilization specified, rc specified by name
0000000000000000000000000000000000000000;;	  kubectl autoscale rc frontend "${kube_flags[@]}" --min=2 --max=3
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'hpa frontend' "{{$hpa_min_field}} {{$hpa_max_field}} {{$hpa_cpu_field}}" '2 3 80'
0000000000000000000000000000000000000000;;	  kubectl delete hpa frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # autoscale without specifying --max should fail
0000000000000000000000000000000000000000;;	  ! kubectl autoscale rc frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete rc frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Set resource limits/request of a deployment
0000000000000000000000000000000000000000;;	  # Pre-condition: no deployment exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Set resources of a local file without talking to the server
0000000000000000000000000000000000000000;;	  kubectl set resources -f hack/testdata/deployment-multicontainer-resources.yaml -c=perl --limits=cpu=300m --requests=cpu=300m --local -o yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  ! kubectl set resources -f hack/testdata/deployment-multicontainer-resources.yaml -c=perl --limits=cpu=300m --requests=cpu=300m --dry-run -o yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Create a deployment
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/deployment-multicontainer-resources.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" 'nginx-deployment-resources:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R1}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_second_image_field}}:{{end}}" "${IMAGE_PERL}:"
0000000000000000000000000000000000000000;;	  # Set the deployment's cpu limits
0000000000000000000000000000000000000000;;	  kubectl set resources deployment nginx-deployment-resources --limits=cpu=100m "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{(index .spec.template.spec.containers 0).resources.limits.cpu}}:{{end}}" "100m:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{(index .spec.template.spec.containers 1).resources.limits.cpu}}:{{end}}" "100m:"
0000000000000000000000000000000000000000;;	  # Set a non-existing container should fail
0000000000000000000000000000000000000000;;	  ! kubectl set resources deployment nginx-deployment-resources -c=redis --limits=cpu=100m
0000000000000000000000000000000000000000;;	  # Set the limit of a specific container in deployment
0000000000000000000000000000000000000000;;	  kubectl set resources deployment nginx-deployment-resources -c=nginx --limits=cpu=200m "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{(index .spec.template.spec.containers 0).resources.limits.cpu}}:{{end}}" "200m:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{(index .spec.template.spec.containers 1).resources.limits.cpu}}:{{end}}" "100m:"
0000000000000000000000000000000000000000;;	  # Set limits/requests of a deployment specified by a file
0000000000000000000000000000000000000000;;	  kubectl set resources -f hack/testdata/deployment-multicontainer-resources.yaml -c=perl --limits=cpu=300m --requests=cpu=300m "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{(index .spec.template.spec.containers 0).resources.limits.cpu}}:{{end}}" "200m:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{(index .spec.template.spec.containers 1).resources.limits.cpu}}:{{end}}" "300m:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{(index .spec.template.spec.containers 1).resources.requests.cpu}}:{{end}}" "300m:"
0000000000000000000000000000000000000000;;	  # Show dry-run works on running deployments
0000000000000000000000000000000000000000;;	  kubectl set resources deployment nginx-deployment-resources -c=perl --limits=cpu=400m --requests=cpu=400m --dry-run -o yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  ! kubectl set resources deployment nginx-deployment-resources -c=perl --limits=cpu=400m --requests=cpu=400m --local -o yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{(index .spec.template.spec.containers 0).resources.limits.cpu}}:{{end}}" "200m:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{(index .spec.template.spec.containers 1).resources.limits.cpu}}:{{end}}" "300m:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{(index .spec.template.spec.containers 1).resources.requests.cpu}}:{{end}}" "300m:"
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete deployment nginx-deployment-resources "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_deployment_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing deployments"
0000000000000000000000000000000000000000;;	  # Test kubectl create deployment (using default - old generator)
0000000000000000000000000000000000000000;;	  kubectl create deployment test-nginx-extensions --image=gcr.io/google-containers/nginx:test-cmd
0000000000000000000000000000000000000000;;	  # Post-Condition: Deployment "nginx" is created.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deploy test-nginx-extensions' "{{$container_name_field}}" 'nginx'
0000000000000000000000000000000000000000;;	  # and old generator was used, iow. old defaults are applied
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get deployment.extensions/test-nginx-extensions -o jsonpath='{.spec.revisionHistoryLimit}')
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" '2'
0000000000000000000000000000000000000000;;	  # Ensure we can interact with deployments through extensions and apps endpoints
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get deployment.extensions -o=jsonpath='{.items[0].apiVersion}' 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'extensions/v1beta1'
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get deployment.apps -o=jsonpath='{.items[0].apiVersion}' 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'apps/v1beta1'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete deployment test-nginx-extensions "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test kubectl create deployment
0000000000000000000000000000000000000000;;	  kubectl create deployment test-nginx-apps --image=gcr.io/google-containers/nginx:test-cmd --generator=deployment-basic/apps.v1beta1
0000000000000000000000000000000000000000;;	  # Post-Condition: Deployment "nginx" is created.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'deploy test-nginx-apps' "{{$container_name_field}}" 'nginx'
0000000000000000000000000000000000000000;;	  # and new generator was used, iow. new defaults are applied
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get deployment/test-nginx-apps -o jsonpath='{.spec.revisionHistoryLimit}')
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" '2'
0000000000000000000000000000000000000000;;	  # Ensure we can interact with deployments through extensions and apps endpoints
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get deployment.extensions -o=jsonpath='{.items[0].apiVersion}' 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'extensions/v1beta1'
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get deployment.apps -o=jsonpath='{.items[0].apiVersion}' 2>&1 "${kube_flags[@]}")
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'apps/v1beta1'
0000000000000000000000000000000000000000;;	  # Describe command (resource only) should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_assert rs "Name:" "Pod Template:" "Labels:" "Selector:" "Controlled By" "Replicas:" "Pods Status:" "Volumes:"
0000000000000000000000000000000000000000;;	  # Describe command (resource only) should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_assert pods "Name:" "Image:" "Node:" "Labels:" "Status:" "Created By" "Controlled By"
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete deployment test-nginx-apps "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test kubectl create deployment should not fail validation
0000000000000000000000000000000000000000;;	  # Pre-Condition: No deployment exists.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/deployment-with-UnixUserID.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-Condition: Deployment "deployment-with-unixuserid" is created.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" 'deployment-with-unixuserid:'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete deployment deployment-with-unixuserid "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test cascading deletion
0000000000000000000000000000000000000000;;	  ## Test that rs is deleted when deployment is deleted.
0000000000000000000000000000000000000000;;	  # Pre-condition: no deployment exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Create deployment
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/deployment.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Wait for rs to come up.
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert rs "{{range.items}}{{$rs_replicas_field}}{{end}}" '3'
0000000000000000000000000000000000000000;;	  # Deleting the deployment should delete the rs.
0000000000000000000000000000000000000000;;	  kubectl delete deployment nginx-deployment "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## Test that rs is not deleted when deployment is deleted with cascade set to false.
0000000000000000000000000000000000000000;;	  # Pre-condition: no deployment and rs exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Create deployment
0000000000000000000000000000000000000000;;	  kubectl create deployment nginx-deployment --image=gcr.io/google-containers/nginx:test-cmd
0000000000000000000000000000000000000000;;	  # Wait for rs to come up.
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert rs "{{range.items}}{{$rs_replicas_field}}{{end}}" '1'
0000000000000000000000000000000000000000;;	  # Delete the deployment with cascade set to false.
0000000000000000000000000000000000000000;;	  kubectl delete deployment nginx-deployment "${kube_flags[@]}" --cascade=false
0000000000000000000000000000000000000000;;	  # Wait for the deployment to be deleted and then verify that rs is not
0000000000000000000000000000000000000000;;	  # deleted.
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$rs_replicas_field}}{{end}}" '1'
0000000000000000000000000000000000000000;;	  # Cleanup
0000000000000000000000000000000000000000;;	  # Find the name of the rs to be deleted.
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get rs "${kube_flags[@]}" -o template --template={{range.items}}{{$id_field}}{{end}})
0000000000000000000000000000000000000000;;	  kubectl delete rs ${output_message} "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Auto scale deployment
0000000000000000000000000000000000000000;;	  # Pre-condition: no deployment exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/deployment.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" 'nginx-deployment:'
0000000000000000000000000000000000000000;;	  # autoscale 2~3 pods, no CPU utilization specified
0000000000000000000000000000000000000000;;	  kubectl-with-retry autoscale deployment nginx-deployment "${kube_flags[@]}" --min=2 --max=3
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'hpa nginx-deployment' "{{$hpa_min_field}} {{$hpa_max_field}} {{$hpa_cpu_field}}" '2 3 80'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  # Note that we should delete hpa first, otherwise it may fight with the deployment reaper.
0000000000000000000000000000000000000000;;	  kubectl delete hpa nginx-deployment "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl delete deployment.extensions nginx-deployment "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Rollback a deployment
0000000000000000000000000000000000000000;;	  # Pre-condition: no deployment exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  # Create a deployment (revision 1)
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/deployment-revision1.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" 'nginx:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R1}:"
0000000000000000000000000000000000000000;;	  # Rollback to revision 1 - should be no-op
0000000000000000000000000000000000000000;;	  kubectl rollout undo deployment nginx --to-revision=1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R1}:"
0000000000000000000000000000000000000000;;	  # Update the deployment (revision 2)
0000000000000000000000000000000000000000;;	  kubectl apply -f hack/testdata/deployment-revision2.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment.extensions "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R2}:"
0000000000000000000000000000000000000000;;	  # Rollback to revision 1 with dry-run - should be no-op
0000000000000000000000000000000000000000;;	  kubectl rollout undo deployment nginx --dry-run=true "${kube_flags[@]}" | grep "test-cmd"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment.extensions "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R2}:"
0000000000000000000000000000000000000000;;	  # Rollback to revision 1
0000000000000000000000000000000000000000;;	  kubectl rollout undo deployment nginx --to-revision=1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  sleep 1
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R1}:"
0000000000000000000000000000000000000000;;	  # Rollback to revision 1000000 - should be no-op
0000000000000000000000000000000000000000;;	  kubectl rollout undo deployment nginx --to-revision=1000000 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R1}:"
0000000000000000000000000000000000000000;;	  # Rollback to last revision
0000000000000000000000000000000000000000;;	  kubectl rollout undo deployment nginx "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  sleep 1
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R2}:"
0000000000000000000000000000000000000000;;	  # Pause the deployment
0000000000000000000000000000000000000000;;	  kubectl-with-retry rollout pause deployment nginx "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # A paused deployment cannot be rolled back
0000000000000000000000000000000000000000;;	  ! kubectl rollout undo deployment nginx "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Resume the deployment
0000000000000000000000000000000000000000;;	  kubectl-with-retry rollout resume deployment nginx "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # The resumed deployment can now be rolled back
0000000000000000000000000000000000000000;;	  kubectl rollout undo deployment nginx "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Check that the new replica set has all old revisions stored in an annotation
0000000000000000000000000000000000000000;;	  newrs="$(kubectl describe deployment nginx | grep NewReplicaSet | awk '{print $2}')"
0000000000000000000000000000000000000000;;	  kubectl get rs "${newrs}" -o yaml | grep "deployment.kubernetes.io/revision-history: 1,3"
0000000000000000000000000000000000000000;;	  # Check that trying to watch the status of a superseded revision returns an error
0000000000000000000000000000000000000000;;	  ! kubectl rollout status deployment/nginx --revision=3
0000000000000000000000000000000000000000;;	  cat hack/testdata/deployment-revision1.yaml | $SED "s/name: nginx$/name: nginx2/" | kubectl create -f - "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Deletion of both deployments should not be blocked
0000000000000000000000000000000000000000;;	   kubectl delete deployment nginx2 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete deployment nginx "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Set image of a deployment
0000000000000000000000000000000000000000;;	  # Pre-condition: no deployment exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Create a deployment
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/deployment-multicontainer.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$id_field}}:{{end}}" 'nginx-deployment:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R1}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_second_image_field}}:{{end}}" "${IMAGE_PERL}:"
0000000000000000000000000000000000000000;;	  # Set the deployment's image
0000000000000000000000000000000000000000;;	  kubectl set image deployment nginx-deployment nginx="${IMAGE_DEPLOYMENT_R2}" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R2}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_second_image_field}}:{{end}}" "${IMAGE_PERL}:"
0000000000000000000000000000000000000000;;	  # Set non-existing container should fail
0000000000000000000000000000000000000000;;	  ! kubectl set image deployment nginx-deployment redis=redis "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Set image of deployments without specifying name
0000000000000000000000000000000000000000;;	  kubectl set image deployments --all nginx="${IMAGE_DEPLOYMENT_R1}" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R1}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_second_image_field}}:{{end}}" "${IMAGE_PERL}:"
0000000000000000000000000000000000000000;;	  # Set image of a deployment specified by file
0000000000000000000000000000000000000000;;	  kubectl set image -f hack/testdata/deployment-multicontainer.yaml nginx="${IMAGE_DEPLOYMENT_R2}" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R2}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_second_image_field}}:{{end}}" "${IMAGE_PERL}:"
0000000000000000000000000000000000000000;;	  # Set image of a local file without talking to the server
0000000000000000000000000000000000000000;;	  kubectl set image -f hack/testdata/deployment-multicontainer.yaml nginx="${IMAGE_DEPLOYMENT_R1}" "${kube_flags[@]}" --local -o yaml
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R2}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_second_image_field}}:{{end}}" "${IMAGE_PERL}:"
0000000000000000000000000000000000000000;;	  # Set image of all containers of the deployment
0000000000000000000000000000000000000000;;	  kubectl set image deployment nginx-deployment "*"="${IMAGE_DEPLOYMENT_R1}" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R1}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert deployment "{{range.items}}{{$deployment_second_image_field}}:{{end}}" "${IMAGE_DEPLOYMENT_R1}:"
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete deployment nginx-deployment "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_rs_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:replicasets)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create and stop a replica set, make sure it doesn't leak pods
0000000000000000000000000000000000000000;;	  # Pre-condition: no replica set exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-replicaset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::log::status "Deleting rs"
0000000000000000000000000000000000000000;;	  kubectl delete rs frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no pods from frontend replica set
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods -l "tier=frontend"' "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create and then delete a replica set with cascade=false, make sure it doesn't delete pods.
0000000000000000000000000000000000000000;;	  # Pre-condition: no replica set exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-replicaset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::log::status "Deleting rs"
0000000000000000000000000000000000000000;;	  kubectl delete rs frontend "${kube_flags[@]}" --cascade=false
0000000000000000000000000000000000000000;;	  # Wait for the rs to be deleted.
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Post-condition: All 3 pods still remain from frontend replica set
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'pods -l "tier=frontend"' "{{range.items}}{{$pod_container_name_field}}:{{end}}" 'php-redis:php-redis:php-redis:'
0000000000000000000000000000000000000000;;	  # Cleanup
0000000000000000000000000000000000000000;;	  kubectl delete pods -l "tier=frontend" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create replica set frontend from YAML
0000000000000000000000000000000000000000;;	  # Pre-condition: no replica set exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-replicaset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: frontend replica set is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" 'frontend:'
0000000000000000000000000000000000000000;;	  # Describe command should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_object_assert rs 'frontend' "Name:" "Pod Template:" "Labels:" "Selector:" "Replicas:" "Pods Status:" "Volumes:"
0000000000000000000000000000000000000000;;	  # Describe command should print events information by default
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert rs 'frontend'
0000000000000000000000000000000000000000;;	  # Describe command should not print events information when show-events=false
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert rs 'frontend' false
0000000000000000000000000000000000000000;;	  # Describe command should print events information when show-events=true
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert rs 'frontend' true
0000000000000000000000000000000000000000;;	  # Describe command (resource only) should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_assert rs "Name:" "Pod Template:" "Labels:" "Selector:" "Replicas:" "Pods Status:" "Volumes:"
0000000000000000000000000000000000000000;;	  # Describe command should print events information by default
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert rs
0000000000000000000000000000000000000000;;	  # Describe command should not print events information when show-events=false
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert rs false
0000000000000000000000000000000000000000;;	  # Describe command should print events information when show-events=true
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert rs true
0000000000000000000000000000000000000000;;	  # Describe command (resource only) should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_assert pods "Name:" "Image:" "Node:" "Labels:" "Status:" "Created By" "Controlled By"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Scale replica set frontend with current-replicas and replicas
0000000000000000000000000000000000000000;;	  # Pre-condition: 3 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rs frontend' "{{$rs_replicas_field}}" '3'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl scale --current-replicas=3 --replicas=2 replicasets frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: 2 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rs frontend' "{{$rs_replicas_field}}" '2'
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete rs frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Expose replica set as service
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-replicaset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Pre-condition: 3 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'rs frontend' "{{$rs_replicas_field}}" '3'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl expose rs frontend --port=80 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: service exists and the port is unnamed
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service frontend' "{{$port_name}} {{$port_field}}" '<no value> 80'
0000000000000000000000000000000000000000;;	  # Create a service using service/v1 generator
0000000000000000000000000000000000000000;;	  kubectl expose rs frontend --port=80 --name=frontend-2 --generator=service/v1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: service exists and the port is named default.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'service frontend-2' "{{$port_name}} {{$port_field}}" 'default 80'
0000000000000000000000000000000000000000;;	  # Cleanup services
0000000000000000000000000000000000000000;;	  kubectl delete service frontend{,-2} "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete replica set with id
0000000000000000000000000000000000000000;;	  # Pre-condition: frontend replica set exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" 'frontend:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete rs frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no replica set exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create two replica sets
0000000000000000000000000000000000000000;;	  # Pre-condition: no replica set exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/frontend-replicaset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/redis-slave-replicaset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: frontend and redis-slave
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" 'frontend:redis-slave:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete multiple replica sets at once
0000000000000000000000000000000000000000;;	  # Pre-condition: frontend and redis-slave
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" 'frontend:redis-slave:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete rs frontend redis-slave "${kube_flags[@]}" # delete multiple replica sets at once
0000000000000000000000000000000000000000;;	  # Post-condition: no replica set exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${horizontalpodautoscalers}" ; then
0000000000000000000000000000000000000000;;	    ### Auto scale replica set
0000000000000000000000000000000000000000;;	    # Pre-condition: no replica set exists
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	    # Command
0000000000000000000000000000000000000000;;	    kubectl create -f hack/testdata/frontend-replicaset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert rs "{{range.items}}{{$id_field}}:{{end}}" 'frontend:'
0000000000000000000000000000000000000000;;	    # autoscale 1~2 pods, CPU utilization 70%, replica set specified by file
0000000000000000000000000000000000000000;;	    kubectl autoscale -f hack/testdata/frontend-replicaset.yaml "${kube_flags[@]}" --max=2 --cpu-percent=70
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'hpa frontend' "{{$hpa_min_field}} {{$hpa_max_field}} {{$hpa_cpu_field}}" '1 2 70'
0000000000000000000000000000000000000000;;	    kubectl delete hpa frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	    # autoscale 2~3 pods, no CPU utilization specified, replica set specified by name
0000000000000000000000000000000000000000;;	    kubectl autoscale rs frontend "${kube_flags[@]}" --min=2 --max=3
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'hpa frontend' "{{$hpa_min_field}} {{$hpa_max_field}} {{$hpa_cpu_field}}" '2 3 80'
0000000000000000000000000000000000000000;;	    kubectl delete hpa frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	    # autoscale without specifying --max should fail
0000000000000000000000000000000000000000;;	    ! kubectl autoscale rs frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	    # Clean up
0000000000000000000000000000000000000000;;	    kubectl delete rs frontend "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_daemonset_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:daemonsets)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a rolling update DaemonSet
0000000000000000000000000000000000000000;;	  # Pre-condition: no DaemonSet exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonsets "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl apply -f hack/testdata/rollingupdate-daemonset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Template Generation should be 1
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'daemonsets bind' "{{${template_generation_field}}}" '1'
0000000000000000000000000000000000000000;;	  kubectl apply -f hack/testdata/rollingupdate-daemonset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Template Generation should stay 1
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'daemonsets bind' "{{${template_generation_field}}}" '1'
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/rollingupdate-daemonset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_daemonset_history_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:daemonsets, v1:controllerrevisions)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Test rolling back a DaemonSet
0000000000000000000000000000000000000000;;	  # Pre-condition: no DaemonSet or its pods exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonsets "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  # Create a DaemonSet (revision 1)
0000000000000000000000000000000000000000;;	  kubectl apply -f hack/testdata/rollingupdate-daemonset.yaml --record "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert controllerrevisions "{{range.items}}{{$annotations_field}}:{{end}}" ".*rollingupdate-daemonset.yaml --record.*"
0000000000000000000000000000000000000000;;	  # Rollback to revision 1 - should be no-op
0000000000000000000000000000000000000000;;	  kubectl rollout undo daemonset --to-revision=1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonset "{{range.items}}{{$daemonset_image_field0}}:{{end}}" "${IMAGE_DAEMONSET_R1}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonset "{{range.items}}{{$container_len}}{{end}}" "1"
0000000000000000000000000000000000000000;;	  # Update the DaemonSet (revision 2)
0000000000000000000000000000000000000000;;	  kubectl apply -f hack/testdata/rollingupdate-daemonset-rv2.yaml --record "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert daemonset "{{range.items}}{{$daemonset_image_field0}}:{{end}}" "${IMAGE_DAEMONSET_R2}:"
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert daemonset "{{range.items}}{{$daemonset_image_field1}}:{{end}}" "${IMAGE_DAEMONSET_R2_2}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonset "{{range.items}}{{$container_len}}{{end}}" "2"
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert controllerrevisions "{{range.items}}{{$annotations_field}}:{{end}}" ".*rollingupdate-daemonset-rv2.yaml --record.*"
0000000000000000000000000000000000000000;;	  # Rollback to revision 1 with dry-run - should be no-op
0000000000000000000000000000000000000000;;	  kubectl rollout undo daemonset --dry-run=true "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonset "{{range.items}}{{$daemonset_image_field0}}:{{end}}" "${IMAGE_DAEMONSET_R2}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonset "{{range.items}}{{$daemonset_image_field1}}:{{end}}" "${IMAGE_DAEMONSET_R2_2}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonset "{{range.items}}{{$container_len}}{{end}}" "2"
0000000000000000000000000000000000000000;;	  # Rollback to revision 1
0000000000000000000000000000000000000000;;	  kubectl rollout undo daemonset --to-revision=1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert daemonset "{{range.items}}{{$daemonset_image_field0}}:{{end}}" "${IMAGE_DAEMONSET_R1}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonset "{{range.items}}{{$container_len}}{{end}}" "1"
0000000000000000000000000000000000000000;;	  # Rollback to revision 1000000 - should fail
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl rollout undo daemonset --to-revision=1000000 "${kube_flags[@]}" 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "unable to find specified revision"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonset "{{range.items}}{{$daemonset_image_field0}}:{{end}}" "${IMAGE_DAEMONSET_R1}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonset "{{range.items}}{{$container_len}}{{end}}" "1"
0000000000000000000000000000000000000000;;	  # Rollback to last revision
0000000000000000000000000000000000000000;;	  kubectl rollout undo daemonset "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert daemonset "{{range.items}}{{$daemonset_image_field0}}:{{end}}" "${IMAGE_DAEMONSET_R2}:"
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert daemonset "{{range.items}}{{$daemonset_image_field1}}:{{end}}" "${IMAGE_DAEMONSET_R2_2}:"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert daemonset "{{range.items}}{{$container_len}}{{end}}" "2"
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/rollingupdate-daemonset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_multi_resources_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:multiple resources)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  FILES="hack/testdata/multi-resource-yaml
0000000000000000000000000000000000000000;;	  hack/testdata/multi-resource-list
0000000000000000000000000000000000000000;;	  hack/testdata/multi-resource-json
0000000000000000000000000000000000000000;;	  hack/testdata/multi-resource-rclist
0000000000000000000000000000000000000000;;	  hack/testdata/multi-resource-svclist"
0000000000000000000000000000000000000000;;	  YAML=".yaml"
0000000000000000000000000000000000000000;;	  JSON=".json"
0000000000000000000000000000000000000000;;	  for file in $FILES; do
0000000000000000000000000000000000000000;;	    if [ -f $file$YAML ]
0000000000000000000000000000000000000000;;	    then
0000000000000000000000000000000000000000;;	      file=$file$YAML
0000000000000000000000000000000000000000;;	      replace_file="${file%.yaml}-modify.yaml"
0000000000000000000000000000000000000000;;	    else
0000000000000000000000000000000000000000;;	      file=$file$JSON
0000000000000000000000000000000000000000;;	      replace_file="${file%.json}-modify.json"
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    has_svc=true
0000000000000000000000000000000000000000;;	    has_rc=true
0000000000000000000000000000000000000000;;	    two_rcs=false
0000000000000000000000000000000000000000;;	    two_svcs=false
0000000000000000000000000000000000000000;;	    if [[ "${file}" == *rclist* ]]; then
0000000000000000000000000000000000000000;;	      has_svc=false
0000000000000000000000000000000000000000;;	      two_rcs=true
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    if [[ "${file}" == *svclist* ]]; then
0000000000000000000000000000000000000000;;	      has_rc=false
0000000000000000000000000000000000000000;;	      two_svcs=true
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    ### Create, get, describe, replace, label, annotate, and then delete service nginxsvc and replication controller my-nginx from 5 types of files:
0000000000000000000000000000000000000000;;	    ### 1) YAML, separated by ---; 2) JSON, with a List type; 3) JSON, with JSON object concatenation
0000000000000000000000000000000000000000;;	    ### 4) JSON, with a ReplicationControllerList type; 5) JSON, with a ServiceList type
0000000000000000000000000000000000000000;;	    echo "Testing with file ${file} and replace with file ${replace_file}"
0000000000000000000000000000000000000000;;	    # Pre-condition: no service (other than default kubernetes services) or replication controller exists
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	    # Command
0000000000000000000000000000000000000000;;	    kubectl create -f "${file}" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	    # Post-condition: mock service (and mock2) exists
0000000000000000000000000000000000000000;;	    if [ "$has_svc" = true ]; then
0000000000000000000000000000000000000000;;	      if [ "$two_svcs" = true ]; then
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'mock:mock2:'
0000000000000000000000000000000000000000;;	      else
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'mock:'
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    # Post-condition: mock rc (and mock2) exists
0000000000000000000000000000000000000000;;	    if [ "$has_rc" = true ]; then
0000000000000000000000000000000000000000;;	      if [ "$two_rcs" = true ]; then
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'mock:mock2:'
0000000000000000000000000000000000000000;;	      else
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'mock:'
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    # Command
0000000000000000000000000000000000000000;;	    kubectl get -f "${file}" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	    # Command: watching multiple resources should return "not supported" error
0000000000000000000000000000000000000000;;	    WATCH_ERROR_FILE="${KUBE_TEMP}/kubectl-watch-error"
0000000000000000000000000000000000000000;;	    kubectl get -f "${file}" "${kube_flags[@]}" "--watch" 2> ${WATCH_ERROR_FILE} || true
0000000000000000000000000000000000000000;;	    if ! grep -q "watch is only supported on individual resources and resource collections" "${WATCH_ERROR_FILE}"; then
0000000000000000000000000000000000000000;;	      kube::log::error_exit "kubectl watch multiple resource returns unexpected error or non-error: $(cat ${WATCH_ERROR_FILE})" "1"
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    kubectl describe -f "${file}" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	    # Command
0000000000000000000000000000000000000000;;	    kubectl replace -f $replace_file --force --cascade "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	    # Post-condition: mock service (and mock2) and mock rc (and mock2) are replaced
0000000000000000000000000000000000000000;;	    if [ "$has_svc" = true ]; then
0000000000000000000000000000000000000000;;	      kube::test::get_object_assert 'services mock' "{{${labels_field}.status}}" 'replaced'
0000000000000000000000000000000000000000;;	      if [ "$two_svcs" = true ]; then
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert 'services mock2' "{{${labels_field}.status}}" 'replaced'
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    if [ "$has_rc" = true ]; then
0000000000000000000000000000000000000000;;	      kube::test::get_object_assert 'rc mock' "{{${labels_field}.status}}" 'replaced'
0000000000000000000000000000000000000000;;	      if [ "$two_rcs" = true ]; then
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert 'rc mock2' "{{${labels_field}.status}}" 'replaced'
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    # Command: kubectl edit multiple resources
0000000000000000000000000000000000000000;;	    temp_editor="${KUBE_TEMP}/tmp-editor.sh"
0000000000000000000000000000000000000000;;	    echo -e "#!/bin/bash\n$SED -i \"s/status\:\ replaced/status\:\ edited/g\" \$@" > "${temp_editor}"
0000000000000000000000000000000000000000;;	    chmod +x "${temp_editor}"
0000000000000000000000000000000000000000;;	    EDITOR="${temp_editor}" kubectl edit "${kube_flags[@]}" -f "${file}"
0000000000000000000000000000000000000000;;	    # Post-condition: mock service (and mock2) and mock rc (and mock2) are edited
0000000000000000000000000000000000000000;;	    if [ "$has_svc" = true ]; then
0000000000000000000000000000000000000000;;	      kube::test::get_object_assert 'services mock' "{{${labels_field}.status}}" 'edited'
0000000000000000000000000000000000000000;;	      if [ "$two_svcs" = true ]; then
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert 'services mock2' "{{${labels_field}.status}}" 'edited'
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    if [ "$has_rc" = true ]; then
0000000000000000000000000000000000000000;;	      kube::test::get_object_assert 'rc mock' "{{${labels_field}.status}}" 'edited'
0000000000000000000000000000000000000000;;	      if [ "$two_rcs" = true ]; then
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert 'rc mock2' "{{${labels_field}.status}}" 'edited'
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    # cleaning
0000000000000000000000000000000000000000;;	    rm "${temp_editor}"
0000000000000000000000000000000000000000;;	    # Command
0000000000000000000000000000000000000000;;	    # We need to set --overwrite, because otherwise, if the first attempt to run "kubectl label"
0000000000000000000000000000000000000000;;	    # fails on some, but not all, of the resources, retries will fail because it tries to modify
0000000000000000000000000000000000000000;;	    # existing labels.
0000000000000000000000000000000000000000;;	    kubectl-with-retry label -f $file labeled=true --overwrite "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	    # Post-condition: mock service and mock rc (and mock2) are labeled
0000000000000000000000000000000000000000;;	    if [ "$has_svc" = true ]; then
0000000000000000000000000000000000000000;;	      kube::test::get_object_assert 'services mock' "{{${labels_field}.labeled}}" 'true'
0000000000000000000000000000000000000000;;	      if [ "$two_svcs" = true ]; then
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert 'services mock2' "{{${labels_field}.labeled}}" 'true'
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    if [ "$has_rc" = true ]; then
0000000000000000000000000000000000000000;;	      kube::test::get_object_assert 'rc mock' "{{${labels_field}.labeled}}" 'true'
0000000000000000000000000000000000000000;;	      if [ "$two_rcs" = true ]; then
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert 'rc mock2' "{{${labels_field}.labeled}}" 'true'
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    # Command
0000000000000000000000000000000000000000;;	    # Command
0000000000000000000000000000000000000000;;	    # We need to set --overwrite, because otherwise, if the first attempt to run "kubectl annotate"
0000000000000000000000000000000000000000;;	    # fails on some, but not all, of the resources, retries will fail because it tries to modify
0000000000000000000000000000000000000000;;	    # existing annotations.
0000000000000000000000000000000000000000;;	    kubectl-with-retry annotate -f $file annotated=true --overwrite "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	    # Post-condition: mock service (and mock2) and mock rc (and mock2) are annotated
0000000000000000000000000000000000000000;;	    if [ "$has_svc" = true ]; then
0000000000000000000000000000000000000000;;	      kube::test::get_object_assert 'services mock' "{{${annotations_field}.annotated}}" 'true'
0000000000000000000000000000000000000000;;	      if [ "$two_svcs" = true ]; then
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert 'services mock2' "{{${annotations_field}.annotated}}" 'true'
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    if [ "$has_rc" = true ]; then
0000000000000000000000000000000000000000;;	      kube::test::get_object_assert 'rc mock' "{{${annotations_field}.annotated}}" 'true'
0000000000000000000000000000000000000000;;	      if [ "$two_rcs" = true ]; then
0000000000000000000000000000000000000000;;	        kube::test::get_object_assert 'rc mock2' "{{${annotations_field}.annotated}}" 'true'
0000000000000000000000000000000000000000;;	      fi
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	    # Cleanup resources created
0000000000000000000000000000000000000000;;	    kubectl delete -f "${file}" "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  done
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #############################
0000000000000000000000000000000000000000;;	  # Multiple Resources via URL#
0000000000000000000000000000000000000000;;	  #############################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Pre-condition: no service (other than default kubernetes services) or replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/master/hack/testdata/multi-resource-yaml.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Post-condition: service(mock) and rc(mock) exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" 'mock:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" 'mock:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete -f https://raw.githubusercontent.com/kubernetes/kubernetes/master/hack/testdata/multi-resource-yaml.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Post-condition: no service (other than default kubernetes services) or replication controller exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert services "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_config_set_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:config set)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl config set-cluster test-cluster --server="https://does-not-work"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Get the api cert and add a comment to avoid flag parsing problems
0000000000000000000000000000000000000000;;	  cert_data=$(echo "#Comment" && cat "${TMPDIR:-/tmp}/apiserver.crt")
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl config set clusters.test-cluster.certificate-authority-data "$cert_data" --set-raw-bytes
0000000000000000000000000000000000000000;;	  r_writen=$(kubectl config view --raw -o jsonpath='{.clusters[?(@.name == "test-cluster")].cluster.certificate-authority-data}')
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  encoded=$(echo -n "$cert_data" | base64)
0000000000000000000000000000000000000000;;	  kubectl config set clusters.test-cluster.certificate-authority-data "$encoded"
0000000000000000000000000000000000000000;;	  e_writen=$(kubectl config view --raw -o jsonpath='{.clusters[?(@.name == "test-cluster")].cluster.certificate-authority-data}')
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  test "$e_writen" == "$r_writen"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_local_proxy_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl local proxy"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Make sure the UI can be proxied
0000000000000000000000000000000000000000;;	  start-proxy
0000000000000000000000000000000000000000;;	  check-curl-proxy-code /ui 307
0000000000000000000000000000000000000000;;	  check-curl-proxy-code /api/ui 404
0000000000000000000000000000000000000000;;	  check-curl-proxy-code /api/v1/namespaces 200
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${metrics}" ; then
0000000000000000000000000000000000000000;;	    check-curl-proxy-code /metrics 200
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${static}" ; then
0000000000000000000000000000000000000000;;	    check-curl-proxy-code /static/ 200
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  stop-proxy
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Make sure the in-development api is accessible by default
0000000000000000000000000000000000000000;;	  start-proxy
0000000000000000000000000000000000000000;;	  check-curl-proxy-code /apis 200
0000000000000000000000000000000000000000;;	  check-curl-proxy-code /apis/extensions/ 200
0000000000000000000000000000000000000000;;	  stop-proxy
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Custom paths let you see everything.
0000000000000000000000000000000000000000;;	  start-proxy /custom
0000000000000000000000000000000000000000;;	  check-curl-proxy-code /custom/ui 307
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${metrics}" ; then
0000000000000000000000000000000000000000;;	    check-curl-proxy-code /custom/metrics 200
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  check-curl-proxy-code /custom/api/v1/namespaces 200
0000000000000000000000000000000000000000;;	  stop-proxy
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_RESTMapper_evaluation_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing RESTMapper"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  RESTMAPPER_ERROR_FILE="${KUBE_TEMP}/restmapper-error"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Non-existent resource type should give a recognizeable error
0000000000000000000000000000000000000000;;	  # Pre-condition: None
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl get "${kube_flags[@]}" unknownresourcetype 2>${RESTMAPPER_ERROR_FILE} || true
0000000000000000000000000000000000000000;;	  if grep -q "the server doesn't have a resource type" "${RESTMAPPER_ERROR_FILE}"; then
0000000000000000000000000000000000000000;;	    kube::log::status "\"kubectl get unknownresourcetype\" returns error as expected: $(cat ${RESTMAPPER_ERROR_FILE})"
0000000000000000000000000000000000000000;;	  else
0000000000000000000000000000000000000000;;	    kube::log::status "\"kubectl get unknownresourcetype\" returns unexpected error or non-error: $(cat ${RESTMAPPER_ERROR_FILE})"
0000000000000000000000000000000000000000;;	    exit 1
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  rm "${RESTMAPPER_ERROR_FILE}"
0000000000000000000000000000000000000000;;	  # Post-condition: None
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_clusterroles_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing clusterroles"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # make sure the server was properly bootstrapped with clusterroles and bindings
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterroles/cluster-admin "{{.metadata.name}}" 'cluster-admin'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrolebindings/cluster-admin "{{.metadata.name}}" 'cluster-admin'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # test `kubectl create clusterrole`
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" clusterrole pod-admin --verb=* --resource=pods
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/pod-admin "{{range.rules}}{{range.verbs}}{{.}}:{{end}}{{end}}" '\*:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/pod-admin "{{range.rules}}{{range.resources}}{{.}}:{{end}}{{end}}" 'pods:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/pod-admin "{{range.rules}}{{range.apiGroups}}{{.}}:{{end}}{{end}}" ':'
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" clusterrole resource-reader --verb=get,list --resource=pods,deployments.extensions
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/resource-reader "{{range.rules}}{{range.verbs}}{{.}}:{{end}}{{end}}" 'get:list:get:list:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/resource-reader "{{range.rules}}{{range.resources}}{{.}}:{{end}}{{end}}" 'pods:deployments:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/resource-reader "{{range.rules}}{{range.apiGroups}}{{.}}:{{end}}{{end}}" ':extensions:'
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" clusterrole resourcename-reader --verb=get,list --resource=pods --resource-name=foo
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/resourcename-reader "{{range.rules}}{{range.verbs}}{{.}}:{{end}}{{end}}" 'get:list:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/resourcename-reader "{{range.rules}}{{range.resources}}{{.}}:{{end}}{{end}}" 'pods:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/resourcename-reader "{{range.rules}}{{range.apiGroups}}{{.}}:{{end}}{{end}}" ':'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/resourcename-reader "{{range.rules}}{{range.resourceNames}}{{.}}:{{end}}{{end}}" 'foo:'
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" clusterrole url-reader --verb=get --non-resource-url=/logs/* --non-resource-url=/healthz/*
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/url-reader "{{range.rules}}{{range.verbs}}{{.}}:{{end}}{{end}}" 'get:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrole/url-reader "{{range.rules}}{{range.nonResourceURLs}}{{.}}:{{end}}{{end}}" '/logs/\*:/healthz/\*:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # test `kubectl create rolebinding/clusterrolebinding`
0000000000000000000000000000000000000000;;	  # test `kubectl set subject rolebinding/clusterrolebinding`
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" clusterrolebinding super-admin --clusterrole=admin --user=super-admin
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrolebinding/super-admin "{{range.subjects}}{{.name}}:{{end}}" 'super-admin:'
0000000000000000000000000000000000000000;;	  kubectl set subject "${kube_flags[@]}" clusterrolebinding super-admin --user=foo
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrolebinding/super-admin "{{range.subjects}}{{.name}}:{{end}}" 'super-admin:foo:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" clusterrolebinding super-group --clusterrole=admin --group=the-group
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrolebinding/super-group "{{range.subjects}}{{.name}}:{{end}}" 'the-group:'
0000000000000000000000000000000000000000;;	  kubectl set subject "${kube_flags[@]}" clusterrolebinding super-group --group=foo
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrolebinding/super-group "{{range.subjects}}{{.name}}:{{end}}" 'the-group:foo:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" clusterrolebinding super-sa --clusterrole=admin --serviceaccount=otherns:sa-name
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrolebinding/super-sa "{{range.subjects}}{{.namespace}}:{{end}}" 'otherns:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrolebinding/super-sa "{{range.subjects}}{{.name}}:{{end}}" 'sa-name:'
0000000000000000000000000000000000000000;;	  kubectl set subject "${kube_flags[@]}" clusterrolebinding super-sa --serviceaccount=otherfoo:foo
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrolebinding/super-sa "{{range.subjects}}{{.namespace}}:{{end}}" 'otherns:otherfoo:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert clusterrolebinding/super-sa "{{range.subjects}}{{.name}}:{{end}}" 'sa-name:foo:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" rolebinding admin --clusterrole=admin --user=default-admin
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rolebinding/admin "{{range.subjects}}{{.name}}:{{end}}" 'default-admin:'
0000000000000000000000000000000000000000;;	  kubectl set subject "${kube_flags[@]}" rolebinding admin --user=foo
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rolebinding/admin "{{range.subjects}}{{.name}}:{{end}}" 'default-admin:foo:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" rolebinding localrole --role=localrole --group=the-group
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rolebinding/localrole "{{range.subjects}}{{.name}}:{{end}}" 'the-group:'
0000000000000000000000000000000000000000;;	  kubectl set subject "${kube_flags[@]}" rolebinding localrole --group=foo
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rolebinding/localrole "{{range.subjects}}{{.name}}:{{end}}" 'the-group:foo:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" rolebinding sarole --role=localrole --serviceaccount=otherns:sa-name
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rolebinding/sarole "{{range.subjects}}{{.namespace}}:{{end}}" 'otherns:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rolebinding/sarole "{{range.subjects}}{{.name}}:{{end}}" 'sa-name:'
0000000000000000000000000000000000000000;;	  kubectl set subject "${kube_flags[@]}" rolebinding sarole --serviceaccount=otherfoo:foo
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rolebinding/sarole "{{range.subjects}}{{.namespace}}:{{end}}" 'otherns:otherfoo:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert rolebinding/sarole "{{range.subjects}}{{.name}}:{{end}}" 'sa-name:foo:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_role_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing role"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Create Role from command (only resource)
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" role pod-admin --verb=* --resource=pods
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/pod-admin "{{range.rules}}{{range.verbs}}{{.}}:{{end}}{{end}}" '\*:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/pod-admin "{{range.rules}}{{range.resources}}{{.}}:{{end}}{{end}}" 'pods:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/pod-admin "{{range.rules}}{{range.apiGroups}}{{.}}:{{end}}{{end}}" ':'
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl create "${kube_flags[@]}" role invalid-pod-admin --verb=* --resource=invalid-resource 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "the server doesn't have a resource type \"invalid-resource\""
0000000000000000000000000000000000000000;;	  # Create Role from command (resource + group)
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" role group-reader --verb=get,list --resource=deployments.extensions
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/group-reader "{{range.rules}}{{range.verbs}}{{.}}:{{end}}{{end}}" 'get:list:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/group-reader "{{range.rules}}{{range.resources}}{{.}}:{{end}}{{end}}" 'deployments:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/group-reader "{{range.rules}}{{range.apiGroups}}{{.}}:{{end}}{{end}}" 'extensions:'
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl create "${kube_flags[@]}" role invalid-group --verb=get,list --resource=deployments.invalid-group 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "the server doesn't have a resource type \"deployments\" in group \"invalid-group\""
0000000000000000000000000000000000000000;;	  # Create Role from command (resource / subresource)
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" role subresource-reader --verb=get,list --resource=pods/status
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/subresource-reader "{{range.rules}}{{range.verbs}}{{.}}:{{end}}{{end}}" 'get:list:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/subresource-reader "{{range.rules}}{{range.resources}}{{.}}:{{end}}{{end}}" 'pods/status:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/subresource-reader "{{range.rules}}{{range.apiGroups}}{{.}}:{{end}}{{end}}" ':'
0000000000000000000000000000000000000000;;	  # Create Role from command (resource + group / subresource)
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" role group-subresource-reader --verb=get,list --resource=replicasets.extensions/scale
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/group-subresource-reader "{{range.rules}}{{range.verbs}}{{.}}:{{end}}{{end}}" 'get:list:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/group-subresource-reader "{{range.rules}}{{range.resources}}{{.}}:{{end}}{{end}}" 'replicasets/scale:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/group-subresource-reader "{{range.rules}}{{range.apiGroups}}{{.}}:{{end}}{{end}}" 'extensions:'
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl create "${kube_flags[@]}" role invalid-group --verb=get,list --resource=rs.invalid-group/scale 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "the server doesn't have a resource type \"rs\" in group \"invalid-group\""
0000000000000000000000000000000000000000;;	  # Create Role from command (resource + resourcename)
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" role resourcename-reader --verb=get,list --resource=pods --resource-name=foo
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/resourcename-reader "{{range.rules}}{{range.verbs}}{{.}}:{{end}}{{end}}" 'get:list:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/resourcename-reader "{{range.rules}}{{range.resources}}{{.}}:{{end}}{{end}}" 'pods:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/resourcename-reader "{{range.rules}}{{range.apiGroups}}{{.}}:{{end}}{{end}}" ':'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/resourcename-reader "{{range.rules}}{{range.resourceNames}}{{.}}:{{end}}{{end}}" 'foo:'
0000000000000000000000000000000000000000;;	  # Create Role from command (multi-resources)
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" role resource-reader --verb=get,list --resource=pods/status,deployments.extensions
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/resource-reader "{{range.rules}}{{range.verbs}}{{.}}:{{end}}{{end}}" 'get:list:get:list:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/resource-reader "{{range.rules}}{{range.resources}}{{.}}:{{end}}{{end}}" 'pods/status:deployments:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert role/resource-reader "{{range.rules}}{{range.apiGroups}}{{.}}:{{end}}{{end}}" ':extensions:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_assert_short_name_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing assert short name"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing propagation of short names for resources"
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get --raw=/api/v1)
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## test if a short name is exported during discovery
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" '{"name":"configmaps","singularName":"","namespaced":true,"kind":"ConfigMap","verbs":\["create","delete","deletecollection","get","list","patch","update","watch"\],"shortNames":\["cm"\]}'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_assert_categories_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing propagation of categories for resources"
0000000000000000000000000000000000000000;;	  output_message=$(kubectl get --raw=/api/v1 | grep -Po '"name":"pods".*?}')
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" '"categories":\["all"\]'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_create_error_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl create with error"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Passing no arguments to create is an error
0000000000000000000000000000000000000000;;	  ! kubectl create
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## kubectl create should not panic on empty string lists in a template
0000000000000000000000000000000000000000;;	  ERROR_FILE="${KUBE_TEMP}/validation-error"
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/invalid-rc-with-empty-args.yaml "${kube_flags[@]}" 2> "${ERROR_FILE}" || true
0000000000000000000000000000000000000000;;	  # Post-condition: should get an error reporting the empty string
0000000000000000000000000000000000000000;;	  if grep -q "unexpected nil value for field" "${ERROR_FILE}"; then
0000000000000000000000000000000000000000;;	    kube::log::status "\"kubectl create with empty string list returns error as expected: $(cat ${ERROR_FILE})"
0000000000000000000000000000000000000000;;	  else
0000000000000000000000000000000000000000;;	    kube::log::status "\"kubectl create with empty string list returns unexpected error or non-error: $(cat ${ERROR_FILE})"
0000000000000000000000000000000000000000;;	    exit 1
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  rm "${ERROR_FILE}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_cmd_with_img_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing cmd with image"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Test that a valid image reference value is provided as the value of --image in `kubectl run <name> --image`
0000000000000000000000000000000000000000;;	  output_message=$(kubectl run test1 --image=validname)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'deployment "test1" created'
0000000000000000000000000000000000000000;;	  kubectl delete deployments test1
0000000000000000000000000000000000000000;;	  # test invalid image name
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl run test2 --image=InvalidImageName 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'error: Invalid image name "InvalidImageName": invalid reference format'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_client_config_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing client config"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  # Pre-condition: kubeconfig "missing" is not a file or directory
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pod --context="" --kubeconfig=missing 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "missing: no such file or directory"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Pre-condition: kubeconfig "missing" is not a file or directory
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pod --user="" --kubeconfig=missing 2>&1)
0000000000000000000000000000000000000000;;	  # Post-condition: --user contains a valid / empty value, missing config file returns error
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "missing: no such file or directory"
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pod --cluster="" --kubeconfig=missing 2>&1)
0000000000000000000000000000000000000000;;	  # Post-condition: --cluster contains a "valid" value, missing config file returns error
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "missing: no such file or directory"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Pre-condition: context "missing-context" does not exist
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pod --context="missing-context" 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'context "missing-context" does not exist'
0000000000000000000000000000000000000000;;	  # Post-condition: invalid or missing context returns error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Pre-condition: cluster "missing-cluster" does not exist
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pod --cluster="missing-cluster" 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'cluster "missing-cluster" does not exist'
0000000000000000000000000000000000000000;;	  # Post-condition: invalid or missing cluster returns error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Pre-condition: user "missing-user" does not exist
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pod --user="missing-user" 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'auth info "missing-user" does not exist'
0000000000000000000000000000000000000000;;	  # Post-condition: invalid or missing user returns error
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # test invalid config
0000000000000000000000000000000000000000;;	  kubectl config view | sed -E "s/apiVersion: .*/apiVersion: v-1/g" > "${TMPDIR:-/tmp}"/newconfig.yaml
0000000000000000000000000000000000000000;;	  output_message=$(! "${KUBE_OUTPUT_HOSTBIN}/kubectl" get pods --context="" --user="" --kubeconfig="${TMPDIR:-/tmp}"/newconfig.yaml 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" "Error loading config file"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pod --kubeconfig=missing-config 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'no such file or directory'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_service_accounts_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing service accounts"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a new namespace
0000000000000000000000000000000000000000;;	  # Pre-condition: the test-service-accounts namespace does not exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'namespaces' '{{range.items}}{{ if eq $id_field \"test-service-accounts\" }}found{{end}}{{end}}:' ':'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create namespace test-service-accounts
0000000000000000000000000000000000000000;;	  # Post-condition: namespace 'test-service-accounts' is created.
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'namespaces/test-service-accounts' "{{$id_field}}" 'test-service-accounts'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a service account in a specific namespace
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create serviceaccount test-service-account --namespace=test-service-accounts
0000000000000000000000000000000000000000;;	  # Post-condition: secret exists and has expected values
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'serviceaccount/test-service-account --namespace=test-service-accounts' "{{$id_field}}" 'test-service-account'
0000000000000000000000000000000000000000;;	  # Clean-up
0000000000000000000000000000000000000000;;	  kubectl delete serviceaccount test-service-account --namespace=test-service-accounts
0000000000000000000000000000000000000000;;	  # Clean up
0000000000000000000000000000000000000000;;	  kubectl delete namespace test-service-accounts
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_pod_templates_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing pod templates"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create PODTEMPLATE
0000000000000000000000000000000000000000;;	  # Pre-condition: no PODTEMPLATE
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert podtemplates "{{range.items}}{{.metadata.name}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/walkthrough/podtemplate.json "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: nginx PODTEMPLATE is available
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert podtemplates "{{range.items}}{{.metadata.name}}:{{end}}" 'nginx:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Printing pod templates works
0000000000000000000000000000000000000000;;	  kubectl get podtemplates "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  [[ "$(kubectl get podtemplates -o yaml "${kube_flags[@]}" | grep nginx)" ]]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete nginx pod template by name
0000000000000000000000000000000000000000;;	  # Pre-condition: nginx pod template is available
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert podtemplates "{{range.items}}{{.metadata.name}}:{{end}}" 'nginx:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete podtemplate nginx "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: No templates exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert podtemplate "{{range.items}}{{.metadata.name}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_stateful_set_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:statefulsets)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create and stop statefulset, make sure it doesn't leak pods
0000000000000000000000000000000000000000;;	  # Pre-condition: no statefulset exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert statefulset "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command: create statefulset
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/nginx-statefulset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Scale statefulset test with current-replicas and replicas
0000000000000000000000000000000000000000;;	  # Pre-condition: 0 replicas
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'statefulset nginx' "{{$statefulset_replicas_field}}" '0'
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert 'statefulset nginx' "{{$statefulset_observed_generation}}" '1'
0000000000000000000000000000000000000000;;	  # Command: Scale up
0000000000000000000000000000000000000000;;	  kubectl scale --current-replicas=0 --replicas=1 statefulset nginx "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: 1 replica, named nginx-0
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'statefulset nginx' "{{$statefulset_replicas_field}}" '1'
0000000000000000000000000000000000000000;;	  kube::test::wait_object_assert 'statefulset nginx' "{{$statefulset_observed_generation}}" '2'
0000000000000000000000000000000000000000;;	  # Typically we'd wait and confirm that N>1 replicas are up, but this framework
0000000000000000000000000000000000000000;;	  # doesn't start  the scheduler, so pet-0 will block all others.
0000000000000000000000000000000000000000;;	  # TODO: test robust scaling in an e2e.
0000000000000000000000000000000000000000;;	  wait-for-pods-with-label "app=nginx-statefulset" "nginx-0"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Clean up
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/nginx-statefulset.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no pods from statefulset controller
0000000000000000000000000000000000000000;;	  wait-for-pods-with-label "app=nginx-statefulset" ""
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_lists_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:lists)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create a List with objects from multiple versions
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/list.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Delete the List with objects from multiple versions
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete service/list-service-test deployment/list-deployment-test
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_persistent_volumes_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing persistent volumes"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create and delete persistent volume examples
0000000000000000000000000000000000000000;;	  # Pre-condition: no persistent volumes currently exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pv "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/persistent-volumes/volumes/local-01.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pv "{{range.items}}{{$id_field}}:{{end}}" 'pv0001:'
0000000000000000000000000000000000000000;;	  kubectl delete pv pv0001 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/persistent-volumes/volumes/local-02.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pv "{{range.items}}{{$id_field}}:{{end}}" 'pv0002:'
0000000000000000000000000000000000000000;;	  kubectl delete pv pv0002 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/persistent-volumes/volumes/gce.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pv "{{range.items}}{{$id_field}}:{{end}}" 'pv0003:'
0000000000000000000000000000000000000000;;	  kubectl delete pv pv0003 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no PVs
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pv "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_persistent_volume_claims_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing persistent volumes claims"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create and delete persistent volume claim examples
0000000000000000000000000000000000000000;;	  # Pre-condition: no persistent volume claims currently exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pvc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/persistent-volumes/claims/claim-01.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pvc "{{range.items}}{{$id_field}}:{{end}}" 'myclaim-1:'
0000000000000000000000000000000000000000;;	  kubectl delete pvc myclaim-1 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/persistent-volumes/claims/claim-02.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pvc "{{range.items}}{{$id_field}}:{{end}}" 'myclaim-2:'
0000000000000000000000000000000000000000;;	  kubectl delete pvc myclaim-2 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/doc-yaml/user-guide/persistent-volumes/claims/claim-03.json "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pvc "{{range.items}}{{$id_field}}:{{end}}" 'myclaim-3:'
0000000000000000000000000000000000000000;;	  kubectl delete pvc myclaim-3 "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no PVCs
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pvc "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_storage_class_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing storage class"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create and delete storage class
0000000000000000000000000000000000000000;;	  # Pre-condition: no storage classes currently exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert storageclass "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create -f - "${kube_flags[@]}" << __EOF__
0000000000000000000000000000000000000000;;	{
0000000000000000000000000000000000000000;;	  "kind": "StorageClass",
0000000000000000000000000000000000000000;;	  "apiVersion": "storage.k8s.io/v1",
0000000000000000000000000000000000000000;;	  "metadata": {
0000000000000000000000000000000000000000;;	  "name": "storage-class-name"
0000000000000000000000000000000000000000;;	  },
0000000000000000000000000000000000000000;;	  "provisioner": "kubernetes.io/fake-provisioner-type",
0000000000000000000000000000000000000000;;	  "parameters": {
0000000000000000000000000000000000000000;;	  "zone":"us-east-1b",
0000000000000000000000000000000000000000;;	  "type":"ssd"
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	__EOF__
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert storageclass "{{range.items}}{{$id_field}}:{{end}}" 'storage-class-name:'
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert sc "{{range.items}}{{$id_field}}:{{end}}" 'storage-class-name:'
0000000000000000000000000000000000000000;;	  kubectl delete storageclass storage-class-name "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  # Post-condition: no storage classes
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert storageclass "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_nodes_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:nodes)"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert nodes "{{range.items}}{{$id_field}}:{{end}}" '127.0.0.1:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::test::describe_object_assert nodes "127.0.0.1" "Name:" "Labels:" "CreationTimestamp:" "Conditions:" "Addresses:" "Capacity:" "Pods:"
0000000000000000000000000000000000000000;;	  # Describe command should print events information by default
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert nodes "127.0.0.1"
0000000000000000000000000000000000000000;;	  # Describe command should not print events information when show-events=false
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert nodes "127.0.0.1" false
0000000000000000000000000000000000000000;;	  # Describe command should print events information when show-events=true
0000000000000000000000000000000000000000;;	  kube::test::describe_object_events_assert nodes "127.0.0.1" true
0000000000000000000000000000000000000000;;	  # Describe command (resource only) should print detailed information
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_assert nodes "Name:" "Labels:" "CreationTimestamp:" "Conditions:" "Addresses:" "Capacity:" "Pods:"
0000000000000000000000000000000000000000;;	  # Describe command should print events information by default
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert nodes
0000000000000000000000000000000000000000;;	  # Describe command should not print events information when show-events=false
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert nodes false
0000000000000000000000000000000000000000;;	  # Describe command should print events information when show-events=true
0000000000000000000000000000000000000000;;	  kube::test::describe_resource_events_assert nodes true
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### kubectl patch update can mark node unschedulable
0000000000000000000000000000000000000000;;	  # Pre-condition: node is schedulable
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert "nodes 127.0.0.1" "{{.spec.unschedulable}}" '<no value>'
0000000000000000000000000000000000000000;;	  kubectl patch "${kube_flags[@]}" nodes "127.0.0.1" -p='{"spec":{"unschedulable":true}}'
0000000000000000000000000000000000000000;;	  # Post-condition: node is unschedulable
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert "nodes 127.0.0.1" "{{.spec.unschedulable}}" 'true'
0000000000000000000000000000000000000000;;	  kubectl patch "${kube_flags[@]}" nodes "127.0.0.1" -p='{"spec":{"unschedulable":null}}'
0000000000000000000000000000000000000000;;	  # Post-condition: node is schedulable
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert "nodes 127.0.0.1" "{{.spec.unschedulable}}" '<no value>'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # check webhook token authentication endpoint, kubectl doesn't actually display the returned object so this isn't super useful
0000000000000000000000000000000000000000;;	  # but it proves that works
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/pkg/kubectl/cmd/create/tokenreview-v1beta1.json --validate=false
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/pkg/kubectl/cmd/create/tokenreview-v1.json --validate=false
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_authorization_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing authorization"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # check remote authorization endpoint, kubectl doesn't actually display the returned object so this isn't super useful
0000000000000000000000000000000000000000;;	  # but it proves that works
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/pkg/kubectl/cmd/create/sar-v1.json --validate=false
0000000000000000000000000000000000000000;;	  kubectl create -f test/fixtures/pkg/kubectl/cmd/create/sar-v1beta1.json --validate=false
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  SAR_RESULT_FILE="${KUBE_TEMP}/sar-result.json"
0000000000000000000000000000000000000000;;	  curl -k -H "Content-Type:" http://localhost:8080/apis/authorization.k8s.io/v1beta1/subjectaccessreviews -XPOST -d @test/fixtures/pkg/kubectl/cmd/create/sar-v1beta1.json > "${SAR_RESULT_FILE}"
0000000000000000000000000000000000000000;;	  if grep -q '"allowed": true' "${SAR_RESULT_FILE}"; then
0000000000000000000000000000000000000000;;	    kube::log::status "\"authorization.k8s.io/subjectaccessreviews\" returns as expected: $(cat "${SAR_RESULT_FILE}")"
0000000000000000000000000000000000000000;;	  else
0000000000000000000000000000000000000000;;	    kube::log::status "\"authorization.k8s.io/subjectaccessreviews\" does not return as expected: $(cat "${SAR_RESULT_FILE}")"
0000000000000000000000000000000000000000;;	    exit 1
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  rm "${SAR_RESULT_FILE}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  SAR_RESULT_FILE="${KUBE_TEMP}/sar-result.json"
0000000000000000000000000000000000000000;;	  curl -k -H "Content-Type:" http://localhost:8080/apis/authorization.k8s.io/v1/subjectaccessreviews -XPOST -d @test/fixtures/pkg/kubectl/cmd/create/sar-v1.json > "${SAR_RESULT_FILE}"
0000000000000000000000000000000000000000;;	  if grep -q '"allowed": true' "${SAR_RESULT_FILE}"; then
0000000000000000000000000000000000000000;;	    kube::log::status "\"authorization.k8s.io/subjectaccessreviews\" returns as expected: $(cat "${SAR_RESULT_FILE}")"
0000000000000000000000000000000000000000;;	  else
0000000000000000000000000000000000000000;;	    kube::log::status "\"authorization.k8s.io/subjectaccessreviews\" does not return as expected: $(cat "${SAR_RESULT_FILE}")"
0000000000000000000000000000000000000000;;	    exit 1
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  rm "${SAR_RESULT_FILE}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_retrieve_multiple_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # switch back to the default namespace
0000000000000000000000000000000000000000;;	  kubectl config set-context "${CONTEXT}" --namespace=""
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:multiget)"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'nodes/127.0.0.1 service/kubernetes' "{{range.items}}{{$id_field}}:{{end}}" '127.0.0.1:kubernetes:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_resource_aliasing_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace
0000000000000000000000000000000000000000;;	  kube::log::status "Testing resource aliasing"
0000000000000000000000000000000000000000;;	  kubectl create -f examples/storage/cassandra/cassandra-controller.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl create -f examples/storage/cassandra/cassandra-service.yaml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  object="all -l'app=cassandra'"
0000000000000000000000000000000000000000;;	  request="{{range.items}}{{range .metadata.labels}}{{.}}:{{end}}{{end}}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # all 4 cassandra's might not be in the request immediately...
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert "$object" "$request" 'cassandra:cassandra:cassandra:cassandra:' || \
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert "$object" "$request" 'cassandra:cassandra:cassandra:' || \
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert "$object" "$request" 'cassandra:cassandra:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl delete all -l app=cassandra "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_explain_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl(v1:explain)"
0000000000000000000000000000000000000000;;	  kubectl explain pods
0000000000000000000000000000000000000000;;	  # shortcuts work
0000000000000000000000000000000000000000;;	  kubectl explain po
0000000000000000000000000000000000000000;;	  kubectl explain po.status.message
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_swagger_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing swagger"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Verify schema
0000000000000000000000000000000000000000;;	  file="${KUBE_TEMP}/schema-v1.json"
0000000000000000000000000000000000000000;;	  curl -s "http://127.0.0.1:${API_PORT}/swaggerapi/api/v1" > "${file}"
0000000000000000000000000000000000000000;;	  [[ "$(grep "list of returned" "${file}")" ]]
0000000000000000000000000000000000000000;;	  [[ "$(grep "List of services" "${file}")" ]]
0000000000000000000000000000000000000000;;	  [[ "$(grep "Watch for changes to the described resources" "${file}")" ]]
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_sort_by_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl --sort-by"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### sort-by should not panic if no pod exists
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl get pods --sort-by="{metadata.name}"
0000000000000000000000000000000000000000;;	  kubectl get pods --sort-by="{metadata.creationTimestamp}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_kubectl_all_namespace_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl --all-namespace"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Pre-condition: the "default" namespace exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert namespaces "{{range.items}}{{if eq $id_field \\\"default\\\"}}{{$id_field}}:{{end}}{{end}}" 'default:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Create POD
0000000000000000000000000000000000000000;;	  # Pre-condition: no POD exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl create "${kube_flags[@]}" -f test/fixtures/doc-yaml/admin/limitrange/valid-pod.yaml
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod is created
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Verify a specific namespace is ignored when all-namespaces is provided
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl get pods --all-namespaces --namespace=default
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ### Clean up
0000000000000000000000000000000000000000;;	  # Pre-condition: valid-pod exists
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" 'valid-pod:'
0000000000000000000000000000000000000000;;	  # Command
0000000000000000000000000000000000000000;;	  kubectl delete "${kube_flags[@]}" pod valid-pod --grace-period=0 --force
0000000000000000000000000000000000000000;;	  # Post-condition: valid-pod doesn't exist
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert pods "{{range.items}}{{$id_field}}:{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_certificates_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing certificates"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # approve
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/csr.yml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'csr/foo' '{{range.status.conditions}}{{.type}}{{end}}' ''
0000000000000000000000000000000000000000;;	  kubectl certificate approve foo "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl get csr "${kube_flags[@]}" -o json
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'csr/foo' '{{range.status.conditions}}{{.type}}{{end}}' 'Approved'
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/csr.yml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert csr "{{range.items}}{{$id_field}}{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/csr.yml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'csr/foo' '{{range.status.conditions}}{{.type}}{{end}}' ''
0000000000000000000000000000000000000000;;	  kubectl certificate approve -f hack/testdata/csr.yml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl get csr "${kube_flags[@]}" -o json
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'csr/foo' '{{range.status.conditions}}{{.type}}{{end}}' 'Approved'
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/csr.yml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert csr "{{range.items}}{{$id_field}}{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # deny
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/csr.yml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'csr/foo' '{{range.status.conditions}}{{.type}}{{end}}' ''
0000000000000000000000000000000000000000;;	  kubectl certificate deny foo "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl get csr "${kube_flags[@]}" -o json
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'csr/foo' '{{range.status.conditions}}{{.type}}{{end}}' 'Denied'
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/csr.yml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert csr "{{range.items}}{{$id_field}}{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kubectl create -f hack/testdata/csr.yml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'csr/foo' '{{range.status.conditions}}{{.type}}{{end}}' ''
0000000000000000000000000000000000000000;;	  kubectl certificate deny -f hack/testdata/csr.yml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kubectl get csr "${kube_flags[@]}" -o json
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert 'csr/foo' '{{range.status.conditions}}{{.type}}{{end}}' 'Denied'
0000000000000000000000000000000000000000;;	  kubectl delete -f hack/testdata/csr.yml "${kube_flags[@]}"
0000000000000000000000000000000000000000;;	  kube::test::get_object_assert csr "{{range.items}}{{$id_field}}{{end}}" ''
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_plugins_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing kubectl plugins"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # top-level plugin command
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins kubectl -h 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'plugin\s\+Runs a command-line plugin'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # no plugins
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl plugin 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'no plugins installed'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # single plugins path
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins kubectl plugin 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'echo\s\+Echoes for test-cmd'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'get\s\+The wonderful new plugin-based get!'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'error\s\+The tremendous plugin that always fails!'
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'The hello plugin'
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'Incomplete plugin'
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'no plugins installed'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # multiple plugins path
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins/:test/fixtures/pkg/kubectl/plugins2/ kubectl plugin -h 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'echo\s\+Echoes for test-cmd'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'get\s\+The wonderful new plugin-based get!'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'error\s\+The tremendous plugin that always fails!'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'hello\s\+The hello plugin'
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'Incomplete plugin'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # don't override existing commands
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins/:test/fixtures/pkg/kubectl/plugins2/ kubectl get -h 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'Display one or many resources'
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "$output_message{output_message}" 'The wonderful new plugin-based get'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # plugin help
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins/:test/fixtures/pkg/kubectl/plugins2/ kubectl plugin hello -h 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'The hello plugin is a new plugin used by test-cmd to test multiple plugin locations.'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'Usage:'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # run plugin
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins/:test/fixtures/pkg/kubectl/plugins2/ kubectl plugin hello 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" '#hello#'
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins/:test/fixtures/pkg/kubectl/plugins2/ kubectl plugin echo 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'This plugin works!'
0000000000000000000000000000000000000000;;	  output_message=$(! KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins/ kubectl plugin hello 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'unknown command'
0000000000000000000000000000000000000000;;	  output_message=$(! KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins/ kubectl plugin error 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'error: exit status 1'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # plugin tree
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins kubectl plugin tree 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'Plugin with a tree of commands'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'child1\s\+The first child of a tree'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'child2\s\+The second child of a tree'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'child3\s\+The third child of a tree'
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins kubectl plugin tree child1 --help 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'The first child of a tree'
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'The second child'
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'child2'
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins kubectl plugin tree child1 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'child one'
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'child1'
0000000000000000000000000000000000000000;;	  kube::test::if_has_not_string "${output_message}" 'The first child'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # plugin env
0000000000000000000000000000000000000000;;	  output_message=$(KUBECTL_PLUGINS_PATH=test/fixtures/pkg/kubectl/plugins kubectl plugin env 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'KUBECTL_PLUGINS_CURRENT_NAMESPACE'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'KUBECTL_PLUGINS_CALLER'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'KUBECTL_PLUGINS_DESCRIPTOR_COMMAND=./env.sh'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'KUBECTL_PLUGINS_DESCRIPTOR_SHORT_DESC=The plugin envs plugin'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'KUBECTL_PLUGINS_GLOBAL_FLAG_KUBECONFIG'
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'KUBECTL_PLUGINS_GLOBAL_FLAG_REQUEST_TIMEOUT=0'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	run_impersonation_tests() {
0000000000000000000000000000000000000000;;	  set -o nounset
0000000000000000000000000000000000000000;;	  set -o errexit
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::log::status "Testing impersonation"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  output_message=$(! kubectl get pods "${kube_flags_with_token[@]}" --as-group=foo 2>&1)
0000000000000000000000000000000000000000;;	  kube::test::if_has_string "${output_message}" 'without impersonating a user'
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${csr}" ; then
0000000000000000000000000000000000000000;;	    # --as
0000000000000000000000000000000000000000;;	    kubectl create -f hack/testdata/csr.yml "${kube_flags_with_token[@]}" --as=user1
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'csr/foo' '{{.spec.username}}' 'user1'
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'csr/foo' '{{range .spec.groups}}{{.}}{{end}}' 'system:authenticated'
0000000000000000000000000000000000000000;;	    kubectl delete -f hack/testdata/csr.yml "${kube_flags_with_token[@]}"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	    # --as-group
0000000000000000000000000000000000000000;;	    kubectl create -f hack/testdata/csr.yml "${kube_flags_with_token[@]}" --as=user1 --as-group=group2 --as-group=group1 --as-group=,,,chameleon
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'csr/foo' '{{len .spec.groups}}' '3'
0000000000000000000000000000000000000000;;	    kube::test::get_object_assert 'csr/foo' '{{range .spec.groups}}{{.}} {{end}}' 'group2 group1 ,,,chameleon '
0000000000000000000000000000000000000000;;	    kubectl delete -f hack/testdata/csr.yml "${kube_flags_with_token[@]}"
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  set +o nounset
0000000000000000000000000000000000000000;;	  set +o errexit
0000000000000000000000000000000000000000;;	}
0000000000000000000000000000000000000000;;	# Runs all kubectl tests.
0000000000000000000000000000000000000000;;	# Requires an env var SUPPORTED_RESOURCES which is a comma separated list of
0000000000000000000000000000000000000000;;	# resources for which tests should be run.
0000000000000000000000000000000000000000;;	runTests() {
0000000000000000000000000000000000000000;;	  foundError="False"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if [ -z "${SUPPORTED_RESOURCES:-}" ]; then
0000000000000000000000000000000000000000;;	    echo "Need to set SUPPORTED_RESOURCES env var. It is a list of resources that are supported and hence should be tested. Set it to (*) to test all resources"
0000000000000000000000000000000000000000;;	    exit 1
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  kube::log::status "Checking kubectl version"
0000000000000000000000000000000000000000;;	  kubectl version
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # use timestamp as the name of namespace because increasing the variable inside subshell
0000000000000000000000000000000000000000;;	  # does not affect the value of the variable outside the subshell.
0000000000000000000000000000000000000000;;	  create_and_use_new_namespace() {
0000000000000000000000000000000000000000;;	    namespace_number=$(date +%s%N)
0000000000000000000000000000000000000000;;	    kube::log::status "Creating namespace namespace${namespace_number}"
0000000000000000000000000000000000000000;;	    kubectl create namespace "namespace${namespace_number}"
0000000000000000000000000000000000000000;;	    kubectl config set-context "${CONTEXT}" --namespace="namespace${namespace_number}"
0000000000000000000000000000000000000000;;	  }
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube_flags=(
0000000000000000000000000000000000000000;;	    -s "http://127.0.0.1:${API_PORT}"
0000000000000000000000000000000000000000;;	  )
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube_flags_with_token=(
0000000000000000000000000000000000000000;;	    -s "https://127.0.0.1:${SECURE_API_PORT}" --token=admin/system:masters --insecure-skip-tls-verify=true
0000000000000000000000000000000000000000;;	  )
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if [[ -z "${ALLOW_SKEW:-}" ]]; then
0000000000000000000000000000000000000000;;	    kube_flags+=("--match-server-version")
0000000000000000000000000000000000000000;;	    kube_flags_with_token+=("--match-server-version")
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${nodes}" ; then
0000000000000000000000000000000000000000;;	    [ "$(kubectl get nodes -o go-template='{{ .apiVersion }}' "${kube_flags[@]}")" == "v1" ]
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  id_field=".metadata.name"
0000000000000000000000000000000000000000;;	  labels_field=".metadata.labels"
0000000000000000000000000000000000000000;;	  annotations_field=".metadata.annotations"
0000000000000000000000000000000000000000;;	  service_selector_field=".spec.selector"
0000000000000000000000000000000000000000;;	  rc_replicas_field=".spec.replicas"
0000000000000000000000000000000000000000;;	  rc_status_replicas_field=".status.replicas"
0000000000000000000000000000000000000000;;	  rc_container_image_field=".spec.template.spec.containers"
0000000000000000000000000000000000000000;;	  rs_replicas_field=".spec.replicas"
0000000000000000000000000000000000000000;;	  port_field="(index .spec.ports 0).port"
0000000000000000000000000000000000000000;;	  port_name="(index .spec.ports 0).name"
0000000000000000000000000000000000000000;;	  second_port_field="(index .spec.ports 1).port"
0000000000000000000000000000000000000000;;	  second_port_name="(index .spec.ports 1).name"
0000000000000000000000000000000000000000;;	  image_field="(index .spec.containers 0).image"
0000000000000000000000000000000000000000;;	  pod_container_name_field="(index .spec.containers 0).name"
0000000000000000000000000000000000000000;;	  container_name_field="(index .spec.template.spec.containers 0).name"
0000000000000000000000000000000000000000;;	  hpa_min_field=".spec.minReplicas"
0000000000000000000000000000000000000000;;	  hpa_max_field=".spec.maxReplicas"
0000000000000000000000000000000000000000;;	  hpa_cpu_field=".spec.targetCPUUtilizationPercentage"
0000000000000000000000000000000000000000;;	  statefulset_replicas_field=".spec.replicas"
0000000000000000000000000000000000000000;;	  statefulset_observed_generation=".status.observedGeneration"
0000000000000000000000000000000000000000;;	  job_parallelism_field=".spec.parallelism"
0000000000000000000000000000000000000000;;	  deployment_replicas=".spec.replicas"
0000000000000000000000000000000000000000;;	  secret_data=".data"
0000000000000000000000000000000000000000;;	  secret_type=".type"
0000000000000000000000000000000000000000;;	  deployment_image_field="(index .spec.template.spec.containers 0).image"
0000000000000000000000000000000000000000;;	  deployment_second_image_field="(index .spec.template.spec.containers 1).image"
0000000000000000000000000000000000000000;;	  change_cause_annotation='.*kubernetes.io/change-cause.*'
0000000000000000000000000000000000000000;;	  pdb_min_available=".spec.minAvailable"
0000000000000000000000000000000000000000;;	  pdb_max_unavailable=".spec.maxUnavailable"
0000000000000000000000000000000000000000;;	  template_generation_field=".spec.templateGeneration"
0000000000000000000000000000000000000000;;	  container_len="(len .spec.template.spec.containers)"
0000000000000000000000000000000000000000;;	  daemonset_image_field0="(index .spec.template.spec.containers 0).image"
0000000000000000000000000000000000000000;;	  daemonset_image_field1="(index .spec.template.spec.containers 1).image"
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Make sure "default" namespace exists.
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${namespaces}" ; then
0000000000000000000000000000000000000000;;	    output_message=$(kubectl get "${kube_flags[@]}" namespaces)
0000000000000000000000000000000000000000;;	    if [[ ! $(echo "${output_message}" | grep "default") ]]; then
0000000000000000000000000000000000000000;;	      # Create default namespace
0000000000000000000000000000000000000000;;	      kubectl create "${kube_flags[@]}" ns default
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # Make sure "kubernetes" service exists.
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${services}" ; then
0000000000000000000000000000000000000000;;	    # Attempt to create the kubernetes service, tolerating failure (since it might already exist)
0000000000000000000000000000000000000000;;	    kubectl create "${kube_flags[@]}" -f hack/testdata/kubernetes-service.yaml || true
0000000000000000000000000000000000000000;;	    # Require the service to exist (either we created it or the API server did)
0000000000000000000000000000000000000000;;	    kubectl get "${kube_flags[@]}" -f hack/testdata/kubernetes-service.yaml
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #########################
0000000000000000000000000000000000000000;;	  # Kubectl version #
0000000000000000000000000000000000000000;;	  #########################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  record_command run_kubectl_version_tests
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #######################
0000000000000000000000000000000000000000;;	  # kubectl config set #
0000000000000000000000000000000000000000;;	  #######################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  record_command run_kubectl_config_set_tests
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #######################
0000000000000000000000000000000000000000;;	  # kubectl local proxy #
0000000000000000000000000000000000000000;;	  #######################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  record_command run_kubectl_local_proxy_tests
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #########################
0000000000000000000000000000000000000000;;	  # RESTMapper evaluation #
0000000000000000000000000000000000000000;;	  #########################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  record_command run_RESTMapper_evaluation_tests
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ################
0000000000000000000000000000000000000000;;	  # Cluster Role #
0000000000000000000000000000000000000000;;	  ################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${clusterroles}" ; then
0000000000000000000000000000000000000000;;	    record_command run_clusterroles_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ########
0000000000000000000000000000000000000000;;	  # Role #
0000000000000000000000000000000000000000;;	  ########
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${roles}" ; then
0000000000000000000000000000000000000000;;	      record_command run_role_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #########################
0000000000000000000000000000000000000000;;	  # Assert short name     #
0000000000000000000000000000000000000000;;	  #########################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  record_command run_assert_short_name_tests
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #########################
0000000000000000000000000000000000000000;;	  # Assert categories     #
0000000000000000000000000000000000000000;;	  #########################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ## test if a category is exported during discovery
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    record_command run_assert_categories_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ###########################
0000000000000000000000000000000000000000;;	  # POD creation / deletion #
0000000000000000000000000000000000000000;;	  ###########################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    record_command run_pod_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    record_command run_save_config_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    record_command run_kubectl_create_error_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    # TODO: Move apply tests to run on rs instead of pods so that they can be
0000000000000000000000000000000000000000;;	    # run for federation apiserver as well.
0000000000000000000000000000000000000000;;	    record_command run_kubectl_apply_tests
0000000000000000000000000000000000000000;;	    record_command run_kubectl_run_tests
0000000000000000000000000000000000000000;;	    record_command run_kubectl_using_deprecated_commands_test
0000000000000000000000000000000000000000;;	    record_command run_kubectl_create_filter_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${deployments}" ; then
0000000000000000000000000000000000000000;;	    record_command run_kubectl_apply_deployments_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ###############
0000000000000000000000000000000000000000;;	  # Kubectl get #
0000000000000000000000000000000000000000;;	  ###############
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    # TODO: Move get tests to run on rs instead of pods so that they can be
0000000000000000000000000000000000000000;;	    # run for federation apiserver as well.
0000000000000000000000000000000000000000;;	    record_command run_kubectl_get_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ##################
0000000000000000000000000000000000000000;;	  # Global timeout #
0000000000000000000000000000000000000000;;	  ##################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    # TODO: Move request timeout tests to run on rs instead of pods so that they
0000000000000000000000000000000000000000;;	    # can be run for federation apiserver as well.
0000000000000000000000000000000000000000;;	    record_command run_kubectl_request_timeout_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #####################################
0000000000000000000000000000000000000000;;	  # Third Party Resources             #
0000000000000000000000000000000000000000;;	  #####################################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  # customresourcedefinitions cleanup after themselves.  Run these first, then TPRs
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${customresourcedefinitions}" ; then
0000000000000000000000000000000000000000;;	    record_command run_crd_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #################
0000000000000000000000000000000000000000;;	  # Run cmd w img #
0000000000000000000000000000000000000000;;	  #################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${deployments}" ; then
0000000000000000000000000000000000000000;;	    record_command run_cmd_with_img_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #####################################
0000000000000000000000000000000000000000;;	  # Recursive Resources via directory #
0000000000000000000000000000000000000000;;	  #####################################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    record_command run_recursive_resources_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ##############
0000000000000000000000000000000000000000;;	  # Namespaces #
0000000000000000000000000000000000000000;;	  ##############
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${namespaces}" ; then
0000000000000000000000000000000000000000;;	    record_command run_namespace_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ###########
0000000000000000000000000000000000000000;;	  # Secrets #
0000000000000000000000000000000000000000;;	  ###########
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${namespaces}" ; then
0000000000000000000000000000000000000000;;	    if kube::test::if_supports_resource "${secrets}" ; then
0000000000000000000000000000000000000000;;	      record_command run_secrets_test
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	  # ConfigMap          #
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${namespaces}"; then
0000000000000000000000000000000000000000;;	    if kube::test::if_supports_resource "${configmaps}" ; then
0000000000000000000000000000000000000000;;	      record_command run_configmap_tests
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ####################
0000000000000000000000000000000000000000;;	  # Client Config    #
0000000000000000000000000000000000000000;;	  ####################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  record_command run_client_config_tests
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ####################
0000000000000000000000000000000000000000;;	  # Service Accounts #
0000000000000000000000000000000000000000;;	  ####################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${namespaces}" && kube::test::if_supports_resource "${serviceaccounts}" ; then
0000000000000000000000000000000000000000;;	    record_command run_service_accounts_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #################
0000000000000000000000000000000000000000;;	  # Pod templates #
0000000000000000000000000000000000000000;;	  #################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${podtemplates}" ; then
0000000000000000000000000000000000000000;;	    record_command run_pod_templates_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ############
0000000000000000000000000000000000000000;;	  # Services #
0000000000000000000000000000000000000000;;	  ############
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${services}" ; then
0000000000000000000000000000000000000000;;	    record_command run_service_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ##################
0000000000000000000000000000000000000000;;	  # DaemonSets     #
0000000000000000000000000000000000000000;;	  ##################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${daemonsets}" ; then
0000000000000000000000000000000000000000;;	    record_command run_daemonset_tests
0000000000000000000000000000000000000000;;	    if kube::test::if_supports_resource "${controllerrevisions}"; then
0000000000000000000000000000000000000000;;	      record_command run_daemonset_history_tests
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ###########################
0000000000000000000000000000000000000000;;	  # Replication controllers #
0000000000000000000000000000000000000000;;	  ###########################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${namespaces}" ; then
0000000000000000000000000000000000000000;;	    if kube::test::if_supports_resource "${replicationcontrollers}" ; then
0000000000000000000000000000000000000000;;	      record_command run_rc_tests
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	  # Deployments       #
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${deployments}" ; then
0000000000000000000000000000000000000000;;	    record_command run_deployment_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	  # Replica Sets       #
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${replicasets}" ; then
0000000000000000000000000000000000000000;;	    record_command run_rs_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #################
0000000000000000000000000000000000000000;;	  # Stateful Sets #
0000000000000000000000000000000000000000;;	  #################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${statefulsets}" ; then
0000000000000000000000000000000000000000;;	    record_command run_stateful_set_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	  # Lists              #
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${services}" ; then
0000000000000000000000000000000000000000;;	    if kube::test::if_supports_resource "${deployments}" ; then
0000000000000000000000000000000000000000;;	      record_command run_lists_tests
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	  # Multiple Resources #
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${services}" ; then
0000000000000000000000000000000000000000;;	    if kube::test::if_supports_resource "${replicationcontrollers}" ; then
0000000000000000000000000000000000000000;;	      record_command run_multi_resources_tests
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	  # Persistent Volumes #
0000000000000000000000000000000000000000;;	  ######################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${persistentvolumes}" ; then
0000000000000000000000000000000000000000;;	    record_command run_persistent_volumes_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ############################
0000000000000000000000000000000000000000;;	  # Persistent Volume Claims #
0000000000000000000000000000000000000000;;	  ############################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${persistentvolumeclaims}" ; then
0000000000000000000000000000000000000000;;	    record_command run_persistent_volume_claims_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ############################
0000000000000000000000000000000000000000;;	  # Storage Classes #
0000000000000000000000000000000000000000;;	  ############################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${storageclass}" ; then
0000000000000000000000000000000000000000;;	    record_command run_storage_class_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #########
0000000000000000000000000000000000000000;;	  # Nodes #
0000000000000000000000000000000000000000;;	  #########
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${nodes}" ; then
0000000000000000000000000000000000000000;;	    record_command run_nodes_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ########################
0000000000000000000000000000000000000000;;	  # authorization.k8s.io #
0000000000000000000000000000000000000000;;	  ########################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${subjectaccessreviews}" ; then
0000000000000000000000000000000000000000;;	    record_command run_authorization_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #####################
0000000000000000000000000000000000000000;;	  # Retrieve multiple #
0000000000000000000000000000000000000000;;	  #####################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${nodes}" ; then
0000000000000000000000000000000000000000;;	    if kube::test::if_supports_resource "${services}" ; then
0000000000000000000000000000000000000000;;	      record_command run_retrieve_multiple_tests
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #####################
0000000000000000000000000000000000000000;;	  # Resource aliasing #
0000000000000000000000000000000000000000;;	  #####################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${services}" ; then
0000000000000000000000000000000000000000;;	    if kube::test::if_supports_resource "${replicationcontrollers}" ; then
0000000000000000000000000000000000000000;;	      record_command run_resource_aliasing_tests
0000000000000000000000000000000000000000;;	    fi
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ###########
0000000000000000000000000000000000000000;;	  # Explain #
0000000000000000000000000000000000000000;;	  ###########
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    record_command run_kubectl_explain_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ###########
0000000000000000000000000000000000000000;;	  # Swagger #
0000000000000000000000000000000000000000;;	  ###########
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  record_command run_swagger_tests
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #####################
0000000000000000000000000000000000000000;;	  # Kubectl --sort-by #
0000000000000000000000000000000000000000;;	  #####################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    record_command run_kubectl_sort_by_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ############################
0000000000000000000000000000000000000000;;	  # Kubectl --all-namespaces #
0000000000000000000000000000000000000000;;	  ############################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${pods}" ; then
0000000000000000000000000000000000000000;;	    record_command run_kubectl_all_namespace_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ################
0000000000000000000000000000000000000000;;	  # Certificates #
0000000000000000000000000000000000000000;;	  ################
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if kube::test::if_supports_resource "${csr}" ; then
0000000000000000000000000000000000000000;;	    record_command run_certificates_tests
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  ###########
0000000000000000000000000000000000000000;;	  # Plugins #
0000000000000000000000000000000000000000;;	  ###########
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  record_command run_plugins_tests
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  #################
0000000000000000000000000000000000000000;;	  # Impersonation #
0000000000000000000000000000000000000000;;	  #################
0000000000000000000000000000000000000000;;	  record_command run_impersonation_tests
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  kube::test::clear_all
0000000000000000000000000000000000000000;;	
0000000000000000000000000000000000000000;;	  if [ "$foundError" == "True" ]; then
0000000000000000000000000000000000000000;;	    echo "TEST FAILED"
0000000000000000000000000000000000000000;;	    exit 1
0000000000000000000000000000000000000000;;	  fi
0000000000000000000000000000000000000000;;	}
